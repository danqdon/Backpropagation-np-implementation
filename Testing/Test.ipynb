{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Implementaci√≥n para el Dataset XOR"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bb05a5dc5b5facb"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss Train(MSE): 0.3096142571298148, R2 Train: -0.2384570285192591\n",
      "Epoch 1, Loss Train(MSE): 0.3014727121808202, R2 Train: -0.2058908487232809\n",
      "Epoch 2, Loss Train(MSE): 0.2939199453588857, R2 Train: -0.17567978143554286\n",
      "Epoch 3, Loss Train(MSE): 0.28699319381457367, R2 Train: -0.14797277525829466\n",
      "Epoch 4, Loss Train(MSE): 0.2807049997230917, R2 Train: -0.12281999889236683\n",
      "Epoch 5, Loss Train(MSE): 0.2750461601888631, R2 Train: -0.1001846407554523\n",
      "Epoch 6, Loss Train(MSE): 0.2699899586432579, R2 Train: -0.07995983457303169\n",
      "Epoch 7, Loss Train(MSE): 0.2655018848122961, R2 Train: -0.06200753924918434\n",
      "Epoch 8, Loss Train(MSE): 0.26154810630174136, R2 Train: -0.046192425206965426\n",
      "Epoch 9, Loss Train(MSE): 0.2580428472028133, R2 Train: -0.03217138881125314\n",
      "Epoch 10, Loss Train(MSE): 0.25497557905784113, R2 Train: -0.019902316231364514\n",
      "Epoch 11, Loss Train(MSE): 0.2522576703103918, R2 Train: -0.009030681241567251\n",
      "Epoch 12, Loss Train(MSE): 0.24988113121202352, R2 Train: 0.00047547515190593526\n",
      "Epoch 13, Loss Train(MSE): 0.24775870962310648, R2 Train: 0.008965161507574093\n",
      "Epoch 14, Loss Train(MSE): 0.24590322774372017, R2 Train: 0.016387089025119317\n",
      "Epoch 15, Loss Train(MSE): 0.24428825736553722, R2 Train: 0.02284697053785112\n",
      "Epoch 16, Loss Train(MSE): 0.24281597842352698, R2 Train: 0.028736086305892083\n",
      "Epoch 17, Loss Train(MSE): 0.24148279404265394, R2 Train: 0.034068823829384254\n",
      "Epoch 18, Loss Train(MSE): 0.24030937000847422, R2 Train: 0.03876251996610314\n",
      "Epoch 19, Loss Train(MSE): 0.2392291950258975, R2 Train: 0.04308321989640995\n",
      "Epoch 20, Loss Train(MSE): 0.23820920585356958, R2 Train: 0.04716317658572167\n",
      "Epoch 21, Loss Train(MSE): 0.23730789349216141, R2 Train: 0.05076842603135434\n",
      "Epoch 22, Loss Train(MSE): 0.23643512384971027, R2 Train: 0.054259504601158914\n",
      "Epoch 23, Loss Train(MSE): 0.2356197529554087, R2 Train: 0.05752098817836515\n",
      "Epoch 24, Loss Train(MSE): 0.23487658047218393, R2 Train: 0.06049367811126427\n",
      "Epoch 25, Loss Train(MSE): 0.23412579722627158, R2 Train: 0.06349681109491367\n",
      "Epoch 26, Loss Train(MSE): 0.2334596571262971, R2 Train: 0.0661613714948116\n",
      "Epoch 27, Loss Train(MSE): 0.2327742988998775, R2 Train: 0.06890280440049001\n",
      "Epoch 28, Loss Train(MSE): 0.23213714578034636, R2 Train: 0.07145141687861456\n",
      "Epoch 29, Loss Train(MSE): 0.2315125285969539, R2 Train: 0.07394988561218441\n",
      "Epoch 30, Loss Train(MSE): 0.23089331159523835, R2 Train: 0.0764267536190466\n",
      "Epoch 31, Loss Train(MSE): 0.23031408468793269, R2 Train: 0.07874366124826926\n",
      "Epoch 32, Loss Train(MSE): 0.22970556503966957, R2 Train: 0.0811777398413217\n",
      "Epoch 33, Loss Train(MSE): 0.22916050513867695, R2 Train: 0.08335797944529222\n",
      "Epoch 34, Loss Train(MSE): 0.22855952442788258, R2 Train: 0.0857619022884697\n",
      "Epoch 35, Loss Train(MSE): 0.22803681479040452, R2 Train: 0.0878527408383819\n",
      "Epoch 36, Loss Train(MSE): 0.2274492399981647, R2 Train: 0.09020304000734125\n",
      "Epoch 37, Loss Train(MSE): 0.22692887819990223, R2 Train: 0.09228448720039106\n",
      "Epoch 38, Loss Train(MSE): 0.22635842449006577, R2 Train: 0.09456630203973693\n",
      "Epoch 39, Loss Train(MSE): 0.22583897143582185, R2 Train: 0.0966441142567126\n",
      "Epoch 40, Loss Train(MSE): 0.22528148163834105, R2 Train: 0.09887407344663579\n",
      "Epoch 41, Loss Train(MSE): 0.22476211472212693, R2 Train: 0.1009515411114923\n",
      "Epoch 42, Loss Train(MSE): 0.22421436244680043, R2 Train: 0.10314255021279828\n",
      "Epoch 43, Loss Train(MSE): 0.22369465733119948, R2 Train: 0.1052213706752021\n",
      "Epoch 44, Loss Train(MSE): 0.22315409385669355, R2 Train: 0.10738362457322581\n",
      "Epoch 45, Loss Train(MSE): 0.22263387947186597, R2 Train: 0.10946448211253612\n",
      "Epoch 46, Loss Train(MSE): 0.2220984530063147, R2 Train: 0.1116061879747412\n",
      "Epoch 47, Loss Train(MSE): 0.22157771609358815, R2 Train: 0.11368913562564742\n",
      "Epoch 48, Loss Train(MSE): 0.22104574183438028, R2 Train: 0.11581703266247889\n",
      "Epoch 49, Loss Train(MSE): 0.2205245649020251, R2 Train: 0.1179017403918996\n",
      "Epoch 50, Loss Train(MSE): 0.21999463090533627, R2 Train: 0.1200214763786549\n",
      "Epoch 51, Loss Train(MSE): 0.21947315263085965, R2 Train: 0.12210738947656141\n",
      "Epoch 52, Loss Train(MSE): 0.21894405105694664, R2 Train: 0.12422379577221343\n",
      "Epoch 53, Loss Train(MSE): 0.218422441695507, R2 Train: 0.12631023321797197\n",
      "Epoch 54, Loss Train(MSE): 0.2178931181449667, R2 Train: 0.1284275274201332\n",
      "Epoch 55, Loss Train(MSE): 0.21737156490364723, R2 Train: 0.13051374038541108\n",
      "Epoch 56, Loss Train(MSE): 0.21684108073867925, R2 Train: 0.132635677045283\n",
      "Epoch 57, Loss Train(MSE): 0.21631977970865066, R2 Train: 0.13472088116539738\n",
      "Epoch 58, Loss Train(MSE): 0.21578728376401698, R2 Train: 0.13685086494393206\n",
      "Epoch 59, Loss Train(MSE): 0.21526643611203933, R2 Train: 0.1389342555518427\n",
      "Epoch 60, Loss Train(MSE): 0.2147311432509644, R2 Train: 0.1410754269961424\n",
      "Epoch 61, Loss Train(MSE): 0.21421095412533905, R2 Train: 0.14315618349864379\n",
      "Epoch 62, Loss Train(MSE): 0.21367212882888273, R2 Train: 0.14531148468446908\n",
      "Epoch 63, Loss Train(MSE): 0.21315280794660163, R2 Train: 0.14738876821359348\n",
      "Epoch 64, Loss Train(MSE): 0.21270324511395064, R2 Train: 0.14918701954419744\n",
      "Epoch 65, Loss Train(MSE): 0.21221137222823466, R2 Train: 0.15115451108706135\n",
      "Epoch 66, Loss Train(MSE): 0.2116339169942964, R2 Train: 0.1534643320228144\n",
      "Epoch 67, Loss Train(MSE): 0.21114687194409437, R2 Train: 0.1554125122236225\n",
      "Epoch 68, Loss Train(MSE): 0.2105966941381404, R2 Train: 0.15761322344743844\n",
      "Epoch 69, Loss Train(MSE): 0.21016571936111791, R2 Train: 0.15933712255552834\n",
      "Epoch 70, Loss Train(MSE): 0.2096482689780711, R2 Train: 0.16140692408771562\n",
      "Epoch 71, Loss Train(MSE): 0.2090646597795563, R2 Train: 0.16374136088177482\n",
      "Epoch 72, Loss Train(MSE): 0.2085932786869465, R2 Train: 0.16562688525221403\n",
      "Epoch 73, Loss Train(MSE): 0.20811078470462474, R2 Train: 0.16755686118150104\n",
      "Epoch 74, Loss Train(MSE): 0.2075229304742195, R2 Train: 0.16990827810312203\n",
      "Epoch 75, Loss Train(MSE): 0.20701911502379966, R2 Train: 0.17192353990480136\n",
      "Epoch 76, Loss Train(MSE): 0.20646768297470686, R2 Train: 0.17412926810117257\n",
      "Epoch 77, Loss Train(MSE): 0.20601314948521218, R2 Train: 0.17594740205915127\n",
      "Epoch 78, Loss Train(MSE): 0.20550304862043822, R2 Train: 0.1779878055182471\n",
      "Epoch 79, Loss Train(MSE): 0.20490729816341913, R2 Train: 0.1803708073463235\n",
      "Epoch 80, Loss Train(MSE): 0.20440907829155683, R2 Train: 0.1823636868337727\n",
      "Epoch 81, Loss Train(MSE): 0.20393515173740073, R2 Train: 0.18425939305039707\n",
      "Epoch 82, Loss Train(MSE): 0.20333421807724356, R2 Train: 0.18666312769102578\n",
      "Epoch 83, Loss Train(MSE): 0.2028102314245055, R2 Train: 0.18875907430197802\n",
      "Epoch 84, Loss Train(MSE): 0.20225561229553554, R2 Train: 0.19097755081785783\n",
      "Epoch 85, Loss Train(MSE): 0.20177267397785034, R2 Train: 0.19290930408859863\n",
      "Epoch 86, Loss Train(MSE): 0.20126966001040852, R2 Train: 0.1949213599583659\n",
      "Epoch 87, Loss Train(MSE): 0.20065947752093127, R2 Train: 0.1973620899162749\n",
      "Epoch 88, Loss Train(MSE): 0.20012941700637907, R2 Train: 0.19948233197448373\n",
      "Epoch 89, Loss Train(MSE): 0.19966440396197166, R2 Train: 0.20134238415211336\n",
      "Epoch 90, Loss Train(MSE): 0.19904834292008258, R2 Train: 0.20380662831966967\n",
      "Epoch 91, Loss Train(MSE): 0.19850105848739055, R2 Train: 0.2059957660504378\n",
      "Epoch 92, Loss Train(MSE): 0.19794230484489028, R2 Train: 0.20823078062043887\n",
      "Epoch 93, Loss Train(MSE): 0.19742566464772054, R2 Train: 0.21029734140911782\n",
      "Epoch 94, Loss Train(MSE): 0.1969311809226414, R2 Train: 0.21227527630943444\n",
      "Epoch 95, Loss Train(MSE): 0.19630494598865295, R2 Train: 0.21478021604538822\n",
      "Epoch 96, Loss Train(MSE): 0.19574211854288698, R2 Train: 0.2170315258284521\n",
      "Epoch 97, Loss Train(MSE): 0.1951802963486658, R2 Train: 0.21927881460533682\n",
      "Epoch 98, Loss Train(MSE): 0.19467567384153248, R2 Train: 0.2212973046338701\n",
      "Epoch 99, Loss Train(MSE): 0.1941521653480945, R2 Train: 0.22339133860762195\n",
      "Epoch 100, Loss Train(MSE): 0.19351541308265408, R2 Train: 0.2259383476693837\n",
      "Epoch 101, Loss Train(MSE): 0.19295734850978355, R2 Train: 0.2281706059608658\n",
      "Epoch 102, Loss Train(MSE): 0.1924766566011327, R2 Train: 0.23009337359546922\n",
      "Epoch 103, Loss Train(MSE): 0.1918334820654533, R2 Train: 0.23266607173818676\n",
      "Epoch 104, Loss Train(MSE): 0.19124196401341043, R2 Train: 0.2350321439463583\n",
      "Epoch 105, Loss Train(MSE): 0.19067782825632182, R2 Train: 0.23728868697471273\n",
      "Epoch 106, Loss Train(MSE): 0.19012912831211123, R2 Train: 0.23948348675155506\n",
      "Epoch 107, Loss Train(MSE): 0.1896213342690577, R2 Train: 0.24151466292376922\n",
      "Epoch 108, Loss Train(MSE): 0.18896751055206554, R2 Train: 0.24412995779173785\n",
      "Epoch 109, Loss Train(MSE): 0.1883610388112415, R2 Train: 0.246555844755034\n",
      "Epoch 110, Loss Train(MSE): 0.18790024147808576, R2 Train: 0.24839903408765696\n",
      "Epoch 111, Loss Train(MSE): 0.18724006330438886, R2 Train: 0.25103974678244456\n",
      "Epoch 112, Loss Train(MSE): 0.1866166964491664, R2 Train: 0.2535332142033344\n",
      "Epoch 113, Loss Train(MSE): 0.1860533166047802, R2 Train: 0.2557867335808792\n",
      "Epoch 114, Loss Train(MSE): 0.18545297351488463, R2 Train: 0.2581881059404615\n",
      "Epoch 115, Loss Train(MSE): 0.18496860914398414, R2 Train: 0.26012556342406346\n",
      "Epoch 116, Loss Train(MSE): 0.18429817946345606, R2 Train: 0.26280728214617577\n",
      "Epoch 117, Loss Train(MSE): 0.1836545099897796, R2 Train: 0.26538196004088155\n",
      "Epoch 118, Loss Train(MSE): 0.1830927203063752, R2 Train: 0.26762911877449924\n",
      "Epoch 119, Loss Train(MSE): 0.1824961270383674, R2 Train: 0.2700154918465304\n",
      "Epoch 120, Loss Train(MSE): 0.18199128790165, R2 Train: 0.2720348483934\n",
      "Epoch 121, Loss Train(MSE): 0.181311122343261, R2 Train: 0.274755510626956\n",
      "Epoch 122, Loss Train(MSE): 0.18064898452208986, R2 Train: 0.2774040619116406\n",
      "Epoch 123, Loss Train(MSE): 0.1802000121272203, R2 Train: 0.27919995149111876\n",
      "Epoch 124, Loss Train(MSE): 0.17951428406754313, R2 Train: 0.28194286372982746\n",
      "Epoch 125, Loss Train(MSE): 0.17883331730896315, R2 Train: 0.2846667307641474\n",
      "Epoch 126, Loss Train(MSE): 0.17828119124582073, R2 Train: 0.2868752350167171\n",
      "Epoch 127, Loss Train(MSE): 0.17761897420998085, R2 Train: 0.2895241031600766\n",
      "Epoch 128, Loss Train(MSE): 0.1773291201768267, R2 Train: 0.2906835192926932\n",
      "Epoch 129, Loss Train(MSE): 0.1769743599731416, R2 Train: 0.29210256010743363\n",
      "Epoch 130, Loss Train(MSE): 0.17661479990938947, R2 Train: 0.2935408003624421\n",
      "Epoch 131, Loss Train(MSE): 0.17632413799213503, R2 Train: 0.29470344803145987\n",
      "Epoch 132, Loss Train(MSE): 0.1761786439677595, R2 Train: 0.29528542412896197\n",
      "Epoch 133, Loss Train(MSE): 0.17566591580816948, R2 Train: 0.2973363367673221\n",
      "Epoch 134, Loss Train(MSE): 0.17550558443577322, R2 Train: 0.29797766225690714\n",
      "Epoch 135, Loss Train(MSE): 0.17527050419502954, R2 Train: 0.29891798321988183\n",
      "Epoch 136, Loss Train(MSE): 0.1747783628206083, R2 Train: 0.3008865487175668\n",
      "Epoch 137, Loss Train(MSE): 0.1747137065368064, R2 Train: 0.3011451738527744\n",
      "Epoch 138, Loss Train(MSE): 0.17431116605000624, R2 Train: 0.30275533579997504\n",
      "Epoch 139, Loss Train(MSE): 0.1739772968983193, R2 Train: 0.30409081240672275\n",
      "Epoch 140, Loss Train(MSE): 0.17392073557763454, R2 Train: 0.3043170576894618\n",
      "Epoch 141, Loss Train(MSE): 0.17360864666187176, R2 Train: 0.30556541335251297\n",
      "Epoch 142, Loss Train(MSE): 0.1732184917695087, R2 Train: 0.30712603292196516\n",
      "Epoch 143, Loss Train(MSE): 0.17309445098200107, R2 Train: 0.3076221960719957\n",
      "Epoch 144, Loss Train(MSE): 0.17281530795838226, R2 Train: 0.30873876816647094\n",
      "Epoch 145, Loss Train(MSE): 0.17245561776438462, R2 Train: 0.31017752894246153\n",
      "Epoch 146, Loss Train(MSE): 0.1723135654533133, R2 Train: 0.3107457381867468\n",
      "Epoch 147, Loss Train(MSE): 0.1719244010607324, R2 Train: 0.3123023957570704\n",
      "Epoch 148, Loss Train(MSE): 0.17175528827011505, R2 Train: 0.3129788469195398\n",
      "Epoch 149, Loss Train(MSE): 0.17162014730296465, R2 Train: 0.3135194107881414\n",
      "Epoch 150, Loss Train(MSE): 0.17117736899218655, R2 Train: 0.3152905240312538\n",
      "Epoch 151, Loss Train(MSE): 0.1709712837998046, R2 Train: 0.31611486480078155\n",
      "Epoch 152, Loss Train(MSE): 0.17088524432001917, R2 Train: 0.3164590227199233\n",
      "Epoch 153, Loss Train(MSE): 0.17046893157313026, R2 Train: 0.318124273707479\n",
      "Epoch 154, Loss Train(MSE): 0.1701534779471404, R2 Train: 0.31938608821143843\n",
      "Epoch 155, Loss Train(MSE): 0.17009517372411806, R2 Train: 0.31961930510352776\n",
      "Epoch 156, Loss Train(MSE): 0.16975453873809376, R2 Train: 0.32098184504762495\n",
      "Epoch 157, Loss Train(MSE): 0.16952916540440044, R2 Train: 0.32188333838239824\n",
      "Epoch 158, Loss Train(MSE): 0.16938523144394727, R2 Train: 0.32245907422421094\n",
      "Epoch 159, Loss Train(MSE): 0.16906684105669034, R2 Train: 0.32373263577323863\n",
      "Epoch 160, Loss Train(MSE): 0.16883460447089144, R2 Train: 0.32466158211643426\n",
      "Epoch 161, Loss Train(MSE): 0.16859310276734352, R2 Train: 0.32562758893062593\n",
      "Epoch 162, Loss Train(MSE): 0.16840919222893844, R2 Train: 0.32636323108424625\n",
      "Epoch 163, Loss Train(MSE): 0.16810953624683578, R2 Train: 0.3275618550126569\n",
      "Epoch 164, Loss Train(MSE): 0.1677591025156163, R2 Train: 0.32896358993753483\n",
      "Epoch 165, Loss Train(MSE): 0.1678364323312563, R2 Train: 0.32865427067497477\n",
      "Epoch 166, Loss Train(MSE): 0.16746034979685032, R2 Train: 0.33015860081259873\n",
      "Epoch 167, Loss Train(MSE): 0.16713298984195443, R2 Train: 0.33146804063218227\n",
      "Epoch 168, Loss Train(MSE): 0.16690390869032637, R2 Train: 0.3323843652386945\n",
      "Epoch 169, Loss Train(MSE): 0.1668922875769215, R2 Train: 0.33243084969231396\n",
      "Epoch 170, Loss Train(MSE): 0.16655068227412553, R2 Train: 0.33379727090349787\n",
      "Epoch 171, Loss Train(MSE): 0.1661641820135212, R2 Train: 0.3353432719459152\n",
      "Epoch 172, Loss Train(MSE): 0.1661497515403945, R2 Train: 0.33540099383842203\n",
      "Epoch 173, Loss Train(MSE): 0.165873079643867, R2 Train: 0.336507681424532\n",
      "Epoch 174, Loss Train(MSE): 0.16566935257528487, R2 Train: 0.33732258969886053\n",
      "Epoch 175, Loss Train(MSE): 0.16533784133172327, R2 Train: 0.3386486346731069\n",
      "Epoch 176, Loss Train(MSE): 0.1652823448230004, R2 Train: 0.33887062070799845\n",
      "Epoch 177, Loss Train(MSE): 0.16494549037911527, R2 Train: 0.34021803848353893\n",
      "Epoch 178, Loss Train(MSE): 0.16473132670916166, R2 Train: 0.34107469316335337\n",
      "Epoch 179, Loss Train(MSE): 0.1645908545939633, R2 Train: 0.3416365816241468\n",
      "Epoch 180, Loss Train(MSE): 0.164368468393957, R2 Train: 0.342526126424172\n",
      "Epoch 181, Loss Train(MSE): 0.1641063662578589, R2 Train: 0.3435745349685644\n",
      "Epoch 182, Loss Train(MSE): 0.16376812666658278, R2 Train: 0.3449274933336689\n",
      "Epoch 183, Loss Train(MSE): 0.16370923190831044, R2 Train: 0.34516307236675825\n",
      "Epoch 184, Loss Train(MSE): 0.1636029802699906, R2 Train: 0.3455880789200376\n",
      "Epoch 185, Loss Train(MSE): 0.16329374428795457, R2 Train: 0.34682502284818173\n",
      "Epoch 186, Loss Train(MSE): 0.16292457785742184, R2 Train: 0.34830168857031263\n",
      "Epoch 187, Loss Train(MSE): 0.1629629606229221, R2 Train: 0.34814815750831163\n",
      "Epoch 188, Loss Train(MSE): 0.16275342510120774, R2 Train: 0.348986299595169\n",
      "Epoch 189, Loss Train(MSE): 0.1623876725291511, R2 Train: 0.3504493098833956\n",
      "Epoch 190, Loss Train(MSE): 0.16212701664146414, R2 Train: 0.3514919334341434\n",
      "Epoch 191, Loss Train(MSE): 0.162112081506655, R2 Train: 0.35155167397338005\n",
      "Epoch 192, Loss Train(MSE): 0.16185594225572195, R2 Train: 0.3525762309771122\n",
      "Epoch 193, Loss Train(MSE): 0.16162448963086992, R2 Train: 0.3535020414765203\n",
      "Epoch 194, Loss Train(MSE): 0.16138208711524432, R2 Train: 0.3544716515390227\n",
      "Epoch 195, Loss Train(MSE): 0.16133733849170712, R2 Train: 0.35465064603317153\n",
      "Epoch 196, Loss Train(MSE): 0.16106960095869266, R2 Train: 0.35572159616522936\n",
      "Epoch 197, Loss Train(MSE): 0.16075970824539976, R2 Train: 0.35696116701840097\n",
      "Epoch 198, Loss Train(MSE): 0.16060963347270946, R2 Train: 0.3575614661091622\n",
      "Epoch 199, Loss Train(MSE): 0.1605934206225522, R2 Train: 0.35762631750979124\n",
      "Epoch 200, Loss Train(MSE): 0.1602791299539745, R2 Train: 0.358883480184102\n",
      "Epoch 201, Loss Train(MSE): 0.16005289622348243, R2 Train: 0.3597884151060703\n",
      "Epoch 202, Loss Train(MSE): 0.1598273636099894, R2 Train: 0.36069054556004243\n",
      "Epoch 203, Loss Train(MSE): 0.15973660619609487, R2 Train: 0.36105357521562054\n",
      "Epoch 204, Loss Train(MSE): 0.15958128452653625, R2 Train: 0.361674861893855\n",
      "Epoch 205, Loss Train(MSE): 0.15927297252747408, R2 Train: 0.3629081098901037\n",
      "Epoch 206, Loss Train(MSE): 0.15901572971601252, R2 Train: 0.3639370811359499\n",
      "Epoch 207, Loss Train(MSE): 0.1589466231453436, R2 Train: 0.36421350741862557\n",
      "Epoch 208, Loss Train(MSE): 0.15890332773851445, R2 Train: 0.3643866890459422\n",
      "Epoch 209, Loss Train(MSE): 0.1585720755237386, R2 Train: 0.36571169790504565\n",
      "Epoch 210, Loss Train(MSE): 0.1583057969275019, R2 Train: 0.36677681228999237\n",
      "Epoch 211, Loss Train(MSE): 0.158129266210905, R2 Train: 0.36748293515638\n",
      "Epoch 212, Loss Train(MSE): 0.1581424864178796, R2 Train: 0.36743005432848164\n",
      "Epoch 213, Loss Train(MSE): 0.15792376209325318, R2 Train: 0.3683049516269873\n",
      "Epoch 214, Loss Train(MSE): 0.15760296992295428, R2 Train: 0.3695881203081829\n",
      "Epoch 215, Loss Train(MSE): 0.1573718651385511, R2 Train: 0.37051253944579565\n",
      "Epoch 216, Loss Train(MSE): 0.15740515587006634, R2 Train: 0.3703793765197346\n",
      "Epoch 217, Loss Train(MSE): 0.15717337134921128, R2 Train: 0.3713065146031549\n",
      "Epoch 218, Loss Train(MSE): 0.1569065031154202, R2 Train: 0.3723739875383192\n",
      "Epoch 219, Loss Train(MSE): 0.15677011316657746, R2 Train: 0.37291954733369015\n",
      "Epoch 220, Loss Train(MSE): 0.1566610355245387, R2 Train: 0.3733558579018452\n",
      "Epoch 221, Loss Train(MSE): 0.1565080477128178, R2 Train: 0.37396780914872885\n",
      "Epoch 222, Loss Train(MSE): 0.1562643124278921, R2 Train: 0.3749427502884316\n",
      "Epoch 223, Loss Train(MSE): 0.15601886811266896, R2 Train: 0.37592452754932415\n",
      "Epoch 224, Loss Train(MSE): 0.15579349834768566, R2 Train: 0.37682600660925736\n",
      "Epoch 225, Loss Train(MSE): 0.15596730985727414, R2 Train: 0.37613076057090344\n",
      "Epoch 226, Loss Train(MSE): 0.15568883223619562, R2 Train: 0.37724467105521753\n",
      "Epoch 227, Loss Train(MSE): 0.15539003786328515, R2 Train: 0.3784398485468594\n",
      "Epoch 228, Loss Train(MSE): 0.1551642293760835, R2 Train: 0.37934308249566595\n",
      "Epoch 229, Loss Train(MSE): 0.15508046265801842, R2 Train: 0.3796781493679263\n",
      "Epoch 230, Loss Train(MSE): 0.15501535545243972, R2 Train: 0.3799385781902411\n",
      "Epoch 231, Loss Train(MSE): 0.15482218338571982, R2 Train: 0.3807112664571207\n",
      "Epoch 232, Loss Train(MSE): 0.15464450535544025, R2 Train: 0.381421978578239\n",
      "Epoch 233, Loss Train(MSE): 0.15435795938742122, R2 Train: 0.3825681624503151\n",
      "Epoch 234, Loss Train(MSE): 0.15436525339304827, R2 Train: 0.3825389864278069\n",
      "Epoch 235, Loss Train(MSE): 0.15419869251462726, R2 Train: 0.383205229941491\n",
      "Epoch 236, Loss Train(MSE): 0.15399771514086308, R2 Train: 0.3840091394365477\n",
      "Epoch 237, Loss Train(MSE): 0.1538412362573232, R2 Train: 0.38463505497070716\n",
      "Epoch 238, Loss Train(MSE): 0.15358707465907473, R2 Train: 0.3856517013637011\n",
      "Epoch 239, Loss Train(MSE): 0.15362934101887396, R2 Train: 0.38548263592450416\n",
      "Epoch 240, Loss Train(MSE): 0.15341333985212863, R2 Train: 0.3863466405914855\n",
      "Epoch 241, Loss Train(MSE): 0.15318337593769146, R2 Train: 0.38726649624923415\n",
      "Epoch 242, Loss Train(MSE): 0.15295173810301932, R2 Train: 0.3881930475879227\n",
      "Epoch 243, Loss Train(MSE): 0.15292679672149567, R2 Train: 0.3882928131140173\n",
      "Epoch 244, Loss Train(MSE): 0.1529327139068113, R2 Train: 0.38826914437275484\n",
      "Epoch 245, Loss Train(MSE): 0.152661370511526, R2 Train: 0.389354517953896\n",
      "Epoch 246, Loss Train(MSE): 0.15243956062355152, R2 Train: 0.39024175750579393\n",
      "Epoch 247, Loss Train(MSE): 0.15221358086368456, R2 Train: 0.3911456765452618\n",
      "Epoch 248, Loss Train(MSE): 0.15216409203469392, R2 Train: 0.39134363186122434\n",
      "Epoch 249, Loss Train(MSE): 0.15220560429681382, R2 Train: 0.39117758281274473\n",
      "Epoch 250, Loss Train(MSE): 0.15194216395044322, R2 Train: 0.39223134419822714\n",
      "Epoch 251, Loss Train(MSE): 0.15172519970897597, R2 Train: 0.39309920116409613\n",
      "Epoch 252, Loss Train(MSE): 0.15150745651486497, R2 Train: 0.3939701739405401\n",
      "Epoch 253, Loss Train(MSE): 0.1513977674016138, R2 Train: 0.39440893039354485\n",
      "Epoch 254, Loss Train(MSE): 0.15140510313185362, R2 Train: 0.3943795874725855\n",
      "Epoch 255, Loss Train(MSE): 0.15122115739777112, R2 Train: 0.39511537040891553\n",
      "Epoch 256, Loss Train(MSE): 0.15106554838484698, R2 Train: 0.3957378064606121\n",
      "Epoch 257, Loss Train(MSE): 0.15091394457034846, R2 Train: 0.39634422171860617\n",
      "Epoch 258, Loss Train(MSE): 0.1506983448248547, R2 Train: 0.3972066207005812\n",
      "Epoch 259, Loss Train(MSE): 0.1507359679100035, R2 Train: 0.39705612835998605\n",
      "Epoch 260, Loss Train(MSE): 0.1505766133126663, R2 Train: 0.39769354674933477\n",
      "Epoch 261, Loss Train(MSE): 0.15043301984773733, R2 Train: 0.3982679206090507\n",
      "Epoch 262, Loss Train(MSE): 0.15019090618412967, R2 Train: 0.3992363752634813\n",
      "Epoch 263, Loss Train(MSE): 0.1499917936004882, R2 Train: 0.4000328255980472\n",
      "Epoch 264, Loss Train(MSE): 0.15004766708357267, R2 Train: 0.3998093316657093\n",
      "Epoch 265, Loss Train(MSE): 0.14993020852393235, R2 Train: 0.4002791659042706\n",
      "Epoch 266, Loss Train(MSE): 0.1497088941701979, R2 Train: 0.40116442331920843\n",
      "Epoch 267, Loss Train(MSE): 0.14954245741864644, R2 Train: 0.40183017032541424\n",
      "Epoch 268, Loss Train(MSE): 0.14934340382439365, R2 Train: 0.4026263847024254\n",
      "Epoch 269, Loss Train(MSE): 0.14939042561129356, R2 Train: 0.40243829755482574\n",
      "Epoch 270, Loss Train(MSE): 0.14934603457556897, R2 Train: 0.4026158616977241\n",
      "Epoch 271, Loss Train(MSE): 0.14911379834322694, R2 Train: 0.40354480662709225\n",
      "Epoch 272, Loss Train(MSE): 0.14894706971930263, R2 Train: 0.40421172112278947\n",
      "Epoch 273, Loss Train(MSE): 0.14873916814245616, R2 Train: 0.40504332743017535\n",
      "Epoch 274, Loss Train(MSE): 0.1485782677603914, R2 Train: 0.4056869289584344\n",
      "Epoch 275, Loss Train(MSE): 0.14866292095452432, R2 Train: 0.4053483161819027\n",
      "Epoch 276, Loss Train(MSE): 0.14862402447618442, R2 Train: 0.4055039020952623\n",
      "Epoch 277, Loss Train(MSE): 0.14840048798000552, R2 Train: 0.4063980480799779\n",
      "Epoch 278, Loss Train(MSE): 0.14818027393453953, R2 Train: 0.4072789042618419\n",
      "Epoch 279, Loss Train(MSE): 0.14802363741415792, R2 Train: 0.40790545034336834\n",
      "Epoch 280, Loss Train(MSE): 0.147886805004417, R2 Train: 0.408452779982332\n",
      "Epoch 281, Loss Train(MSE): 0.14797761138822113, R2 Train: 0.4080895544471155\n",
      "Epoch 282, Loss Train(MSE): 0.1477905677976575, R2 Train: 0.40883772880936997\n",
      "Epoch 283, Loss Train(MSE): 0.14772102001699153, R2 Train: 0.4091159199320339\n",
      "Epoch 284, Loss Train(MSE): 0.14750905719683627, R2 Train: 0.4099637712126549\n",
      "Epoch 285, Loss Train(MSE): 0.14730012640399545, R2 Train: 0.4107994943840182\n",
      "Epoch 286, Loss Train(MSE): 0.14727599414618525, R2 Train: 0.410896023415259\n",
      "Epoch 287, Loss Train(MSE): 0.14726453508776008, R2 Train: 0.4109418596489597\n",
      "Epoch 288, Loss Train(MSE): 0.14711420411248635, R2 Train: 0.4115431835500546\n",
      "Epoch 289, Loss Train(MSE): 0.14692799450742883, R2 Train: 0.4122880219702847\n",
      "Epoch 290, Loss Train(MSE): 0.14676860281270884, R2 Train: 0.41292558874916463\n",
      "Epoch 291, Loss Train(MSE): 0.14666149661340466, R2 Train: 0.41335401354638135\n",
      "Epoch 292, Loss Train(MSE): 0.14669775125899467, R2 Train: 0.4132089949640213\n",
      "Epoch 293, Loss Train(MSE): 0.14664264837947216, R2 Train: 0.41342940648211135\n",
      "Epoch 294, Loss Train(MSE): 0.14644093681013048, R2 Train: 0.41423625275947806\n",
      "Epoch 295, Loss Train(MSE): 0.14629670132487915, R2 Train: 0.4148131947004834\n",
      "Epoch 296, Loss Train(MSE): 0.146114981892761, R2 Train: 0.41554007242895596\n",
      "Epoch 297, Loss Train(MSE): 0.1459755589023607, R2 Train: 0.41609776439055723\n",
      "Epoch 298, Loss Train(MSE): 0.14595835922697797, R2 Train: 0.4161665630920881\n",
      "Epoch 299, Loss Train(MSE): 0.14605798804469433, R2 Train: 0.4157680478212227\n",
      "Epoch 300, Loss Train(MSE): 0.1458631819127206, R2 Train: 0.4165472723491176\n",
      "Epoch 301, Loss Train(MSE): 0.14567137314033232, R2 Train: 0.4173145074386707\n",
      "Epoch 302, Loss Train(MSE): 0.1455199575801638, R2 Train: 0.41792016967934476\n",
      "Epoch 303, Loss Train(MSE): 0.14535467363290086, R2 Train: 0.41858130546839656\n",
      "Epoch 304, Loss Train(MSE): 0.14529487346053516, R2 Train: 0.4188205061578594\n",
      "Epoch 305, Loss Train(MSE): 0.14535360083790413, R2 Train: 0.41858559664838346\n",
      "Epoch 306, Loss Train(MSE): 0.1452143565086296, R2 Train: 0.4191425739654816\n",
      "Epoch 307, Loss Train(MSE): 0.14509157435558123, R2 Train: 0.41963370257767507\n",
      "Epoch 308, Loss Train(MSE): 0.1450067869323624, R2 Train: 0.41997285227055037\n",
      "Epoch 309, Loss Train(MSE): 0.14482643647895682, R2 Train: 0.4206942540841727\n",
      "Epoch 310, Loss Train(MSE): 0.1446486413563685, R2 Train: 0.42140543457452595\n",
      "Epoch 311, Loss Train(MSE): 0.1447601540907884, R2 Train: 0.4209593836368464\n",
      "Epoch 312, Loss Train(MSE): 0.14464750413061347, R2 Train: 0.4214099834775461\n",
      "Epoch 313, Loss Train(MSE): 0.14451216601490122, R2 Train: 0.42195133594039513\n",
      "Epoch 314, Loss Train(MSE): 0.1443472947028523, R2 Train: 0.42261082118859084\n",
      "Epoch 315, Loss Train(MSE): 0.1442947196038539, R2 Train: 0.4228211215845844\n",
      "Epoch 316, Loss Train(MSE): 0.14415670710559256, R2 Train: 0.42337317157762977\n",
      "Epoch 317, Loss Train(MSE): 0.14408452440304786, R2 Train: 0.42366190238780854\n",
      "Epoch 318, Loss Train(MSE): 0.14411538001370106, R2 Train: 0.4235384799451958\n",
      "Epoch 319, Loss Train(MSE): 0.14399156385044368, R2 Train: 0.4240337445982253\n",
      "Epoch 320, Loss Train(MSE): 0.14383488384707901, R2 Train: 0.42466046461168394\n",
      "Epoch 321, Loss Train(MSE): 0.1437052930469958, R2 Train: 0.42517882781201677\n",
      "Epoch 322, Loss Train(MSE): 0.14356446745697465, R2 Train: 0.4257421301721014\n",
      "Epoch 323, Loss Train(MSE): 0.14342724254381736, R2 Train: 0.42629102982473055\n",
      "Epoch 324, Loss Train(MSE): 0.14354316063523978, R2 Train: 0.4258273574590409\n",
      "Epoch 325, Loss Train(MSE): 0.1435419841997778, R2 Train: 0.42583206320088884\n",
      "Epoch 326, Loss Train(MSE): 0.1433761859823383, R2 Train: 0.42649525607064676\n",
      "Epoch 327, Loss Train(MSE): 0.14321296713350448, R2 Train: 0.4271481314659821\n",
      "Epoch 328, Loss Train(MSE): 0.14308879316422957, R2 Train: 0.42764482734308173\n",
      "Epoch 329, Loss Train(MSE): 0.14294323887662364, R2 Train: 0.42822704449350546\n",
      "Epoch 330, Loss Train(MSE): 0.14283398073297948, R2 Train: 0.42866407706808207\n",
      "Epoch 331, Loss Train(MSE): 0.14290506940382602, R2 Train: 0.4283797223846959\n",
      "Epoch 332, Loss Train(MSE): 0.14286133879925225, R2 Train: 0.428554644802991\n",
      "Epoch 333, Loss Train(MSE): 0.14276053926221377, R2 Train: 0.4289578429511449\n",
      "Epoch 334, Loss Train(MSE): 0.1426911114828534, R2 Train: 0.4292355540685864\n",
      "Epoch 335, Loss Train(MSE): 0.1425367543612677, R2 Train: 0.4298529825549292\n",
      "Epoch 336, Loss Train(MSE): 0.1423846455941329, R2 Train: 0.43046141762346835\n",
      "Epoch 337, Loss Train(MSE): 0.1422505285673469, R2 Train: 0.4309978857306124\n",
      "Epoch 338, Loss Train(MSE): 0.14231549130282337, R2 Train: 0.43073803478870654\n",
      "Epoch 339, Loss Train(MSE): 0.14229885648510565, R2 Train: 0.4308045740595774\n",
      "Epoch 340, Loss Train(MSE): 0.14216469148429914, R2 Train: 0.43134123406280345\n",
      "Epoch 341, Loss Train(MSE): 0.142053804272067, R2 Train: 0.43178478291173195\n",
      "Epoch 342, Loss Train(MSE): 0.1420074594101459, R2 Train: 0.43197016235941643\n",
      "Epoch 343, Loss Train(MSE): 0.14186143711774998, R2 Train: 0.43255425152900007\n",
      "Epoch 344, Loss Train(MSE): 0.14171749433593112, R2 Train: 0.43313002265627554\n",
      "Epoch 345, Loss Train(MSE): 0.14170301978948197, R2 Train: 0.43318792084207214\n",
      "Epoch 346, Loss Train(MSE): 0.14175936940114134, R2 Train: 0.43296252239543465\n",
      "Epoch 347, Loss Train(MSE): 0.14164985095569135, R2 Train: 0.4334005961772346\n",
      "Epoch 348, Loss Train(MSE): 0.14151393349860603, R2 Train: 0.4339442660055759\n",
      "Epoch 349, Loss Train(MSE): 0.14141552334956695, R2 Train: 0.4343379066017322\n",
      "Epoch 350, Loss Train(MSE): 0.14128448929825457, R2 Train: 0.4348620428069817\n",
      "Epoch 351, Loss Train(MSE): 0.1411813340736291, R2 Train: 0.4352746637054836\n",
      "Epoch 352, Loss Train(MSE): 0.14107822744497267, R2 Train: 0.4356870902201093\n",
      "Epoch 353, Loss Train(MSE): 0.14130010689905742, R2 Train: 0.4347995724037703\n",
      "Epoch 354, Loss Train(MSE): 0.14118629691549067, R2 Train: 0.4352548123380373\n",
      "Epoch 355, Loss Train(MSE): 0.14104760723973883, R2 Train: 0.4358095710410447\n",
      "Epoch 356, Loss Train(MSE): 0.1409165724885231, R2 Train: 0.4363337100459076\n",
      "Epoch 357, Loss Train(MSE): 0.14081828339888897, R2 Train: 0.43672686640444414\n",
      "Epoch 358, Loss Train(MSE): 0.14069919835275738, R2 Train: 0.4372032065889705\n",
      "Epoch 359, Loss Train(MSE): 0.14059552357824395, R2 Train: 0.4376179056870242\n",
      "Epoch 360, Loss Train(MSE): 0.14054152308941795, R2 Train: 0.4378339076423282\n",
      "Epoch 361, Loss Train(MSE): 0.14065271694262513, R2 Train: 0.4373891322294995\n",
      "Epoch 362, Loss Train(MSE): 0.1405788239831483, R2 Train: 0.4376847040674068\n",
      "Epoch 363, Loss Train(MSE): 0.14051705816768095, R2 Train: 0.4379317673292762\n",
      "Epoch 364, Loss Train(MSE): 0.14038713006440828, R2 Train: 0.43845147974236687\n",
      "Epoch 365, Loss Train(MSE): 0.14025912200100962, R2 Train: 0.4389635119959615\n",
      "Epoch 366, Loss Train(MSE): 0.14014499232782962, R2 Train: 0.4394200306886815\n",
      "Epoch 367, Loss Train(MSE): 0.14004738117102775, R2 Train: 0.439810475315889\n",
      "Epoch 368, Loss Train(MSE): 0.1400297746042008, R2 Train: 0.4398809015831968\n",
      "Epoch 369, Loss Train(MSE): 0.14011096959899866, R2 Train: 0.43955612160400537\n",
      "Epoch 370, Loss Train(MSE): 0.1400145682423416, R2 Train: 0.4399417270306336\n",
      "Epoch 371, Loss Train(MSE): 0.13989702540437998, R2 Train: 0.44041189838248007\n",
      "Epoch 372, Loss Train(MSE): 0.13981927972594838, R2 Train: 0.4407228810962065\n",
      "Epoch 373, Loss Train(MSE): 0.13977708588363644, R2 Train: 0.44089165646545425\n",
      "Epoch 374, Loss Train(MSE): 0.13965664139757195, R2 Train: 0.4413734344097122\n",
      "Epoch 375, Loss Train(MSE): 0.1395378721401774, R2 Train: 0.4418485114392904\n",
      "Epoch 376, Loss Train(MSE): 0.13948579690987853, R2 Train: 0.4420568123604859\n",
      "Epoch 377, Loss Train(MSE): 0.13960744068671074, R2 Train: 0.441570237253157\n",
      "Epoch 378, Loss Train(MSE): 0.13950933066040197, R2 Train: 0.4419626773583921\n",
      "Epoch 379, Loss Train(MSE): 0.13940317263163807, R2 Train: 0.44238730947344773\n",
      "Epoch 380, Loss Train(MSE): 0.1393136108377372, R2 Train: 0.44274555664905124\n",
      "Epoch 381, Loss Train(MSE): 0.13920519250165203, R2 Train: 0.4431792299933919\n",
      "Epoch 382, Loss Train(MSE): 0.1391248498328625, R2 Train: 0.44350060066855\n",
      "Epoch 383, Loss Train(MSE): 0.1390191756548552, R2 Train: 0.44392329738057923\n",
      "Epoch 384, Loss Train(MSE): 0.13893613132470015, R2 Train: 0.4442554747011994\n",
      "Epoch 385, Loss Train(MSE): 0.13908423913933024, R2 Train: 0.44366304344267904\n",
      "Epoch 386, Loss Train(MSE): 0.1390957456290543, R2 Train: 0.4436170174837828\n",
      "Epoch 387, Loss Train(MSE): 0.13898100145478567, R2 Train: 0.4440759941808573\n",
      "Epoch 388, Loss Train(MSE): 0.13886801294085446, R2 Train: 0.44452794823658215\n",
      "Epoch 389, Loss Train(MSE): 0.13875671405876389, R2 Train: 0.44497314376494446\n",
      "Epoch 390, Loss Train(MSE): 0.13867182663829875, R2 Train: 0.445312693446805\n",
      "Epoch 391, Loss Train(MSE): 0.13857231422142724, R2 Train: 0.44571074311429104\n",
      "Epoch 392, Loss Train(MSE): 0.1384965857703145, R2 Train: 0.44601365691874195\n",
      "Epoch 393, Loss Train(MSE): 0.13843566267833546, R2 Train: 0.44625734928665817\n",
      "Epoch 394, Loss Train(MSE): 0.13858004547273203, R2 Train: 0.44567981810907187\n",
      "Epoch 395, Loss Train(MSE): 0.13848379009354017, R2 Train: 0.4460648396258393\n",
      "Epoch 396, Loss Train(MSE): 0.13839506467485377, R2 Train: 0.4464197413005849\n",
      "Epoch 397, Loss Train(MSE): 0.13834320104875086, R2 Train: 0.44662719580499655\n",
      "Epoch 398, Loss Train(MSE): 0.13829798164820653, R2 Train: 0.4468080734071739\n",
      "Epoch 399, Loss Train(MSE): 0.13819389371527302, R2 Train: 0.4472244251389079\n",
      "Epoch 400, Loss Train(MSE): 0.13809124144085672, R2 Train: 0.44763503423657314\n",
      "Epoch 401, Loss Train(MSE): 0.13798998136618398, R2 Train: 0.44804007453526407\n",
      "Epoch 402, Loss Train(MSE): 0.137971277614897, R2 Train: 0.448114889540412\n",
      "Epoch 403, Loss Train(MSE): 0.13807671387081208, R2 Train: 0.4476931445167517\n",
      "Epoch 404, Loss Train(MSE): 0.13800137006763671, R2 Train: 0.44799451972945314\n",
      "Epoch 405, Loss Train(MSE): 0.1379026598159213, R2 Train: 0.44838936073631475\n",
      "Epoch 406, Loss Train(MSE): 0.13783101368037243, R2 Train: 0.4486759452785103\n",
      "Epoch 407, Loss Train(MSE): 0.13773760860424308, R2 Train: 0.44904956558302767\n",
      "Epoch 408, Loss Train(MSE): 0.13766334151354145, R2 Train: 0.4493466339458342\n",
      "Epoch 409, Loss Train(MSE): 0.13759447926116414, R2 Train: 0.44962208295534345\n",
      "Epoch 410, Loss Train(MSE): 0.13758076981453837, R2 Train: 0.4496769207418465\n",
      "Epoch 411, Loss Train(MSE): 0.1374996129098105, R2 Train: 0.450001548360758\n",
      "Epoch 412, Loss Train(MSE): 0.1376414868057919, R2 Train: 0.44943405277683235\n",
      "Epoch 413, Loss Train(MSE): 0.13754238752619952, R2 Train: 0.44983044989520193\n",
      "Epoch 414, Loss Train(MSE): 0.1374567335614518, R2 Train: 0.4501730657541928\n",
      "Epoch 415, Loss Train(MSE): 0.13737796789622098, R2 Train: 0.4504881284151161\n",
      "Epoch 416, Loss Train(MSE): 0.1372990745551492, R2 Train: 0.4508037017794032\n",
      "Epoch 417, Loss Train(MSE): 0.13721835878251748, R2 Train: 0.4511265648699301\n",
      "Epoch 418, Loss Train(MSE): 0.1371467402450328, R2 Train: 0.45141303901986884\n",
      "Epoch 419, Loss Train(MSE): 0.13706309502271283, R2 Train: 0.4517476199091487\n",
      "Epoch 420, Loss Train(MSE): 0.13699917316347254, R2 Train: 0.45200330734610983\n",
      "Epoch 421, Loss Train(MSE): 0.13705618030653147, R2 Train: 0.45177527877387413\n",
      "Epoch 422, Loss Train(MSE): 0.13709662864861105, R2 Train: 0.4516134854055558\n",
      "Epoch 423, Loss Train(MSE): 0.13705566569309457, R2 Train: 0.4517773372276217\n",
      "Epoch 424, Loss Train(MSE): 0.1370163117622219, R2 Train: 0.4519347529511124\n",
      "Epoch 425, Loss Train(MSE): 0.13692550466831677, R2 Train: 0.45229798132673293\n",
      "Epoch 426, Loss Train(MSE): 0.13683598324270213, R2 Train: 0.4526560670291915\n",
      "Epoch 427, Loss Train(MSE): 0.13674770472680708, R2 Train: 0.45300918109277166\n",
      "Epoch 428, Loss Train(MSE): 0.1366646152484824, R2 Train: 0.45334153900607044\n",
      "Epoch 429, Loss Train(MSE): 0.13660129439015212, R2 Train: 0.45359482243939153\n",
      "Epoch 430, Loss Train(MSE): 0.1365258712013985, R2 Train: 0.453896515194406\n",
      "Epoch 431, Loss Train(MSE): 0.13662430564898922, R2 Train: 0.4535027774040431\n",
      "Epoch 432, Loss Train(MSE): 0.13663338229052263, R2 Train: 0.4534664708379095\n",
      "Epoch 433, Loss Train(MSE): 0.13655528611193213, R2 Train: 0.4537788555522715\n",
      "Epoch 434, Loss Train(MSE): 0.13648733693469275, R2 Train: 0.454050652261229\n",
      "Epoch 435, Loss Train(MSE): 0.13640900081855425, R2 Train: 0.454363996725783\n",
      "Epoch 436, Loss Train(MSE): 0.13638985725906277, R2 Train: 0.4544405709637489\n",
      "Epoch 437, Loss Train(MSE): 0.13634194584793405, R2 Train: 0.4546322166082638\n",
      "Epoch 438, Loss Train(MSE): 0.13625938037421495, R2 Train: 0.4549624785031402\n",
      "Epoch 439, Loss Train(MSE): 0.1361778925504794, R2 Train: 0.4552884297980824\n",
      "Epoch 440, Loss Train(MSE): 0.13609745204862084, R2 Train: 0.45561019180551665\n",
      "Epoch 441, Loss Train(MSE): 0.13614580103864965, R2 Train: 0.4554167958454014\n",
      "Epoch 442, Loss Train(MSE): 0.13620103071963655, R2 Train: 0.4551958771214538\n",
      "Epoch 443, Loss Train(MSE): 0.13613585338336123, R2 Train: 0.45545658646655507\n",
      "Epoch 444, Loss Train(MSE): 0.13606008453569374, R2 Train: 0.45575966185722505\n",
      "Epoch 445, Loss Train(MSE): 0.1359996335492285, R2 Train: 0.456001465803086\n",
      "Epoch 446, Loss Train(MSE): 0.13592335668165853, R2 Train: 0.4563065732733659\n",
      "Epoch 447, Loss Train(MSE): 0.135868142103158, R2 Train: 0.45652743158736797\n",
      "Epoch 448, Loss Train(MSE): 0.13579424507355473, R2 Train: 0.4568230197057811\n",
      "Epoch 449, Loss Train(MSE): 0.13573720412555212, R2 Train: 0.4570511834977915\n",
      "Epoch 450, Loss Train(MSE): 0.1356767090770012, R2 Train: 0.45729316369199524\n",
      "Epoch 451, Loss Train(MSE): 0.1356844216169793, R2 Train: 0.4572623135320828\n",
      "Epoch 452, Loss Train(MSE): 0.13584014525503463, R2 Train: 0.4566394189798615\n",
      "Epoch 453, Loss Train(MSE): 0.13576071235407539, R2 Train: 0.45695715058369846\n",
      "Epoch 454, Loss Train(MSE): 0.1356824724320151, R2 Train: 0.45727011027193964\n",
      "Epoch 455, Loss Train(MSE): 0.1356053802688324, R2 Train: 0.4575784789246704\n",
      "Epoch 456, Loss Train(MSE): 0.13554810369154632, R2 Train: 0.4578075852338147\n",
      "Epoch 457, Loss Train(MSE): 0.1354771784973151, R2 Train: 0.45809128601073956\n",
      "Epoch 458, Loss Train(MSE): 0.13542491799354953, R2 Train: 0.4583003280258019\n",
      "Epoch 459, Loss Train(MSE): 0.1353556702538082, R2 Train: 0.45857731898476717\n",
      "Epoch 460, Loss Train(MSE): 0.13530245757303896, R2 Train: 0.45879016970784414\n",
      "Epoch 461, Loss Train(MSE): 0.1352386381077326, R2 Train: 0.4590454475690696\n",
      "Epoch 462, Loss Train(MSE): 0.13521834564191146, R2 Train: 0.45912661743235417\n",
      "Epoch 463, Loss Train(MSE): 0.13535490998634028, R2 Train: 0.45858036005463887\n",
      "Epoch 464, Loss Train(MSE): 0.13528936333999292, R2 Train: 0.4588425466400283\n",
      "Epoch 465, Loss Train(MSE): 0.13524528038450795, R2 Train: 0.4590188784619682\n",
      "Epoch 466, Loss Train(MSE): 0.13523474764756643, R2 Train: 0.45906100940973427\n",
      "Epoch 467, Loss Train(MSE): 0.13516297030226138, R2 Train: 0.4593481187909545\n",
      "Epoch 468, Loss Train(MSE): 0.13509216118778425, R2 Train: 0.459631355248863\n",
      "Epoch 469, Loss Train(MSE): 0.1350222896805036, R2 Train: 0.45991084127798565\n",
      "Epoch 470, Loss Train(MSE): 0.13495332716299405, R2 Train: 0.4601866913480238\n",
      "Epoch 471, Loss Train(MSE): 0.13490271929906608, R2 Train: 0.4603891228037357\n",
      "Epoch 472, Loss Train(MSE): 0.1348395838201858, R2 Train: 0.4606416647192568\n",
      "Epoch 473, Loss Train(MSE): 0.13479272243272636, R2 Train: 0.46082911026909457\n",
      "Epoch 474, Loss Train(MSE): 0.13492489306112584, R2 Train: 0.4603004277554966\n",
      "Epoch 475, Loss Train(MSE): 0.13490465865128698, R2 Train: 0.4603813653948521\n",
      "Epoch 476, Loss Train(MSE): 0.1348453311808607, R2 Train: 0.4606186752765572\n",
      "Epoch 477, Loss Train(MSE): 0.13478690161192997, R2 Train: 0.46085239355228014\n",
      "Epoch 478, Loss Train(MSE): 0.13473129660345295, R2 Train: 0.4610748135861882\n",
      "Epoch 479, Loss Train(MSE): 0.1346725526959822, R2 Train: 0.46130978921607124\n",
      "Epoch 480, Loss Train(MSE): 0.1346210919177616, R2 Train: 0.46151563232895365\n",
      "Epoch 481, Loss Train(MSE): 0.13456129221336185, R2 Train: 0.4617548311465526\n",
      "Epoch 482, Loss Train(MSE): 0.1345594018576037, R2 Train: 0.4617623925695852\n",
      "Epoch 483, Loss Train(MSE): 0.13452070293504328, R2 Train: 0.46191718825982686\n",
      "Epoch 484, Loss Train(MSE): 0.13445744015430364, R2 Train: 0.46217023938278545\n",
      "Epoch 485, Loss Train(MSE): 0.13442724972290443, R2 Train: 0.4622910011083823\n",
      "Epoch 486, Loss Train(MSE): 0.13455243869916583, R2 Train: 0.46179024520333667\n",
      "Epoch 487, Loss Train(MSE): 0.134486289969229, R2 Train: 0.46205484012308395\n",
      "Epoch 488, Loss Train(MSE): 0.13443882385212544, R2 Train: 0.46224470459149825\n",
      "Epoch 489, Loss Train(MSE): 0.13437628632042822, R2 Train: 0.4624948547182871\n",
      "Epoch 490, Loss Train(MSE): 0.13433168204072582, R2 Train: 0.4626732718370967\n",
      "Epoch 491, Loss Train(MSE): 0.13427163484619964, R2 Train: 0.46291346061520144\n",
      "Epoch 492, Loss Train(MSE): 0.1342259887254082, R2 Train: 0.4630960450983672\n",
      "Epoch 493, Loss Train(MSE): 0.13417025455010606, R2 Train: 0.46331898179957576\n",
      "Epoch 494, Loss Train(MSE): 0.13412297627223543, R2 Train: 0.46350809491105827\n",
      "Epoch 495, Loss Train(MSE): 0.13407183795654837, R2 Train: 0.46371264817380653\n",
      "Epoch 496, Loss Train(MSE): 0.1340224287091339, R2 Train: 0.46391028516346444\n",
      "Epoch 497, Loss Train(MSE): 0.13399111993817273, R2 Train: 0.4640355202473091\n",
      "Epoch 498, Loss Train(MSE): 0.1341395346545685, R2 Train: 0.46344186138172605\n",
      "Epoch 499, Loss Train(MSE): 0.1341360686682571, R2 Train: 0.46345572532697155\n",
      "Epoch 500, Loss Train(MSE): 0.134098326112787, R2 Train: 0.46360669554885203\n",
      "Epoch 501, Loss Train(MSE): 0.13403768583184847, R2 Train: 0.4638492566726061\n",
      "Epoch 502, Loss Train(MSE): 0.13397786339319362, R2 Train: 0.4640885464272255\n",
      "Epoch 503, Loss Train(MSE): 0.1339188320195533, R2 Train: 0.46432467192178684\n",
      "Epoch 504, Loss Train(MSE): 0.1338605666608795, R2 Train: 0.464557733356482\n",
      "Epoch 505, Loss Train(MSE): 0.1338121415013725, R2 Train: 0.46475143399451\n",
      "Epoch 506, Loss Train(MSE): 0.13376352716616985, R2 Train: 0.4649458913353206\n",
      "Epoch 507, Loss Train(MSE): 0.13371923363789973, R2 Train: 0.4651230654484011\n",
      "Epoch 508, Loss Train(MSE): 0.13366879297466425, R2 Train: 0.465324828101343\n",
      "Epoch 509, Loss Train(MSE): 0.13362886336677776, R2 Train: 0.46548454653288895\n",
      "Epoch 510, Loss Train(MSE): 0.13368539913594832, R2 Train: 0.4652584034562067\n",
      "Epoch 511, Loss Train(MSE): 0.1337483924467248, R2 Train: 0.46500643021310084\n",
      "Epoch 512, Loss Train(MSE): 0.13369592701365526, R2 Train: 0.46521629194537895\n",
      "Epoch 513, Loss Train(MSE): 0.13365013254673017, R2 Train: 0.46539946981307934\n",
      "Epoch 514, Loss Train(MSE): 0.13360040089301242, R2 Train: 0.4655983964279503\n",
      "Epoch 515, Loss Train(MSE): 0.1335546623483412, R2 Train: 0.46578135060663517\n",
      "Epoch 516, Loss Train(MSE): 0.1335109133846501, R2 Train: 0.4659563464613996\n",
      "Epoch 517, Loss Train(MSE): 0.13352420525842257, R2 Train: 0.46590317896630973\n",
      "Epoch 518, Loss Train(MSE): 0.13347020066551027, R2 Train: 0.4661191973379589\n",
      "Epoch 519, Loss Train(MSE): 0.13341684395171693, R2 Train: 0.46633262419313226\n",
      "Epoch 520, Loss Train(MSE): 0.1333641179328175, R2 Train: 0.46654352826873\n",
      "Epoch 521, Loss Train(MSE): 0.13331200638516102, R2 Train: 0.4667519744593559\n",
      "Epoch 522, Loss Train(MSE): 0.13326074100175128, R2 Train: 0.4669570359929949\n",
      "Epoch 523, Loss Train(MSE): 0.1333095849967583, R2 Train: 0.4667616600129668\n",
      "Epoch 524, Loss Train(MSE): 0.1333838819057722, R2 Train: 0.46646447237691124\n",
      "Epoch 525, Loss Train(MSE): 0.13334141750474895, R2 Train: 0.4666343299810042\n",
      "Epoch 526, Loss Train(MSE): 0.1332924458707756, R2 Train: 0.4668302165168976\n",
      "Epoch 527, Loss Train(MSE): 0.1332504304755085, R2 Train: 0.466998278097966\n",
      "Epoch 528, Loss Train(MSE): 0.13320410406047065, R2 Train: 0.4671835837581174\n",
      "Epoch 529, Loss Train(MSE): 0.13316191777207928, R2 Train: 0.4673523289116829\n",
      "Epoch 530, Loss Train(MSE): 0.13311855327104383, R2 Train: 0.4675257869158247\n",
      "Epoch 531, Loss Train(MSE): 0.1330756646736781, R2 Train: 0.4676973413052876\n",
      "Epoch 532, Loss Train(MSE): 0.13303553403777824, R2 Train: 0.46785786384888706\n",
      "Epoch 533, Loss Train(MSE): 0.13299148774112618, R2 Train: 0.4680340490354953\n",
      "Epoch 534, Loss Train(MSE): 0.1329548236081519, R2 Train: 0.46818070556739244\n",
      "Epoch 535, Loss Train(MSE): 0.13290937344182863, R2 Train: 0.4683625062326855\n",
      "Epoch 536, Loss Train(MSE): 0.13287615150592466, R2 Train: 0.46849539397630136\n",
      "Epoch 537, Loss Train(MSE): 0.1330286655954325, R2 Train: 0.46788533761827\n",
      "Epoch 538, Loss Train(MSE): 0.13305477558572282, R2 Train: 0.46778089765710873\n",
      "Epoch 539, Loss Train(MSE): 0.1330038986260114, R2 Train: 0.46798440549595444\n",
      "Epoch 540, Loss Train(MSE): 0.13295370876243787, R2 Train: 0.46818516495024853\n",
      "Epoch 541, Loss Train(MSE): 0.1329041828396723, R2 Train: 0.46838326864131075\n",
      "Epoch 542, Loss Train(MSE): 0.13285529916054412, R2 Train: 0.46857880335782354\n",
      "Epoch 543, Loss Train(MSE): 0.13280703737159147, R2 Train: 0.4687718505136341\n",
      "Epoch 544, Loss Train(MSE): 0.13276507912954638, R2 Train: 0.4689396834818145\n",
      "Epoch 545, Loss Train(MSE): 0.13272648945762236, R2 Train: 0.46909404216951056\n",
      "Epoch 546, Loss Train(MSE): 0.13268753967666114, R2 Train: 0.46924984129335545\n",
      "Epoch 547, Loss Train(MSE): 0.13264784158607082, R2 Train: 0.4694086336557167\n",
      "Epoch 548, Loss Train(MSE): 0.1326121133434921, R2 Train: 0.4695515466260316\n",
      "Epoch 549, Loss Train(MSE): 0.1325709532545267, R2 Train: 0.4697161869818932\n",
      "Epoch 550, Loss Train(MSE): 0.1325386285700353, R2 Train: 0.46984548571985885\n",
      "Epoch 551, Loss Train(MSE): 0.132626386086227, R2 Train: 0.46949445565509196\n",
      "Epoch 552, Loss Train(MSE): 0.1326605537299736, R2 Train: 0.4693577850801056\n",
      "Epoch 553, Loss Train(MSE): 0.1326174510360283, R2 Train: 0.46953019585588684\n",
      "Epoch 554, Loss Train(MSE): 0.13257992078926747, R2 Train: 0.4696803168429301\n",
      "Epoch 555, Loss Train(MSE): 0.1325387112483738, R2 Train: 0.4698451550065048\n",
      "Epoch 556, Loss Train(MSE): 0.13250150976717673, R2 Train: 0.4699939609312931\n",
      "Epoch 557, Loss Train(MSE): 0.13249219591657702, R2 Train: 0.47003121633369194\n",
      "Epoch 558, Loss Train(MSE): 0.1324822650760917, R2 Train: 0.4700709396956332\n",
      "Epoch 559, Loss Train(MSE): 0.13243792907213892, R2 Train: 0.47024828371144434\n",
      "Epoch 560, Loss Train(MSE): 0.1323941067165896, R2 Train: 0.4704235731336416\n",
      "Epoch 561, Loss Train(MSE): 0.13235078454346588, R2 Train: 0.47059686182613647\n",
      "Epoch 562, Loss Train(MSE): 0.1323079498101062, R2 Train: 0.47076820075957515\n",
      "Epoch 563, Loss Train(MSE): 0.13226559044453773, R2 Train: 0.4709376382218491\n",
      "Epoch 564, Loss Train(MSE): 0.1322275774847442, R2 Train: 0.4710896900610232\n",
      "Epoch 565, Loss Train(MSE): 0.13219493883815203, R2 Train: 0.4712202446473919\n",
      "Epoch 566, Loss Train(MSE): 0.13233474236588075, R2 Train: 0.470661030536477\n",
      "Epoch 567, Loss Train(MSE): 0.13231560947605403, R2 Train: 0.4707375620957839\n",
      "Epoch 568, Loss Train(MSE): 0.1322807551876479, R2 Train: 0.4708769792494084\n",
      "Epoch 569, Loss Train(MSE): 0.13224045006454835, R2 Train: 0.4710381997418066\n",
      "Epoch 570, Loss Train(MSE): 0.1322071794858522, R2 Train: 0.4711712820565912\n",
      "Epoch 571, Loss Train(MSE): 0.13216732860577357, R2 Train: 0.4713306855769057\n",
      "Epoch 572, Loss Train(MSE): 0.13213593001910923, R2 Train: 0.4714562799235631\n",
      "Epoch 573, Loss Train(MSE): 0.13209607108119747, R2 Train: 0.4716157156752101\n",
      "Epoch 574, Loss Train(MSE): 0.13206679577035402, R2 Train: 0.47173281691858393\n",
      "Epoch 575, Loss Train(MSE): 0.13202771658078083, R2 Train: 0.4718891336768767\n",
      "Epoch 576, Loss Train(MSE): 0.13199846659901796, R2 Train: 0.4720061336039282\n",
      "Epoch 577, Loss Train(MSE): 0.13196168716806977, R2 Train: 0.4721532513277209\n",
      "Epoch 578, Loss Train(MSE): 0.13193125561887104, R2 Train: 0.47227497752451586\n",
      "Epoch 579, Loss Train(MSE): 0.13193143763392393, R2 Train: 0.4722742494643043\n",
      "Epoch 580, Loss Train(MSE): 0.13192091832181768, R2 Train: 0.4723163267127293\n",
      "Epoch 581, Loss Train(MSE): 0.1319965794611525, R2 Train: 0.47201368215538997\n",
      "Epoch 582, Loss Train(MSE): 0.13202862820933192, R2 Train: 0.4718854871626723\n",
      "Epoch 583, Loss Train(MSE): 0.13198713729091224, R2 Train: 0.472051450836351\n",
      "Epoch 584, Loss Train(MSE): 0.1319461925188225, R2 Train: 0.47221522992471\n",
      "Epoch 585, Loss Train(MSE): 0.1319057757746393, R2 Train: 0.4723768969014428\n",
      "Epoch 586, Loss Train(MSE): 0.13186587002232453, R2 Train: 0.4725365199107019\n",
      "Epoch 587, Loss Train(MSE): 0.13183739663116398, R2 Train: 0.4726504134753441\n",
      "Epoch 588, Loss Train(MSE): 0.13179950876342017, R2 Train: 0.4728019649463193\n",
      "Epoch 589, Loss Train(MSE): 0.13177205787792692, R2 Train: 0.4729117684882923\n",
      "Epoch 590, Loss Train(MSE): 0.13173574914955852, R2 Train: 0.47305700340176593\n",
      "Epoch 591, Loss Train(MSE): 0.1317076043762461, R2 Train: 0.4731695824950156\n",
      "Epoch 592, Loss Train(MSE): 0.13167369054797304, R2 Train: 0.47330523780810785\n",
      "Epoch 593, Loss Train(MSE): 0.1316445607740268, R2 Train: 0.47342175690389277\n",
      "Epoch 594, Loss Train(MSE): 0.131613199412081, R2 Train: 0.473547202351676\n",
      "Epoch 595, Loss Train(MSE): 0.13158283164757983, R2 Train: 0.47366867340968066\n",
      "Epoch 596, Loss Train(MSE): 0.1315541588474995, R2 Train: 0.47378336461000203\n",
      "Epoch 597, Loss Train(MSE): 0.13164388560935897, R2 Train: 0.4734244575625641\n",
      "Epoch 598, Loss Train(MSE): 0.1316782502598743, R2 Train: 0.47328699896050275\n",
      "Epoch 599, Loss Train(MSE): 0.13164077294417542, R2 Train: 0.4734369082232983\n",
      "Epoch 600, Loss Train(MSE): 0.1316248074153148, R2 Train: 0.47350077033874083\n",
      "Epoch 601, Loss Train(MSE): 0.13162795827735152, R2 Train: 0.4734881668905939\n",
      "Epoch 602, Loss Train(MSE): 0.13159070014668064, R2 Train: 0.4736371994132774\n",
      "Epoch 603, Loss Train(MSE): 0.13155388459508463, R2 Train: 0.4737844616196615\n",
      "Epoch 604, Loss Train(MSE): 0.13151749890347902, R2 Train: 0.4739300043860839\n",
      "Epoch 605, Loss Train(MSE): 0.13148153104887392, R2 Train: 0.47407387580450433\n",
      "Epoch 606, Loss Train(MSE): 0.1314459696557718, R2 Train: 0.4742161213769128\n",
      "Epoch 607, Loss Train(MSE): 0.1314108039512897, R2 Train: 0.4743567841948412\n",
      "Epoch 608, Loss Train(MSE): 0.13138576794917176, R2 Train: 0.474456928203313\n",
      "Epoch 609, Loss Train(MSE): 0.13135292294501805, R2 Train: 0.4745883082199278\n",
      "Epoch 610, Loss Train(MSE): 0.1313282177125409, R2 Train: 0.47468712914983635\n",
      "Epoch 611, Loss Train(MSE): 0.13129716690528462, R2 Train: 0.47481133237886153\n",
      "Epoch 612, Loss Train(MSE): 0.13127117646361122, R2 Train: 0.4749152941455551\n",
      "Epoch 613, Loss Train(MSE): 0.13125609442337235, R2 Train: 0.4749756223065106\n",
      "Epoch 614, Loss Train(MSE): 0.13139176493201607, R2 Train: 0.4744329402719357\n",
      "Epoch 615, Loss Train(MSE): 0.1313625483893932, R2 Train: 0.47454980644242717\n",
      "Epoch 616, Loss Train(MSE): 0.13133037588661747, R2 Train: 0.4746784964535301\n",
      "Epoch 617, Loss Train(MSE): 0.13130213846232958, R2 Train: 0.4747914461506817\n",
      "Epoch 618, Loss Train(MSE): 0.1312706107300702, R2 Train: 0.47491755707971917\n",
      "Epoch 619, Loss Train(MSE): 0.13124359440984948, R2 Train: 0.4750256223606021\n",
      "Epoch 620, Loss Train(MSE): 0.13121233629311158, R2 Train: 0.4751506548275537\n",
      "Epoch 621, Loss Train(MSE): 0.13118675443796396, R2 Train: 0.47525298224814416\n",
      "Epoch 622, Loss Train(MSE): 0.131155436489029, R2 Train: 0.475378254043884\n",
      "Epoch 623, Loss Train(MSE): 0.1311314770071938, R2 Train: 0.4754740919712248\n",
      "Epoch 624, Loss Train(MSE): 0.13110018228988676, R2 Train: 0.47559927084045295\n",
      "Epoch 625, Loss Train(MSE): 0.13107731156988003, R2 Train: 0.47569075372047986\n",
      "Epoch 626, Loss Train(MSE): 0.13105637079313961, R2 Train: 0.47577451682744154\n",
      "Epoch 627, Loss Train(MSE): 0.13107385761834703, R2 Train: 0.4757045695266119\n",
      "Epoch 628, Loss Train(MSE): 0.1310423443741128, R2 Train: 0.4758306225035488\n",
      "Epoch 629, Loss Train(MSE): 0.1310111383221225, R2 Train: 0.47595544671151\n",
      "Epoch 630, Loss Train(MSE): 0.13098023333769512, R2 Train: 0.4760790666492195\n",
      "Epoch 631, Loss Train(MSE): 0.13108348082078344, R2 Train: 0.4756660767168662\n",
      "Epoch 632, Loss Train(MSE): 0.13108932783874455, R2 Train: 0.4756426886450218\n",
      "Epoch 633, Loss Train(MSE): 0.1310560602719438, R2 Train: 0.4757757589122248\n",
      "Epoch 634, Loss Train(MSE): 0.1310232107391346, R2 Train: 0.47590715704346165\n",
      "Epoch 635, Loss Train(MSE): 0.13099973407914128, R2 Train: 0.47600106368343487\n",
      "Epoch 636, Loss Train(MSE): 0.13096808451304237, R2 Train: 0.4761276619478305\n",
      "Epoch 637, Loss Train(MSE): 0.1309456583154816, R2 Train: 0.4762173667380736\n",
      "Epoch 638, Loss Train(MSE): 0.13091500116282787, R2 Train: 0.47633999534868854\n",
      "Epoch 639, Loss Train(MSE): 0.13089246607566568, R2 Train: 0.4764301356973373\n",
      "Epoch 640, Loss Train(MSE): 0.13086335811804947, R2 Train: 0.4765465675278021\n",
      "Epoch 641, Loss Train(MSE): 0.13084045091708127, R2 Train: 0.4766381963316749\n",
      "Epoch 642, Loss Train(MSE): 0.13081304292910034, R2 Train: 0.47674782828359863\n",
      "Epoch 643, Loss Train(MSE): 0.1307895318353837, R2 Train: 0.4768418726584652\n",
      "Epoch 644, Loss Train(MSE): 0.13076395644680802, R2 Train: 0.4769441742127679\n",
      "Epoch 645, Loss Train(MSE): 0.1307396374644546, R2 Train: 0.4770414501421816\n",
      "Epoch 646, Loss Train(MSE): 0.13071601104429748, R2 Train: 0.4771359558228101\n",
      "Epoch 647, Loss Train(MSE): 0.13069070477073266, R2 Train: 0.47723718091706935\n",
      "Epoch 648, Loss Train(MSE): 0.13066912909237463, R2 Train: 0.4773234836305015\n",
      "Epoch 649, Loss Train(MSE): 0.13075525657051448, R2 Train: 0.47697897371794207\n",
      "Epoch 650, Loss Train(MSE): 0.13079023773364795, R2 Train: 0.4768390490654082\n",
      "Epoch 651, Loss Train(MSE): 0.1307592454613045, R2 Train: 0.47696301815478204\n",
      "Epoch 652, Loss Train(MSE): 0.13073728657975336, R2 Train: 0.47705085368098654\n",
      "Epoch 653, Loss Train(MSE): 0.13072167634627024, R2 Train: 0.477113294614919\n",
      "Epoch 654, Loss Train(MSE): 0.13073303147452672, R2 Train: 0.4770678741018931\n",
      "Epoch 655, Loss Train(MSE): 0.13070345401925956, R2 Train: 0.47718618392296175\n",
      "Epoch 656, Loss Train(MSE): 0.13067420212515218, R2 Train: 0.47730319149939127\n",
      "Epoch 657, Loss Train(MSE): 0.13064526718832725, R2 Train: 0.477418931246691\n",
      "Epoch 658, Loss Train(MSE): 0.13061664103570472, R2 Train: 0.4775334358571811\n",
      "Epoch 659, Loss Train(MSE): 0.1305883158973834, R2 Train: 0.4776467364104664\n",
      "Epoch 660, Loss Train(MSE): 0.13056028438097259, R2 Train: 0.47775886247610966\n",
      "Epoch 661, Loss Train(MSE): 0.13053253944772836, R2 Train: 0.47786984220908657\n",
      "Epoch 662, Loss Train(MSE): 0.13050657026165344, R2 Train: 0.47797371895338625\n",
      "Epoch 663, Loss Train(MSE): 0.130486067326344, R2 Train: 0.47805573069462404\n",
      "Epoch 664, Loss Train(MSE): 0.13046188825910301, R2 Train: 0.47815244696358794\n",
      "Epoch 665, Loss Train(MSE): 0.1304404779023719, R2 Train: 0.4782380883905124\n",
      "Epoch 666, Loss Train(MSE): 0.13041818352022944, R2 Train: 0.47832726591908226\n",
      "Epoch 667, Loss Train(MSE): 0.1303957206485566, R2 Train: 0.47841711740577364\n",
      "Epoch 668, Loss Train(MSE): 0.13049310786810733, R2 Train: 0.47802756852757067\n",
      "Epoch 669, Loss Train(MSE): 0.13051276844885906, R2 Train: 0.47794892620456375\n",
      "Epoch 670, Loss Train(MSE): 0.1304917875516644, R2 Train: 0.4780328497933424\n",
      "Epoch 671, Loss Train(MSE): 0.1304640656652622, R2 Train: 0.4781437373389512\n",
      "Epoch 672, Loss Train(MSE): 0.1304436663665639, R2 Train: 0.4782253345337444\n",
      "Epoch 673, Loss Train(MSE): 0.13041657498392314, R2 Train: 0.4783337000643074\n",
      "Epoch 674, Loss Train(MSE): 0.13039694607936136, R2 Train: 0.47841221568255454\n",
      "Epoch 675, Loss Train(MSE): 0.13037039731027228, R2 Train: 0.4785184107589109\n",
      "Epoch 676, Loss Train(MSE): 0.13035136207773254, R2 Train: 0.47859455168906984\n",
      "Epoch 677, Loss Train(MSE): 0.1303257447123075, R2 Train: 0.47869702115076995\n",
      "Epoch 678, Loss Train(MSE): 0.1303065320721761, R2 Train: 0.4787738717112956\n",
      "Epoch 679, Loss Train(MSE): 0.1302822179466162, R2 Train: 0.47887112821353517\n",
      "Epoch 680, Loss Train(MSE): 0.1302626278016561, R2 Train: 0.4789494887933756\n",
      "Epoch 681, Loss Train(MSE): 0.13025661111752493, R2 Train: 0.4789735555299003\n",
      "Epoch 682, Loss Train(MSE): 0.13026506797895093, R2 Train: 0.47893972808419627\n",
      "Epoch 683, Loss Train(MSE): 0.13023992046132654, R2 Train: 0.47904031815469383\n",
      "Epoch 684, Loss Train(MSE): 0.13021500514826095, R2 Train: 0.4791399794069562\n",
      "Epoch 685, Loss Train(MSE): 0.13019031752661078, R2 Train: 0.47923872989355687\n",
      "Epoch 686, Loss Train(MSE): 0.13016585326417746, R2 Train: 0.4793365869432902\n",
      "Epoch 687, Loss Train(MSE): 0.13014160819931372, R2 Train: 0.4794335672027451\n",
      "Epoch 688, Loss Train(MSE): 0.13026190052386435, R2 Train: 0.4789523979045426\n",
      "Epoch 689, Loss Train(MSE): 0.13024944002743521, R2 Train: 0.47900223989025914\n",
      "Epoch 690, Loss Train(MSE): 0.1302242713182547, R2 Train: 0.4791029147269812\n",
      "Epoch 691, Loss Train(MSE): 0.13020446436349348, R2 Train: 0.4791821425460261\n",
      "Epoch 692, Loss Train(MSE): 0.1301799261452985, R2 Train: 0.479280295418806\n",
      "Epoch 693, Loss Train(MSE): 0.1301605728278295, R2 Train: 0.479357708688682\n",
      "Epoch 694, Loss Train(MSE): 0.1301368299463528, R2 Train: 0.47945268021458876\n",
      "Epoch 695, Loss Train(MSE): 0.13011768555928624, R2 Train: 0.47952925776285504\n",
      "Epoch 696, Loss Train(MSE): 0.13009488533033547, R2 Train: 0.4796204586786581\n",
      "Epoch 697, Loss Train(MSE): 0.13007573183550244, R2 Train: 0.4796970726579902\n",
      "Epoch 698, Loss Train(MSE): 0.13005400581762971, R2 Train: 0.47978397672948114\n",
      "Epoch 699, Loss Train(MSE): 0.1300346489086585, R2 Train: 0.47986140436536595\n",
      "Epoch 700, Loss Train(MSE): 0.1300141144714845, R2 Train: 0.479943542114062\n",
      "Epoch 701, Loss Train(MSE): 0.12999438099763047, R2 Train: 0.4800224760094781\n",
      "Epoch 702, Loss Train(MSE): 0.12997514271297372, R2 Train: 0.4800994291481051\n",
      "Epoch 703, Loss Train(MSE): 0.12995487841442063, R2 Train: 0.48018048634231747\n",
      "Epoch 704, Loss Train(MSE): 0.12993702929332146, R2 Train: 0.48025188282671416\n",
      "Epoch 705, Loss Train(MSE): 0.12991609680605515, R2 Train: 0.4803356127757794\n",
      "Epoch 706, Loss Train(MSE): 0.12989971940139095, R2 Train: 0.4804011223944362\n",
      "Epoch 707, Loss Train(MSE): 0.12987861806803133, R2 Train: 0.4804855277278747\n",
      "Epoch 708, Loss Train(MSE): 0.1299045627964828, R2 Train: 0.4803817488140688\n",
      "Epoch 709, Loss Train(MSE): 0.1299947523233757, R2 Train: 0.4800209907064972\n",
      "Epoch 710, Loss Train(MSE): 0.1299750679179184, R2 Train: 0.48009972832832637\n",
      "Epoch 711, Loss Train(MSE): 0.1299618882224506, R2 Train: 0.48015244711019756\n",
      "Epoch 712, Loss Train(MSE): 0.1299758895055278, R2 Train: 0.4800964419778888\n",
      "Epoch 713, Loss Train(MSE): 0.12995211127590256, R2 Train: 0.48019155489638976\n",
      "Epoch 714, Loss Train(MSE): 0.12992858710003255, R2 Train: 0.4802856515998698\n",
      "Epoch 715, Loss Train(MSE): 0.12990531032747887, R2 Train: 0.4803787586900845\n",
      "Epoch 716, Loss Train(MSE): 0.1298822746243207, R2 Train: 0.4804709015027172\n",
      "Epoch 717, Loss Train(MSE): 0.12985947395429934, R2 Train: 0.48056210418280265\n",
      "Epoch 718, Loss Train(MSE): 0.1298369025611964, R2 Train: 0.4806523897552144\n",
      "Epoch 719, Loss Train(MSE): 0.12981455495235986, R2 Train: 0.4807417801905606\n",
      "Epoch 720, Loss Train(MSE): 0.1297924258832975, R2 Train: 0.48083029646680997\n",
      "Epoch 721, Loss Train(MSE): 0.12977236607131767, R2 Train: 0.4809105357147293\n",
      "Epoch 722, Loss Train(MSE): 0.12975526554866737, R2 Train: 0.4809789378053305\n",
      "Epoch 723, Loss Train(MSE): 0.1297364134466915, R2 Train: 0.48105434621323395\n",
      "Epoch 724, Loss Train(MSE): 0.12971879042818904, R2 Train: 0.48112483828724384\n",
      "Epoch 725, Loss Train(MSE): 0.12970122874696396, R2 Train: 0.48119508501214414\n",
      "Epoch 726, Loss Train(MSE): 0.12968296214113045, R2 Train: 0.4812681514354782\n",
      "Epoch 727, Loss Train(MSE): 0.12966676451873915, R2 Train: 0.4813329419250434\n",
      "Epoch 728, Loss Train(MSE): 0.1296477462262379, R2 Train: 0.4814090150950484\n",
      "Epoch 729, Loss Train(MSE): 0.12963297806085658, R2 Train: 0.4814680877565737\n",
      "Epoch 730, Loss Train(MSE): 0.12970751376445386, R2 Train: 0.48116994494218457\n",
      "Epoch 731, Loss Train(MSE): 0.12974555989115438, R2 Train: 0.48101776043538247\n",
      "Epoch 732, Loss Train(MSE): 0.12972501161209188, R2 Train: 0.4810999535516325\n",
      "Epoch 733, Loss Train(MSE): 0.1297071188658039, R2 Train: 0.48117152453678436\n",
      "Epoch 734, Loss Train(MSE): 0.12968695690773394, R2 Train: 0.48125217236906426\n",
      "Epoch 735, Loss Train(MSE): 0.12966957715797045, R2 Train: 0.4813216913681182\n",
      "Epoch 736, Loss Train(MSE): 0.12964994358600676, R2 Train: 0.48140022565597296\n",
      "Epoch 737, Loss Train(MSE): 0.12963287116674074, R2 Train: 0.48146851533303703\n",
      "Epoch 738, Loss Train(MSE): 0.12961389389146818, R2 Train: 0.48154442443412726\n",
      "Epoch 739, Loss Train(MSE): 0.12959694416992723, R2 Train: 0.4816122233202911\n",
      "Epoch 740, Loss Train(MSE): 0.12957873831399458, R2 Train: 0.4816850467440217\n",
      "Epoch 741, Loss Train(MSE): 0.1295617454964655, R2 Train: 0.48175301801413795\n",
      "Epoch 742, Loss Train(MSE): 0.12954441461035215, R2 Train: 0.4818223415585914\n",
      "Epoch 743, Loss Train(MSE): 0.1295272298040865, R2 Train: 0.48189108078365395\n",
      "Epoch 744, Loss Train(MSE): 0.12953139915193684, R2 Train: 0.4818744033922526\n",
      "Epoch 745, Loss Train(MSE): 0.12953413868405766, R2 Train: 0.48186344526376934\n",
      "Epoch 746, Loss Train(MSE): 0.1295143580204218, R2 Train: 0.4819425679183128\n",
      "Epoch 747, Loss Train(MSE): 0.1294947482733981, R2 Train: 0.4820210069064076\n",
      "Epoch 748, Loss Train(MSE): 0.12947530627604922, R2 Train: 0.4820987748958031\n",
      "Epoch 749, Loss Train(MSE): 0.1294560289804307, R2 Train: 0.48217588407827716\n",
      "Epoch 750, Loss Train(MSE): 0.1294369134512995, R2 Train: 0.48225234619480195\n",
      "Epoch 751, Loss Train(MSE): 0.12941795686020238, R2 Train: 0.48232817255919047\n",
      "Epoch 752, Loss Train(MSE): 0.12939915647991795, R2 Train: 0.4824033740803282\n",
      "Epoch 753, Loss Train(MSE): 0.12950802779636228, R2 Train: 0.4819678888145509\n",
      "Epoch 754, Loss Train(MSE): 0.12950529223642662, R2 Train: 0.4819788310542935\n",
      "Epoch 755, Loss Train(MSE): 0.12948861260242686, R2 Train: 0.48204554959029255\n",
      "Epoch 756, Loss Train(MSE): 0.1294698022241224, R2 Train: 0.48212079110351036\n",
      "Epoch 757, Loss Train(MSE): 0.12945368402957563, R2 Train: 0.48218526388169747\n",
      "Epoch 758, Loss Train(MSE): 0.12943527854696116, R2 Train: 0.48225888581215537\n",
      "Epoch 759, Loss Train(MSE): 0.12941952953660663, R2 Train: 0.4823218818535735\n",
      "Epoch 760, Loss Train(MSE): 0.1294016501515762, R2 Train: 0.4823933993936952\n",
      "Epoch 761, Loss Train(MSE): 0.1293860971553504, R2 Train: 0.4824556113785984\n",
      "Epoch 762, Loss Train(MSE): 0.12936885333704765, R2 Train: 0.4825245866518094\n",
      "Epoch 763, Loss Train(MSE): 0.1293533403258501, R2 Train: 0.48258663869659957\n",
      "Epoch 764, Loss Train(MSE): 0.12933683090432155, R2 Train: 0.4826526763827138\n",
      "Epoch 765, Loss Train(MSE): 0.1293212172677385, R2 Train: 0.48271513092904605\n",
      "Epoch 766, Loss Train(MSE): 0.12930553141121506, R2 Train: 0.48277787435513975\n",
      "Epoch 767, Loss Train(MSE): 0.12928969042893287, R2 Train: 0.4828412382842685\n",
      "Epoch 768, Loss Train(MSE): 0.1292749085189926, R2 Train: 0.48290036592402963\n",
      "Epoch 769, Loss Train(MSE): 0.12925872600159452, R2 Train: 0.4829650959936219\n",
      "Epoch 770, Loss Train(MSE): 0.1292449204184966, R2 Train: 0.48302031832601355\n",
      "Epoch 771, Loss Train(MSE): 0.12922829349669235, R2 Train: 0.4830868260132306\n",
      "Epoch 772, Loss Train(MSE): 0.129215529325506, R2 Train: 0.483137882697976\n",
      "Epoch 773, Loss Train(MSE): 0.12919887127112364, R2 Train: 0.4832045149155054\n",
      "Epoch 774, Loss Train(MSE): 0.12918621259522806, R2 Train: 0.4832551496190878\n",
      "Epoch 775, Loss Train(MSE): 0.12917039111493836, R2 Train: 0.48331843554024656\n",
      "Epoch 776, Loss Train(MSE): 0.12917843934927103, R2 Train: 0.48328624260291586\n",
      "Epoch 777, Loss Train(MSE): 0.12928033046079726, R2 Train: 0.48287867815681096\n",
      "Epoch 778, Loss Train(MSE): 0.12926388542788309, R2 Train: 0.48294445828846766\n",
      "Epoch 779, Loss Train(MSE): 0.1292596599240751, R2 Train: 0.4829613603036996\n",
      "Epoch 780, Loss Train(MSE): 0.12926914082452198, R2 Train: 0.48292343670191207\n",
      "Epoch 781, Loss Train(MSE): 0.129250534866373, R2 Train: 0.482997860534508\n",
      "Epoch 782, Loss Train(MSE): 0.1292321150066802, R2 Train: 0.48307153997327923\n",
      "Epoch 783, Loss Train(MSE): 0.12921387667099843, R2 Train: 0.4831444933160063\n",
      "Epoch 784, Loss Train(MSE): 0.1291958154852559, R2 Train: 0.48321673805897636\n",
      "Epoch 785, Loss Train(MSE): 0.12917792726484698, R2 Train: 0.4832882909406121\n",
      "Epoch 786, Loss Train(MSE): 0.12916020800437777, R2 Train: 0.4833591679824889\n",
      "Epoch 787, Loss Train(MSE): 0.1291426538680225, R2 Train: 0.48342938452790996\n",
      "Epoch 788, Loss Train(MSE): 0.12912526118045198, R2 Train: 0.4834989552781921\n",
      "Epoch 789, Loss Train(MSE): 0.12910802641829774, R2 Train: 0.483567894326809\n",
      "Epoch 790, Loss Train(MSE): 0.12909156587913545, R2 Train: 0.4836337364834582\n",
      "Epoch 791, Loss Train(MSE): 0.1290790237272762, R2 Train: 0.4836839050908952\n",
      "Epoch 792, Loss Train(MSE): 0.12906341857834008, R2 Train: 0.48374632568663967\n",
      "Epoch 793, Loss Train(MSE): 0.12905052203501216, R2 Train: 0.48379791185995136\n",
      "Epoch 794, Loss Train(MSE): 0.12903583582571418, R2 Train: 0.4838566566971433\n",
      "Epoch 795, Loss Train(MSE): 0.1290224951368545, R2 Train: 0.48391001945258205\n",
      "Epoch 796, Loss Train(MSE): 0.1290087851631043, R2 Train: 0.4839648593475828\n",
      "Epoch 797, Loss Train(MSE): 0.12899491927478463, R2 Train: 0.4840203229008615\n",
      "Epoch 798, Loss Train(MSE): 0.1289822371094665, R2 Train: 0.48407105156213404\n",
      "Epoch 799, Loss Train(MSE): 0.12896777287201927, R2 Train: 0.4841289085119229\n",
      "Epoch 800, Loss Train(MSE): 0.12895616484518577, R2 Train: 0.48417534061925693\n",
      "Epoch 801, Loss Train(MSE): 0.12897178538807957, R2 Train: 0.4841128584476817\n",
      "Epoch 802, Loss Train(MSE): 0.1290627009497285, R2 Train: 0.48374919620108603\n",
      "Epoch 803, Loss Train(MSE): 0.12904640200965947, R2 Train: 0.4838143919613621\n",
      "Epoch 804, Loss Train(MSE): 0.12903267484875494, R2 Train: 0.48386930060498023\n",
      "Epoch 805, Loss Train(MSE): 0.1290165555007306, R2 Train: 0.4839337779970776\n",
      "Epoch 806, Loss Train(MSE): 0.1290033087529772, R2 Train: 0.48398676498809123\n",
      "Epoch 807, Loss Train(MSE): 0.1289874771330305, R2 Train: 0.48405009146787803\n",
      "Epoch 808, Loss Train(MSE): 0.1289745594313862, R2 Train: 0.4841017622744552\n",
      "Epoch 809, Loss Train(MSE): 0.12895911389020268, R2 Train: 0.4841635444391893\n",
      "Epoch 810, Loss Train(MSE): 0.1289463879179984, R2 Train: 0.4842144483280064\n",
      "Epoch 811, Loss Train(MSE): 0.12893141789517995, R2 Train: 0.4842743284192802\n",
      "Epoch 812, Loss Train(MSE): 0.12891875904350583, R2 Train: 0.4843249638259767\n",
      "Epoch 813, Loss Train(MSE): 0.12890434585230745, R2 Train: 0.4843826165907702\n",
      "Epoch 814, Loss Train(MSE): 0.12889164102151057, R2 Train: 0.4844334359139577\n",
      "Epoch 815, Loss Train(MSE): 0.12887785855458894, R2 Train: 0.48448856578164423\n",
      "Epoch 816, Loss Train(MSE): 0.12886500508258866, R2 Train: 0.48453997966964535\n",
      "Epoch 817, Loss Train(MSE): 0.12886274559187116, R2 Train: 0.48454901763251534\n",
      "Epoch 818, Loss Train(MSE): 0.1288751917424504, R2 Train: 0.4844992330301984\n",
      "Epoch 819, Loss Train(MSE): 0.12885989697554098, R2 Train: 0.48456041209783607\n",
      "Epoch 820, Loss Train(MSE): 0.1288447237688945, R2 Train: 0.484621104924422\n",
      "Epoch 821, Loss Train(MSE): 0.12882967003853169, R2 Train: 0.48468131984587326\n",
      "Epoch 822, Loss Train(MSE): 0.12881473377205113, R2 Train: 0.48474106491179547\n",
      "Epoch 823, Loss Train(MSE): 0.12879991302519359, R2 Train: 0.48480034789922566\n",
      "Epoch 824, Loss Train(MSE): 0.1287852059185942, R2 Train: 0.48485917632562325\n",
      "Epoch 825, Loss Train(MSE): 0.1287706106347117, R2 Train: 0.48491755746115317\n",
      "Epoch 826, Loss Train(MSE): 0.12875612541492457, R2 Train: 0.48497549834030174\n",
      "Epoch 827, Loss Train(MSE): 0.12874649809451394, R2 Train: 0.48501400762194424\n",
      "Epoch 828, Loss Train(MSE): 0.1288553397365566, R2 Train: 0.4845786410537736\n",
      "Epoch 829, Loss Train(MSE): 0.1288427695183468, R2 Train: 0.4846289219266128\n",
      "Epoch 830, Loss Train(MSE): 0.12882774826194032, R2 Train: 0.4846890069522387\n",
      "Epoch 831, Loss Train(MSE): 0.12881527841753448, R2 Train: 0.48473888632986206\n",
      "Epoch 832, Loss Train(MSE): 0.12880074876745393, R2 Train: 0.4847970049301843\n",
      "Epoch 833, Loss Train(MSE): 0.1287884786701506, R2 Train: 0.48484608531939755\n",
      "Epoch 834, Loss Train(MSE): 0.1287743035408223, R2 Train: 0.48490278583671076\n",
      "Epoch 835, Loss Train(MSE): 0.1287623238919554, R2 Train: 0.48495070443217836\n",
      "Epoch 836, Loss Train(MSE): 0.12874837847539058, R2 Train: 0.48500648609843766\n",
      "Epoch 837, Loss Train(MSE): 0.1287367720573078, R2 Train: 0.4850529117707688\n",
      "Epoch 838, Loss Train(MSE): 0.12872294268629225, R2 Train: 0.485108229254831\n",
      "Epoch 839, Loss Train(MSE): 0.12871178504032174, R2 Train: 0.48515285983871304\n",
      "Epoch 840, Loss Train(MSE): 0.1286979681700332, R2 Train: 0.4852081273198672\n",
      "Epoch 841, Loss Train(MSE): 0.12868732820802684, R2 Train: 0.48525068716789266\n",
      "Epoch 842, Loss Train(MSE): 0.12867342950226976, R2 Train: 0.48530628199092096\n",
      "Epoch 843, Loss Train(MSE): 0.12866337005920642, R2 Train: 0.48534651976317433\n",
      "Epoch 844, Loss Train(MSE): 0.1286497345726008, R2 Train: 0.48540106170959685\n",
      "Epoch 845, Loss Train(MSE): 0.128639465946976, R2 Train: 0.485442136212096\n",
      "Epoch 846, Loss Train(MSE): 0.12862653642820793, R2 Train: 0.4854938542871683\n",
      "Epoch 847, Loss Train(MSE): 0.12861592217038478, R2 Train: 0.48553631131846087\n",
      "Epoch 848, Loss Train(MSE): 0.12860376506519347, R2 Train: 0.4855849397392261\n",
      "Epoch 849, Loss Train(MSE): 0.12859273980722946, R2 Train: 0.48562904077108215\n",
      "Epoch 850, Loss Train(MSE): 0.12858139805883184, R2 Train: 0.48567440776467263\n",
      "Epoch 851, Loss Train(MSE): 0.1285699023590798, R2 Train: 0.48572039056368077\n",
      "Epoch 852, Loss Train(MSE): 0.1285594148843189, R2 Train: 0.4857623404627244\n",
      "Epoch 853, Loss Train(MSE): 0.1285473947246957, R2 Train: 0.4858104211012172\n",
      "Epoch 854, Loss Train(MSE): 0.12853779672960472, R2 Train: 0.4858488130815811\n",
      "Epoch 855, Loss Train(MSE): 0.12859364444869537, R2 Train: 0.48562542220521854\n",
      "Epoch 856, Loss Train(MSE): 0.12863916616009582, R2 Train: 0.4854433353596167\n",
      "Epoch 857, Loss Train(MSE): 0.1286255902644674, R2 Train: 0.4854976389421304\n",
      "Epoch 858, Loss Train(MSE): 0.12861405137360699, R2 Train: 0.48554379450557206\n",
      "Epoch 859, Loss Train(MSE): 0.12861334086036316, R2 Train: 0.48554663655854735\n",
      "Epoch 860, Loss Train(MSE): 0.12862285789676603, R2 Train: 0.4855085684129359\n",
      "Epoch 861, Loss Train(MSE): 0.12860868693897642, R2 Train: 0.4855652522440943\n",
      "Epoch 862, Loss Train(MSE): 0.12859464401252937, R2 Train: 0.4856214239498825\n",
      "Epoch 863, Loss Train(MSE): 0.12858072629528916, R2 Train: 0.48567709481884336\n",
      "Epoch 864, Loss Train(MSE): 0.12856693107584558, R2 Train: 0.4857322756966177\n",
      "Epoch 865, Loss Train(MSE): 0.1285532557480922, R2 Train: 0.4857869770076312\n",
      "Epoch 866, Loss Train(MSE): 0.12853969780609806, R2 Train: 0.48584120877560777\n",
      "Epoch 867, Loss Train(MSE): 0.1285262548392547, R2 Train: 0.4858949806429812\n",
      "Epoch 868, Loss Train(MSE): 0.12851292452768356, R2 Train: 0.48594830188926574\n",
      "Epoch 869, Loss Train(MSE): 0.12849970463788765, R2 Train: 0.4860011814484494\n",
      "Epoch 870, Loss Train(MSE): 0.12848659301863521, R2 Train: 0.48605362792545914\n",
      "Epoch 871, Loss Train(MSE): 0.1284735875970603, R2 Train: 0.48610564961175884\n",
      "Epoch 872, Loss Train(MSE): 0.1284618633715845, R2 Train: 0.486152546513662\n",
      "Epoch 873, Loss Train(MSE): 0.12845166512275003, R2 Train: 0.4861933395089999\n",
      "Epoch 874, Loss Train(MSE): 0.12844057706343898, R2 Train: 0.4862376917462441\n",
      "Epoch 875, Loss Train(MSE): 0.12843008143271925, R2 Train: 0.486279674269123\n",
      "Epoch 876, Loss Train(MSE): 0.12841967271848012, R2 Train: 0.4863213091260795\n",
      "Epoch 877, Loss Train(MSE): 0.1284088208356011, R2 Train: 0.4863647166575956\n",
      "Epoch 878, Loss Train(MSE): 0.12839913081382093, R2 Train: 0.4864034767447163\n",
      "Epoch 879, Loss Train(MSE): 0.12838786894759519, R2 Train: 0.48644852420961926\n",
      "Epoch 880, Loss Train(MSE): 0.12837893342847168, R2 Train: 0.4864842662861133\n",
      "Epoch 881, Loss Train(MSE): 0.12836748006855372, R2 Train: 0.4865300797257851\n",
      "Epoch 882, Loss Train(MSE): 0.12835880662563928, R2 Train: 0.48656477349744287\n",
      "Epoch 883, Loss Train(MSE): 0.12834781473692886, R2 Train: 0.4866087410522846\n",
      "Epoch 884, Loss Train(MSE): 0.12839454355392152, R2 Train: 0.4864218257843139\n",
      "Epoch 885, Loss Train(MSE): 0.12844715491319217, R2 Train: 0.4862113803472313\n",
      "Epoch 886, Loss Train(MSE): 0.1284356133891331, R2 Train: 0.48625754644346764\n",
      "Epoch 887, Loss Train(MSE): 0.12842393179690728, R2 Train: 0.4863042728123709\n",
      "Epoch 888, Loss Train(MSE): 0.128412862842074, R2 Train: 0.48634854863170396\n",
      "Epoch 889, Loss Train(MSE): 0.12840127236559148, R2 Train: 0.48639491053763406\n",
      "Epoch 890, Loss Train(MSE): 0.12839056425549125, R2 Train: 0.486437742978035\n",
      "Epoch 891, Loss Train(MSE): 0.12837914070696171, R2 Train: 0.48648343717215314\n",
      "Epoch 892, Loss Train(MSE): 0.12836869106048554, R2 Train: 0.48652523575805784\n",
      "Epoch 893, Loss Train(MSE): 0.12835750407654944, R2 Train: 0.4865699836938022\n",
      "Epoch 894, Loss Train(MSE): 0.12834721904205215, R2 Train: 0.4866111238317914\n",
      "Epoch 895, Loss Train(MSE): 0.12833633258473715, R2 Train: 0.4866546696610514\n",
      "Epoch 896, Loss Train(MSE): 0.12832612610610997, R2 Train: 0.48669549557556013\n",
      "Epoch 897, Loss Train(MSE): 0.12831559891716482, R2 Train: 0.4867376043313407\n",
      "Epoch 898, Loss Train(MSE): 0.12830539207108443, R2 Train: 0.48677843171566226\n",
      "Epoch 899, Loss Train(MSE): 0.12829527808467583, R2 Train: 0.48681888766129666\n",
      "Epoch 900, Loss Train(MSE): 0.1282849984812865, R2 Train: 0.486860006074854\n",
      "Epoch 901, Loss Train(MSE): 0.12827534719944528, R2 Train: 0.4868986112022189\n",
      "Epoch 902, Loss Train(MSE): 0.12826492843966336, R2 Train: 0.48694028624134655\n",
      "Epoch 903, Loss Train(MSE): 0.12826988997209915, R2 Train: 0.4869204401116034\n",
      "Epoch 904, Loss Train(MSE): 0.12827735925554914, R2 Train: 0.4868905629778034\n",
      "Epoch 905, Loss Train(MSE): 0.1282657999266303, R2 Train: 0.48693680029347886\n",
      "Epoch 906, Loss Train(MSE): 0.1282543233512384, R2 Train: 0.4869827065950464\n",
      "Epoch 907, Loss Train(MSE): 0.1282429282600617, R2 Train: 0.4870282869597532\n",
      "Epoch 908, Loss Train(MSE): 0.12823161342255907, R2 Train: 0.48707354630976374\n",
      "Epoch 909, Loss Train(MSE): 0.12822037764529787, R2 Train: 0.48711848941880853\n",
      "Epoch 910, Loss Train(MSE): 0.12820921977037392, R2 Train: 0.48716312091850433\n",
      "Epoch 911, Loss Train(MSE): 0.12819813867390806, R2 Train: 0.48720744530436777\n",
      "Epoch 912, Loss Train(MSE): 0.1281871332646164, R2 Train: 0.48725146694153443\n",
      "Epoch 913, Loss Train(MSE): 0.12817620248244965, R2 Train: 0.4872951900702014\n",
      "Epoch 914, Loss Train(MSE): 0.1281653452972983, R2 Train: 0.48733861881080676\n",
      "Epoch 915, Loss Train(MSE): 0.12824984397516742, R2 Train: 0.4870006240993303\n",
      "Epoch 916, Loss Train(MSE): 0.12825857166495436, R2 Train: 0.48696571334018257\n",
      "Epoch 917, Loss Train(MSE): 0.12824870292119853, R2 Train: 0.4870051883152059\n",
      "Epoch 918, Loss Train(MSE): 0.12823745133740805, R2 Train: 0.4870501946503678\n",
      "Epoch 919, Loss Train(MSE): 0.1282279898515885, R2 Train: 0.48708804059364597\n",
      "Epoch 920, Loss Train(MSE): 0.12821682443792376, R2 Train: 0.48713270224830496\n",
      "Epoch 921, Loss Train(MSE): 0.1282076734383125, R2 Train: 0.48716930624675003\n",
      "Epoch 922, Loss Train(MSE): 0.1281966607162759, R2 Train: 0.48721335713489644\n",
      "Epoch 923, Loss Train(MSE): 0.12818773124204735, R2 Train: 0.4872490750318106\n",
      "Epoch 924, Loss Train(MSE): 0.1281769324943054, R2 Train: 0.4872922700227784\n",
      "Epoch 925, Loss Train(MSE): 0.12816814273922872, R2 Train: 0.4873274290430851\n",
      "Epoch 926, Loss Train(MSE): 0.1281576144206775, R2 Train: 0.48736954231729\n",
      "Epoch 927, Loss Train(MSE): 0.12814888913907604, R2 Train: 0.4874044434436958\n",
      "Epoch 928, Loss Train(MSE): 0.1281386832508996, R2 Train: 0.48744526699640156\n",
      "Epoch 929, Loss Train(MSE): 0.12812995321924608, R2 Train: 0.48748018712301566\n",
      "Epoch 930, Loss Train(MSE): 0.1281201176497961, R2 Train: 0.48751952940081555\n",
      "Epoch 931, Loss Train(MSE): 0.12811131917809276, R2 Train: 0.48755472328762894\n",
      "Epoch 932, Loss Train(MSE): 0.12810189801397204, R2 Train: 0.48759240794411185\n",
      "Epoch 933, Loss Train(MSE): 0.128092972501747, R2 Train: 0.487628109993012\n",
      "Epoch 934, Loss Train(MSE): 0.12808400631209033, R2 Train: 0.4876639747516387\n",
      "Epoch 935, Loss Train(MSE): 0.12807489984443507, R2 Train: 0.4877004006222597\n",
      "Epoch 936, Loss Train(MSE): 0.12806642594104561, R2 Train: 0.48773429623581754\n",
      "Epoch 937, Loss Train(MSE): 0.12805708892063714, R2 Train: 0.48777164431745146\n",
      "Epoch 938, Loss Train(MSE): 0.128049141596336, R2 Train: 0.48780343361465595\n",
      "Epoch 939, Loss Train(MSE): 0.12803952840784358, R2 Train: 0.4878418863686257\n",
      "Epoch 940, Loss Train(MSE): 0.1280321391551335, R2 Train: 0.487871443379466\n",
      "Epoch 941, Loss Train(MSE): 0.12802247782975137, R2 Train: 0.4879100886809945\n",
      "Epoch 942, Loss Train(MSE): 0.12801514398442426, R2 Train: 0.48793942406230295\n",
      "Epoch 943, Loss Train(MSE): 0.1280059115434781, R2 Train: 0.4879763538260876\n",
      "Epoch 944, Loss Train(MSE): 0.12799816496280228, R2 Train: 0.4880073401487909\n",
      "Epoch 945, Loss Train(MSE): 0.12798959394176238, R2 Train: 0.4880416242329505\n",
      "Epoch 946, Loss Train(MSE): 0.12798140207415934, R2 Train: 0.48807439170336264\n",
      "Epoch 947, Loss Train(MSE): 0.12804109938625854, R2 Train: 0.4878356024549658\n",
      "Epoch 948, Loss Train(MSE): 0.12807448256566273, R2 Train: 0.4877020697373491\n",
      "Epoch 949, Loss Train(MSE): 0.12806593283682477, R2 Train: 0.4877362686527009\n",
      "Epoch 950, Loss Train(MSE): 0.1280553911321463, R2 Train: 0.48777843547141475\n",
      "Epoch 951, Loss Train(MSE): 0.12805570292528126, R2 Train: 0.48777718829887495\n",
      "Epoch 952, Loss Train(MSE): 0.1280660374586976, R2 Train: 0.4877358501652096\n",
      "Epoch 953, Loss Train(MSE): 0.1280552611763186, R2 Train: 0.4877789552947256\n",
      "Epoch 954, Loss Train(MSE): 0.12804457475613598, R2 Train: 0.4878217009754561\n",
      "Epoch 955, Loss Train(MSE): 0.12803397636414032, R2 Train: 0.4878640945434387\n",
      "Epoch 956, Loss Train(MSE): 0.12802346423163627, R2 Train: 0.4879061430734549\n",
      "Epoch 957, Loss Train(MSE): 0.12801303665236122, R2 Train: 0.48794785339055513\n",
      "Epoch 958, Loss Train(MSE): 0.12800269197974437, R2 Train: 0.4879892320810225\n",
      "Epoch 959, Loss Train(MSE): 0.12799242862429833, R2 Train: 0.4880302855028067\n",
      "Epoch 960, Loss Train(MSE): 0.12798224505113756, R2 Train: 0.48807101979544976\n",
      "Epoch 961, Loss Train(MSE): 0.1279721397776153, R2 Train: 0.48811144088953884\n",
      "Epoch 962, Loss Train(MSE): 0.1279621113710746, R2 Train: 0.4881515545157016\n",
      "Epoch 963, Loss Train(MSE): 0.12795215844670627, R2 Train: 0.48819136621317494\n",
      "Epoch 964, Loss Train(MSE): 0.12794227966550878, R2 Train: 0.4882308813379649\n",
      "Epoch 965, Loss Train(MSE): 0.12793247373234526, R2 Train: 0.48827010507061896\n",
      "Epoch 966, Loss Train(MSE): 0.1279251915433708, R2 Train: 0.48829923382651685\n",
      "Epoch 967, Loss Train(MSE): 0.12791590519237706, R2 Train: 0.4883363792304918\n",
      "Epoch 968, Loss Train(MSE): 0.12790903711222917, R2 Train: 0.4883638515510833\n",
      "Epoch 969, Loss Train(MSE): 0.12789983407465094, R2 Train: 0.48840066370139623\n",
      "Epoch 970, Loss Train(MSE): 0.12789289669158085, R2 Train: 0.4884284132336766\n",
      "Epoch 971, Loss Train(MSE): 0.12788411312608755, R2 Train: 0.4884635474956498\n",
      "Epoch 972, Loss Train(MSE): 0.12787689919523756, R2 Train: 0.48849240321904974\n",
      "Epoch 973, Loss Train(MSE): 0.12786863886564448, R2 Train: 0.4885254445374221\n",
      "Epoch 974, Loss Train(MSE): 0.12786111249239485, R2 Train: 0.4885555500304206\n",
      "Epoch 975, Loss Train(MSE): 0.1278534003258104, R2 Train: 0.48858639869675835\n",
      "Epoch 976, Loss Train(MSE): 0.12784552843505195, R2 Train: 0.4886178862597922\n",
      "Epoch 977, Loss Train(MSE): 0.12783838733264746, R2 Train: 0.48864645066941015\n",
      "Epoch 978, Loss Train(MSE): 0.1278301394634463, R2 Train: 0.48867944214621484\n",
      "Epoch 979, Loss Train(MSE): 0.1278235904385066, R2 Train: 0.4887056382459736\n",
      "Epoch 980, Loss Train(MSE): 0.12781520129378046, R2 Train: 0.48873919482487815\n",
      "Epoch 981, Loss Train(MSE): 0.1278685394650407, R2 Train: 0.4885258421398372\n",
      "Epoch 982, Loss Train(MSE): 0.12790608100583156, R2 Train: 0.48837567597667375\n",
      "Epoch 983, Loss Train(MSE): 0.12789784848052674, R2 Train: 0.48840860607789305\n",
      "Epoch 984, Loss Train(MSE): 0.1278884942167267, R2 Train: 0.48844602313309315\n",
      "Epoch 985, Loss Train(MSE): 0.12788063993315626, R2 Train: 0.48847744026737494\n",
      "Epoch 986, Loss Train(MSE): 0.12787129857649204, R2 Train: 0.48851480569403183\n",
      "Epoch 987, Loss Train(MSE): 0.12786374586019167, R2 Train: 0.4885450165592333\n",
      "Epoch 988, Loss Train(MSE): 0.12785447156195312, R2 Train: 0.4885821137521875\n",
      "Epoch 989, Loss Train(MSE): 0.1278471494745935, R2 Train: 0.488611402101626\n",
      "Epoch 990, Loss Train(MSE): 0.1278379924341957, R2 Train: 0.48864803026321724\n",
      "Epoch 991, Loss Train(MSE): 0.12783083532385286, R2 Train: 0.48867665870458854\n",
      "Epoch 992, Loss Train(MSE): 0.12782184207998426, R2 Train: 0.48871263168006296\n",
      "Epoch 993, Loss Train(MSE): 0.12781478917133407, R2 Train: 0.4887408433146637\n",
      "Epoch 994, Loss Train(MSE): 0.12780600286844415, R2 Train: 0.4887759885262234\n",
      "Epoch 995, Loss Train(MSE): 0.12779899788889423, R2 Train: 0.48880400844442307\n",
      "Epoch 996, Loss Train(MSE): 0.12779045852141901, R2 Train: 0.48883816591432394\n",
      "Epoch 997, Loss Train(MSE): 0.12778344935963273, R2 Train: 0.48886620256146907\n",
      "Epoch 998, Loss Train(MSE): 0.1277751939960946, R2 Train: 0.4888992240156216\n",
      "Epoch 999, Loss Train(MSE): 0.12776813238974952, R2 Train: 0.48892747044100193\n",
      "Epoch 1000, Loss Train(MSE): 0.1277601953786371, R2 Train: 0.48895921848545165\n",
      "Epoch 1001, Loss Train(MSE): 0.12775303662860182, R2 Train: 0.4889878534855927\n",
      "Epoch 1002, Loss Train(MSE): 0.12774544978773356, R2 Train: 0.4890182008490658\n",
      "Epoch 1003, Loss Train(MSE): 0.12773815249614753, R2 Train: 0.48904739001540987\n",
      "Epoch 1004, Loss Train(MSE): 0.1277351357333269, R2 Train: 0.4890594570666924\n",
      "Epoch 1005, Loss Train(MSE): 0.1277517536188401, R2 Train: 0.48899298552463955\n",
      "Epoch 1006, Loss Train(MSE): 0.12774315335283878, R2 Train: 0.48902738658864486\n",
      "Epoch 1007, Loss Train(MSE): 0.1277346080353004, R2 Train: 0.4890615678587984\n",
      "Epoch 1008, Loss Train(MSE): 0.1277261169192674, R2 Train: 0.48909553232293035\n",
      "Epoch 1009, Loss Train(MSE): 0.1277176792778913, R2 Train: 0.4891292828884348\n",
      "Epoch 1010, Loss Train(MSE): 0.12770929440366932, R2 Train: 0.4891628223853227\n",
      "Epoch 1011, Loss Train(MSE): 0.12770096160771427, R2 Train: 0.4891961535691429\n",
      "Epoch 1012, Loss Train(MSE): 0.12769268021905636, R2 Train: 0.48922927912377456\n",
      "Epoch 1013, Loss Train(MSE): 0.1276844495839751, R2 Train: 0.4892622016640996\n",
      "Epoch 1014, Loss Train(MSE): 0.12767626906536023, R2 Train: 0.4892949237385591\n",
      "Epoch 1015, Loss Train(MSE): 0.1276681380420998, R2 Train: 0.48932744783160076\n",
      "Epoch 1016, Loss Train(MSE): 0.1276600559084952, R2 Train: 0.48935977636601924\n",
      "Epoch 1017, Loss Train(MSE): 0.1277133339828033, R2 Train: 0.4891466640687868\n",
      "Epoch 1018, Loss Train(MSE): 0.12774478839742182, R2 Train: 0.48902084641031274\n",
      "Epoch 1019, Loss Train(MSE): 0.1277356063778928, R2 Train: 0.4890575744884288\n",
      "Epoch 1020, Loss Train(MSE): 0.12772891647610993, R2 Train: 0.4890843340955603\n",
      "Epoch 1021, Loss Train(MSE): 0.12771997045318953, R2 Train: 0.4891201181872419\n",
      "Epoch 1022, Loss Train(MSE): 0.12771325740341546, R2 Train: 0.48914697038633814\n",
      "Epoch 1023, Loss Train(MSE): 0.1277046114047861, R2 Train: 0.48918155438085564\n",
      "Epoch 1024, Loss Train(MSE): 0.12769792394469864, R2 Train: 0.4892083042212054\n",
      "Epoch 1025, Loss Train(MSE): 0.12768951495271152, R2 Train: 0.4892419401891539\n",
      "Epoch 1026, Loss Train(MSE): 0.12768289840760774, R2 Train: 0.48926840636956903\n",
      "Epoch 1027, Loss Train(MSE): 0.12767466791096577, R2 Train: 0.48930132835613693\n",
      "Epoch 1028, Loss Train(MSE): 0.12766816443649084, R2 Train: 0.4893273422540366\n",
      "Epoch 1029, Loss Train(MSE): 0.12766005809374137, R2 Train: 0.4893597676250345\n",
      "Epoch 1030, Loss Train(MSE): 0.1276537068987479, R2 Train: 0.48938517240500845\n",
      "Epoch 1031, Loss Train(MSE): 0.12764567423024703, R2 Train: 0.4894173030790119\n",
      "Epoch 1032, Loss Train(MSE): 0.12763951178165364, R2 Train: 0.48944195287338543\n",
      "Epoch 1033, Loss Train(MSE): 0.12763150588728764, R2 Train: 0.48947397645084945\n",
      "Epoch 1034, Loss Train(MSE): 0.12762556609860665, R2 Train: 0.4894977356055734\n",
      "Epoch 1035, Loss Train(MSE): 0.1276175944955129, R2 Train: 0.48952962201794836\n",
      "Epoch 1036, Loss Train(MSE): 0.12761181436210517, R2 Train: 0.4895527425515793\n",
      "Epoch 1037, Loss Train(MSE): 0.1276040330747549, R2 Train: 0.4895838677009804\n",
      "Epoch 1038, Loss Train(MSE): 0.12759814630758146, R2 Train: 0.4896074147696742\n",
      "Epoch 1039, Loss Train(MSE): 0.12759069062253675, R2 Train: 0.489637237509853\n",
      "Epoch 1040, Loss Train(MSE): 0.1275846619987033, R2 Train: 0.4896613520051868\n",
      "Epoch 1041, Loss Train(MSE): 0.127577557257875, R2 Train: 0.48968977096849997\n",
      "Epoch 1042, Loss Train(MSE): 0.1275713540667666, R2 Train: 0.48971458373293364\n",
      "Epoch 1043, Loss Train(MSE): 0.12756462378736444, R2 Train: 0.48974150485054224\n",
      "Epoch 1044, Loss Train(MSE): 0.12755821565660552, R2 Train: 0.4897671373735779\n",
      "Epoch 1045, Loss Train(MSE): 0.1275518816499524, R2 Train: 0.4897924734001904\n",
      "Epoch 1046, Loss Train(MSE): 0.12754524038488993, R2 Train: 0.4898190384604403\n",
      "Epoch 1047, Loss Train(MSE): 0.1275393228665158, R2 Train: 0.48984270853393685\n",
      "Epoch 1048, Loss Train(MSE): 0.12753242230204062, R2 Train: 0.4898703107918375\n",
      "Epoch 1049, Loss Train(MSE): 0.1275269399937916, R2 Train: 0.48989224002483356\n",
      "Epoch 1050, Loss Train(MSE): 0.12751990396788726, R2 Train: 0.48992038412845096\n",
      "Epoch 1051, Loss Train(MSE): 0.1275145831800027, R2 Train: 0.4899416672799892\n",
      "Epoch 1052, Loss Train(MSE): 0.12750779558666972, R2 Train: 0.4899688176533211\n",
      "Epoch 1053, Loss Train(MSE): 0.127502133950254, R2 Train: 0.489991464198984\n",
      "Epoch 1054, Loss Train(MSE): 0.1274958452552209, R2 Train: 0.4900166189791164\n",
      "Epoch 1055, Loss Train(MSE): 0.12755782422830553, R2 Train: 0.4897687030867779\n",
      "Epoch 1056, Loss Train(MSE): 0.12758105319309526, R2 Train: 0.48967578722761895\n",
      "Epoch 1057, Loss Train(MSE): 0.12757345102322148, R2 Train: 0.48970619590711406\n",
      "Epoch 1058, Loss Train(MSE): 0.1275665550328892, R2 Train: 0.48973377986844324\n",
      "Epoch 1059, Loss Train(MSE): 0.12755929680606223, R2 Train: 0.48976281277575107\n",
      "Epoch 1060, Loss Train(MSE): 0.12755236101289574, R2 Train: 0.48979055594841703\n",
      "Epoch 1061, Loss Train(MSE): 0.12754538708295926, R2 Train: 0.48981845166816296\n",
      "Epoch 1062, Loss Train(MSE): 0.12754315384324214, R2 Train: 0.48982738462703146\n",
      "Epoch 1063, Loss Train(MSE): 0.12755742097067532, R2 Train: 0.4897703161172987\n",
      "Epoch 1064, Loss Train(MSE): 0.12754952651215767, R2 Train: 0.4898018939513693\n",
      "Epoch 1065, Loss Train(MSE): 0.12754168963995635, R2 Train: 0.4898332414401746\n",
      "Epoch 1066, Loss Train(MSE): 0.12753390933685657, R2 Train: 0.4898643626525737\n",
      "Epoch 1067, Loss Train(MSE): 0.12752618461726276, R2 Train: 0.489895261530949\n",
      "Epoch 1068, Loss Train(MSE): 0.12751851452596985, R2 Train: 0.4899259418961206\n",
      "Epoch 1069, Loss Train(MSE): 0.12751089813698666, R2 Train: 0.4899564074520534\n",
      "Epoch 1070, Loss Train(MSE): 0.12750333455241109, R2 Train: 0.48998666179035566\n",
      "Epoch 1071, Loss Train(MSE): 0.12749582290135203, R2 Train: 0.4900167083945919\n",
      "Epoch 1072, Loss Train(MSE): 0.12748836233889838, R2 Train: 0.4900465506444065\n",
      "Epoch 1073, Loss Train(MSE): 0.12748095204513124, R2 Train: 0.49007619181947504\n",
      "Epoch 1074, Loss Train(MSE): 0.1274735912241779, R2 Train: 0.4901056351032884\n",
      "Epoch 1075, Loss Train(MSE): 0.12746627910330627, R2 Train: 0.4901348835867749\n",
      "Epoch 1076, Loss Train(MSE): 0.12745901493205683, R2 Train: 0.4901639402717727\n",
      "Epoch 1077, Loss Train(MSE): 0.12745179798141115, R2 Train: 0.4901928080743554\n",
      "Epoch 1078, Loss Train(MSE): 0.12744462754299526, R2 Train: 0.49022148982801894\n",
      "Epoch 1079, Loss Train(MSE): 0.12743783722829446, R2 Train: 0.49024865108682214\n",
      "Epoch 1080, Loss Train(MSE): 0.12743250042572035, R2 Train: 0.4902699982971186\n",
      "Epoch 1081, Loss Train(MSE): 0.1274260397160897, R2 Train: 0.4902958411356412\n",
      "Epoch 1082, Loss Train(MSE): 0.1274205237789347, R2 Train: 0.4903179048842612\n",
      "Epoch 1083, Loss Train(MSE): 0.1274144123525744, R2 Train: 0.49034235058970244\n",
      "Epoch 1084, Loss Train(MSE): 0.1274086920816276, R2 Train: 0.49036523167348955\n",
      "Epoch 1085, Loss Train(MSE): 0.12740294823629417, R2 Train: 0.49038820705482333\n",
      "Epoch 1086, Loss Train(MSE): 0.12739700017518532, R2 Train: 0.4904119992992587\n",
      "Epoch 1087, Loss Train(MSE): 0.12739164091366234, R2 Train: 0.49043343634535064\n",
      "Epoch 1088, Loss Train(MSE): 0.12738544323535406, R2 Train: 0.4904582270585838\n",
      "Epoch 1089, Loss Train(MSE): 0.12738048434498428, R2 Train: 0.4904780626200629\n",
      "Epoch 1090, Loss Train(MSE): 0.1273741452860408, R2 Train: 0.4905034188558368\n",
      "Epoch 1091, Loss Train(MSE): 0.1273693487908704, R2 Train: 0.49052260483651844\n",
      "Epoch 1092, Loss Train(MSE): 0.1273632245359664, R2 Train: 0.49054710185613437\n",
      "Epoch 1093, Loss Train(MSE): 0.1273581095132883, R2 Train: 0.4905675619468468\n",
      "Epoch 1094, Loss Train(MSE): 0.12735243995997053, R2 Train: 0.4905902401601179\n",
      "Epoch 1095, Loss Train(MSE): 0.12739154330487304, R2 Train: 0.49043382678050784\n",
      "Epoch 1096, Loss Train(MSE): 0.1274346852901718, R2 Train: 0.4902612588393128\n",
      "Epoch 1097, Loss Train(MSE): 0.12742777638107147, R2 Train: 0.4902888944757141\n",
      "Epoch 1098, Loss Train(MSE): 0.1274215366212801, R2 Train: 0.49031385351487955\n",
      "Epoch 1099, Loss Train(MSE): 0.127414940598075, R2 Train: 0.49034023760769996\n",
      "Epoch 1100, Loss Train(MSE): 0.12740865434998233, R2 Train: 0.4903653826000707\n",
      "Epoch 1101, Loss Train(MSE): 0.1274023191986189, R2 Train: 0.49039072320552435\n",
      "Epoch 1102, Loss Train(MSE): 0.12739602470443603, R2 Train: 0.49041590118225586\n",
      "Epoch 1103, Loss Train(MSE): 0.12738990185893434, R2 Train: 0.49044039256426264\n",
      "Epoch 1104, Loss Train(MSE): 0.12738363488567103, R2 Train: 0.49046546045731587\n",
      "Epoch 1105, Loss Train(MSE): 0.1273776789867326, R2 Train: 0.4904892840530696\n",
      "Epoch 1106, Loss Train(MSE): 0.1273714729903524, R2 Train: 0.4905141080385904\n",
      "Epoch 1107, Loss Train(MSE): 0.1273656416631772, R2 Train: 0.49053743334729116\n",
      "Epoch 1108, Loss Train(MSE): 0.12735952794020677, R2 Train: 0.4905618882391729\n",
      "Epoch 1109, Loss Train(MSE): 0.12735378158979466, R2 Train: 0.4905848736408214\n",
      "Epoch 1110, Loss Train(MSE): 0.1273477894174887, R2 Train: 0.4906088423300452\n",
      "Epoch 1111, Loss Train(MSE): 0.12734209103987265, R2 Train: 0.4906316358405094\n",
      "Epoch 1112, Loss Train(MSE): 0.12733624780592512, R2 Train: 0.49065500877629953\n",
      "Epoch 1113, Loss Train(MSE): 0.1273305628139371, R2 Train: 0.49067774874425163\n",
      "Epoch 1114, Loss Train(MSE): 0.12732489413663645, R2 Train: 0.4907004234534542\n",
      "Epoch 1115, Loss Train(MSE): 0.12731919019894303, R2 Train: 0.4907232392042279\n",
      "Epoch 1116, Loss Train(MSE): 0.12731372003857924, R2 Train: 0.49074511984568303\n",
      "Epoch 1117, Loss Train(MSE): 0.12730796693084662, R2 Train: 0.49076813227661353\n",
      "Epoch 1118, Loss Train(MSE): 0.12730271769310225, R2 Train: 0.490789129227591\n",
      "Epoch 1119, Loss Train(MSE): 0.1272968871602591, R2 Train: 0.49081245135896356\n",
      "Epoch 1120, Loss Train(MSE): 0.12729187979224849, R2 Train: 0.49083248083100606\n",
      "Epoch 1121, Loss Train(MSE): 0.12728594542091243, R2 Train: 0.4908562183163503\n",
      "Epoch 1122, Loss Train(MSE): 0.12728119950046943, R2 Train: 0.4908752019981223\n",
      "Epoch 1123, Loss Train(MSE): 0.12727513660069065, R2 Train: 0.4908994535972374\n",
      "Epoch 1124, Loss Train(MSE): 0.12727317157686713, R2 Train: 0.4909073136925315\n",
      "Epoch 1125, Loss Train(MSE): 0.1272891161771532, R2 Train: 0.49084353529138725\n",
      "Epoch 1126, Loss Train(MSE): 0.12728284789620087, R2 Train: 0.49086860841519653\n",
      "Epoch 1127, Loss Train(MSE): 0.12727661473939433, R2 Train: 0.49089354104242267\n",
      "Epoch 1128, Loss Train(MSE): 0.12727041629443325, R2 Train: 0.490918334822267\n",
      "Epoch 1129, Loss Train(MSE): 0.12726425215857776, R2 Train: 0.490942991365689\n",
      "Epoch 1130, Loss Train(MSE): 0.1272581219383321, R2 Train: 0.49096751224667157\n",
      "Epoch 1131, Loss Train(MSE): 0.12725202524914025, R2 Train: 0.490991899003439\n",
      "Epoch 1132, Loss Train(MSE): 0.1272459617150928, R2 Train: 0.49101615313962876\n",
      "Epoch 1133, Loss Train(MSE): 0.1272399309686458, R2 Train: 0.4910402761254168\n",
      "Epoch 1134, Loss Train(MSE): 0.12723393265034955, R2 Train: 0.4910642693986018\n",
      "Epoch 1135, Loss Train(MSE): 0.12722796640858805, R2 Train: 0.4910881343656478\n",
      "Epoch 1136, Loss Train(MSE): 0.12722203189932826, R2 Train: 0.49111187240268694\n",
      "Epoch 1137, Loss Train(MSE): 0.1272161287858787, R2 Train: 0.49113548485648517\n",
      "Epoch 1138, Loss Train(MSE): 0.12728914886531378, R2 Train: 0.49084340453874487\n",
      "Epoch 1139, Loss Train(MSE): 0.1272926989137799, R2 Train: 0.4908292043448804\n",
      "Epoch 1140, Loss Train(MSE): 0.1272859211453063, R2 Train: 0.49085631541877484\n",
      "Epoch 1141, Loss Train(MSE): 0.12727919594637147, R2 Train: 0.49088321621451414\n",
      "Epoch 1142, Loss Train(MSE): 0.12727379261205576, R2 Train: 0.49090482955177694\n",
      "Epoch 1143, Loss Train(MSE): 0.12726772107718148, R2 Train: 0.49092911569127407\n",
      "Epoch 1144, Loss Train(MSE): 0.1272622886883143, R2 Train: 0.4909508452467428\n",
      "Epoch 1145, Loss Train(MSE): 0.12725642709467064, R2 Train: 0.49097429162131745\n",
      "Epoch 1146, Loss Train(MSE): 0.127250997814955, R2 Train: 0.49099600874018\n",
      "Epoch 1147, Loss Train(MSE): 0.1272453057514981, R2 Train: 0.49101877699400764\n",
      "Epoch 1148, Loss Train(MSE): 0.12723990974825583, R2 Train: 0.4910403610069767\n",
      "Epoch 1149, Loss Train(MSE): 0.1272343493582761, R2 Train: 0.4910626025668956\n",
      "Epoch 1150, Loss Train(MSE): 0.12722901492904343, R2 Train: 0.4910839402838263\n",
      "Epoch 1151, Loss Train(MSE): 0.12722355074119646, R2 Train: 0.4911057970352142\n",
      "Epoch 1152, Loss Train(MSE): 0.127218304431073, R2 Train: 0.49112678227570805\n",
      "Epoch 1153, Loss Train(MSE): 0.12721290320311202, R2 Train: 0.49114838718755194\n",
      "Epoch 1154, Loss Train(MSE): 0.12720776991363758, R2 Train: 0.4911689203454497\n",
      "Epoch 1155, Loss Train(MSE): 0.12720240048776937, R2 Train: 0.4911903980489225\n",
      "Epoch 1156, Loss Train(MSE): 0.12719740357803233, R2 Train: 0.4912103856878707\n",
      "Epoch 1157, Loss Train(MSE): 0.12719203674691962, R2 Train: 0.49123185301232153\n",
      "Epoch 1158, Loss Train(MSE): 0.127187198127533, R2 Train: 0.49125120748986795\n",
      "Epoch 1159, Loss Train(MSE): 0.1271818065100583, R2 Train: 0.49127277395976676\n",
      "Epoch 1160, Loss Train(MSE): 0.12717714673058278, R2 Train: 0.4912914130776689\n",
      "Epoch 1161, Loss Train(MSE): 0.12717170465657018, R2 Train: 0.49131318137371927\n",
      "Epoch 1162, Loss Train(MSE): 0.1271672429869085, R2 Train: 0.49133102805236595\n",
      "Epoch 1163, Loss Train(MSE): 0.1271617263900743, R2 Train: 0.49135309443970276\n",
      "Epoch 1164, Loss Train(MSE): 0.12715748089631573, R2 Train: 0.49137007641473707\n",
      "Epoch 1165, Loss Train(MSE): 0.12715190543003352, R2 Train: 0.4913923782798659\n",
      "Epoch 1166, Loss Train(MSE): 0.12714782097322533, R2 Train: 0.4914087161070987\n",
      "Epoch 1167, Loss Train(MSE): 0.12714236413947044, R2 Train: 0.49143054344211823\n",
      "Epoch 1168, Loss Train(MSE): 0.12713813343472088, R2 Train: 0.49144746626111646\n",
      "Epoch 1169, Loss Train(MSE): 0.12713295009694728, R2 Train: 0.4914681996122109\n",
      "Epoch 1170, Loss Train(MSE): 0.12712855448379332, R2 Train: 0.49148578206482674\n",
      "Epoch 1171, Loss Train(MSE): 0.12712365854968968, R2 Train: 0.49150536580124127\n",
      "Epoch 1172, Loss Train(MSE): 0.1271190805509362, R2 Train: 0.49152367779625517\n",
      "Epoch 1173, Loss Train(MSE): 0.12711448502808087, R2 Train: 0.4915420598876765\n",
      "Epoch 1174, Loss Train(MSE): 0.1271097082790116, R2 Train: 0.49156116688395357\n",
      "Epoch 1175, Loss Train(MSE): 0.12710542532591526, R2 Train: 0.49157829869633896\n",
      "Epoch 1176, Loss Train(MSE): 0.12710043450824446, R2 Train: 0.49159826196702217\n",
      "Epoch 1177, Loss Train(MSE): 0.12709647548214875, R2 Train: 0.491614098071405\n",
      "Epoch 1178, Loss Train(MSE): 0.1270913887779191, R2 Train: 0.49163444488832364\n",
      "Epoch 1179, Loss Train(MSE): 0.12708750249999748, R2 Train: 0.4916499900000101\n",
      "Epoch 1180, Loss Train(MSE): 0.12708261153225872, R2 Train: 0.4916695538709651\n",
      "Epoch 1181, Loss Train(MSE): 0.12707846186842567, R2 Train: 0.4916861525262973\n",
      "Epoch 1182, Loss Train(MSE): 0.1270739344788096, R2 Train: 0.4917042620847616\n",
      "Epoch 1183, Loss Train(MSE): 0.12713858505046377, R2 Train: 0.4914456597981449\n",
      "Epoch 1184, Loss Train(MSE): 0.12715025463599625, R2 Train: 0.491398981456015\n",
      "Epoch 1185, Loss Train(MSE): 0.12714460351925816, R2 Train: 0.49142158592296736\n",
      "Epoch 1186, Loss Train(MSE): 0.12713955619385225, R2 Train: 0.491441775224591\n",
      "Epoch 1187, Loss Train(MSE): 0.12713417165668162, R2 Train: 0.49146331337327354\n",
      "Epoch 1188, Loss Train(MSE): 0.12712906012552205, R2 Train: 0.4914837594979118\n",
      "Epoch 1189, Loss Train(MSE): 0.12712390287777742, R2 Train: 0.49150438848889033\n",
      "Epoch 1190, Loss Train(MSE): 0.12711875675600925, R2 Train: 0.491524972975963\n",
      "Epoch 1191, Loss Train(MSE): 0.1271137898980701, R2 Train: 0.4915448404077196\n",
      "Epoch 1192, Loss Train(MSE): 0.12710863703971775, R2 Train: 0.491565451841129\n",
      "Epoch 1193, Loss Train(MSE): 0.1271038259081869, R2 Train: 0.49158469636725244\n",
      "Epoch 1194, Loss Train(MSE): 0.12709869251437908, R2 Train: 0.4916052299424837\n",
      "Epoch 1195, Loss Train(MSE): 0.12709400453913364, R2 Train: 0.49162398184346545\n",
      "Epoch 1196, Loss Train(MSE): 0.1271021051990841, R2 Train: 0.4915915792036636\n",
      "Epoch 1197, Loss Train(MSE): 0.12710666368934542, R2 Train: 0.4915733452426183\n",
      "Epoch 1198, Loss Train(MSE): 0.12710104932193106, R2 Train: 0.49159580271227576\n",
      "Epoch 1199, Loss Train(MSE): 0.1270954696243103, R2 Train: 0.49161812150275885\n",
      "Epoch 1200, Loss Train(MSE): 0.12708992408832953, R2 Train: 0.4916403036466819\n",
      "Epoch 1201, Loss Train(MSE): 0.1270844122192336, R2 Train: 0.49166235112306556\n",
      "Epoch 1202, Loss Train(MSE): 0.1270789335352157, R2 Train: 0.49168426585913716\n",
      "Epoch 1203, Loss Train(MSE): 0.12707348756698325, R2 Train: 0.491706049732067\n",
      "Epoch 1204, Loss Train(MSE): 0.12706807385734092, R2 Train: 0.4917277045706363\n",
      "Epoch 1205, Loss Train(MSE): 0.1270626919607886, R2 Train: 0.49174923215684563\n",
      "Epoch 1206, Loss Train(MSE): 0.1270573414431346, R2 Train: 0.4917706342274616\n",
      "Epoch 1207, Loss Train(MSE): 0.12705202188112305, R2 Train: 0.4917919124755078\n",
      "Epoch 1208, Loss Train(MSE): 0.12704673286207527, R2 Train: 0.49181306855169893\n",
      "Epoch 1209, Loss Train(MSE): 0.1270414739835439, R2 Train: 0.4918341040658244\n",
      "Epoch 1210, Loss Train(MSE): 0.1270362448529805, R2 Train: 0.49185502058807795\n",
      "Epoch 1211, Loss Train(MSE): 0.12703104508741436, R2 Train: 0.49187581965034255\n",
      "Epoch 1212, Loss Train(MSE): 0.127025874313144, R2 Train: 0.491896502747424\n",
      "Epoch 1213, Loss Train(MSE): 0.12702073216543905, R2 Train: 0.4919170713382438\n",
      "Epoch 1214, Loss Train(MSE): 0.1270156182882536, R2 Train: 0.49193752684698555\n",
      "Epoch 1215, Loss Train(MSE): 0.12701053233394952, R2 Train: 0.49195787066420194\n",
      "Epoch 1216, Loss Train(MSE): 0.12700555984962916, R2 Train: 0.49197776060148335\n",
      "Epoch 1217, Loss Train(MSE): 0.12700192538381008, R2 Train: 0.49199229846475967\n",
      "Epoch 1218, Loss Train(MSE): 0.12699721789373017, R2 Train: 0.4920111284250793\n",
      "Epoch 1219, Loss Train(MSE): 0.12699340839295398, R2 Train: 0.4920263664281841\n",
      "Epoch 1220, Loss Train(MSE): 0.1269889768667744, R2 Train: 0.4920440925329024\n",
      "Epoch 1221, Loss Train(MSE): 0.12698497863092598, R2 Train: 0.4920600854762961\n",
      "Epoch 1222, Loss Train(MSE): 0.12698083334768462, R2 Train: 0.4920766666092615\n",
      "Epoch 1223, Loss Train(MSE): 0.1269766335204038, R2 Train: 0.49209346591838476\n",
      "Epoch 1224, Loss Train(MSE): 0.12697278410580962, R2 Train: 0.49210886357676153\n",
      "Epoch 1225, Loss Train(MSE): 0.12696837062689628, R2 Train: 0.49212651749241487\n",
      "Epoch 1226, Loss Train(MSE): 0.1269648260883739, R2 Train: 0.4921406956465044\n",
      "Epoch 1227, Loss Train(MSE): 0.12696030586382545, R2 Train: 0.4921587765446982\n",
      "Epoch 1228, Loss Train(MSE): 0.12695684087092174, R2 Train: 0.49217263651631304\n",
      "Epoch 1229, Loss Train(MSE): 0.12695249157209781, R2 Train: 0.49219003371160874\n",
      "Epoch 1230, Loss Train(MSE): 0.12694877318534503, R2 Train: 0.4922049072586199\n",
      "Epoch 1231, Loss Train(MSE): 0.12701374404696647, R2 Train: 0.4919450238121341\n",
      "Epoch 1232, Loss Train(MSE): 0.12702273719851362, R2 Train: 0.4919090512059455\n",
      "Epoch 1233, Loss Train(MSE): 0.12701614075478293, R2 Train: 0.4919354369808683\n",
      "Epoch 1234, Loss Train(MSE): 0.12701115995074497, R2 Train: 0.49195536019702013\n",
      "Epoch 1235, Loss Train(MSE): 0.12700666635853186, R2 Train: 0.49197333456587256\n",
      "Epoch 1236, Loss Train(MSE): 0.12700188710664365, R2 Train: 0.4919924515734254\n",
      "Epoch 1237, Loss Train(MSE): 0.1269973588757919, R2 Train: 0.49201056449683245\n",
      "Epoch 1238, Loss Train(MSE): 0.12699274939479946, R2 Train: 0.49202900242080216\n",
      "Epoch 1239, Loss Train(MSE): 0.12698821079301642, R2 Train: 0.4920471568279343\n",
      "Epoch 1240, Loss Train(MSE): 0.1269837411494262, R2 Train: 0.4920650354022952\n",
      "Epoch 1241, Loss Train(MSE): 0.12697921506165533, R2 Train: 0.4920831397533787\n",
      "Epoch 1242, Loss Train(MSE): 0.1269748570563736, R2 Train: 0.4921005717745056\n",
      "Epoch 1243, Loss Train(MSE): 0.12697036506568735, R2 Train: 0.4921185397372506\n",
      "Epoch 1244, Loss Train(MSE): 0.12696609212858576, R2 Train: 0.492135631485657\n",
      "Epoch 1245, Loss Train(MSE): 0.12696165459162181, R2 Train: 0.49215338163351274\n",
      "Epoch 1246, Loss Train(MSE): 0.126957441683402, R2 Train: 0.492170233266392\n",
      "Epoch 1247, Loss Train(MSE): 0.12695307780076684, R2 Train: 0.49218768879693264\n",
      "Epoch 1248, Loss Train(MSE): 0.12694890132155134, R2 Train: 0.4922043947137946\n",
      "Epoch 1249, Loss Train(MSE): 0.12694462920357538, R2 Train: 0.49222148318569847\n",
      "Epoch 1250, Loss Train(MSE): 0.12694046690770452, R2 Train: 0.4922381323691819\n",
      "Epoch 1251, Loss Train(MSE): 0.1269363036359029, R2 Train: 0.49225478545638846\n",
      "Epoch 1252, Loss Train(MSE): 0.1269321345524597, R2 Train: 0.49227146179016124\n",
      "Epoch 1253, Loss Train(MSE): 0.1269280962370202, R2 Train: 0.4922876150519192\n",
      "Epoch 1254, Loss Train(MSE): 0.1269239005956484, R2 Train: 0.4923043976174064\n",
      "Epoch 1255, Loss Train(MSE): 0.12692000242924228, R2 Train: 0.49231999028303086\n",
      "Epoch 1256, Loss Train(MSE): 0.1269157615908575, R2 Train: 0.49233695363657004\n",
      "Epoch 1257, Loss Train(MSE): 0.12691201789904363, R2 Train: 0.49235192840382547\n",
      "Epoch 1258, Loss Train(MSE): 0.12690771429107342, R2 Train: 0.4923691428357063\n",
      "Epoch 1259, Loss Train(MSE): 0.12690413857954347, R2 Train: 0.4923834456818261\n",
      "Epoch 1260, Loss Train(MSE): 0.1268997556353607, R2 Train: 0.49240097745855715\n",
      "Epoch 1261, Loss Train(MSE): 0.1268963606342537, R2 Train: 0.49241455746298524\n",
      "Epoch 1262, Loss Train(MSE): 0.1268919176354572, R2 Train: 0.49243232945817117\n",
      "Epoch 1263, Loss Train(MSE): 0.1268886486683854, R2 Train: 0.4924454053264584\n",
      "Epoch 1264, Loss Train(MSE): 0.1268842985238188, R2 Train: 0.4924628059047248\n",
      "Epoch 1265, Loss Train(MSE): 0.1268808998836434, R2 Train: 0.4924764004654264\n",
      "Epoch 1266, Loss Train(MSE): 0.12687677147111678, R2 Train: 0.49249291411553286\n",
      "Epoch 1267, Loss Train(MSE): 0.12687323003625944, R2 Train: 0.4925070798549622\n",
      "Epoch 1268, Loss Train(MSE): 0.1268693333706492, R2 Train: 0.4925226665174032\n",
      "Epoch 1269, Loss Train(MSE): 0.12686563678272825, R2 Train: 0.492537452869087\n",
      "Epoch 1270, Loss Train(MSE): 0.12686365055479798, R2 Train: 0.4925453977808081\n",
      "Epoch 1271, Loss Train(MSE): 0.1268794888217184, R2 Train: 0.4924820447131264\n",
      "Epoch 1272, Loss Train(MSE): 0.12687505722757098, R2 Train: 0.4924997710897161\n",
      "Epoch 1273, Loss Train(MSE): 0.12687064663753372, R2 Train: 0.4925174134498651\n",
      "Epoch 1274, Loss Train(MSE): 0.12686625685285252, R2 Train: 0.4925349725885899\n",
      "Epoch 1275, Loss Train(MSE): 0.12686188767849033, R2 Train: 0.4925524492860387\n",
      "Epoch 1276, Loss Train(MSE): 0.12685753892302365, R2 Train: 0.4925698443079054\n",
      "Epoch 1277, Loss Train(MSE): 0.12685321039854272, R2 Train: 0.4925871584058291\n",
      "Epoch 1278, Loss Train(MSE): 0.1268489019205546, R2 Train: 0.49260439231778164\n",
      "Epoch 1279, Loss Train(MSE): 0.1268446133078896, R2 Train: 0.4926215467684416\n",
      "Epoch 1280, Loss Train(MSE): 0.12684034438261096, R2 Train: 0.49263862246955614\n",
      "Epoch 1281, Loss Train(MSE): 0.126872073872651, R2 Train: 0.49251170450939596\n",
      "Epoch 1282, Loss Train(MSE): 0.12692217529612104, R2 Train: 0.49231129881551583\n",
      "Epoch 1283, Loss Train(MSE): 0.12691314085411867, R2 Train: 0.4923474365835253\n",
      "Epoch 1284, Loss Train(MSE): 0.12690420789561055, R2 Train: 0.4923831684175578\n",
      "Epoch 1285, Loss Train(MSE): 0.1268953744608787, R2 Train: 0.4924185021564852\n",
      "Epoch 1286, Loss Train(MSE): 0.1268866386431734, R2 Train: 0.4924534454273064\n",
      "Epoch 1287, Loss Train(MSE): 0.12688005299940217, R2 Train: 0.49247978800239134\n",
      "Epoch 1288, Loss Train(MSE): 0.12687536779668182, R2 Train: 0.4924985288132727\n",
      "Epoch 1289, Loss Train(MSE): 0.1268707111942441, R2 Train: 0.49251715522302364\n",
      "Epoch 1290, Loss Train(MSE): 0.12686608276802572, R2 Train: 0.49253566892789713\n",
      "Epoch 1291, Loss Train(MSE): 0.12686148210476503, R2 Train: 0.4925540715809399\n",
      "Epoch 1292, Loss Train(MSE): 0.1268579646926891, R2 Train: 0.49256814122924364\n",
      "Epoch 1293, Loss Train(MSE): 0.12685365168013715, R2 Train: 0.4925853932794514\n",
      "Epoch 1294, Loss Train(MSE): 0.12685021460512325, R2 Train: 0.492599141579507\n",
      "Epoch 1295, Loss Train(MSE): 0.1268459150924959, R2 Train: 0.4926163396300164\n",
      "Epoch 1296, Loss Train(MSE): 0.12684257359316092, R2 Train: 0.4926297056273563\n",
      "Epoch 1297, Loss Train(MSE): 0.12683826901528325, R2 Train: 0.492646923938867\n",
      "Epoch 1298, Loss Train(MSE): 0.12683503749399333, R2 Train: 0.4926598500240267\n",
      "Epoch 1299, Loss Train(MSE): 0.1268307196631615, R2 Train: 0.492677121347354\n",
      "Epoch 1300, Loss Train(MSE): 0.12682759628794008, R2 Train: 0.4926896148482397\n",
      "Epoch 1301, Loss Train(MSE): 0.1268233474158184, R2 Train: 0.49270661033672636\n",
      "Epoch 1302, Loss Train(MSE): 0.12682016423310646, R2 Train: 0.49271934306757414\n",
      "Epoch 1303, Loss Train(MSE): 0.12681607002273826, R2 Train: 0.49273571990904697\n",
      "Epoch 1304, Loss Train(MSE): 0.12681281215898366, R2 Train: 0.4927487513640654\n",
      "Epoch 1305, Loss Train(MSE): 0.12680888411227903, R2 Train: 0.4927644635508839\n",
      "Epoch 1306, Loss Train(MSE): 0.12680553752040863, R2 Train: 0.4927778499183655\n",
      "Epoch 1307, Loss Train(MSE): 0.1268017864964323, R2 Train: 0.49279285401427075\n",
      "Epoch 1308, Loss Train(MSE): 0.12679833791099682, R2 Train: 0.49280664835601273\n",
      "Epoch 1309, Loss Train(MSE): 0.1267947741592329, R2 Train: 0.4928209033630684\n",
      "Epoch 1310, Loss Train(MSE): 0.12679121105429605, R2 Train: 0.4928351557828158\n",
      "Epoch 1311, Loss Train(MSE): 0.12678784424596418, R2 Train: 0.4928486230161433\n",
      "Epoch 1312, Loss Train(MSE): 0.1267841547955476, R2 Train: 0.49286338081780956\n",
      "Epoch 1313, Loss Train(MSE): 0.12678099405310028, R2 Train: 0.49287602378759887\n",
      "Epoch 1314, Loss Train(MSE): 0.12677716709400794, R2 Train: 0.49289133162396825\n",
      "Epoch 1315, Loss Train(MSE): 0.12677422101893163, R2 Train: 0.4929031159242735\n",
      "Epoch 1316, Loss Train(MSE): 0.1267703625031514, R2 Train: 0.49291854998739437\n",
      "Epoch 1317, Loss Train(MSE): 0.12676740856520458, R2 Train: 0.4929303657391817\n",
      "Epoch 1318, Loss Train(MSE): 0.12676371107276646, R2 Train: 0.49294515570893416\n",
      "Epoch 1319, Loss Train(MSE): 0.1267605839870443, R2 Train: 0.49295766405182284\n",
      "Epoch 1320, Loss Train(MSE): 0.12675713050214124, R2 Train: 0.49297147799143504\n",
      "Epoch 1321, Loss Train(MSE): 0.1267538214201473, R2 Train: 0.4929847143194108\n",
      "Epoch 1322, Loss Train(MSE): 0.1267506186850044, R2 Train: 0.4929975252599824\n",
      "Epoch 1323, Loss Train(MSE): 0.1267471192692207, R2 Train: 0.49301152292311723\n",
      "Epoch 1324, Loss Train(MSE): 0.12674417362051046, R2 Train: 0.49302330551795814\n",
      "Epoch 1325, Loss Train(MSE): 0.1267405137199667, R2 Train: 0.4930379451201332\n",
      "Epoch 1326, Loss Train(MSE): 0.12673775765414952, R2 Train: 0.4930489693834019\n",
      "Epoch 1327, Loss Train(MSE): 0.12673417422243738, R2 Train: 0.4930633031102505\n",
      "Epoch 1328, Loss Train(MSE): 0.12673119960540616, R2 Train: 0.49307520157837537\n",
      "Epoch 1329, Loss Train(MSE): 0.1267278965106316, R2 Train: 0.4930884139574736\n",
      "Epoch 1330, Loss Train(MSE): 0.1267246968188415, R2 Train: 0.49310121272463403\n",
      "Epoch 1331, Loss Train(MSE): 0.12672167892540337, R2 Train: 0.4931132842983865\n",
      "Epoch 1332, Loss Train(MSE): 0.1267182480313375, R2 Train: 0.49312700787465\n",
      "Epoch 1333, Loss Train(MSE): 0.1267694988590663, R2 Train: 0.4929220045637348\n",
      "Epoch 1334, Loss Train(MSE): 0.12680356734078194, R2 Train: 0.49278573063687225\n",
      "Epoch 1335, Loss Train(MSE): 0.12679651767050398, R2 Train: 0.49281392931798407\n",
      "Epoch 1336, Loss Train(MSE): 0.12678867865176424, R2 Train: 0.49284528539294303\n",
      "Epoch 1337, Loss Train(MSE): 0.126781835556243, R2 Train: 0.492872657775028\n",
      "Epoch 1338, Loss Train(MSE): 0.12677413336512494, R2 Train: 0.49290346653950023\n",
      "Epoch 1339, Loss Train(MSE): 0.12676746261555022, R2 Train: 0.4929301495377991\n",
      "Epoch 1340, Loss Train(MSE): 0.12675991747221507, R2 Train: 0.4929603301111397\n",
      "Epoch 1341, Loss Train(MSE): 0.12675429644455546, R2 Train: 0.49298281422177814\n",
      "Epoch 1342, Loss Train(MSE): 0.12675032825525548, R2 Train: 0.4929986869789781\n",
      "Epoch 1343, Loss Train(MSE): 0.1267472599495017, R2 Train: 0.4930109602019932\n",
      "Epoch 1344, Loss Train(MSE): 0.12674338231609594, R2 Train: 0.49302647073561623\n",
      "Epoch 1345, Loss Train(MSE): 0.12674030294939403, R2 Train: 0.49303878820242386\n",
      "Epoch 1346, Loss Train(MSE): 0.12673652843459798, R2 Train: 0.49305388626160807\n",
      "Epoch 1347, Loss Train(MSE): 0.12673342278677763, R2 Train: 0.4930663088528895\n",
      "Epoch 1348, Loss Train(MSE): 0.12672976328684685, R2 Train: 0.4930809468526126\n",
      "Epoch 1349, Loss Train(MSE): 0.12672661694849438, R2 Train: 0.4930935322060225\n",
      "Epoch 1350, Loss Train(MSE): 0.12672308372740487, R2 Train: 0.4931076650903805\n",
      "Epoch 1351, Loss Train(MSE): 0.12671988305666448, R2 Train: 0.49312046777334206\n",
      "Epoch 1352, Loss Train(MSE): 0.12671648677829175, R2 Train: 0.493134052886833\n",
      "Epoch 1353, Loss Train(MSE): 0.12671321886027337, R2 Train: 0.4931471245589065\n",
      "Epoch 1354, Loss Train(MSE): 0.1267188216524362, R2 Train: 0.4931247133902552\n",
      "Epoch 1355, Loss Train(MSE): 0.12672637145667448, R2 Train: 0.4930945141733021\n",
      "Epoch 1356, Loss Train(MSE): 0.12672250082377742, R2 Train: 0.4931099967048903\n",
      "Epoch 1357, Loss Train(MSE): 0.12671864855765202, R2 Train: 0.4931254057693919\n",
      "Epoch 1358, Loss Train(MSE): 0.1267148144705311, R2 Train: 0.4931407421178756\n",
      "Epoch 1359, Loss Train(MSE): 0.12671099837836686, R2 Train: 0.49315600648653257\n",
      "Epoch 1360, Loss Train(MSE): 0.12670720010072836, R2 Train: 0.49317119959708655\n",
      "Epoch 1361, Loss Train(MSE): 0.12670341946070207, R2 Train: 0.4931863221571917\n",
      "Epoch 1362, Loss Train(MSE): 0.12669965628479574, R2 Train: 0.49320137486081705\n",
      "Epoch 1363, Loss Train(MSE): 0.12669591040284486, R2 Train: 0.49321635838862055\n",
      "Epoch 1364, Loss Train(MSE): 0.12669218164792276, R2 Train: 0.49323127340830897\n",
      "Epoch 1365, Loss Train(MSE): 0.12668846985625282, R2 Train: 0.49324612057498873\n",
      "Epoch 1366, Loss Train(MSE): 0.12668477486712404, R2 Train: 0.49326090053150384\n",
      "Epoch 1367, Loss Train(MSE): 0.12668109652280893, R2 Train: 0.4932756139087643\n",
      "Epoch 1368, Loss Train(MSE): 0.1266774346684838, R2 Train: 0.4932902613260648\n",
      "Epoch 1369, Loss Train(MSE): 0.1266737891521522, R2 Train: 0.49330484339139125\n",
      "Epoch 1370, Loss Train(MSE): 0.1266701598245696, R2 Train: 0.49331936070172155\n",
      "Epoch 1371, Loss Train(MSE): 0.12666654653917195, R2 Train: 0.4933338138433122\n",
      "Epoch 1372, Loss Train(MSE): 0.12666294915200477, R2 Train: 0.49334820339198093\n",
      "Epoch 1373, Loss Train(MSE): 0.12665936752165566, R2 Train: 0.49336252991337737\n",
      "Epoch 1374, Loss Train(MSE): 0.12665580150918826, R2 Train: 0.49337679396324696\n",
      "Epoch 1375, Loss Train(MSE): 0.1266522509780787, R2 Train: 0.4933909960876852\n",
      "Epoch 1376, Loss Train(MSE): 0.1266487157941532, R2 Train: 0.49340513682338716\n",
      "Epoch 1377, Loss Train(MSE): 0.12664529663229604, R2 Train: 0.49341881347081584\n",
      "Epoch 1378, Loss Train(MSE): 0.12664274388486296, R2 Train: 0.49342902446054815\n",
      "Epoch 1379, Loss Train(MSE): 0.12663959159729216, R2 Train: 0.4934416336108314\n",
      "Epoch 1380, Loss Train(MSE): 0.12663681980270905, R2 Train: 0.4934527207891638\n",
      "Epoch 1381, Loss Train(MSE): 0.1266339383229427, R2 Train: 0.4934642467082292\n",
      "Epoch 1382, Loss Train(MSE): 0.12663094252126866, R2 Train: 0.49347622991492535\n",
      "Epoch 1383, Loss Train(MSE): 0.12662833552706015, R2 Train: 0.4934866578917594\n",
      "Epoch 1384, Loss Train(MSE): 0.12662516558164713, R2 Train: 0.4934993376734115\n",
      "Epoch 1385, Loss Train(MSE): 0.12662272884544787, R2 Train: 0.4935090846182085\n",
      "Epoch 1386, Loss Train(MSE): 0.1266196429408263, R2 Train: 0.49352142823669476\n",
      "Epoch 1387, Loss Train(MSE): 0.12661980985261873, R2 Train: 0.4935207605895251\n",
      "Epoch 1388, Loss Train(MSE): 0.12670569540008503, R2 Train: 0.4931772183996599\n",
      "Epoch 1389, Loss Train(MSE): 0.12669888617431918, R2 Train: 0.4932044553027233\n",
      "Epoch 1390, Loss Train(MSE): 0.12669232470581063, R2 Train: 0.4932307011767575\n",
      "Epoch 1391, Loss Train(MSE): 0.12668566419303062, R2 Train: 0.4932573432278775\n",
      "Epoch 1392, Loss Train(MSE): 0.12667924632809197, R2 Train: 0.49328301468763214\n",
      "Epoch 1393, Loss Train(MSE): 0.12667270653102755, R2 Train: 0.4933091738758898\n",
      "Epoch 1394, Loss Train(MSE): 0.12666644892104847, R2 Train: 0.4933342043158061\n",
      "Epoch 1395, Loss Train(MSE): 0.12666000365217672, R2 Train: 0.4933599853912931\n",
      "Epoch 1396, Loss Train(MSE): 0.12665392174079526, R2 Train: 0.49338431303681896\n",
      "Epoch 1397, Loss Train(MSE): 0.12664763217639954, R2 Train: 0.4934094712944018\n",
      "Epoch 1398, Loss Train(MSE): 0.12664476115478904, R2 Train: 0.49342095538084385\n",
      "Epoch 1399, Loss Train(MSE): 0.12664141123568354, R2 Train: 0.49343435505726585\n",
      "Epoch 1400, Loss Train(MSE): 0.12663865820879802, R2 Train: 0.4934453671648079\n",
      "Epoch 1401, Loss Train(MSE): 0.126635253293195, R2 Train: 0.49345898682722\n",
      "Epoch 1402, Loss Train(MSE): 0.12663262757107244, R2 Train: 0.49346948971571025\n",
      "Epoch 1403, Loss Train(MSE): 0.126629177361252, R2 Train: 0.49348329055499196\n",
      "Epoch 1404, Loss Train(MSE): 0.12662664822316935, R2 Train: 0.4934934071073226\n",
      "Epoch 1405, Loss Train(MSE): 0.12662326042304448, R2 Train: 0.4935069583078221\n",
      "Epoch 1406, Loss Train(MSE): 0.12662064023735367, R2 Train: 0.4935174390505853\n",
      "Epoch 1407, Loss Train(MSE): 0.1266174096806102, R2 Train: 0.4935303612775592\n",
      "Epoch 1408, Loss Train(MSE): 0.1266146888598082, R2 Train: 0.4935412445607672\n",
      "Epoch 1409, Loss Train(MSE): 0.1266116230706071, R2 Train: 0.49355350771757156\n",
      "Epoch 1410, Loss Train(MSE): 0.12660879252665583, R2 Train: 0.4935648298933767\n",
      "Epoch 1411, Loss Train(MSE): 0.12660589863133812, R2 Train: 0.49357640547464754\n",
      "Epoch 1412, Loss Train(MSE): 0.12660294975098998, R2 Train: 0.49358820099604006\n",
      "Epoch 1413, Loss Train(MSE): 0.12660023449688418, R2 Train: 0.4935990620124633\n",
      "Epoch 1414, Loss Train(MSE): 0.1265971591183786, R2 Train: 0.4936113635264856\n",
      "Epoch 1415, Loss Train(MSE): 0.12659462889160777, R2 Train: 0.4936214844335689\n",
      "Epoch 1416, Loss Train(MSE): 0.12659143549117768, R2 Train: 0.49363425803528926\n",
      "Epoch 1417, Loss Train(MSE): 0.12658906569195608, R2 Train: 0.4936437372321757\n",
      "Epoch 1418, Loss Train(MSE): 0.1265859224318068, R2 Train: 0.4936563102727728\n",
      "Epoch 1419, Loss Train(MSE): 0.12658339961450557, R2 Train: 0.4936664015419777\n",
      "Epoch 1420, Loss Train(MSE): 0.12658046350120974, R2 Train: 0.493678145995161\n",
      "Epoch 1421, Loss Train(MSE): 0.12657778109950357, R2 Train: 0.4936888756019857\n",
      "Epoch 1422, Loss Train(MSE): 0.12657505721697257, R2 Train: 0.4936997711321097\n",
      "Epoch 1423, Loss Train(MSE): 0.12657220901950686, R2 Train: 0.49371116392197256\n",
      "Epoch 1424, Loss Train(MSE): 0.12656970216531283, R2 Train: 0.4937211913387487\n",
      "Epoch 1425, Loss Train(MSE): 0.12656668229883614, R2 Train: 0.49373327080465546\n",
      "Epoch 1426, Loss Train(MSE): 0.12656439699726113, R2 Train: 0.4937424120109555\n",
      "Epoch 1427, Loss Train(MSE): 0.12656138534868655, R2 Train: 0.4937544586052538\n",
      "Epoch 1428, Loss Train(MSE): 0.1265589564364054, R2 Train: 0.49376417425437835\n",
      "Epoch 1429, Loss Train(MSE): 0.12655615933142417, R2 Train: 0.49377536267430333\n",
      "Epoch 1430, Loss Train(MSE): 0.1265535383048152, R2 Train: 0.4937858467807392\n",
      "Epoch 1431, Loss Train(MSE): 0.12655097981968963, R2 Train: 0.4937960807212415\n",
      "Epoch 1432, Loss Train(MSE): 0.12654816202249425, R2 Train: 0.493807351910023\n",
      "Epoch 1433, Loss Train(MSE): 0.126545845677178, R2 Train: 0.493816617291288\n",
      "Epoch 1434, Loss Train(MSE): 0.12654293796444877, R2 Train: 0.49382824814220494\n",
      "Epoch 1435, Loss Train(MSE): 0.12654064584006527, R2 Train: 0.4938374166397389\n",
      "Epoch 1436, Loss Train(MSE): 0.12653787592030688, R2 Train: 0.4938484963187725\n",
      "Epoch 1437, Loss Train(MSE): 0.1265353698463157, R2 Train: 0.49385852061473723\n",
      "Epoch 1438, Loss Train(MSE): 0.12653285638080028, R2 Train: 0.4938685744767989\n",
      "Epoch 1439, Loss Train(MSE): 0.1265301327230692, R2 Train: 0.4938794691077232\n",
      "Epoch 1440, Loss Train(MSE): 0.12652787838298324, R2 Train: 0.49388848646806705\n",
      "Epoch 1441, Loss Train(MSE): 0.12652506539710326, R2 Train: 0.49389973841158696\n",
      "Epoch 1442, Loss Train(MSE): 0.12652281044277988, R2 Train: 0.4939087582288805\n",
      "Epoch 1443, Loss Train(MSE): 0.12652314256382954, R2 Train: 0.4939074297446818\n",
      "Epoch 1444, Loss Train(MSE): 0.1265365401171503, R2 Train: 0.49385383953139883\n",
      "Epoch 1445, Loss Train(MSE): 0.12655879513229992, R2 Train: 0.4937648194708003\n",
      "Epoch 1446, Loss Train(MSE): 0.1266187552286785, R2 Train: 0.49352497908528603\n",
      "Epoch 1447, Loss Train(MSE): 0.12661222539017417, R2 Train: 0.49355109843930334\n",
      "Epoch 1448, Loss Train(MSE): 0.12660575781832378, R2 Train: 0.49357696872670487\n",
      "Epoch 1449, Loss Train(MSE): 0.12659935148984613, R2 Train: 0.49360259404061546\n",
      "Epoch 1450, Loss Train(MSE): 0.12659300540523374, R2 Train: 0.493627978379065\n",
      "Epoch 1451, Loss Train(MSE): 0.12658671858805856, R2 Train: 0.49365312564776576\n",
      "Epoch 1452, Loss Train(MSE): 0.126580490084302, R2 Train: 0.493678039662792\n",
      "Epoch 1453, Loss Train(MSE): 0.1265743189617078, R2 Train: 0.49370272415316885\n",
      "Epoch 1454, Loss Train(MSE): 0.1265682043091569, R2 Train: 0.49372718276337235\n",
      "Epoch 1455, Loss Train(MSE): 0.12656214523606393, R2 Train: 0.49375141905574427\n",
      "Epoch 1456, Loss Train(MSE): 0.12655614087179345, R2 Train: 0.4937754365128262\n",
      "Epoch 1457, Loss Train(MSE): 0.12655106736545801, R2 Train: 0.49379573053816794\n",
      "Epoch 1458, Loss Train(MSE): 0.12654783948618628, R2 Train: 0.4938086420552549\n",
      "Epoch 1459, Loss Train(MSE): 0.1265446261928816, R2 Train: 0.49382149522847363\n",
      "Epoch 1460, Loss Train(MSE): 0.12654142733955973, R2 Train: 0.4938342906417611\n",
      "Epoch 1461, Loss Train(MSE): 0.12653824278302445, R2 Train: 0.4938470288679022\n",
      "Epoch 1462, Loss Train(MSE): 0.12653507238279518, R2 Train: 0.49385971046881927\n",
      "Epoch 1463, Loss Train(MSE): 0.12653191600103672, R2 Train: 0.4938723359958531\n",
      "Epoch 1464, Loss Train(MSE): 0.1265287735024906, R2 Train: 0.49388490599003765\n",
      "Epoch 1465, Loss Train(MSE): 0.1265256447544093, R2 Train: 0.4938974209823628\n",
      "Epoch 1466, Loss Train(MSE): 0.12652252962649171, R2 Train: 0.49390988149403314\n",
      "Epoch 1467, Loss Train(MSE): 0.12651942799082058, R2 Train: 0.49392228803671767\n",
      "Epoch 1468, Loss Train(MSE): 0.1265163669589512, R2 Train: 0.49393453216419525\n",
      "Epoch 1469, Loss Train(MSE): 0.12651417814588284, R2 Train: 0.49394328741646865\n",
      "Epoch 1470, Loss Train(MSE): 0.12651130493159976, R2 Train: 0.49395478027360096\n",
      "Epoch 1471, Loss Train(MSE): 0.12650897148858167, R2 Train: 0.4939641140456733\n",
      "Epoch 1472, Loss Train(MSE): 0.12650629082485138, R2 Train: 0.4939748367005945\n",
      "Epoch 1473, Loss Train(MSE): 0.12650380700841518, R2 Train: 0.4939847719663393\n",
      "Epoch 1474, Loss Train(MSE): 0.1265013233719441, R2 Train: 0.49399470651222355\n",
      "Epoch 1475, Loss Train(MSE): 0.1264986837401986, R2 Train: 0.4940052650392056\n",
      "Epoch 1476, Loss Train(MSE): 0.12649640136279594, R2 Train: 0.49401439454881624\n",
      "Epoch 1477, Loss Train(MSE): 0.12649360547592847, R2 Train: 0.4940255780962861\n",
      "Epoch 1478, Loss Train(MSE): 0.12649152026128307, R2 Train: 0.4940339189548677\n",
      "Epoch 1479, Loss Train(MSE): 0.12648875561436917, R2 Train: 0.4940449775425233\n",
      "Epoch 1480, Loss Train(MSE): 0.1264864957518637, R2 Train: 0.49405401699254525\n",
      "Epoch 1481, Loss Train(MSE): 0.1264839481633875, R2 Train: 0.49406420734644996\n",
      "Epoch 1482, Loss Train(MSE): 0.12648150929789193, R2 Train: 0.4940739628084323\n",
      "Epoch 1483, Loss Train(MSE): 0.12647918209934045, R2 Train: 0.4940832716026382\n",
      "Epoch 1484, Loss Train(MSE): 0.12647656011577948, R2 Train: 0.49409375953688206\n",
      "Epoch 1485, Loss Train(MSE): 0.12647445644232375, R2 Train: 0.494102174230705\n",
      "Epoch 1486, Loss Train(MSE): 0.12647177951844804, R2 Train: 0.49411288192620784\n",
      "Epoch 1487, Loss Train(MSE): 0.12646963932389887, R2 Train: 0.4941214427044045\n",
      "Epoch 1488, Loss Train(MSE): 0.1264671181400047, R2 Train: 0.49413152743998123\n",
      "Epoch 1489, Loss Train(MSE): 0.126464779565954, R2 Train: 0.494140881736184\n",
      "Epoch 1490, Loss Train(MSE): 0.12646249469225898, R2 Train: 0.4941500212309641\n",
      "Epoch 1491, Loss Train(MSE): 0.12645995450247852, R2 Train: 0.49416018199008593\n",
      "Epoch 1492, Loss Train(MSE): 0.1264579083403805, R2 Train: 0.49416836663847796\n",
      "Epoch 1493, Loss Train(MSE): 0.12645531597759258, R2 Train: 0.4941787360896297\n",
      "Epoch 1494, Loss Train(MSE): 0.12645320679060987, R2 Train: 0.49418717283756053\n",
      "Epoch 1495, Loss Train(MSE): 0.126450788697044, R2 Train: 0.49419684521182405\n",
      "Epoch 1496, Loss Train(MSE): 0.12644846502201082, R2 Train: 0.4942061399119567\n",
      "Epoch 1497, Loss Train(MSE): 0.1264462963892965, R2 Train: 0.494214814442814\n",
      "Epoch 1498, Loss Train(MSE): 0.12644376141105976, R2 Train: 0.49422495435576097\n",
      "Epoch 1499, Loss Train(MSE): 0.1264418335634062, R2 Train: 0.49423266574637525\n",
      "Epoch 1500, Loss Train(MSE): 0.1264393248154499, R2 Train: 0.4942427007382004\n",
      "Epoch 1501, Loss Train(MSE): 0.12643717118553344, R2 Train: 0.49425131525786625\n",
      "Epoch 1502, Loss Train(MSE): 0.1264349212902634, R2 Train: 0.4942603148389464\n",
      "Epoch 1503, Loss Train(MSE): 0.12643253987674344, R2 Train: 0.49426984049302625\n",
      "Epoch 1504, Loss Train(MSE): 0.1264305501945893, R2 Train: 0.49427779922164283\n",
      "Epoch 1505, Loss Train(MSE): 0.1264280884754462, R2 Train: 0.4942876460982152\n",
      "Epoch 1506, Loss Train(MSE): 0.12643074903692972, R2 Train: 0.4942770038522811\n",
      "Epoch 1507, Loss Train(MSE): 0.12650957969670715, R2 Train: 0.4939616812131714\n",
      "Epoch 1508, Loss Train(MSE): 0.12650422902314343, R2 Train: 0.4939830839074263\n",
      "Epoch 1509, Loss Train(MSE): 0.12649896148491657, R2 Train: 0.4940041540603337\n",
      "Epoch 1510, Loss Train(MSE): 0.12649367064126607, R2 Train: 0.4940253174349357\n",
      "Epoch 1511, Loss Train(MSE): 0.12648855120687713, R2 Train: 0.4940457951724915\n",
      "Epoch 1512, Loss Train(MSE): 0.12648330189126336, R2 Train: 0.49406679243494656\n",
      "Epoch 1513, Loss Train(MSE): 0.12647834158912538, R2 Train: 0.4940866336434985\n",
      "Epoch 1514, Loss Train(MSE): 0.1264731166054327, R2 Train: 0.4941075335782692\n",
      "Epoch 1515, Loss Train(MSE): 0.12646832570822197, R2 Train: 0.4941266971671121\n",
      "Epoch 1516, Loss Train(MSE): 0.1264631088992733, R2 Train: 0.49414756440290675\n",
      "Epoch 1517, Loss Train(MSE): 0.1264584969701675, R2 Train: 0.49416601211933\n",
      "Epoch 1518, Loss Train(MSE): 0.12645328142684176, R2 Train: 0.49418687429263297\n",
      "Epoch 1519, Loss Train(MSE): 0.1264488439700979, R2 Train: 0.4942046241196084\n",
      "Epoch 1520, Loss Train(MSE): 0.1264453530944842, R2 Train: 0.49421858762206317\n",
      "Epoch 1521, Loss Train(MSE): 0.12644322670072558, R2 Train: 0.4942270931970977\n",
      "Epoch 1522, Loss Train(MSE): 0.12644064833734264, R2 Train: 0.49423740665062943\n",
      "Epoch 1523, Loss Train(MSE): 0.126438408659558, R2 Train: 0.494246365361768\n",
      "Epoch 1524, Loss Train(MSE): 0.12643598859443528, R2 Train: 0.4942560456222589\n",
      "Epoch 1525, Loss Train(MSE): 0.12643362982391856, R2 Train: 0.49426548070432574\n",
      "Epoch 1526, Loss Train(MSE): 0.12643137265933513, R2 Train: 0.4942745093626595\n",
      "Epoch 1527, Loss Train(MSE): 0.12642888927470097, R2 Train: 0.4942844429011961\n",
      "Epoch 1528, Loss Train(MSE): 0.12642679937883636, R2 Train: 0.49429280248465457\n",
      "Epoch 1529, Loss Train(MSE): 0.12642419638448285, R2 Train: 0.4943032144620686\n",
      "Epoch 1530, Loss Train(MSE): 0.12642225868414153, R2 Train: 0.4943109652634339\n",
      "Epoch 1531, Loss Train(MSE): 0.12641969080829243, R2 Train: 0.49432123676683026\n",
      "Epoch 1532, Loss Train(MSE): 0.12641760995334317, R2 Train: 0.4943295601866273\n",
      "Epoch 1533, Loss Train(MSE): 0.126415225001422, R2 Train: 0.494339099994312\n",
      "Epoch 1534, Loss Train(MSE): 0.12641299650136623, R2 Train: 0.4943480139945351\n",
      "Epoch 1535, Loss Train(MSE): 0.12641079798573168, R2 Train: 0.49435680805707327\n",
      "Epoch 1536, Loss Train(MSE): 0.12640841758038576, R2 Train: 0.49436632967845695\n",
      "Epoch 1537, Loss Train(MSE): 0.12640640882444934, R2 Train: 0.49437436470220264\n",
      "Epoch 1538, Loss Train(MSE): 0.1264039175554313, R2 Train: 0.49438432977827484\n",
      "Epoch 1539, Loss Train(MSE): 0.12640201263826725, R2 Train: 0.494391949446931\n",
      "Epoch 1540, Loss Train(MSE): 0.12639958861239026, R2 Train: 0.494401645550439\n",
      "Epoch 1541, Loss Train(MSE): 0.12639751663209528, R2 Train: 0.4944099334716189\n",
      "Epoch 1542, Loss Train(MSE): 0.12640721820728812, R2 Train: 0.4943711271708475\n",
      "Epoch 1543, Loss Train(MSE): 0.12641057000157813, R2 Train: 0.4943577199936875\n",
      "Epoch 1544, Loss Train(MSE): 0.12640791585014383, R2 Train: 0.4943683365994247\n",
      "Epoch 1545, Loss Train(MSE): 0.1264052713346096, R2 Train: 0.4943789146615616\n",
      "Epoch 1546, Loss Train(MSE): 0.12640263639250104, R2 Train: 0.4943894544299958\n",
      "Epoch 1547, Loss Train(MSE): 0.12640001096211395, R2 Train: 0.4943999561515442\n",
      "Epoch 1548, Loss Train(MSE): 0.12639739498249883, R2 Train: 0.4944104200700047\n",
      "Epoch 1549, Loss Train(MSE): 0.12639478839344573, R2 Train: 0.4944208464262171\n",
      "Epoch 1550, Loss Train(MSE): 0.12639219113546954, R2 Train: 0.49443123545812184\n",
      "Epoch 1551, Loss Train(MSE): 0.12638960314979564, R2 Train: 0.49444158740081745\n",
      "Epoch 1552, Loss Train(MSE): 0.12638702437834592, R2 Train: 0.4944519024866163\n",
      "Epoch 1553, Loss Train(MSE): 0.12638445476372528, R2 Train: 0.49446218094509886\n",
      "Epoch 1554, Loss Train(MSE): 0.12638189424920804, R2 Train: 0.49447242300316785\n",
      "Epoch 1555, Loss Train(MSE): 0.12637934277872528, R2 Train: 0.49448262888509886\n",
      "Epoch 1556, Loss Train(MSE): 0.12637680029685205, R2 Train: 0.4944927988125918\n",
      "Epoch 1557, Loss Train(MSE): 0.12637426674879526, R2 Train: 0.49450293300481896\n",
      "Epoch 1558, Loss Train(MSE): 0.12637174208038135, R2 Train: 0.4945130316784746\n",
      "Epoch 1559, Loss Train(MSE): 0.1263692262380449, R2 Train: 0.49452309504782044\n",
      "Epoch 1560, Loss Train(MSE): 0.12636671916881706, R2 Train: 0.49453312332473176\n",
      "Epoch 1561, Loss Train(MSE): 0.1263642208203145, R2 Train: 0.49454311671874196\n",
      "Epoch 1562, Loss Train(MSE): 0.12636173114072852, R2 Train: 0.49455307543708593\n",
      "Epoch 1563, Loss Train(MSE): 0.12635925007881435, R2 Train: 0.4945629996847426\n",
      "Epoch 1564, Loss Train(MSE): 0.12635677758388103, R2 Train: 0.4945728896644759\n",
      "Epoch 1565, Loss Train(MSE): 0.12635431360578123, R2 Train: 0.4945827455768751\n",
      "Epoch 1566, Loss Train(MSE): 0.12635185809490132, R2 Train: 0.4945925676203947\n",
      "Epoch 1567, Loss Train(MSE): 0.12634941100215202, R2 Train: 0.4946023559913919\n",
      "Epoch 1568, Loss Train(MSE): 0.12634697227895864, R2 Train: 0.49461211088416546\n",
      "Epoch 1569, Loss Train(MSE): 0.12634483753323666, R2 Train: 0.49462064986705334\n",
      "Epoch 1570, Loss Train(MSE): 0.12634286106519893, R2 Train: 0.4946285557392043\n",
      "Epoch 1571, Loss Train(MSE): 0.1263409817445738, R2 Train: 0.49463607302170476\n",
      "Epoch 1572, Loss Train(MSE): 0.12637669892307496, R2 Train: 0.49449320430770016\n",
      "Epoch 1573, Loss Train(MSE): 0.12641909825301129, R2 Train: 0.49432360698795486\n",
      "Epoch 1574, Loss Train(MSE): 0.1264142351854004, R2 Train: 0.4943430592583984\n",
      "Epoch 1575, Loss Train(MSE): 0.12640960992112088, R2 Train: 0.4943615603155165\n",
      "Epoch 1576, Loss Train(MSE): 0.12640489416064377, R2 Train: 0.49438042335742494\n",
      "Epoch 1577, Loss Train(MSE): 0.12640028376979837, R2 Train: 0.49439886492080654\n",
      "Epoch 1578, Loss Train(MSE): 0.12639572438132948, R2 Train: 0.49441710247468207\n",
      "Epoch 1579, Loss Train(MSE): 0.12639111477125364, R2 Train: 0.4944355409149854\n",
      "Epoch 1580, Loss Train(MSE): 0.1263867202138726, R2 Train: 0.4944531191445096\n",
      "Epoch 1581, Loss Train(MSE): 0.12638209811826398, R2 Train: 0.4944716075269441\n",
      "Epoch 1582, Loss Train(MSE): 0.12637787628089453, R2 Train: 0.4944884948764219\n",
      "Epoch 1583, Loss Train(MSE): 0.12637322921217511, R2 Train: 0.49450708315129954\n",
      "Epoch 1584, Loss Train(MSE): 0.12636918744694778, R2 Train: 0.4945232502122089\n",
      "Epoch 1585, Loss Train(MSE): 0.12636457999069867, R2 Train: 0.49454168003720533\n",
      "Epoch 1586, Loss Train(MSE): 0.12636057510723894, R2 Train: 0.49455769957104423\n",
      "Epoch 1587, Loss Train(MSE): 0.12635679968396016, R2 Train: 0.49457280126415937\n",
      "Epoch 1588, Loss Train(MSE): 0.12635482105344048, R2 Train: 0.4945807157862381\n",
      "Epoch 1589, Loss Train(MSE): 0.12635263919697526, R2 Train: 0.49458944321209897\n",
      "Epoch 1590, Loss Train(MSE): 0.1263505378144637, R2 Train: 0.4945978487421452\n",
      "Epoch 1591, Loss Train(MSE): 0.12634851506270423, R2 Train: 0.4946059397491831\n",
      "Epoch 1592, Loss Train(MSE): 0.12634628663324077, R2 Train: 0.49461485346703693\n",
      "Epoch 1593, Loss Train(MSE): 0.12634442639178628, R2 Train: 0.4946222944328549\n",
      "Epoch 1594, Loss Train(MSE): 0.12634210159566764, R2 Train: 0.49463159361732945\n",
      "Epoch 1595, Loss Train(MSE): 0.12634033862340166, R2 Train: 0.49463864550639336\n",
      "Epoch 1596, Loss Train(MSE): 0.12633806933901306, R2 Train: 0.49464772264394774\n",
      "Epoch 1597, Loss Train(MSE): 0.1263361644760897, R2 Train: 0.4946553420956412\n",
      "Epoch 1598, Loss Train(MSE): 0.12633407030709426, R2 Train: 0.49466371877162296\n",
      "Epoch 1599, Loss Train(MSE): 0.12633202005126704, R2 Train: 0.49467191979493186\n",
      "Epoch 1600, Loss Train(MSE): 0.12633010373901013, R2 Train: 0.4946795850439595\n",
      "Epoch 1601, Loss Train(MSE): 0.12632790476505718, R2 Train: 0.4946883809397713\n",
      "Epoch 1602, Loss Train(MSE): 0.1263261689043351, R2 Train: 0.4946953243826596\n",
      "Epoch 1603, Loss Train(MSE): 0.12632393745746368, R2 Train: 0.4947042501701453\n",
      "Epoch 1604, Loss Train(MSE): 0.12632214661099436, R2 Train: 0.49471141355602255\n",
      "Epoch 1605, Loss Train(MSE): 0.12632005319513037, R2 Train: 0.4947197872194785\n",
      "Epoch 1606, Loss Train(MSE): 0.12631810140854852, R2 Train: 0.49472759436580593\n",
      "Epoch 1607, Loss Train(MSE): 0.12631619881420186, R2 Train: 0.49473520474319255\n",
      "Epoch 1608, Loss Train(MSE): 0.12631408341317102, R2 Train: 0.4947436663473159\n",
      "Epoch 1609, Loss Train(MSE): 0.12631237368600698, R2 Train: 0.4947505052559721\n",
      "Epoch 1610, Loss Train(MSE): 0.12631020895200915, R2 Train: 0.4947591641919634\n",
      "Epoch 1611, Loss Train(MSE): 0.1263084612009621, R2 Train: 0.49476615519615164\n",
      "Epoch 1612, Loss Train(MSE): 0.12630643048747217, R2 Train: 0.4947742780501113\n",
      "Epoch 1613, Loss Train(MSE): 0.1263045086753904, R2 Train: 0.49478196529843843\n",
      "Epoch 1614, Loss Train(MSE): 0.12630267967263273, R2 Train: 0.4947892813094691\n",
      "Epoch 1615, Loss Train(MSE): 0.12630058168303007, R2 Train: 0.4947976732678797\n",
      "Epoch 1616, Loss Train(MSE): 0.126298955963767, R2 Train: 0.494804176144932\n",
      "Epoch 1617, Loss Train(MSE): 0.12629685278651576, R2 Train: 0.494812588853937\n",
      "Epoch 1618, Loss Train(MSE): 0.12629508656770283, R2 Train: 0.4948196537291887\n",
      "Epoch 1619, Loss Train(MSE): 0.12629317234431534, R2 Train: 0.49482731062273866\n",
      "Epoch 1620, Loss Train(MSE): 0.126291221036717, R2 Train: 0.494835115853132\n",
      "Epoch 1621, Loss Train(MSE): 0.1262895176139026, R2 Train: 0.49484192954438955\n",
      "Epoch 1622, Loss Train(MSE): 0.12628745652619394, R2 Train: 0.49485017389522423\n",
      "Epoch 1623, Loss Train(MSE): 0.12628581182924858, R2 Train: 0.49485675268300566\n",
      "Epoch 1624, Loss Train(MSE): 0.12628384284376434, R2 Train: 0.4948646286249426\n",
      "Epoch 1625, Loss Train(MSE): 0.12628200517475685, R2 Train: 0.4948719793009726\n",
      "Epoch 1626, Loss Train(MSE): 0.12628025361133044, R2 Train: 0.49487898555467824\n",
      "Epoch 1627, Loss Train(MSE): 0.12627823241627492, R2 Train: 0.4948870703349003\n",
      "Epoch 1628, Loss Train(MSE): 0.12627667823313593, R2 Train: 0.4948932870674563\n",
      "Epoch 1629, Loss Train(MSE): 0.12627468221785154, R2 Train: 0.49490127112859383\n",
      "Epoch 1630, Loss Train(MSE): 0.12627292810687096, R2 Train: 0.49490828757251615\n",
      "Epoch 1631, Loss Train(MSE): 0.12627115532454872, R2 Train: 0.49491537870180513\n",
      "Epoch 1632, Loss Train(MSE): 0.1262692002556672, R2 Train: 0.49492319897733117\n",
      "Epoch 1633, Loss Train(MSE): 0.1262676513483843, R2 Train: 0.4949293946064628\n",
      "Epoch 1634, Loss Train(MSE): 0.12626568155387874, R2 Train: 0.49493727378448504\n",
      "Epoch 1635, Loss Train(MSE): 0.12626398325775876, R2 Train: 0.494944066968965\n",
      "Epoch 1636, Loss Train(MSE): 0.12626221416255337, R2 Train: 0.4949511433497865\n",
      "Epoch 1637, Loss Train(MSE): 0.126260309031673, R2 Train: 0.49495876387330795\n",
      "Epoch 1638, Loss Train(MSE): 0.12625876868051986, R2 Train: 0.4949649252779206\n",
      "Epoch 1639, Loss Train(MSE): 0.12625683434397156, R2 Train: 0.49497266262411377\n",
      "Epoch 1640, Loss Train(MSE): 0.12625516691392744, R2 Train: 0.49497933234429026\n",
      "Epoch 1641, Loss Train(MSE): 0.1262534238272692, R2 Train: 0.4949863046909232\n",
      "Epoch 1642, Loss Train(MSE): 0.1262863995284246, R2 Train: 0.4948544018863016\n",
      "Epoch 1643, Loss Train(MSE): 0.12632901442952404, R2 Train: 0.4946839422819038\n",
      "Epoch 1644, Loss Train(MSE): 0.1263244736718045, R2 Train: 0.494702105312782\n",
      "Epoch 1645, Loss Train(MSE): 0.12632059960275088, R2 Train: 0.49471760158899647\n",
      "Epoch 1646, Loss Train(MSE): 0.12631617625631014, R2 Train: 0.49473529497475943\n",
      "Epoch 1647, Loss Train(MSE): 0.126312301960846, R2 Train: 0.494750792156616\n",
      "Epoch 1648, Loss Train(MSE): 0.12631651920165807, R2 Train: 0.49473392319336773\n",
      "Epoch 1649, Loss Train(MSE): 0.12632030971963518, R2 Train: 0.4947187611214593\n",
      "Epoch 1650, Loss Train(MSE): 0.12631588253671433, R2 Train: 0.4947364698531427\n",
      "Epoch 1651, Loss Train(MSE): 0.12631148996579603, R2 Train: 0.4947540401368159\n",
      "Epoch 1652, Loss Train(MSE): 0.1263071315401282, R2 Train: 0.49477147383948716\n",
      "Epoch 1653, Loss Train(MSE): 0.12630280680195324, R2 Train: 0.494788772792187\n",
      "Epoch 1654, Loss Train(MSE): 0.1262985153022884, R2 Train: 0.49480593879084644\n",
      "Epoch 1655, Loss Train(MSE): 0.12629425660071184, R2 Train: 0.49482297359715266\n",
      "Epoch 1656, Loss Train(MSE): 0.12629003026515534, R2 Train: 0.49483987893937864\n",
      "Epoch 1657, Loss Train(MSE): 0.1262858358717031, R2 Train: 0.49485665651318755\n",
      "Epoch 1658, Loss Train(MSE): 0.12628167300439572, R2 Train: 0.49487330798241713\n",
      "Epoch 1659, Loss Train(MSE): 0.12627854987023032, R2 Train: 0.4948858005190787\n",
      "Epoch 1660, Loss Train(MSE): 0.12627629514983169, R2 Train: 0.49489481940067326\n",
      "Epoch 1661, Loss Train(MSE): 0.12627404846928272, R2 Train: 0.4949038061228691\n",
      "Epoch 1662, Loss Train(MSE): 0.12627180977194832, R2 Train: 0.4949127609122067\n",
      "Epoch 1663, Loss Train(MSE): 0.12626957900197833, R2 Train: 0.4949216839920867\n",
      "Epoch 1664, Loss Train(MSE): 0.1262673561042914, R2 Train: 0.4949305755828344\n",
      "Epoch 1665, Loss Train(MSE): 0.12626514102455894, R2 Train: 0.49493943590176426\n",
      "Epoch 1666, Loss Train(MSE): 0.12626293370918934, R2 Train: 0.49494826516324264\n",
      "Epoch 1667, Loss Train(MSE): 0.1262607341053129, R2 Train: 0.49495706357874836\n",
      "Epoch 1668, Loss Train(MSE): 0.12625854216076723, R2 Train: 0.49496583135693106\n",
      "Epoch 1669, Loss Train(MSE): 0.12625635782408245, R2 Train: 0.4949745687036702\n",
      "Epoch 1670, Loss Train(MSE): 0.12625418104446717, R2 Train: 0.4949832758221313\n",
      "Epoch 1671, Loss Train(MSE): 0.12625201177179493, R2 Train: 0.4949919529128203\n",
      "Epoch 1672, Loss Train(MSE): 0.12624984995659067, R2 Train: 0.49500060017363734\n",
      "Epoch 1673, Loss Train(MSE): 0.12624769555001747, R2 Train: 0.4950092177999301\n",
      "Epoch 1674, Loss Train(MSE): 0.12624554850386416, R2 Train: 0.49501780598454337\n",
      "Epoch 1675, Loss Train(MSE): 0.12624340877053253, R2 Train: 0.4950263649178699\n",
      "Epoch 1676, Loss Train(MSE): 0.12624127630302537, R2 Train: 0.4950348947878985\n",
      "Epoch 1677, Loss Train(MSE): 0.12623915105493463, R2 Train: 0.49504339578026146\n",
      "Epoch 1678, Loss Train(MSE): 0.12623726091436527, R2 Train: 0.49505095634253893\n",
      "Epoch 1679, Loss Train(MSE): 0.12623556292300006, R2 Train: 0.49505774830799976\n",
      "Epoch 1680, Loss Train(MSE): 0.12623385989870775, R2 Train: 0.495064560405169\n",
      "Epoch 1681, Loss Train(MSE): 0.1262319969392362, R2 Train: 0.4950720122430552\n",
      "Epoch 1682, Loss Train(MSE): 0.1262304823229288, R2 Train: 0.4950780707082848\n",
      "Epoch 1683, Loss Train(MSE): 0.1262285752457002, R2 Train: 0.49508569901719923\n",
      "Epoch 1684, Loss Train(MSE): 0.12622700588052777, R2 Train: 0.49509197647788894\n",
      "Epoch 1685, Loss Train(MSE): 0.1262252351211889, R2 Train: 0.49509905951524436\n",
      "Epoch 1686, Loss Train(MSE): 0.12622349326627896, R2 Train: 0.49510602693488415\n",
      "Epoch 1687, Loss Train(MSE): 0.12622191728676718, R2 Train: 0.49511233085293127\n",
      "Epoch 1688, Loss Train(MSE): 0.12622004666206388, R2 Train: 0.4951198133517445\n",
      "Epoch 1689, Loss Train(MSE): 0.12621857684719948, R2 Train: 0.4951256926112021\n",
      "Epoch 1690, Loss Train(MSE): 0.1262167644448832, R2 Train: 0.4951329422204672\n",
      "Epoch 1691, Loss Train(MSE): 0.12621511546260844, R2 Train: 0.49513953814956624\n",
      "Epoch 1692, Loss Train(MSE): 0.1262135034728145, R2 Train: 0.49514598610874205\n",
      "Epoch 1693, Loss Train(MSE): 0.1262116742388764, R2 Train: 0.49515330304449445\n",
      "Epoch 1694, Loss Train(MSE): 0.12621026339080074, R2 Train: 0.49515894643679703\n",
      "Epoch 1695, Loss Train(MSE): 0.12620843974696128, R2 Train: 0.4951662410121549\n",
      "Epoch 1696, Loss Train(MSE): 0.1262068575039652, R2 Train: 0.49517256998413917\n",
      "Epoch 1697, Loss Train(MSE): 0.1262052330466213, R2 Train: 0.49517906781351484\n",
      "Epoch 1698, Loss Train(MSE): 0.12620346486161893, R2 Train: 0.4951861405535243\n",
      "Epoch 1699, Loss Train(MSE): 0.126202046314805, R2 Train: 0.49519181474078\n",
      "Epoch 1700, Loss Train(MSE): 0.12620025508399393, R2 Train: 0.49519897966402426\n",
      "Epoch 1701, Loss Train(MSE): 0.1261987160182054, R2 Train: 0.4952051359271784\n",
      "Epoch 1702, Loss Train(MSE): 0.12619710025419376, R2 Train: 0.495211598983225\n",
      "Epoch 1703, Loss Train(MSE): 0.12619537022736174, R2 Train: 0.49521851909055303\n",
      "Epoch 1704, Loss Train(MSE): 0.12619396455069545, R2 Train: 0.4952241417972182\n",
      "Epoch 1705, Loss Train(MSE): 0.12619220425458447, R2 Train: 0.49523118298166213\n",
      "Epoch 1706, Loss Train(MSE): 0.1261906868378338, R2 Train: 0.4952372526486648\n",
      "Epoch 1707, Loss Train(MSE): 0.1261890990973057, R2 Train: 0.49524360361077724\n",
      "Epoch 1708, Loss Train(MSE): 0.1261873863100289, R2 Train: 0.4952504547598844\n",
      "Epoch 1709, Loss Train(MSE): 0.12618601229473256, R2 Train: 0.49525595082106977\n",
      "Epoch 1710, Loss Train(MSE): 0.1261842815752294, R2 Train: 0.4952628736990824\n",
      "Epoch 1711, Loss Train(MSE): 0.1261827661375493, R2 Train: 0.49526893544980277\n",
      "Epoch 1712, Loss Train(MSE): 0.1261812240737264, R2 Train: 0.4952751037050944\n",
      "Epoch 1713, Loss Train(MSE): 0.1261795113785251, R2 Train: 0.4952819544858996\n",
      "Epoch 1714, Loss Train(MSE): 0.12617818260535701, R2 Train: 0.49528726957857194\n",
      "Epoch 1715, Loss Train(MSE): 0.12617648242895602, R2 Train: 0.49529407028417594\n",
      "Epoch 1716, Loss Train(MSE): 0.12617495120076955, R2 Train: 0.4953001951969218\n",
      "Epoch 1717, Loss Train(MSE): 0.1262122580432596, R2 Train: 0.49515096782696155\n",
      "Epoch 1718, Loss Train(MSE): 0.1262474373037205, R2 Train: 0.49501025078511796\n",
      "Epoch 1719, Loss Train(MSE): 0.126243940880994, R2 Train: 0.495024236476024\n",
      "Epoch 1720, Loss Train(MSE): 0.12624000269839036, R2 Train: 0.49503998920643855\n",
      "Epoch 1721, Loss Train(MSE): 0.12623649388890865, R2 Train: 0.4950540244443654\n",
      "Epoch 1722, Loss Train(MSE): 0.12623269137393106, R2 Train: 0.49506923450427576\n",
      "Epoch 1723, Loss Train(MSE): 0.12622916061751904, R2 Train: 0.49508335752992383\n",
      "Epoch 1724, Loss Train(MSE): 0.12622549964982666, R2 Train: 0.4950980014006934\n",
      "Epoch 1725, Loss Train(MSE): 0.1262219379075966, R2 Train: 0.49511224836961365\n",
      "Epoch 1726, Loss Train(MSE): 0.1262184239979541, R2 Train: 0.4951263040081836\n",
      "Epoch 1727, Loss Train(MSE): 0.12621482272476106, R2 Train: 0.49514070910095576\n",
      "Epoch 1728, Loss Train(MSE): 0.1262114610348349, R2 Train: 0.49515415586066036\n",
      "Epoch 1729, Loss Train(MSE): 0.12620781215333976, R2 Train: 0.49516875138664096\n",
      "Epoch 1730, Loss Train(MSE): 0.1262046075143457, R2 Train: 0.4951815699426172\n",
      "Epoch 1731, Loss Train(MSE): 0.12620097391835336, R2 Train: 0.49519610432658656\n",
      "Epoch 1732, Loss Train(MSE): 0.12619779168100517, R2 Train: 0.4952088332759793\n",
      "Epoch 1733, Loss Train(MSE): 0.1261942855511838, R2 Train: 0.49522285779526476\n",
      "Epoch 1734, Loss Train(MSE): 0.12619103115583297, R2 Train: 0.49523587537666813\n",
      "Epoch 1735, Loss Train(MSE): 0.126187698840683, R2 Train: 0.49524920463726796\n",
      "Epoch 1736, Loss Train(MSE): 0.1261854638354743, R2 Train: 0.4952581446581028\n",
      "Epoch 1737, Loss Train(MSE): 0.12618392950087443, R2 Train: 0.4952642819965023\n",
      "Epoch 1738, Loss Train(MSE): 0.12618211325748124, R2 Train: 0.495271546970075\n",
      "Epoch 1739, Loss Train(MSE): 0.1261807269969417, R2 Train: 0.49527709201223324\n",
      "Epoch 1740, Loss Train(MSE): 0.12617891029703077, R2 Train: 0.49528435881187693\n",
      "Epoch 1741, Loss Train(MSE): 0.1261774221890021, R2 Train: 0.4952903112439916\n",
      "Epoch 1742, Loss Train(MSE): 0.12617574566967302, R2 Train: 0.49529701732130793\n",
      "Epoch 1743, Loss Train(MSE): 0.1261741241514839, R2 Train: 0.49530350339406437\n",
      "Epoch 1744, Loss Train(MSE): 0.1261726034767603, R2 Train: 0.49530958609295883\n",
      "Epoch 1745, Loss Train(MSE): 0.12617084656180355, R2 Train: 0.4953166137527858\n",
      "Epoch 1746, Loss Train(MSE): 0.12616948328127128, R2 Train: 0.49532206687491487\n",
      "Epoch 1747, Loss Train(MSE): 0.12616771668712218, R2 Train: 0.4953291332515113\n",
      "Epoch 1748, Loss Train(MSE): 0.12616625766978332, R2 Train: 0.49533496932086674\n",
      "Epoch 1749, Loss Train(MSE): 0.1261646316545388, R2 Train: 0.49534147338184475\n",
      "Epoch 1750, Loss Train(MSE): 0.1261630294087353, R2 Train: 0.4953478823650588\n",
      "Epoch 1751, Loss Train(MSE): 0.12616156749959356, R2 Train: 0.49535373000162575\n",
      "Epoch 1752, Loss Train(MSE): 0.1261598352061967, R2 Train: 0.4953606591752132\n",
      "Epoch 1753, Loss Train(MSE): 0.126158509622553, R2 Train: 0.49536596150978796\n",
      "Epoch 1754, Loss Train(MSE): 0.1261568044341747, R2 Train: 0.49537278226330117\n",
      "Epoch 1755, Loss Train(MSE): 0.1261553286094543, R2 Train: 0.4953786855621828\n",
      "Epoch 1756, Loss Train(MSE): 0.12615379352175934, R2 Train: 0.4953848259129626\n",
      "Epoch 1757, Loss Train(MSE): 0.12615216611272123, R2 Train: 0.4953913355491151\n",
      "Epoch 1758, Loss Train(MSE): 0.126150802121628, R2 Train: 0.49539679151348803\n",
      "Epoch 1759, Loss Train(MSE): 0.12614911396695533, R2 Train: 0.4954035441321787\n",
      "Epoch 1760, Loss Train(MSE): 0.12614773829724857, R2 Train: 0.4954090468110057\n",
      "Epoch 1761, Loss Train(MSE): 0.1261461537767553, R2 Train: 0.49541538489297876\n",
      "Epoch 1762, Loss Train(MSE): 0.1261446204759675, R2 Train: 0.49542151809613\n",
      "Epoch 1763, Loss Train(MSE): 0.12615122163912265, R2 Train: 0.4953951134435094\n",
      "Epoch 1764, Loss Train(MSE): 0.12615680101993626, R2 Train: 0.49537279592025496\n",
      "Epoch 1765, Loss Train(MSE): 0.12615494521959453, R2 Train: 0.4953802191216219\n",
      "Epoch 1766, Loss Train(MSE): 0.12615309499811636, R2 Train: 0.49538762000753456\n",
      "Epoch 1767, Loss Train(MSE): 0.1261512503283693, R2 Train: 0.4953949986865228\n",
      "Epoch 1768, Loss Train(MSE): 0.12614941118344625, R2 Train: 0.495402355266215\n",
      "Epoch 1769, Loss Train(MSE): 0.12614757753666264, R2 Train: 0.49540968985334943\n",
      "Epoch 1770, Loss Train(MSE): 0.12614574936155307, R2 Train: 0.49541700255378773\n",
      "Epoch 1771, Loss Train(MSE): 0.12614392663186835, R2 Train: 0.4954242934725266\n",
      "Epoch 1772, Loss Train(MSE): 0.1261421093215722, R2 Train: 0.49543156271371125\n",
      "Epoch 1773, Loss Train(MSE): 0.12614029740483879, R2 Train: 0.49543881038064486\n",
      "Epoch 1774, Loss Train(MSE): 0.12613849085604933, R2 Train: 0.49544603657580266\n",
      "Epoch 1775, Loss Train(MSE): 0.12613668964978952, R2 Train: 0.4954532414008419\n",
      "Epoch 1776, Loss Train(MSE): 0.1261348937608468, R2 Train: 0.4954604249566128\n",
      "Epoch 1777, Loss Train(MSE): 0.12613310316420737, R2 Train: 0.49546758734317053\n",
      "Epoch 1778, Loss Train(MSE): 0.12613131783505366, R2 Train: 0.49547472865978537\n",
      "Epoch 1779, Loss Train(MSE): 0.1261295377487618, R2 Train: 0.49548184900495285\n",
      "Epoch 1780, Loss Train(MSE): 0.12612776288089886, R2 Train: 0.49548894847640457\n",
      "Epoch 1781, Loss Train(MSE): 0.12612599320722054, R2 Train: 0.49549602717111785\n",
      "Epoch 1782, Loss Train(MSE): 0.12612422870366838, R2 Train: 0.4955030851853265\n",
      "Epoch 1783, Loss Train(MSE): 0.12612246934636764, R2 Train: 0.4955101226145294\n",
      "Epoch 1784, Loss Train(MSE): 0.12612071511162476, R2 Train: 0.49551713955350096\n",
      "Epoch 1785, Loss Train(MSE): 0.12611896597592517, R2 Train: 0.4955241360962993\n",
      "Epoch 1786, Loss Train(MSE): 0.12611722191593067, R2 Train: 0.49553111233627734\n",
      "Epoch 1787, Loss Train(MSE): 0.1261154829084776, R2 Train: 0.4955380683660896\n",
      "Epoch 1788, Loss Train(MSE): 0.1261137489305743, R2 Train: 0.49554500427770276\n",
      "Epoch 1789, Loss Train(MSE): 0.12611201995939916, R2 Train: 0.49555192016240335\n",
      "Epoch 1790, Loss Train(MSE): 0.1261102959722983, R2 Train: 0.49555881611080677\n",
      "Epoch 1791, Loss Train(MSE): 0.12610857694678376, R2 Train: 0.49556569221286495\n",
      "Epoch 1792, Loss Train(MSE): 0.12610686286053113, R2 Train: 0.49557254855787547\n",
      "Epoch 1793, Loss Train(MSE): 0.12610515369137779, R2 Train: 0.49557938523448886\n",
      "Epoch 1794, Loss Train(MSE): 0.1261034494173208, R2 Train: 0.4955862023307168\n",
      "Epoch 1795, Loss Train(MSE): 0.12610217575858204, R2 Train: 0.49559129696567183\n",
      "Epoch 1796, Loss Train(MSE): 0.12610067766264832, R2 Train: 0.4955972893494067\n",
      "Epoch 1797, Loss Train(MSE): 0.12613041178593493, R2 Train: 0.4954783528562603\n",
      "Epoch 1798, Loss Train(MSE): 0.12617049656163776, R2 Train: 0.49531801375344897\n",
      "Epoch 1799, Loss Train(MSE): 0.12616710028739256, R2 Train: 0.49533159885042977\n",
      "Epoch 1800, Loss Train(MSE): 0.12616389580443982, R2 Train: 0.4953444167822407\n",
      "Epoch 1801, Loss Train(MSE): 0.1261604769228359, R2 Train: 0.49535809230865635\n",
      "Epoch 1802, Loss Train(MSE): 0.1261573985911474, R2 Train: 0.49537040563541035\n",
      "Epoch 1803, Loss Train(MSE): 0.1261539493513136, R2 Train: 0.4953842025947456\n",
      "Epoch 1804, Loss Train(MSE): 0.12615100199009843, R2 Train: 0.49539599203960627\n",
      "Epoch 1805, Loss Train(MSE): 0.12614760565529293, R2 Train: 0.4954095773788283\n",
      "Epoch 1806, Loss Train(MSE): 0.12614461430890705, R2 Train: 0.4954215427643718\n",
      "Epoch 1807, Loss Train(MSE): 0.12614136100998372, R2 Train: 0.49543455596006514\n",
      "Epoch 1808, Loss Train(MSE): 0.1261383159468419, R2 Train: 0.49544673621263235\n",
      "Epoch 1809, Loss Train(MSE): 0.12613520993593708, R2 Train: 0.4954591602562517\n",
      "Epoch 1810, Loss Train(MSE): 0.12613210486534107, R2 Train: 0.4954715805386357\n",
      "Epoch 1811, Loss Train(MSE): 0.12612914988966326, R2 Train: 0.495483400441347\n",
      "Epoch 1812, Loss Train(MSE): 0.12612597885726756, R2 Train: 0.49549608457092975\n",
      "Epoch 1813, Loss Train(MSE): 0.1261231784244971, R2 Train: 0.4955072863020116\n",
      "Epoch 1814, Loss Train(MSE): 0.12612001561103844, R2 Train: 0.4955199375558462\n",
      "Epoch 1815, Loss Train(MSE): 0.12611721491558586, R2 Train: 0.49553114033765655\n",
      "Epoch 1816, Loss Train(MSE): 0.12611417810511222, R2 Train: 0.49554328757955113\n",
      "Epoch 1817, Loss Train(MSE): 0.12611129283473937, R2 Train: 0.4955548286610425\n",
      "Epoch 1818, Loss Train(MSE): 0.1261094400608006, R2 Train: 0.4955622397567976\n",
      "Epoch 1819, Loss Train(MSE): 0.12610789424657015, R2 Train: 0.4955684230137194\n",
      "Epoch 1820, Loss Train(MSE): 0.12610661054337072, R2 Train: 0.49557355782651713\n",
      "Epoch 1821, Loss Train(MSE): 0.12610500823414209, R2 Train: 0.49557996706343166\n",
      "Epoch 1822, Loss Train(MSE): 0.1261037312317822, R2 Train: 0.4955850750728712\n",
      "Epoch 1823, Loss Train(MSE): 0.1261022094164696, R2 Train: 0.4955911623341216\n",
      "Epoch 1824, Loss Train(MSE): 0.12610080157723774, R2 Train: 0.49559679369104903\n",
      "Epoch 1825, Loss Train(MSE): 0.12609942884325084, R2 Train: 0.4956022846269966\n",
      "Epoch 1826, Loss Train(MSE): 0.1260978887689329, R2 Train: 0.49560844492426837\n",
      "Epoch 1827, Loss Train(MSE): 0.12609666619149498, R2 Train: 0.49561333523402007\n",
      "Epoch 1828, Loss Train(MSE): 0.12609510453677167, R2 Train: 0.4956195818529133\n",
      "Epoch 1829, Loss Train(MSE): 0.12609380965099432, R2 Train: 0.49562476139602274\n",
      "Epoch 1830, Loss Train(MSE): 0.1260923705717998, R2 Train: 0.49563051771280076\n",
      "Epoch 1831, Loss Train(MSE): 0.12609093753071857, R2 Train: 0.49563624987712573\n",
      "Epoch 1832, Loss Train(MSE): 0.12608965369505468, R2 Train: 0.4956413852197813\n",
      "Epoch 1833, Loss Train(MSE): 0.12608812002528677, R2 Train: 0.4956475198988529\n",
      "Epoch 1834, Loss Train(MSE): 0.12608691542494724, R2 Train: 0.49565233830021105\n",
      "Epoch 1835, Loss Train(MSE): 0.12608543050421586, R2 Train: 0.49565827798313655\n",
      "Epoch 1836, Loss Train(MSE): 0.12608408243540523, R2 Train: 0.4956636702583791\n",
      "Epoch 1837, Loss Train(MSE): 0.12608275730919205, R2 Train: 0.4956689707632318\n",
      "Epoch 1838, Loss Train(MSE): 0.12608126484620072, R2 Train: 0.4956749406151971\n",
      "Epoch 1839, Loss Train(MSE): 0.12608010017964422, R2 Train: 0.49567959928142313\n",
      "Epoch 1840, Loss Train(MSE): 0.1260786026981519, R2 Train: 0.49568558920739236\n",
      "Epoch 1841, Loss Train(MSE): 0.12607731901785507, R2 Train: 0.4956907239285797\n",
      "Epoch 1842, Loss Train(MSE): 0.12607597128903242, R2 Train: 0.49569611484387033\n",
      "Epoch 1843, Loss Train(MSE): 0.12607453861931806, R2 Train: 0.4957018455227278\n",
      "Epoch 1844, Loss Train(MSE): 0.12607335526787772, R2 Train: 0.4957065789284891\n",
      "Epoch 1845, Loss Train(MSE): 0.12607188282519505, R2 Train: 0.4957124686992198\n",
      "Epoch 1846, Loss Train(MSE): 0.1260706448536867, R2 Train: 0.49571742058525325\n",
      "Epoch 1847, Loss Train(MSE): 0.1260692914356108, R2 Train: 0.4957228342575568\n",
      "Epoch 1848, Loss Train(MSE): 0.1260679003708393, R2 Train: 0.49572839851664285\n",
      "Epoch 1849, Loss Train(MSE): 0.12606671481136963, R2 Train: 0.49573314075452146\n",
      "Epoch 1850, Loss Train(MSE): 0.12606526631940038, R2 Train: 0.4957389347223985\n",
      "Epoch 1851, Loss Train(MSE): 0.12606405686398206, R2 Train: 0.49574377254407176\n",
      "Epoch 1852, Loss Train(MSE): 0.12606271332371233, R2 Train: 0.4957491467051507\n",
      "Epoch 1853, Loss Train(MSE): 0.12606134712068792, R2 Train: 0.4957546115172483\n",
      "Epoch 1854, Loss Train(MSE): 0.1260601745200401, R2 Train: 0.49575930191983963\n",
      "Epoch 1855, Loss Train(MSE): 0.12605874897399702, R2 Train: 0.4957650041040119\n",
      "Epoch 1856, Loss Train(MSE): 0.12605755220968945, R2 Train: 0.4957697911612422\n",
      "Epoch 1857, Loss Train(MSE): 0.1260562328731653, R2 Train: 0.49577506850733877\n",
      "Epoch 1858, Loss Train(MSE): 0.12605487611840413, R2 Train: 0.4957804955263835\n",
      "Epoch 1859, Loss Train(MSE): 0.12605373043534632, R2 Train: 0.49578507825861473\n",
      "Epoch 1860, Loss Train(MSE): 0.12605232690573176, R2 Train: 0.49579069237707296\n",
      "Epoch 1861, Loss Train(MSE): 0.12605112826671633, R2 Train: 0.4957954869331347\n",
      "Epoch 1862, Loss Train(MSE): 0.12604984631474772, R2 Train: 0.4958006147410091\n",
      "Epoch 1863, Loss Train(MSE): 0.12604848481939682, R2 Train: 0.49580606072241273\n",
      "Epoch 1864, Loss Train(MSE): 0.1260473788976782, R2 Train: 0.4958104844092872\n",
      "Epoch 1865, Loss Train(MSE): 0.1260459965231274, R2 Train: 0.4958160139074904\n",
      "Epoch 1866, Loss Train(MSE): 0.1260447826038398, R2 Train: 0.4958208695846408\n",
      "Epoch 1867, Loss Train(MSE): 0.12604355015982407, R2 Train: 0.4958257993607037\n",
      "Epoch 1868, Loss Train(MSE): 0.12604218072644177, R2 Train: 0.4958312770942329\n",
      "Epoch 1869, Loss Train(MSE): 0.1260411069030026, R2 Train: 0.4958355723879896\n",
      "Epoch 1870, Loss Train(MSE): 0.12603975493618527, R2 Train: 0.4958409802552589\n",
      "Epoch 1871, Loss Train(MSE): 0.12603851353703605, R2 Train: 0.4958459458518558\n",
      "Epoch 1872, Loss Train(MSE): 0.12603734159415614, R2 Train: 0.49585063362337545\n",
      "Epoch 1873, Loss Train(MSE): 0.12603599202700724, R2 Train: 0.495856031891971\n",
      "Epoch 1874, Loss Train(MSE): 0.1260348813765642, R2 Train: 0.49586047449374315\n",
      "Epoch 1875, Loss Train(MSE): 0.12603359855615798, R2 Train: 0.4958656057753681\n",
      "Epoch 1876, Loss Train(MSE): 0.1260323183363124, R2 Train: 0.4958707266547504\n",
      "Epoch 1877, Loss Train(MSE): 0.126031217132368, R2 Train: 0.495875131470528\n",
      "Epoch 1878, Loss Train(MSE): 0.12602988672182816, R2 Train: 0.49588045311268736\n",
      "Epoch 1879, Loss Train(MSE): 0.12602872854054384, R2 Train: 0.49588508583782465\n",
      "Epoch 1880, Loss Train(MSE): 0.12602752451563595, R2 Train: 0.4958899019374562\n",
      "Epoch 1881, Loss Train(MSE): 0.1260262058531831, R2 Train: 0.49589517658726756\n",
      "Epoch 1882, Loss Train(MSE): 0.12604008226886448, R2 Train: 0.49583967092454206\n",
      "Epoch 1883, Loss Train(MSE): 0.12609268921837055, R2 Train: 0.4956292431265178\n",
      "Epoch 1884, Loss Train(MSE): 0.12608977015669404, R2 Train: 0.49564091937322385\n",
      "Epoch 1885, Loss Train(MSE): 0.1260868526376807, R2 Train: 0.49565258944927715\n",
      "Epoch 1886, Loss Train(MSE): 0.12608390832957372, R2 Train: 0.4956643666817051\n",
      "Epoch 1887, Loss Train(MSE): 0.12608110296523184, R2 Train: 0.49567558813907264\n",
      "Epoch 1888, Loss Train(MSE): 0.12607812702391188, R2 Train: 0.49568749190435246\n",
      "Epoch 1889, Loss Train(MSE): 0.1260807157972267, R2 Train: 0.49567713681109316\n",
      "Epoch 1890, Loss Train(MSE): 0.126086488741363, R2 Train: 0.495654045034548\n",
      "Epoch 1891, Loss Train(MSE): 0.12608338445563322, R2 Train: 0.4956664621774671\n",
      "Epoch 1892, Loss Train(MSE): 0.12608030083376057, R2 Train: 0.49567879666495773\n",
      "Epoch 1893, Loss Train(MSE): 0.12607723763821424, R2 Train: 0.49569104944714304\n",
      "Epoch 1894, Loss Train(MSE): 0.1260741946353816, R2 Train: 0.49570322145847356\n",
      "Epoch 1895, Loss Train(MSE): 0.12607117159548595, R2 Train: 0.4957153136180562\n",
      "Epoch 1896, Loss Train(MSE): 0.12606816829250594, R2 Train: 0.4957273268299762\n",
      "Epoch 1897, Loss Train(MSE): 0.12606518450409754, R2 Train: 0.49573926198360985\n",
      "Epoch 1898, Loss Train(MSE): 0.12606222001151743, R2 Train: 0.4957511199539303\n",
      "Epoch 1899, Loss Train(MSE): 0.1260592745995486, R2 Train: 0.4957629016018056\n",
      "Epoch 1900, Loss Train(MSE): 0.12605634805642765, R2 Train: 0.4957746077742894\n",
      "Epoch 1901, Loss Train(MSE): 0.12605344017377398, R2 Train: 0.49578623930490406\n",
      "Epoch 1902, Loss Train(MSE): 0.12605055074652066, R2 Train: 0.49579779701391735\n",
      "Epoch 1903, Loss Train(MSE): 0.12604767957284682, R2 Train: 0.4958092817086127\n",
      "Epoch 1904, Loss Train(MSE): 0.1260449412240382, R2 Train: 0.4958202351038472\n",
      "Epoch 1905, Loss Train(MSE): 0.1260433765832069, R2 Train: 0.49582649366717235\n",
      "Epoch 1906, Loss Train(MSE): 0.12604181654248367, R2 Train: 0.4958327338300653\n",
      "Epoch 1907, Loss Train(MSE): 0.12604026107702157, R2 Train: 0.4958389556919137\n",
      "Epoch 1908, Loss Train(MSE): 0.12603871016223286, R2 Train: 0.49584515935106854\n",
      "Epoch 1909, Loss Train(MSE): 0.12603716377378468, R2 Train: 0.49585134490486127\n",
      "Epoch 1910, Loss Train(MSE): 0.12603562188759493, R2 Train: 0.4958575124496203\n",
      "Epoch 1911, Loss Train(MSE): 0.12603408447982786, R2 Train: 0.49586366208068855\n",
      "Epoch 1912, Loss Train(MSE): 0.1260325515268903, R2 Train: 0.4958697938924388\n",
      "Epoch 1913, Loss Train(MSE): 0.12603102300542746, R2 Train: 0.4958759079782902\n",
      "Epoch 1914, Loss Train(MSE): 0.12602949889231893, R2 Train: 0.4958820044307243\n",
      "Epoch 1915, Loss Train(MSE): 0.1260279791646752, R2 Train: 0.4958880833412992\n",
      "Epoch 1916, Loss Train(MSE): 0.12602646379983362, R2 Train: 0.4958941448006655\n",
      "Epoch 1917, Loss Train(MSE): 0.12602495277535486, R2 Train: 0.49590018889858056\n",
      "Epoch 1918, Loss Train(MSE): 0.12602344606901913, R2 Train: 0.49590621572392346\n",
      "Epoch 1919, Loss Train(MSE): 0.12602194365882313, R2 Train: 0.49591222536470747\n",
      "Epoch 1920, Loss Train(MSE): 0.12602044552297587, R2 Train: 0.49591821790809654\n",
      "Epoch 1921, Loss Train(MSE): 0.12601895163989604, R2 Train: 0.49592419344041583\n",
      "Epoch 1922, Loss Train(MSE): 0.12601746198820826, R2 Train: 0.49593015204716695\n",
      "Epoch 1923, Loss Train(MSE): 0.1260159765467399, R2 Train: 0.4959360938130404\n",
      "Epoch 1924, Loss Train(MSE): 0.12601471980425716, R2 Train: 0.49594112078297137\n",
      "Epoch 1925, Loss Train(MSE): 0.1260134703444754, R2 Train: 0.49594611862209836\n",
      "Epoch 1926, Loss Train(MSE): 0.12601236227315066, R2 Train: 0.49595055090739737\n",
      "Epoch 1927, Loss Train(MSE): 0.12601103515686216, R2 Train: 0.4959558593725514\n",
      "Epoch 1928, Loss Train(MSE): 0.12600995983026925, R2 Train: 0.495960160678923\n",
      "Epoch 1929, Loss Train(MSE): 0.12600869891089903, R2 Train: 0.4959652043564039\n",
      "Epoch 1930, Loss Train(MSE): 0.1260074846713659, R2 Train: 0.49597006131453636\n",
      "Epoch 1931, Loss Train(MSE): 0.12600637543431867, R2 Train: 0.4959744982627253\n",
      "Epoch 1932, Loss Train(MSE): 0.126005069030524, R2 Train: 0.495979723877904\n",
      "Epoch 1933, Loss Train(MSE): 0.12600401762484761, R2 Train: 0.49598392950060954\n",
      "Epoch 1934, Loss Train(MSE): 0.12600276600477794, R2 Train: 0.4959889359808882\n",
      "Epoch 1935, Loss Train(MSE): 0.12600157252965208, R2 Train: 0.4959937098813917\n",
      "Epoch 1936, Loss Train(MSE): 0.12600047527057995, R2 Train: 0.4959980989176802\n",
      "Epoch 1937, Loss Train(MSE): 0.12599918874268998, R2 Train: 0.4960032450292401\n",
      "Epoch 1938, Loss Train(MSE): 0.12599814756351016, R2 Train: 0.49600740974595936\n",
      "Epoch 1939, Loss Train(MSE): 0.1259969176868997, R2 Train: 0.49601232925240124\n",
      "Epoch 1940, Loss Train(MSE): 0.12599573162345884, R2 Train: 0.49601707350616464\n",
      "Epoch 1941, Loss Train(MSE): 0.12599465848040095, R2 Train: 0.4960213660783962\n",
      "Epoch 1942, Loss Train(MSE): 0.12599339105216203, R2 Train: 0.49602643579135186\n",
      "Epoch 1943, Loss Train(MSE): 0.12599234745200233, R2 Train: 0.4960306101919907\n",
      "Epoch 1944, Loss Train(MSE): 0.1259911508076781, R2 Train: 0.49603539676928765\n",
      "Epoch 1945, Loss Train(MSE): 0.12598995982260675, R2 Train: 0.496040160709573\n",
      "Epoch 1946, Loss Train(MSE): 0.1259889220023958, R2 Train: 0.49604431199041676\n",
      "Epoch 1947, Loss Train(MSE): 0.12598767295237248, R2 Train: 0.4960493081905101\n",
      "Epoch 1948, Loss Train(MSE): 0.12598661525171148, R2 Train: 0.4960535389931541\n",
      "Epoch 1949, Loss Train(MSE): 0.12598546244350958, R2 Train: 0.49605815022596167\n",
      "Epoch 1950, Loss Train(MSE): 0.12598425514630252, R2 Train: 0.49606297941478994\n",
      "Epoch 1951, Loss Train(MSE): 0.12598326299287368, R2 Train: 0.49606694802850526\n",
      "Epoch 1952, Loss Train(MSE): 0.12598203164949734, R2 Train: 0.49607187340201064\n",
      "Epoch 1953, Loss Train(MSE): 0.12598094906444457, R2 Train: 0.4960762037422217\n",
      "Epoch 1954, Loss Train(MSE): 0.12597984987579125, R2 Train: 0.496080600496835\n",
      "Epoch 1955, Loss Train(MSE): 0.12597862938627677, R2 Train: 0.4960854824548929\n",
      "Epoch 1956, Loss Train(MSE): 0.12597766536963909, R2 Train: 0.49608933852144366\n",
      "Epoch 1957, Loss Train(MSE): 0.1259764649124044, R2 Train: 0.49609414035038235\n",
      "Epoch 1958, Loss Train(MSE): 0.12597534759465295, R2 Train: 0.4960986096213882\n",
      "Epoch 1959, Loss Train(MSE): 0.12597431092802347, R2 Train: 0.4961027562879061\n",
      "Epoch 1960, Loss Train(MSE): 0.12597310713425766, R2 Train: 0.49610757146296935\n",
      "Epoch 1961, Loss Train(MSE): 0.12597210074227333, R2 Train: 0.4961115970309067\n",
      "Epoch 1962, Loss Train(MSE): 0.12597096989752427, R2 Train: 0.49611612040990294\n",
      "Epoch 1963, Loss Train(MSE): 0.12596980866723936, R2 Train: 0.49612076533104255\n",
      "Epoch 1964, Loss Train(MSE): 0.1259688428340832, R2 Train: 0.49612462866366724\n",
      "Epoch 1965, Loss Train(MSE): 0.12596765517660138, R2 Train: 0.49612937929359446\n",
      "Epoch 1966, Loss Train(MSE): 0.1259665977278699, R2 Train: 0.49613360908852044\n",
      "Epoch 1967, Loss Train(MSE): 0.1259655443444698, R2 Train: 0.4961378226221208\n",
      "Epoch 1968, Loss Train(MSE): 0.1259643665999098, R2 Train: 0.49614253360036076\n",
      "Epoch 1969, Loss Train(MSE): 0.12596340768264275, R2 Train: 0.496146369269429\n",
      "Epoch 1970, Loss Train(MSE): 0.12596227168293214, R2 Train: 0.49615091326827143\n",
      "Epoch 1971, Loss Train(MSE): 0.125961155242902, R2 Train: 0.496155379028392\n",
      "Epoch 1972, Loss Train(MSE): 0.1259601864639702, R2 Train: 0.4961592541441192\n",
      "Epoch 1973, Loss Train(MSE): 0.1259590240078372, R2 Train: 0.4961639039686512\n",
      "Epoch 1974, Loss Train(MSE): 0.12595799972461394, R2 Train: 0.49616800110154424\n",
      "Epoch 1975, Loss Train(MSE): 0.1259932534571301, R2 Train: 0.49602698617147956\n",
      "Epoch 1976, Loss Train(MSE): 0.12602193254104954, R2 Train: 0.49591226983580183\n",
      "Epoch 1977, Loss Train(MSE): 0.12601950954261948, R2 Train: 0.4959219618295221\n",
      "Epoch 1978, Loss Train(MSE): 0.1260167804490117, R2 Train: 0.49593287820395315\n",
      "Epoch 1979, Loss Train(MSE): 0.12601432343889518, R2 Train: 0.4959427062444193\n",
      "Epoch 1980, Loss Train(MSE): 0.12601170075110601, R2 Train: 0.49595319699557594\n",
      "Epoch 1981, Loss Train(MSE): 0.12600920464656892, R2 Train: 0.4959631814137243\n",
      "Epoch 1982, Loss Train(MSE): 0.12600669160519198, R2 Train: 0.49597323357923206\n",
      "Epoch 1983, Loss Train(MSE): 0.12600415157030423, R2 Train: 0.4959833937187831\n",
      "Epoch 1984, Loss Train(MSE): 0.1260017512345475, R2 Train: 0.49599299506181005\n",
      "Epoch 1985, Loss Train(MSE): 0.12599916266887162, R2 Train: 0.4960033493245135\n",
      "Epoch 1986, Loss Train(MSE): 0.1259968779250034, R2 Train: 0.49601248829998645\n",
      "Epoch 1987, Loss Train(MSE): 0.12599429216401464, R2 Train: 0.49602283134394143\n",
      "Epoch 1988, Loss Train(MSE): 0.12599201548215472, R2 Train: 0.49603193807138113\n",
      "Epoch 1989, Loss Train(MSE): 0.12598952053869253, R2 Train: 0.4960419178452299\n",
      "Epoch 1990, Loss Train(MSE): 0.12598718093933622, R2 Train: 0.4960512762426551\n",
      "Epoch 1991, Loss Train(MSE): 0.12598481182367258, R2 Train: 0.4960607527053097\n",
      "Epoch 1992, Loss Train(MSE): 0.12598240552213824, R2 Train: 0.49607037791144704\n",
      "Epoch 1993, Loss Train(MSE): 0.12598016450743657, R2 Train: 0.4960793419702537\n",
      "Epoch 1994, Loss Train(MSE): 0.12597770313690712, R2 Train: 0.4960891874523715\n",
      "Epoch 1995, Loss Train(MSE): 0.1259755629823935, R2 Train: 0.49609774807042595\n",
      "Epoch 1996, Loss Train(MSE): 0.12597314898747775, R2 Train: 0.496107404050089\n",
      "Epoch 1997, Loss Train(MSE): 0.12597092993033165, R2 Train: 0.4961162802786734\n",
      "Epoch 1998, Loss Train(MSE): 0.12596865256905457, R2 Train: 0.49612538972378173\n",
      "Epoch 1999, Loss Train(MSE): 0.12596635151552163, R2 Train: 0.4961345939379135\n",
      "Epoch 2000, Loss Train(MSE): 0.12596454703358412, R2 Train: 0.49614181186566353\n",
      "Epoch 2001, Loss Train(MSE): 0.1259633046262562, R2 Train: 0.49614678149497515\n",
      "Epoch 2002, Loss Train(MSE): 0.12596232384192713, R2 Train: 0.4961507046322915\n",
      "Epoch 2003, Loss Train(MSE): 0.1259611279787725, R2 Train: 0.49615548808490995\n",
      "Epoch 2004, Loss Train(MSE): 0.12596003556112453, R2 Train: 0.49615985775550187\n",
      "Epoch 2005, Loss Train(MSE): 0.1259589637260096, R2 Train: 0.49616414509596163\n",
      "Epoch 2006, Loss Train(MSE): 0.1259577588776256, R2 Train: 0.49616896448949765\n",
      "Epoch 2007, Loss Train(MSE): 0.1259568116786968, R2 Train: 0.4961727532852128\n",
      "Epoch 2008, Loss Train(MSE): 0.1259555968201177, R2 Train: 0.4961776127195292\n",
      "Epoch 2009, Loss Train(MSE): 0.1259545687945244, R2 Train: 0.49618172482190237\n",
      "Epoch 2010, Loss Train(MSE): 0.12595346435042606, R2 Train: 0.49618614259829574\n",
      "Epoch 2011, Loss Train(MSE): 0.12595232018408162, R2 Train: 0.49619071926367353\n",
      "Epoch 2012, Loss Train(MSE): 0.12595134359347945, R2 Train: 0.4961946256260822\n",
      "Epoch 2013, Loss Train(MSE): 0.12595014784706499, R2 Train: 0.49619940861174006\n",
      "Epoch 2014, Loss Train(MSE): 0.12594916948057988, R2 Train: 0.4962033220776805\n",
      "Epoch 2015, Loss Train(MSE): 0.12594804587555952, R2 Train: 0.4962078164977619\n",
      "Epoch 2016, Loss Train(MSE): 0.12594694801380732, R2 Train: 0.4962122079447707\n",
      "Epoch 2017, Loss Train(MSE): 0.12594595516153573, R2 Train: 0.4962161793538571\n",
      "Epoch 2018, Loss Train(MSE): 0.12594477773598736, R2 Train: 0.49622088905605055\n",
      "Epoch 2019, Loss Train(MSE): 0.12594383536907375, R2 Train: 0.496224658523705\n",
      "Epoch 2020, Loss Train(MSE): 0.1259427050738303, R2 Train: 0.49622917970467884\n",
      "Epoch 2021, Loss Train(MSE): 0.12594164018470155, R2 Train: 0.4962334392611938\n",
      "Epoch 2022, Loss Train(MSE): 0.12594064324779733, R2 Train: 0.4962374270088107\n",
      "Epoch 2023, Loss Train(MSE): 0.12593948340864392, R2 Train: 0.49624206636542434\n",
      "Epoch 2024, Loss Train(MSE): 0.12593856437558187, R2 Train: 0.4962457424976725\n",
      "Epoch 2025, Loss Train(MSE): 0.1259374389541028, R2 Train: 0.49625024418358876\n",
      "Epoch 2026, Loss Train(MSE): 0.12593639467391324, R2 Train: 0.49625442130434705\n",
      "Epoch 2027, Loss Train(MSE): 0.12593540494506325, R2 Train: 0.496258380219747\n",
      "Epoch 2028, Loss Train(MSE): 0.12593426200957886, R2 Train: 0.4962629519616846\n",
      "Epoch 2029, Loss Train(MSE): 0.12593335456552532, R2 Train: 0.4962665817378987\n",
      "Epoch 2030, Loss Train(MSE): 0.1259405685758878, R2 Train: 0.49623772569644875\n",
      "Epoch 2031, Loss Train(MSE): 0.1259443720929868, R2 Train: 0.49622251162805275\n",
      "Epoch 2032, Loss Train(MSE): 0.1259430821650278, R2 Train: 0.49622767133988877\n",
      "Epoch 2033, Loss Train(MSE): 0.12594179550576043, R2 Train: 0.4962328179769583\n",
      "Epoch 2034, Loss Train(MSE): 0.12594051210243334, R2 Train: 0.4962379515902666\n",
      "Epoch 2035, Loss Train(MSE): 0.12593923194237244, R2 Train: 0.49624307223051023\n",
      "Epoch 2036, Loss Train(MSE): 0.12593795501298008, R2 Train: 0.4962481799480797\n",
      "Epoch 2037, Loss Train(MSE): 0.12593668130173458, R2 Train: 0.4962532747930617\n",
      "Epoch 2038, Loss Train(MSE): 0.1259354107961891, R2 Train: 0.49625835681524355\n",
      "Epoch 2039, Loss Train(MSE): 0.1259341434839713, R2 Train: 0.4962634260641148\n",
      "Epoch 2040, Loss Train(MSE): 0.12593287935278208, R2 Train: 0.4962684825888717\n",
      "Epoch 2041, Loss Train(MSE): 0.12593161839039552, R2 Train: 0.49627352643841793\n",
      "Epoch 2042, Loss Train(MSE): 0.12593036058465762, R2 Train: 0.4962785576613695\n",
      "Epoch 2043, Loss Train(MSE): 0.12592910592348594, R2 Train: 0.49628357630605624\n",
      "Epoch 2044, Loss Train(MSE): 0.12592785439486864, R2 Train: 0.4962885824205254\n",
      "Epoch 2045, Loss Train(MSE): 0.12592660598686425, R2 Train: 0.496293576052543\n",
      "Epoch 2046, Loss Train(MSE): 0.1259253606876004, R2 Train: 0.49629855724959837\n",
      "Epoch 2047, Loss Train(MSE): 0.12592411848527368, R2 Train: 0.49630352605890526\n",
      "Epoch 2048, Loss Train(MSE): 0.12592287936814878, R2 Train: 0.4963084825274049\n",
      "Epoch 2049, Loss Train(MSE): 0.12592164332455785, R2 Train: 0.4963134267017686\n",
      "Epoch 2050, Loss Train(MSE): 0.12592041034289986, R2 Train: 0.49631835862840057\n",
      "Epoch 2051, Loss Train(MSE): 0.12591918041164005, R2 Train: 0.4963232783534398\n",
      "Epoch 2052, Loss Train(MSE): 0.12591795351930937, R2 Train: 0.4963281859227625\n",
      "Epoch 2053, Loss Train(MSE): 0.12591672965450362, R2 Train: 0.4963330813819855\n",
      "Epoch 2054, Loss Train(MSE): 0.12591550880588318, R2 Train: 0.4963379647764673\n",
      "Epoch 2055, Loss Train(MSE): 0.12591429096217227, R2 Train: 0.4963428361513109\n",
      "Epoch 2056, Loss Train(MSE): 0.12591307611215832, R2 Train: 0.4963476955513667\n",
      "Epoch 2057, Loss Train(MSE): 0.1259118642446916, R2 Train: 0.49635254302123355\n",
      "Epoch 2058, Loss Train(MSE): 0.12591065534868442, R2 Train: 0.4963573786052623\n",
      "Epoch 2059, Loss Train(MSE): 0.1259094494131107, R2 Train: 0.49636220234755724\n",
      "Epoch 2060, Loss Train(MSE): 0.1259082464270054, R2 Train: 0.4963670142919784\n",
      "Epoch 2061, Loss Train(MSE): 0.12590704637946415, R2 Train: 0.4963718144821434\n",
      "Epoch 2062, Loss Train(MSE): 0.12590584925964235, R2 Train: 0.4963766029614306\n",
      "Epoch 2063, Loss Train(MSE): 0.12590465505675508, R2 Train: 0.4963813797729797\n",
      "Epoch 2064, Loss Train(MSE): 0.12590346376007622, R2 Train: 0.49638614495969513\n",
      "Epoch 2065, Loss Train(MSE): 0.1259022753589381, R2 Train: 0.49639089856424756\n",
      "Epoch 2066, Loss Train(MSE): 0.1259010898427311, R2 Train: 0.4963956406290756\n",
      "Epoch 2067, Loss Train(MSE): 0.125899907200903, R2 Train: 0.496400371196388\n",
      "Epoch 2068, Loss Train(MSE): 0.1258988134530709, R2 Train: 0.49640474618771635\n",
      "Epoch 2069, Loss Train(MSE): 0.1258979149618692, R2 Train: 0.4964083401525232\n",
      "Epoch 2070, Loss Train(MSE): 0.12589697483689133, R2 Train: 0.4964121006524347\n",
      "Epoch 2071, Loss Train(MSE): 0.1258959517338395, R2 Train: 0.496416193064642\n",
      "Epoch 2072, Loss Train(MSE): 0.1258951234205064, R2 Train: 0.4964195063179744\n",
      "Epoch 2073, Loss Train(MSE): 0.12590118113771, R2 Train: 0.49639527544915996\n",
      "Epoch 2074, Loss Train(MSE): 0.12595633182358854, R2 Train: 0.49617467270564586\n",
      "Epoch 2075, Loss Train(MSE): 0.1259541701825362, R2 Train: 0.49618331926985515\n",
      "Epoch 2076, Loss Train(MSE): 0.12595172982224692, R2 Train: 0.4961930807110123\n",
      "Epoch 2077, Loss Train(MSE): 0.125949622452784, R2 Train: 0.49620151018886405\n",
      "Epoch 2078, Loss Train(MSE): 0.12594723565449895, R2 Train: 0.4962110573820042\n",
      "Epoch 2079, Loss Train(MSE): 0.1259450853530247, R2 Train: 0.4962196585879012\n",
      "Epoch 2080, Loss Train(MSE): 0.12594280048864362, R2 Train: 0.4962287980454255\n",
      "Epoch 2081, Loss Train(MSE): 0.12594060337251903, R2 Train: 0.4962375865099239\n",
      "Epoch 2082, Loss Train(MSE): 0.12593842291730312, R2 Train: 0.49624630833078753\n",
      "Epoch 2083, Loss Train(MSE): 0.12593617528649578, R2 Train: 0.49625529885401687\n",
      "Epoch 2084, Loss Train(MSE): 0.12593410158014454, R2 Train: 0.49626359367942185\n",
      "Epoch 2085, Loss Train(MSE): 0.12593180892424602, R2 Train: 0.4962727643030159\n",
      "Epoch 2086, Loss Train(MSE): 0.12592982713415724, R2 Train: 0.49628069146337106\n",
      "Epoch 2087, Loss Train(MSE): 0.12592757298638482, R2 Train: 0.4962897080544607\n",
      "Epoch 2088, Loss Train(MSE): 0.125925528944016, R2 Train: 0.49629788422393595\n",
      "Epoch 2089, Loss Train(MSE): 0.12592338997823255, R2 Train: 0.4963064400870698\n",
      "Epoch 2090, Loss Train(MSE): 0.1259212806205164, R2 Train: 0.49631487751793435\n",
      "Epoch 2091, Loss Train(MSE): 0.1259192586926226, R2 Train: 0.4963229652295096\n",
      "Epoch 2092, Loss Train(MSE): 0.12591708110464772, R2 Train: 0.4963316755814091\n",
      "Epoch 2093, Loss Train(MSE): 0.12591517796126397, R2 Train: 0.4963392881549441\n",
      "Epoch 2094, Loss Train(MSE): 0.12591301752384756, R2 Train: 0.49634792990460974\n",
      "Epoch 2095, Loss Train(MSE): 0.12591105939630776, R2 Train: 0.496355762414769\n",
      "Epoch 2096, Loss Train(MSE): 0.12590901363171042, R2 Train: 0.4963639454731583\n",
      "Epoch 2097, Loss Train(MSE): 0.12590697765910575, R2 Train: 0.496372089363577\n",
      "Epoch 2098, Loss Train(MSE): 0.12590505744884645, R2 Train: 0.4963797702046142\n",
      "Epoch 2099, Loss Train(MSE): 0.12590296565737444, R2 Train: 0.49638813737050225\n",
      "Epoch 2100, Loss Train(MSE): 0.12590112436427, R2 Train: 0.49639550254292\n",
      "Epoch 2101, Loss Train(MSE): 0.1258997409785371, R2 Train: 0.4964010360858516\n",
      "Epoch 2102, Loss Train(MSE): 0.12589877999409024, R2 Train: 0.49640488002363903\n",
      "Epoch 2103, Loss Train(MSE): 0.12589782830123833, R2 Train: 0.4964086867950467\n",
      "Epoch 2104, Loss Train(MSE): 0.1258967610157605, R2 Train: 0.496412955936958\n",
      "Epoch 2105, Loss Train(MSE): 0.1258959256036109, R2 Train: 0.4964162975855564\n",
      "Epoch 2106, Loss Train(MSE): 0.12589485299359196, R2 Train: 0.49642058802563216\n",
      "Epoch 2107, Loss Train(MSE): 0.12589393151123848, R2 Train: 0.49642427395504607\n",
      "Epoch 2108, Loss Train(MSE): 0.1258929663145273, R2 Train: 0.49642813474189085\n",
      "Epoch 2109, Loss Train(MSE): 0.12589193573533727, R2 Train: 0.4964322570586509\n",
      "Epoch 2110, Loss Train(MSE): 0.12589108924678852, R2 Train: 0.49643564301284593\n",
      "Epoch 2111, Loss Train(MSE): 0.12589003226633422, R2 Train: 0.49643987093466313\n",
      "Epoch 2112, Loss Train(MSE): 0.12588913877946065, R2 Train: 0.4964434448821574\n",
      "Epoch 2113, Loss Train(MSE): 0.1258881706224517, R2 Train: 0.4964473175101932\n",
      "Epoch 2114, Loss Train(MSE): 0.12588716550692958, R2 Train: 0.4964513379722817\n",
      "Epoch 2115, Loss Train(MSE): 0.12588631824775587, R2 Train: 0.4964547270089765\n",
      "Epoch 2116, Loss Train(MSE): 0.12588527630039256, R2 Train: 0.49645889479842975\n",
      "Epoch 2117, Loss Train(MSE): 0.12588440010561988, R2 Train: 0.4964623995775205\n",
      "Epoch 2118, Loss Train(MSE): 0.12588343879606126, R2 Train: 0.49646624481575496\n",
      "Epoch 2119, Loss Train(MSE): 0.1258824486851259, R2 Train: 0.49647020525949637\n",
      "Epoch 2120, Loss Train(MSE): 0.12588161024263506, R2 Train: 0.4964735590294598\n",
      "Epoch 2121, Loss Train(MSE): 0.12588058277206138, R2 Train: 0.4964776689117545\n",
      "Epoch 2122, Loss Train(MSE): 0.1258797139128557, R2 Train: 0.49648114434857715\n",
      "Epoch 2123, Loss Train(MSE): 0.12587876857311703, R2 Train: 0.4964849257075319\n",
      "Epoch 2124, Loss Train(MSE): 0.1258777837364361, R2 Train: 0.4964888650542556\n",
      "Epoch 2125, Loss Train(MSE): 0.12587696302853615, R2 Train: 0.4964921478858554\n",
      "Epoch 2126, Loss Train(MSE): 0.12587594951521885, R2 Train: 0.4964962019391246\n",
      "Epoch 2127, Loss Train(MSE): 0.12587507872995624, R2 Train: 0.49649968508017506\n",
      "Epoch 2128, Loss Train(MSE): 0.12587415784358968, R2 Train: 0.49650336862564126\n",
      "Epoch 2129, Loss Train(MSE): 0.1258731692290975, R2 Train: 0.49650732308361\n",
      "Epoch 2130, Loss Train(MSE): 0.12587237454961134, R2 Train: 0.49651050180155465\n",
      "Epoch 2131, Loss Train(MSE): 0.12587137450770425, R2 Train: 0.496514501969183\n",
      "Epoch 2132, Loss Train(MSE): 0.12587049318178956, R2 Train: 0.4965180272728418\n",
      "Epoch 2133, Loss Train(MSE): 0.12586960463655614, R2 Train: 0.49652158145377545\n",
      "Epoch 2134, Loss Train(MSE): 0.12586861286427506, R2 Train: 0.49652554854289976\n",
      "Epoch 2135, Loss Train(MSE): 0.1258678340103248, R2 Train: 0.49652866395870077\n",
      "Epoch 2136, Loss Train(MSE): 0.12586685614146345, R2 Train: 0.4965325754341462\n",
      "Epoch 2137, Loss Train(MSE): 0.12586595635819073, R2 Train: 0.4965361745672371\n",
      "Epoch 2138, Loss Train(MSE): 0.1258651073811517, R2 Train: 0.4965395704753932\n",
      "Epoch 2139, Loss Train(MSE): 0.12586412834542404, R2 Train: 0.49654348661830383\n",
      "Epoch 2140, Loss Train(MSE): 0.1258633249452144, R2 Train: 0.4965467002191424\n",
      "Epoch 2141, Loss Train(MSE): 0.1258623923295346, R2 Train: 0.49655043068186155\n",
      "Epoch 2142, Loss Train(MSE): 0.1258614666423643, R2 Train: 0.4965541334305428\n",
      "Epoch 2143, Loss Train(MSE): 0.1258606640440897, R2 Train: 0.49655734382364125\n",
      "Epoch 2144, Loss Train(MSE): 0.12585969733565636, R2 Train: 0.4965612106573746\n",
      "Epoch 2145, Loss Train(MSE): 0.12585886230774404, R2 Train: 0.49656455076902384\n",
      "Epoch 2146, Loss Train(MSE): 0.12585798141560148, R2 Train: 0.4965680743375941\n",
      "Epoch 2147, Loss Train(MSE): 0.12585702290192405, R2 Train: 0.4965719083923038\n",
      "Epoch 2148, Loss Train(MSE): 0.125856273008222, R2 Train: 0.496574907967112\n",
      "Epoch 2149, Loss Train(MSE): 0.12585531824249996, R2 Train: 0.49657872703000017\n",
      "Epoch 2150, Loss Train(MSE): 0.12585444500588308, R2 Train: 0.4965822199764677\n",
      "Epoch 2151, Loss Train(MSE): 0.12585362184431115, R2 Train: 0.4965855126227554\n",
      "Epoch 2152, Loss Train(MSE): 0.12585267442957784, R2 Train: 0.4965893022816886\n",
      "Epoch 2153, Loss Train(MSE): 0.12585188251980817, R2 Train: 0.49659246992076733\n",
      "Epoch 2154, Loss Train(MSE): 0.12585098982179074, R2 Train: 0.49659604071283703\n",
      "Epoch 2155, Loss Train(MSE): 0.1258500723213606, R2 Train: 0.49659971071455755\n",
      "Epoch 2156, Loss Train(MSE): 0.12584931239734562, R2 Train: 0.4966027504106175\n",
      "Epoch 2157, Loss Train(MSE): 0.12584837633898788, R2 Train: 0.4966064946440485\n",
      "Epoch 2158, Loss Train(MSE): 0.12584753531418616, R2 Train: 0.49660985874325536\n",
      "Epoch 2159, Loss Train(MSE): 0.1258467103826024, R2 Train: 0.4966131584695904\n",
      "Epoch 2160, Loss Train(MSE): 0.1258457813212969, R2 Train: 0.4966168747148124\n",
      "Epoch 2161, Loss Train(MSE): 0.1258450131572677, R2 Train: 0.4966199473709292\n",
      "Epoch 2162, Loss Train(MSE): 0.12584412663438593, R2 Train: 0.49662349346245627\n",
      "Epoch 2163, Loss Train(MSE): 0.12584323115383309, R2 Train: 0.49662707538466766\n",
      "Epoch 2164, Loss Train(MSE): 0.1258424788323408, R2 Train: 0.49663008467063685\n",
      "Epoch 2165, Loss Train(MSE): 0.125841560595236, R2 Train: 0.49663375761905604\n",
      "Epoch 2166, Loss Train(MSE): 0.12584073360177392, R2 Train: 0.4966370655929043\n",
      "Epoch 2167, Loss Train(MSE): 0.12583992376985212, R2 Train: 0.4966403049205915\n",
      "Epoch 2168, Loss Train(MSE): 0.12583901220779906, R2 Train: 0.4966439511688038\n",
      "Epoch 2169, Loss Train(MSE): 0.12583825038946536, R2 Train: 0.49664699844213855\n",
      "Epoch 2170, Loss Train(MSE): 0.12583738617817547, R2 Train: 0.4966504552872981\n",
      "Epoch 2171, Loss Train(MSE): 0.12583649565459498, R2 Train: 0.49665401738162007\n",
      "Epoch 2172, Loss Train(MSE): 0.12583576676182123, R2 Train: 0.49665693295271507\n",
      "Epoch 2173, Loss Train(MSE): 0.1258348655393388, R2 Train: 0.49666053784264474\n",
      "Epoch 2174, Loss Train(MSE): 0.12583403624731482, R2 Train: 0.49666385501074073\n",
      "Epoch 2175, Loss Train(MSE): 0.12583325665175335, R2 Train: 0.4966669733929866\n",
      "Epoch 2176, Loss Train(MSE): 0.1258323618104287, R2 Train: 0.4966705527582852\n",
      "Epoch 2177, Loss Train(MSE): 0.12583159071200936, R2 Train: 0.49667363715196255\n",
      "Epoch 2178, Loss Train(MSE): 0.12583664648396692, R2 Train: 0.4966534140641323\n",
      "Epoch 2179, Loss Train(MSE): 0.1258892275515024, R2 Train: 0.49644308979399043\n",
      "Epoch 2180, Loss Train(MSE): 0.1258873572460249, R2 Train: 0.4964505710159004\n",
      "Epoch 2181, Loss Train(MSE): 0.12588522700492866, R2 Train: 0.49645909198028537\n",
      "Epoch 2182, Loss Train(MSE): 0.12588332147947173, R2 Train: 0.49646671408211307\n",
      "Epoch 2183, Loss Train(MSE): 0.12588935341876145, R2 Train: 0.4964425863249542\n",
      "Epoch 2184, Loss Train(MSE): 0.1258914193079938, R2 Train: 0.4964343227680248\n",
      "Epoch 2185, Loss Train(MSE): 0.12588924203792473, R2 Train: 0.4964430318483011\n",
      "Epoch 2186, Loss Train(MSE): 0.12588707720765113, R2 Train: 0.49645169116939547\n",
      "Epoch 2187, Loss Train(MSE): 0.12588492469419924, R2 Train: 0.49646030122320306\n",
      "Epoch 2188, Loss Train(MSE): 0.1258827843763442, R2 Train: 0.4964688624946232\n",
      "Epoch 2189, Loss Train(MSE): 0.12588065613457863, R2 Train: 0.49647737546168547\n",
      "Epoch 2190, Loss Train(MSE): 0.12587853985108122, R2 Train: 0.49648584059567513\n",
      "Epoch 2191, Loss Train(MSE): 0.12587643540968615, R2 Train: 0.4964942583612554\n",
      "Epoch 2192, Loss Train(MSE): 0.12587434269585374, R2 Train: 0.49650262921658506\n",
      "Epoch 2193, Loss Train(MSE): 0.12587226159664097, R2 Train: 0.4965109536134361\n",
      "Epoch 2194, Loss Train(MSE): 0.125870192000673, R2 Train: 0.49651923199730796\n",
      "Epoch 2195, Loss Train(MSE): 0.12586813379811543, R2 Train: 0.4965274648075383\n",
      "Epoch 2196, Loss Train(MSE): 0.12586608688064674, R2 Train: 0.49653565247741305\n",
      "Epoch 2197, Loss Train(MSE): 0.12586405114143165, R2 Train: 0.4965437954342734\n",
      "Epoch 2198, Loss Train(MSE): 0.12586202647509512, R2 Train: 0.49655189409961953\n",
      "Epoch 2199, Loss Train(MSE): 0.12586001277769643, R2 Train: 0.4965599488892143\n",
      "Epoch 2200, Loss Train(MSE): 0.12585800994670432, R2 Train: 0.4965679602131827\n",
      "Epoch 2201, Loss Train(MSE): 0.12585601788097223, R2 Train: 0.4965759284761111\n",
      "Epoch 2202, Loss Train(MSE): 0.12585403648071442, R2 Train: 0.49658385407714234\n",
      "Epoch 2203, Loss Train(MSE): 0.12585206564748244, R2 Train: 0.49659173741007023\n",
      "Epoch 2204, Loss Train(MSE): 0.12585010528414178, R2 Train: 0.49659957886343287\n",
      "Epoch 2205, Loss Train(MSE): 0.12584815529484975, R2 Train: 0.496607378820601\n",
      "Epoch 2206, Loss Train(MSE): 0.12584621558503287, R2 Train: 0.49661513765986853\n",
      "Epoch 2207, Loss Train(MSE): 0.12584428606136566, R2 Train: 0.49662285575453735\n",
      "Epoch 2208, Loss Train(MSE): 0.12584311556690211, R2 Train: 0.49662753773239154\n",
      "Epoch 2209, Loss Train(MSE): 0.12584205501656462, R2 Train: 0.4966317799337415\n",
      "Epoch 2210, Loss Train(MSE): 0.1258409970258581, R2 Train: 0.4966360118965676\n",
      "Epoch 2211, Loss Train(MSE): 0.12583994158423595, R2 Train: 0.4966402336630562\n",
      "Epoch 2212, Loss Train(MSE): 0.125838888681231, R2 Train: 0.496644445275076\n",
      "Epoch 2213, Loss Train(MSE): 0.12583783830645429, R2 Train: 0.49664864677418286\n",
      "Epoch 2214, Loss Train(MSE): 0.12583679044959464, R2 Train: 0.4966528382016214\n",
      "Epoch 2215, Loss Train(MSE): 0.12583574510041723, R2 Train: 0.4966570195983311\n",
      "Epoch 2216, Loss Train(MSE): 0.12583470224876264, R2 Train: 0.49666119100494943\n",
      "Epoch 2217, Loss Train(MSE): 0.12583366188454614, R2 Train: 0.4966653524618154\n",
      "Epoch 2218, Loss Train(MSE): 0.12583262399775655, R2 Train: 0.4966695040089738\n",
      "Epoch 2219, Loss Train(MSE): 0.12583158857845544, R2 Train: 0.4966736456861782\n",
      "Epoch 2220, Loss Train(MSE): 0.12583055561677628, R2 Train: 0.4966777775328949\n",
      "Epoch 2221, Loss Train(MSE): 0.12582952510292325, R2 Train: 0.496681899588307\n",
      "Epoch 2222, Loss Train(MSE): 0.1258284970271709, R2 Train: 0.49668601189131645\n",
      "Epoch 2223, Loss Train(MSE): 0.1258274713798627, R2 Train: 0.4966901144805492\n",
      "Epoch 2224, Loss Train(MSE): 0.1258264481514106, R2 Train: 0.49669420739435755\n",
      "Epoch 2225, Loss Train(MSE): 0.12582543793154996, R2 Train: 0.49669824827380016\n",
      "Epoch 2226, Loss Train(MSE): 0.12582472277671108, R2 Train: 0.49670110889315566\n",
      "Epoch 2227, Loss Train(MSE): 0.12582382642547701, R2 Train: 0.49670469429809194\n",
      "Epoch 2228, Loss Train(MSE): 0.1258230045729929, R2 Train: 0.49670798170802843\n",
      "Epoch 2229, Loss Train(MSE): 0.12582222213319905, R2 Train: 0.4967111114672038\n",
      "Epoch 2230, Loss Train(MSE): 0.12582132236490837, R2 Train: 0.49671471054036653\n",
      "Epoch 2231, Loss Train(MSE): 0.12582059623702738, R2 Train: 0.4967176150518905\n",
      "Epoch 2232, Loss Train(MSE): 0.12581972963820193, R2 Train: 0.4967210814471923\n",
      "Epoch 2233, Loss Train(MSE): 0.12581889539079863, R2 Train: 0.4967244184368055\n",
      "Epoch 2234, Loss Train(MSE): 0.1258181439112047, R2 Train: 0.49672742435518125\n",
      "Epoch 2235, Loss Train(MSE): 0.1258172553656897, R2 Train: 0.49673097853724124\n",
      "Epoch 2236, Loss Train(MSE): 0.12581651137003838, R2 Train: 0.4967339545198465\n",
      "Epoch 2237, Loss Train(MSE): 0.12581568085509637, R2 Train: 0.49673727657961453\n",
      "Epoch 2238, Loss Train(MSE): 0.1258148274666031, R2 Train: 0.49674069013358757\n",
      "Epoch 2239, Loss Train(MSE): 0.12581411314329816, R2 Train: 0.4967435474268074\n",
      "Epoch 2240, Loss Train(MSE): 0.12581323546681727, R2 Train: 0.4967470581327309\n",
      "Epoch 2241, Loss Train(MSE): 0.12581246717282532, R2 Train: 0.4967501313086987\n",
      "Epoch 2242, Loss Train(MSE): 0.12581167864324622, R2 Train: 0.49675328542701513\n",
      "Epoch 2243, Loss Train(MSE): 0.12581080765616207, R2 Train: 0.49675676937535174\n",
      "Epoch 2244, Loss Train(MSE): 0.12581012071956876, R2 Train: 0.49675951712172495\n",
      "Epoch 2245, Loss Train(MSE): 0.1258092615195246, R2 Train: 0.4967629539219016\n",
      "Epoch 2246, Loss Train(MSE): 0.125808462996578, R2 Train: 0.496766148013688\n",
      "Epoch 2247, Loss Train(MSE): 0.12580772187795666, R2 Train: 0.49676911248817335\n",
      "Epoch 2248, Loss Train(MSE): 0.12580686122083357, R2 Train: 0.49677255511666574\n",
      "Epoch 2249, Loss Train(MSE): 0.12580613938828084, R2 Train: 0.49677544244687666\n",
      "Epoch 2250, Loss Train(MSE): 0.1258053319691709, R2 Train: 0.4967786721233164\n",
      "Epoch 2251, Loss Train(MSE): 0.12580449763070825, R2 Train: 0.496782009477167\n",
      "Epoch 2252, Loss Train(MSE): 0.12580380904164679, R2 Train: 0.49678476383341286\n",
      "Epoch 2253, Loss Train(MSE): 0.125802958412545, R2 Train: 0.49678816634982004\n",
      "Epoch 2254, Loss Train(MSE): 0.12580219636370296, R2 Train: 0.49679121454518815\n",
      "Epoch 2255, Loss Train(MSE): 0.1258014455947652, R2 Train: 0.4967942176209392\n",
      "Epoch 2256, Loss Train(MSE): 0.12580060114754324, R2 Train: 0.49679759540982704\n",
      "Epoch 2257, Loss Train(MSE): 0.12579990812860545, R2 Train: 0.4968003674855782\n",
      "Epoch 2258, Loss Train(MSE): 0.12579909826685381, R2 Train: 0.49680360693258474\n",
      "Epoch 2259, Loss Train(MSE): 0.125798291091381, R2 Train: 0.49680683563447603\n",
      "Epoch 2260, Loss Train(MSE): 0.12579760145054217, R2 Train: 0.49680959419783133\n",
      "Epoch 2261, Loss Train(MSE): 0.12579676656877373, R2 Train: 0.4968129337249051\n",
      "Epoch 2262, Loss Train(MSE): 0.12579602443856377, R2 Train: 0.4968159022457449\n",
      "Epoch 2263, Loss Train(MSE): 0.12579527943455723, R2 Train: 0.49681888226177107\n",
      "Epoch 2264, Loss Train(MSE): 0.12579445045460996, R2 Train: 0.49682219818156015\n",
      "Epoch 2265, Loss Train(MSE): 0.12579377037385864, R2 Train: 0.49682491850456545\n",
      "Epoch 2266, Loss Train(MSE): 0.1257929728449149, R2 Train: 0.49682810862034044\n",
      "Epoch 2267, Loss Train(MSE): 0.12579217725316882, R2 Train: 0.4968312909873247\n",
      "Epoch 2268, Loss Train(MSE): 0.12579150106283365, R2 Train: 0.4968339957486654\n",
      "Epoch 2269, Loss Train(MSE): 0.12579068122622292, R2 Train: 0.49683727509510833\n",
      "Epoch 2270, Loss Train(MSE): 0.1257899440755076, R2 Train: 0.49684022369796965\n",
      "Epoch 2271, Loss Train(MSE): 0.1257892187357666, R2 Train: 0.4968431250569336\n",
      "Epoch 2272, Loss Train(MSE): 0.12578840454485754, R2 Train: 0.49684638182056984\n",
      "Epoch 2273, Loss Train(MSE): 0.12578772307894284, R2 Train: 0.49684910768422863\n",
      "Epoch 2274, Loss Train(MSE): 0.12578695120173933, R2 Train: 0.4968521951930427\n",
      "Epoch 2275, Loss Train(MSE): 0.12578615313522784, R2 Train: 0.49685538745908864\n",
      "Epoch 2276, Loss Train(MSE): 0.12578550346964085, R2 Train: 0.4968579861214366\n",
      "Epoch 2277, Loss Train(MSE): 0.12578469803569278, R2 Train: 0.49686120785722887\n",
      "Epoch 2278, Loss Train(MSE): 0.12578395238612852, R2 Train: 0.4968641904554859\n",
      "Epoch 2279, Loss Train(MSE): 0.12578325923748865, R2 Train: 0.4968669630500454\n",
      "Epoch 2280, Loss Train(MSE): 0.12578245921432582, R2 Train: 0.4968701631426967\n",
      "Epoch 2281, Loss Train(MSE): 0.12578176344327283, R2 Train: 0.4968729462269087\n",
      "Epoch 2282, Loss Train(MSE): 0.12578102921746512, R2 Train: 0.49687588313013953\n",
      "Epoch 2283, Loss Train(MSE): 0.12578023451719034, R2 Train: 0.49687906193123865\n",
      "Epoch 2284, Loss Train(MSE): 0.12577958617099527, R2 Train: 0.4968816553160189\n",
      "Epoch 2285, Loss Train(MSE): 0.12577881319343845, R2 Train: 0.4968847472262462\n",
      "Epoch 2286, Loss Train(MSE): 0.12577804690190028, R2 Train: 0.4968878123923989\n",
      "Epoch 2287, Loss Train(MSE): 0.12577739720859368, R2 Train: 0.49689041116562527\n",
      "Epoch 2288, Loss Train(MSE): 0.12577661077809957, R2 Train: 0.49689355688760173\n",
      "Epoch 2289, Loss Train(MSE): 0.12577588907262158, R2 Train: 0.4968964437095137\n",
      "Epoch 2290, Loss Train(MSE): 0.1257752032763855, R2 Train: 0.496899186894458\n",
      "Epoch 2291, Loss Train(MSE): 0.125774421959653, R2 Train: 0.496902312161388\n",
      "Epoch 2292, Loss Train(MSE): 0.1257882507859048, R2 Train: 0.4968469968563808\n",
      "Epoch 2293, Loss Train(MSE): 0.12582928331293364, R2 Train: 0.49668286674826545\n",
      "Epoch 2294, Loss Train(MSE): 0.12582744497241027, R2 Train: 0.4966902201103589\n",
      "Epoch 2295, Loss Train(MSE): 0.12582576704508772, R2 Train: 0.49669693181964913\n",
      "Epoch 2296, Loss Train(MSE): 0.1258238970336379, R2 Train: 0.49670441186544845\n",
      "Epoch 2297, Loss Train(MSE): 0.1258222899034181, R2 Train: 0.4967108403863276\n",
      "Epoch 2298, Loss Train(MSE): 0.12582044560671443, R2 Train: 0.4967182175731423\n",
      "Epoch 2299, Loss Train(MSE): 0.12581879705642768, R2 Train: 0.4967248117742893\n",
      "Epoch 2300, Loss Train(MSE): 0.12581703444230813, R2 Train: 0.4967318622307675\n",
      "Epoch 2301, Loss Train(MSE): 0.12581534197346644, R2 Train: 0.4967386321061342\n",
      "Epoch 2302, Loss Train(MSE): 0.12581366268446292, R2 Train: 0.4967453492621483\n",
      "Epoch 2303, Loss Train(MSE): 0.12581192390614843, R2 Train: 0.4967523043754063\n",
      "Epoch 2304, Loss Train(MSE): 0.12581032950282778, R2 Train: 0.4967586819886889\n",
      "Epoch 2305, Loss Train(MSE): 0.12580856042165164, R2 Train: 0.4967657583133934\n",
      "Epoch 2306, Loss Train(MSE): 0.12580701648020645, R2 Train: 0.4967719340791742\n",
      "Epoch 2307, Loss Train(MSE): 0.12580528600670016, R2 Train: 0.4967788559731994\n",
      "Epoch 2308, Loss Train(MSE): 0.12580368799264766, R2 Train: 0.4967852480294094\n",
      "Epoch 2309, Loss Train(MSE): 0.12580204813827656, R2 Train: 0.4967918074468938\n",
      "Epoch 2310, Loss Train(MSE): 0.12580039404087698, R2 Train: 0.4967984238364921\n",
      "Epoch 2311, Loss Train(MSE): 0.1257988460702121, R2 Train: 0.4968046157191516\n",
      "Epoch 2312, Loss Train(MSE): 0.12579714925815366, R2 Train: 0.49681140296738535\n",
      "Epoch 2313, Loss Train(MSE): 0.12579566442567744, R2 Train: 0.49681734229729024\n",
      "Epoch 2314, Loss Train(MSE): 0.12579400175453587, R2 Train: 0.49682399298185653\n",
      "Epoch 2315, Loss Train(MSE): 0.12579245411276194, R2 Train: 0.49683018354895225\n",
      "Epoch 2316, Loss Train(MSE): 0.1257908882244911, R2 Train: 0.49683644710203556\n",
      "Epoch 2317, Loss Train(MSE): 0.12578927609236867, R2 Train: 0.4968428956305253\n",
      "Epoch 2318, Loss Train(MSE): 0.1257878079956293, R2 Train: 0.4968487680174828\n",
      "Epoch 2319, Loss Train(MSE): 0.12578617823059488, R2 Train: 0.4968552870776205\n",
      "Epoch 2320, Loss Train(MSE): 0.1257847125501796, R2 Train: 0.49686114979928164\n",
      "Epoch 2321, Loss Train(MSE): 0.12578314877945573, R2 Train: 0.49686740488217707\n",
      "Epoch 2322, Loss Train(MSE): 0.12578161278894587, R2 Train: 0.49687354884421653\n",
      "Epoch 2323, Loss Train(MSE): 0.12578015098083542, R2 Train: 0.4968793960766583\n",
      "Epoch 2324, Loss Train(MSE): 0.12577875296712854, R2 Train: 0.49688498813148585\n",
      "Epoch 2325, Loss Train(MSE): 0.1257780905246776, R2 Train: 0.49688763790128965\n",
      "Epoch 2326, Loss Train(MSE): 0.12577728349416759, R2 Train: 0.49689086602332966\n",
      "Epoch 2327, Loss Train(MSE): 0.12577653399132147, R2 Train: 0.49689386403471414\n",
      "Epoch 2328, Loss Train(MSE): 0.12577582072572152, R2 Train: 0.4968967170971139\n",
      "Epoch 2329, Loss Train(MSE): 0.1257749970936175, R2 Train: 0.49690001162553\n",
      "Epoch 2330, Loss Train(MSE): 0.12577435154213512, R2 Train: 0.4969025938314595\n",
      "Epoch 2331, Loss Train(MSE): 0.12577354510057087, R2 Train: 0.4969058195977165\n",
      "Epoch 2332, Loss Train(MSE): 0.1257728107726426, R2 Train: 0.4969087569094296\n",
      "Epoch 2333, Loss Train(MSE): 0.12577209959787813, R2 Train: 0.49691160160848746\n",
      "Epoch 2334, Loss Train(MSE): 0.12577128648048752, R2 Train: 0.4969148540780499\n",
      "Epoch 2335, Loss Train(MSE): 0.12577065045597685, R2 Train: 0.4969173981760926\n",
      "Epoch 2336, Loss Train(MSE): 0.12576985140559843, R2 Train: 0.4969205943776063\n",
      "Epoch 2337, Loss Train(MSE): 0.12576912504005755, R2 Train: 0.4969234998397698\n",
      "Epoch 2338, Loss Train(MSE): 0.12576842262038515, R2 Train: 0.4969263095184594\n",
      "Epoch 2339, Loss Train(MSE): 0.12576761966739028, R2 Train: 0.4969295213304389\n",
      "Epoch 2340, Loss Train(MSE): 0.12576698627108598, R2 Train: 0.49693205491565606\n",
      "Epoch 2341, Loss Train(MSE): 0.1257662009837254, R2 Train: 0.4969351960650984\n",
      "Epoch 2342, Loss Train(MSE): 0.1257654758231377, R2 Train: 0.49693809670744915\n",
      "Epoch 2343, Loss Train(MSE): 0.12576478840145613, R2 Train: 0.49694084639417546\n",
      "Epoch 2344, Loss Train(MSE): 0.1257639952834721, R2 Train: 0.4969440188661116\n",
      "Epoch 2345, Loss Train(MSE): 0.12576335805263752, R2 Train: 0.4969465677894499\n",
      "Epoch 2346, Loss Train(MSE): 0.12576259249616087, R2 Train: 0.4969496300153565\n",
      "Epoch 2347, Loss Train(MSE): 0.12576186220972121, R2 Train: 0.49695255116111514\n",
      "Epoch 2348, Loss Train(MSE): 0.12576119563337387, R2 Train: 0.4969552174665045\n",
      "Epoch 2349, Loss Train(MSE): 0.1257604120403579, R2 Train: 0.4969583518385684\n",
      "Epoch 2350, Loss Train(MSE): 0.12575976492114269, R2 Train: 0.49696094031542926\n",
      "Epoch 2351, Loss Train(MSE): 0.12576450286230592, R2 Train: 0.4969419885507763\n",
      "Epoch 2352, Loss Train(MSE): 0.12576952198492045, R2 Train: 0.4969219120603182\n",
      "Epoch 2353, Loss Train(MSE): 0.1257686282731772, R2 Train: 0.4969254869072912\n",
      "Epoch 2354, Loss Train(MSE): 0.1257677364856851, R2 Train: 0.4969290540572596\n",
      "Epoch 2355, Loss Train(MSE): 0.12576684661618004, R2 Train: 0.49693261353527984\n",
      "Epoch 2356, Loss Train(MSE): 0.12576595865842824, R2 Train: 0.49693616536628704\n",
      "Epoch 2357, Loss Train(MSE): 0.125765072606226, R2 Train: 0.49693970957509603\n",
      "Epoch 2358, Loss Train(MSE): 0.12576418845339918, R2 Train: 0.4969432461864033\n",
      "Epoch 2359, Loss Train(MSE): 0.12576330619380333, R2 Train: 0.4969467752247867\n",
      "Epoch 2360, Loss Train(MSE): 0.12576242582132338, R2 Train: 0.4969502967147065\n",
      "Epoch 2361, Loss Train(MSE): 0.12576154732987324, R2 Train: 0.49695381068050704\n",
      "Epoch 2362, Loss Train(MSE): 0.1257606707133958, R2 Train: 0.4969573171464168\n",
      "Epoch 2363, Loss Train(MSE): 0.12575979596586262, R2 Train: 0.4969608161365495\n",
      "Epoch 2364, Loss Train(MSE): 0.1257589230812737, R2 Train: 0.4969643076749052\n",
      "Epoch 2365, Loss Train(MSE): 0.12575805205365728, R2 Train: 0.49696779178537087\n",
      "Epoch 2366, Loss Train(MSE): 0.1257571828770698, R2 Train: 0.4969712684917208\n",
      "Epoch 2367, Loss Train(MSE): 0.12575631554559535, R2 Train: 0.4969747378176186\n",
      "Epoch 2368, Loss Train(MSE): 0.12575545005334582, R2 Train: 0.4969781997866167\n",
      "Epoch 2369, Loss Train(MSE): 0.12575458639446044, R2 Train: 0.49698165442215825\n",
      "Epoch 2370, Loss Train(MSE): 0.12575372456310577, R2 Train: 0.49698510174757693\n",
      "Epoch 2371, Loss Train(MSE): 0.12575286455347534, R2 Train: 0.49698854178609864\n",
      "Epoch 2372, Loss Train(MSE): 0.12575200635978967, R2 Train: 0.4969919745608413\n",
      "Epoch 2373, Loss Train(MSE): 0.12575114997629586, R2 Train: 0.4969954000948166\n",
      "Epoch 2374, Loss Train(MSE): 0.12575029539726745, R2 Train: 0.4969988184109302\n",
      "Epoch 2375, Loss Train(MSE): 0.12574944261700444, R2 Train: 0.49700222953198225\n",
      "Epoch 2376, Loss Train(MSE): 0.1257485916298327, R2 Train: 0.4970056334806692\n",
      "Epoch 2377, Loss Train(MSE): 0.12574774243010423, R2 Train: 0.4970090302795831\n",
      "Epoch 2378, Loss Train(MSE): 0.12574689501219674, R2 Train: 0.49701241995121304\n",
      "Epoch 2379, Loss Train(MSE): 0.12574604937051334, R2 Train: 0.49701580251794664\n",
      "Epoch 2380, Loss Train(MSE): 0.12574520549948282, R2 Train: 0.49701917800206874\n",
      "Epoch 2381, Loss Train(MSE): 0.12574436339355896, R2 Train: 0.49702254642576416\n",
      "Epoch 2382, Loss Train(MSE): 0.12574352304722067, R2 Train: 0.49702590781111733\n",
      "Epoch 2383, Loss Train(MSE): 0.12574268445497166, R2 Train: 0.49702926218011334\n",
      "Epoch 2384, Loss Train(MSE): 0.1257418476113404, R2 Train: 0.49703260955463835\n",
      "Epoch 2385, Loss Train(MSE): 0.12574101251088, R2 Train: 0.49703594995648004\n",
      "Epoch 2386, Loss Train(MSE): 0.12574017914816768, R2 Train: 0.4970392834073293\n",
      "Epoch 2387, Loss Train(MSE): 0.1257393475178051, R2 Train: 0.4970426099287796\n",
      "Epoch 2388, Loss Train(MSE): 0.12573851761441782, R2 Train: 0.49704592954232873\n",
      "Epoch 2389, Loss Train(MSE): 0.1257376894326554, R2 Train: 0.4970492422693784\n",
      "Epoch 2390, Loss Train(MSE): 0.12573686296719094, R2 Train: 0.4970525481312362\n",
      "Epoch 2391, Loss Train(MSE): 0.1257360382127213, R2 Train: 0.49705584714911477\n",
      "Epoch 2392, Loss Train(MSE): 0.12573521516396666, R2 Train: 0.49705913934413337\n",
      "Epoch 2393, Loss Train(MSE): 0.12573439381567045, R2 Train: 0.4970624247373182\n",
      "Epoch 2394, Loss Train(MSE): 0.12573357416259912, R2 Train: 0.49706570334960354\n",
      "Epoch 2395, Loss Train(MSE): 0.12573275619954227, R2 Train: 0.4970689752018309\n",
      "Epoch 2396, Loss Train(MSE): 0.12573193992131212, R2 Train: 0.4970722403147515\n",
      "Epoch 2397, Loss Train(MSE): 0.12573124164728344, R2 Train: 0.49707503341086623\n",
      "Epoch 2398, Loss Train(MSE): 0.12573056476637937, R2 Train: 0.49707774093448254\n",
      "Epoch 2399, Loss Train(MSE): 0.12572997644331563, R2 Train: 0.4970800942267375\n",
      "Epoch 2400, Loss Train(MSE): 0.12572927352471097, R2 Train: 0.4970829059011561\n",
      "Epoch 2401, Loss Train(MSE): 0.12572863670807907, R2 Train: 0.4970854531676837\n",
      "Epoch 2402, Loss Train(MSE): 0.1257280155678545, R2 Train: 0.49708793772858195\n",
      "Epoch 2403, Loss Train(MSE): 0.12572731702237255, R2 Train: 0.4970907319105098\n",
      "Epoch 2404, Loss Train(MSE): 0.12572671832168322, R2 Train: 0.4970931267132671\n",
      "Epoch 2405, Loss Train(MSE): 0.12572606621529558, R2 Train: 0.49709573513881766\n",
      "Epoch 2406, Loss Train(MSE): 0.12572537197829373, R2 Train: 0.4970985120868251\n",
      "Epoch 2407, Loss Train(MSE): 0.12572480950679454, R2 Train: 0.4971007619728218\n",
      "Epoch 2408, Loss Train(MSE): 0.1257241282265629, R2 Train: 0.4971034870937484\n",
      "Epoch 2409, Loss Train(MSE): 0.12572345952815245, R2 Train: 0.4971061618873902\n",
      "Epoch 2410, Loss Train(MSE): 0.12572288883200433, R2 Train: 0.4971084446719827\n",
      "Epoch 2411, Loss Train(MSE): 0.12572220130334444, R2 Train: 0.49711119478662225\n",
      "Epoch 2412, Loss Train(MSE): 0.1257215666963242, R2 Train: 0.4971137332147032\n",
      "Epoch 2413, Loss Train(MSE): 0.12572096882377073, R2 Train: 0.49711612470491706\n",
      "Epoch 2414, Loss Train(MSE): 0.12572028544922192, R2 Train: 0.4971188582031123\n",
      "Epoch 2415, Loss Train(MSE): 0.12572702911673872, R2 Train: 0.4970918835330451\n",
      "Epoch 2416, Loss Train(MSE): 0.12577290311002465, R2 Train: 0.4969083875599014\n",
      "Epoch 2417, Loss Train(MSE): 0.1257712776923451, R2 Train: 0.49691488923061955\n",
      "Epoch 2418, Loss Train(MSE): 0.125769814757764, R2 Train: 0.496920740968944\n",
      "Epoch 2419, Loss Train(MSE): 0.1257681722892838, R2 Train: 0.4969273108428648\n",
      "Epoch 2420, Loss Train(MSE): 0.12576674361214835, R2 Train: 0.4969330255514066\n",
      "Epoch 2421, Loss Train(MSE): 0.12576513764076794, R2 Train: 0.49693944943692825\n",
      "Epoch 2422, Loss Train(MSE): 0.1257636687479837, R2 Train: 0.4969453250080652\n",
      "Epoch 2423, Loss Train(MSE): 0.12576213639451203, R2 Train: 0.4969514544219519\n",
      "Epoch 2424, Loss Train(MSE): 0.12576062527689424, R2 Train: 0.496957498892423\n",
      "Epoch 2425, Loss Train(MSE): 0.12575916787938482, R2 Train: 0.4969633284824607\n",
      "Epoch 2426, Loss Train(MSE): 0.12575761261073445, R2 Train: 0.4969695495570622\n",
      "Epoch 2427, Loss Train(MSE): 0.12575623144327167, R2 Train: 0.49697507422691334\n",
      "Epoch 2428, Loss Train(MSE): 0.12575467275415006, R2 Train: 0.49698130898339976\n",
      "Epoch 2429, Loss Train(MSE): 0.12575328444583608, R2 Train: 0.49698686221665567\n",
      "Epoch 2430, Loss Train(MSE): 0.12575178525612196, R2 Train: 0.49699285897551215\n",
      "Epoch 2431, Loss Train(MSE): 0.12575034646567093, R2 Train: 0.4969986141373163\n",
      "Epoch 2432, Loss Train(MSE): 0.12574892823859235, R2 Train: 0.4970042870456306\n",
      "Epoch 2433, Loss Train(MSE): 0.12574743733491195, R2 Train: 0.4970102506603522\n",
      "Epoch 2434, Loss Train(MSE): 0.12574610111224324, R2 Train: 0.49701559555102703\n",
      "Epoch 2435, Loss Train(MSE): 0.12574460270180193, R2 Train: 0.49702158919279227\n",
      "Epoch 2436, Loss Train(MSE): 0.1257432576707244, R2 Train: 0.49702696931710244\n",
      "Epoch 2437, Loss Train(MSE): 0.125741821195086, R2 Train: 0.497032715219656\n",
      "Epoch 2438, Loss Train(MSE): 0.12574041852918966, R2 Train: 0.49703832588324137\n",
      "Epoch 2439, Loss Train(MSE): 0.12573906813289384, R2 Train: 0.4970437274684246\n",
      "Epoch 2440, Loss Train(MSE): 0.12573761063287178, R2 Train: 0.49704955746851287\n",
      "Epoch 2441, Loss Train(MSE): 0.12573633931827236, R2 Train: 0.4970546427269106\n",
      "Epoch 2442, Loss Train(MSE): 0.12573490099300055, R2 Train: 0.4970603960279978\n",
      "Epoch 2443, Loss Train(MSE): 0.12573356705888414, R2 Train: 0.49706573176446345\n",
      "Epoch 2444, Loss Train(MSE): 0.12573221844939464, R2 Train: 0.49707112620242144\n",
      "Epoch 2445, Loss Train(MSE): 0.12573082069339606, R2 Train: 0.4970767172264158\n",
      "Epoch 2446, Loss Train(MSE): 0.12572956250407089, R2 Train: 0.49708174998371646\n",
      "Epoch 2447, Loss Train(MSE): 0.12572815840829085, R2 Train: 0.4970873663668366\n",
      "Epoch 2448, Loss Train(MSE): 0.12572687452253498, R2 Train: 0.4970925019098601\n",
      "Epoch 2449, Loss Train(MSE): 0.12572554308127862, R2 Train: 0.49709782767488553\n",
      "Epoch 2450, Loss Train(MSE): 0.1257241910379892, R2 Train: 0.49710323584804317\n",
      "Epoch 2451, Loss Train(MSE): 0.12572322742210656, R2 Train: 0.49710709031157374\n",
      "Epoch 2452, Loss Train(MSE): 0.12572250521434178, R2 Train: 0.4971099791426329\n",
      "Epoch 2453, Loss Train(MSE): 0.12572189520061386, R2 Train: 0.49711241919754456\n",
      "Epoch 2454, Loss Train(MSE): 0.12572122996946156, R2 Train: 0.4971150801221538\n",
      "Epoch 2455, Loss Train(MSE): 0.12572053871096572, R2 Train: 0.4971178451561371\n",
      "Epoch 2456, Loss Train(MSE): 0.12571996002082847, R2 Train: 0.4971201599166861\n",
      "Epoch 2457, Loss Train(MSE): 0.12571924636606555, R2 Train: 0.4971230145357378\n",
      "Epoch 2458, Loss Train(MSE): 0.12571863644922732, R2 Train: 0.49712545420309073\n",
      "Epoch 2459, Loss Train(MSE): 0.12571798492892874, R2 Train: 0.497128060284285\n",
      "Epoch 2460, Loss Train(MSE): 0.12571729259279155, R2 Train: 0.4971308296288338\n",
      "Epoch 2461, Loss Train(MSE): 0.12571672863786962, R2 Train: 0.49713308544852153\n",
      "Epoch 2462, Loss Train(MSE): 0.1257160232733888, R2 Train: 0.4971359069064448\n",
      "Epoch 2463, Loss Train(MSE): 0.12571540806199746, R2 Train: 0.49713836775201015\n",
      "Epoch 2464, Loss Train(MSE): 0.12571477524889382, R2 Train: 0.4971408990044247\n",
      "Epoch 2465, Loss Train(MSE): 0.12571407654849567, R2 Train: 0.4971436938060173\n",
      "Epoch 2466, Loss Train(MSE): 0.12571353222896625, R2 Train: 0.497145871084135\n",
      "Epoch 2467, Loss Train(MSE): 0.12571283490672827, R2 Train: 0.4971486603730869\n",
      "Epoch 2468, Loss Train(MSE): 0.12571220933502286, R2 Train: 0.49715116265990855\n",
      "Epoch 2469, Loss Train(MSE): 0.1257115999222248, R2 Train: 0.4971536003111008\n",
      "Epoch 2470, Loss Train(MSE): 0.12571090755627554, R2 Train: 0.49715636977489785\n",
      "Epoch 2471, Loss Train(MSE): 0.12571035224370536, R2 Train: 0.49715859102517856\n",
      "Epoch 2472, Loss Train(MSE): 0.12570968046523456, R2 Train: 0.49716127813906175\n",
      "Epoch 2473, Loss Train(MSE): 0.12570903983245593, R2 Train: 0.49716384067017627\n",
      "Epoch 2474, Loss Train(MSE): 0.12570845816385742, R2 Train: 0.4971661673445703\n",
      "Epoch 2475, Loss Train(MSE): 0.12570777346051729, R2 Train: 0.49716890615793086\n",
      "Epoch 2476, Loss Train(MSE): 0.12570719943543926, R2 Train: 0.49717120225824296\n",
      "Epoch 2477, Loss Train(MSE): 0.12570655884207246, R2 Train: 0.49717376463171015\n",
      "Epoch 2478, Loss Train(MSE): 0.12570589867909152, R2 Train: 0.49717640528363394\n",
      "Epoch 2479, Loss Train(MSE): 0.12570534889171775, R2 Train: 0.497178604433129\n",
      "Epoch 2480, Loss Train(MSE): 0.12570467163703217, R2 Train: 0.49718131345187133\n",
      "Epoch 2481, Loss Train(MSE): 0.1257040746194433, R2 Train: 0.4971837015222268\n",
      "Epoch 2482, Loss Train(MSE): 0.12570346917036035, R2 Train: 0.4971861233185586\n",
      "Epoch 2483, Loss Train(MSE): 0.12570279651303995, R2 Train: 0.4971888139478402\n",
      "Epoch 2484, Loss Train(MSE): 0.12570226010838687, R2 Train: 0.49719095956645254\n",
      "Epoch 2485, Loss Train(MSE): 0.1257016014071559, R2 Train: 0.49719359437137645\n",
      "Epoch 2486, Loss Train(MSE): 0.12570097742255415, R2 Train: 0.4971960903097834\n",
      "Epoch 2487, Loss Train(MSE): 0.12570041078391614, R2 Train: 0.49719835686433544\n",
      "Epoch 2488, Loss Train(MSE): 0.1256997452463139, R2 Train: 0.49720101901474445\n",
      "Epoch 2489, Loss Train(MSE): 0.12569917870899813, R2 Train: 0.49720328516400747\n",
      "Epoch 2490, Loss Train(MSE): 0.1256985618015465, R2 Train: 0.49720575279381396\n",
      "Epoch 2491, Loss Train(MSE): 0.1256979070794484, R2 Train: 0.49720837168220644\n",
      "Epoch 2492, Loss Train(MSE): 0.12569738273414202, R2 Train: 0.4972104690634319\n",
      "Epoch 2493, Loss Train(MSE): 0.12569672413045613, R2 Train: 0.4972131034781755\n",
      "Epoch 2494, Loss Train(MSE): 0.1256961238524557, R2 Train: 0.49721550459017716\n",
      "Epoch 2495, Loss Train(MSE): 0.12569555206848199, R2 Train: 0.49721779172607206\n",
      "Epoch 2496, Loss Train(MSE): 0.12569489774992565, R2 Train: 0.4972204090002974\n",
      "Epoch 2497, Loss Train(MSE): 0.12569434967513082, R2 Train: 0.4972226012994767\n",
      "Epoch 2498, Loss Train(MSE): 0.12569373258622782, R2 Train: 0.4972250696550887\n",
      "Epoch 2499, Loss Train(MSE): 0.12569309521464786, R2 Train: 0.49722761914140856\n",
      "Epoch 2500, Loss Train(MSE): 0.1256925716386095, R2 Train: 0.497229713445562\n",
      "Epoch 2501, Loss Train(MSE): 0.125691923967063, R2 Train: 0.497232304131748\n",
      "Epoch 2502, Loss Train(MSE): 0.12569133605171812, R2 Train: 0.49723465579312753\n",
      "Epoch 2503, Loss Train(MSE): 0.12569076975866483, R2 Train: 0.49723692096534067\n",
      "Epoch 2504, Loss Train(MSE): 0.12569012619772468, R2 Train: 0.49723949520910127\n",
      "Epoch 2505, Loss Train(MSE): 0.12568958566113744, R2 Train: 0.49724165735545023\n",
      "Epoch 2506, Loss Train(MSE): 0.12568897862982722, R2 Train: 0.4972440854806911\n",
      "Epoch 2507, Loss Train(MSE): 0.12568834786734248, R2 Train: 0.4972466085306301\n",
      "Epoch 2508, Loss Train(MSE): 0.12568783512959536, R2 Train: 0.49724865948161856\n",
      "Epoch 2509, Loss Train(MSE): 0.12568719795136526, R2 Train: 0.49725120819453894\n",
      "Epoch 2510, Loss Train(MSE): 0.12568661205475162, R2 Train: 0.4972535517809935\n",
      "Epoch 2511, Loss Train(MSE): 0.12568606094502208, R2 Train: 0.4972557562199117\n",
      "Epoch 2512, Loss Train(MSE): 0.1256854277163361, R2 Train: 0.49725828913465564\n",
      "Epoch 2513, Loss Train(MSE): 0.12568488475795458, R2 Train: 0.49726046096818166\n",
      "Epoch 2514, Loss Train(MSE): 0.12568429711268914, R2 Train: 0.49726281154924346\n",
      "Epoch 2515, Loss Train(MSE): 0.12568366777312243, R2 Train: 0.4972653289075103\n",
      "Epoch 2516, Loss Train(MSE): 0.12568316588372208, R2 Train: 0.4972673364651117\n",
      "Epoch 2517, Loss Train(MSE): 0.12568254348375876, R2 Train: 0.49726982606496495\n",
      "Epoch 2518, Loss Train(MSE): 0.12568195019214126, R2 Train: 0.49727219923143495\n",
      "Epoch 2519, Loss Train(MSE): 0.12568142307445448, R2 Train: 0.49727430770218206\n",
      "Epoch 2520, Loss Train(MSE): 0.1256807997821286, R2 Train: 0.4972768008714856\n",
      "Epoch 2521, Loss Train(MSE): 0.12568024534423924, R2 Train: 0.497279018623043\n",
      "Epoch 2522, Loss Train(MSE): 0.12567968555567902, R2 Train: 0.49728125777728394\n",
      "Epoch 2523, Loss Train(MSE): 0.12567906600795964, R2 Train: 0.49728373596816144\n",
      "Epoch 2524, Loss Train(MSE): 0.12567854868742673, R2 Train: 0.4972858052502931\n",
      "Epoch 2525, Loss Train(MSE): 0.125677957882391, R2 Train: 0.497288168470436\n",
      "Epoch 2526, Loss Train(MSE): 0.1256773486052527, R2 Train: 0.4972906055789892\n",
      "Epoch 2527, Loss Train(MSE): 0.12567685351383368, R2 Train: 0.4972925859446653\n",
      "Epoch 2528, Loss Train(MSE): 0.12567623979413944, R2 Train: 0.4972950408234422\n",
      "Epoch 2529, Loss Train(MSE): 0.12567566560822788, R2 Train: 0.4972973375670885\n",
      "Epoch 2530, Loss Train(MSE): 0.12567514140451397, R2 Train: 0.4972994343819441\n",
      "Epoch 2531, Loss Train(MSE): 0.125674531295303, R2 Train: 0.49730187481878796\n",
      "Epoch 2532, Loss Train(MSE): 0.1256739905864167, R2 Train: 0.4973040376543332\n",
      "Epoch 2533, Loss Train(MSE): 0.12567343880841003, R2 Train: 0.4973062447663599\n",
      "Epoch 2534, Loss Train(MSE): 0.1256728322590588, R2 Train: 0.49730867096376485\n",
      "Epoch 2535, Loss Train(MSE): 0.12567232346114085, R2 Train: 0.4973107061554366\n",
      "Epoch 2536, Loss Train(MSE): 0.1256732270645078, R2 Train: 0.49730709174196885\n",
      "Epoch 2537, Loss Train(MSE): 0.12568155063011344, R2 Train: 0.49727379747954625\n",
      "Epoch 2538, Loss Train(MSE): 0.12568083474924371, R2 Train: 0.49727666100302514\n",
      "Epoch 2539, Loss Train(MSE): 0.12568012025023662, R2 Train: 0.49727951899905354\n",
      "Epoch 2540, Loss Train(MSE): 0.12567940712921097, R2 Train: 0.4972823714831561\n",
      "Epoch 2541, Loss Train(MSE): 0.12567869538229995, R2 Train: 0.4972852184708002\n",
      "Epoch 2542, Loss Train(MSE): 0.12567798500565117, R2 Train: 0.49728805997739534\n",
      "Epoch 2543, Loss Train(MSE): 0.12567727599542675, R2 Train: 0.497290896018293\n",
      "Epoch 2544, Loss Train(MSE): 0.12567656834780305, R2 Train: 0.4972937266087878\n",
      "Epoch 2545, Loss Train(MSE): 0.12567586205897066, R2 Train: 0.49729655176411736\n",
      "Epoch 2546, Loss Train(MSE): 0.12567515712513444, R2 Train: 0.49729937149946224\n",
      "Epoch 2547, Loss Train(MSE): 0.12569770989973503, R2 Train: 0.4972091604010599\n",
      "Epoch 2548, Loss Train(MSE): 0.12572427341224676, R2 Train: 0.49710290635101295\n",
      "Epoch 2549, Loss Train(MSE): 0.1257227644691631, R2 Train: 0.49710894212334755\n",
      "Epoch 2550, Loss Train(MSE): 0.12572126295052055, R2 Train: 0.4971149481979178\n",
      "Epoch 2551, Loss Train(MSE): 0.125719768792901, R2 Train: 0.497120924828396\n",
      "Epoch 2552, Loss Train(MSE): 0.12571828193366694, R2 Train: 0.49712687226533225\n",
      "Epoch 2553, Loss Train(MSE): 0.12571680231094928, R2 Train: 0.49713279075620287\n",
      "Epoch 2554, Loss Train(MSE): 0.1257153298636353, R2 Train: 0.49713868054545884\n",
      "Epoch 2555, Loss Train(MSE): 0.12571386453135672, R2 Train: 0.49714454187457313\n",
      "Epoch 2556, Loss Train(MSE): 0.12571240625447816, R2 Train: 0.49715037498208736\n",
      "Epoch 2557, Loss Train(MSE): 0.12571095497408566, R2 Train: 0.49715618010365736\n",
      "Epoch 2558, Loss Train(MSE): 0.12570951063197544, R2 Train: 0.49716195747209824\n",
      "Epoch 2559, Loss Train(MSE): 0.12570807317064311, R2 Train: 0.49716770731742754\n",
      "Epoch 2560, Loss Train(MSE): 0.12570664253327268, R2 Train: 0.49717342986690927\n",
      "Epoch 2561, Loss Train(MSE): 0.12570521866372608, R2 Train: 0.4971791253450957\n",
      "Epoch 2562, Loss Train(MSE): 0.12570380150653263, R2 Train: 0.4971847939738695\n",
      "Epoch 2563, Loss Train(MSE): 0.12570239100687905, R2 Train: 0.4971904359724838\n",
      "Epoch 2564, Loss Train(MSE): 0.12570098711059938, R2 Train: 0.4971960515576025\n",
      "Epoch 2565, Loss Train(MSE): 0.12569958976416493, R2 Train: 0.4972016409433403\n",
      "Epoch 2566, Loss Train(MSE): 0.12569819891467493, R2 Train: 0.4972072043413003\n",
      "Epoch 2567, Loss Train(MSE): 0.1256968145098469, R2 Train: 0.4972127419606124\n",
      "Epoch 2568, Loss Train(MSE): 0.12569543649800732, R2 Train: 0.4972182540079707\n",
      "Epoch 2569, Loss Train(MSE): 0.12569406482808265, R2 Train: 0.4972237406876694\n",
      "Epoch 2570, Loss Train(MSE): 0.12569269944959, R2 Train: 0.49722920220164\n",
      "Epoch 2571, Loss Train(MSE): 0.1256913403126288, R2 Train: 0.4972346387494848\n",
      "Epoch 2572, Loss Train(MSE): 0.12568998736787176, R2 Train: 0.49724005052851294\n",
      "Epoch 2573, Loss Train(MSE): 0.12568864056655654, R2 Train: 0.49724543773377383\n",
      "Epoch 2574, Loss Train(MSE): 0.12568729986047739, R2 Train: 0.49725080055809046\n",
      "Epoch 2575, Loss Train(MSE): 0.12568596520197692, R2 Train: 0.49725613919209233\n",
      "Epoch 2576, Loss Train(MSE): 0.12568463654393794, R2 Train: 0.49726145382424825\n",
      "Epoch 2577, Loss Train(MSE): 0.1256833138397757, R2 Train: 0.49726674464089715\n",
      "Epoch 2578, Loss Train(MSE): 0.12568199704343022, R2 Train: 0.4972720118262791\n",
      "Epoch 2579, Loss Train(MSE): 0.12568068610935818, R2 Train: 0.4972772555625673\n",
      "Epoch 2580, Loss Train(MSE): 0.125679380992526, R2 Train: 0.49728247602989595\n",
      "Epoch 2581, Loss Train(MSE): 0.12567808164840205, R2 Train: 0.4972876734063918\n",
      "Epoch 2582, Loss Train(MSE): 0.1256767880329496, R2 Train: 0.4972928478682016\n",
      "Epoch 2583, Loss Train(MSE): 0.12567550010261958, R2 Train: 0.4972979995895217\n",
      "Epoch 2584, Loss Train(MSE): 0.12567421781434368, R2 Train: 0.4973031287426253\n",
      "Epoch 2585, Loss Train(MSE): 0.1256729411255275, R2 Train: 0.49730823549789005\n",
      "Epoch 2586, Loss Train(MSE): 0.12567198247431938, R2 Train: 0.4973120701027225\n",
      "Epoch 2587, Loss Train(MSE): 0.12567142674294193, R2 Train: 0.49731429302823227\n",
      "Epoch 2588, Loss Train(MSE): 0.12567079999812092, R2 Train: 0.4973168000075163\n",
      "Epoch 2589, Loss Train(MSE): 0.12567030654441452, R2 Train: 0.4973187738223419\n",
      "Epoch 2590, Loss Train(MSE): 0.12566969057046856, R2 Train: 0.49732123771812575\n",
      "Epoch 2591, Loss Train(MSE): 0.12566912204036618, R2 Train: 0.4973235118385353\n",
      "Epoch 2592, Loss Train(MSE): 0.12566858536567563, R2 Train: 0.4973256585372975\n",
      "Epoch 2593, Loss Train(MSE): 0.12566796540223674, R2 Train: 0.49732813839105305\n",
      "Epoch 2594, Loss Train(MSE): 0.1256674607403124, R2 Train: 0.4973301570387504\n",
      "Epoch 2595, Loss Train(MSE): 0.12566686698103782, R2 Train: 0.4973325320758487\n",
      "Epoch 2596, Loss Train(MSE): 0.12566628643977415, R2 Train: 0.4973348542409034\n",
      "Epoch 2597, Loss Train(MSE): 0.12566577267568046, R2 Train: 0.49733690929727814\n",
      "Epoch 2598, Loss Train(MSE): 0.12566515930581498, R2 Train: 0.49733936277674007\n",
      "Epoch 2599, Loss Train(MSE): 0.1256646394559331, R2 Train: 0.49734144217626763\n",
      "Epoch 2600, Loss Train(MSE): 0.1256640716088007, R2 Train: 0.49734371356479723\n",
      "Epoch 2601, Loss Train(MSE): 0.12566347515002838, R2 Train: 0.4973460993998865\n",
      "Epoch 2602, Loss Train(MSE): 0.12566298792627004, R2 Train: 0.49734804829491985\n",
      "Epoch 2603, Loss Train(MSE): 0.1256623809718343, R2 Train: 0.4973504761126628\n",
      "Epoch 2604, Loss Train(MSE): 0.12566184218491463, R2 Train: 0.49735263126034146\n",
      "Epoch 2605, Loss Train(MSE): 0.125661303731462, R2 Train: 0.497354785074152\n",
      "Epoch 2606, Loss Train(MSE): 0.1256607007397799, R2 Train: 0.49735719704088044\n",
      "Epoch 2607, Loss Train(MSE): 0.12566021742012767, R2 Train: 0.4973591303194893\n",
      "Epoch 2608, Loss Train(MSE): 0.12565962983895865, R2 Train: 0.4973614806441654\n",
      "Epoch 2609, Loss Train(MSE): 0.12565906862417173, R2 Train: 0.4973637255033131\n",
      "Epoch 2610, Loss Train(MSE): 0.12565856279764875, R2 Train: 0.497365748809405\n",
      "Epoch 2611, Loss Train(MSE): 0.1256579659468926, R2 Train: 0.4973681362124296\n",
      "Epoch 2612, Loss Train(MSE): 0.12565745742712978, R2 Train: 0.4973701702914809\n",
      "Epoch 2613, Loss Train(MSE): 0.1256569050926041, R2 Train: 0.4973723796295836\n",
      "Epoch 2614, Loss Train(MSE): 0.12565631812623876, R2 Train: 0.49737472749504497\n",
      "Epoch 2615, Loss Train(MSE): 0.12565584800962776, R2 Train: 0.49737660796148897\n",
      "Epoch 2616, Loss Train(MSE): 0.12565525714431078, R2 Train: 0.4973789714227569\n",
      "Epoch 2617, Loss Train(MSE): 0.12565472023671953, R2 Train: 0.4973811190531219\n",
      "Epoch 2618, Loss Train(MSE): 0.12565420610333752, R2 Train: 0.4973831755866499\n",
      "Epoch 2619, Loss Train(MSE): 0.12565361893963575, R2 Train: 0.497385524241457\n",
      "Epoch 2620, Loss Train(MSE): 0.12565313012985713, R2 Train: 0.4973874794805715\n",
      "Epoch 2621, Loss Train(MSE): 0.12565257385078935, R2 Train: 0.4973897045968426\n",
      "Epoch 2622, Loss Train(MSE): 0.12565200558512352, R2 Train: 0.49739197765950593\n",
      "Epoch 2623, Loss Train(MSE): 0.12565153239786594, R2 Train: 0.4973938704085362\n",
      "Epoch 2624, Loss Train(MSE): 0.12565095097903625, R2 Train: 0.497396196083855\n",
      "Epoch 2625, Loss Train(MSE): 0.12565042838985047, R2 Train: 0.49739828644059814\n",
      "Epoch 2626, Loss Train(MSE): 0.12564991534458342, R2 Train: 0.4974003386216663\n",
      "Epoch 2627, Loss Train(MSE): 0.12564933748088097, R2 Train: 0.4974026500764761\n",
      "Epoch 2628, Loss Train(MSE): 0.1256488587443732, R2 Train: 0.4974045650225072\n",
      "Epoch 2629, Loss Train(MSE): 0.1256483075818829, R2 Train: 0.49740676967246844\n",
      "Epoch 2630, Loss Train(MSE): 0.12564774853450716, R2 Train: 0.49740900586197134\n",
      "Epoch 2631, Loss Train(MSE): 0.12564728119391058, R2 Train: 0.4974108752243577\n",
      "Epoch 2632, Loss Train(MSE): 0.12564670885286744, R2 Train: 0.49741316458853024\n",
      "Epoch 2633, Loss Train(MSE): 0.1256461914339454, R2 Train: 0.4974152342642184\n",
      "Epoch 2634, Loss Train(MSE): 0.12564568807684842, R2 Train: 0.4974172476926063\n",
      "Epoch 2635, Loss Train(MSE): 0.1256451191553898, R2 Train: 0.49741952337844075\n",
      "Epoch 2636, Loss Train(MSE): 0.12564464166733824, R2 Train: 0.49742143333064703\n",
      "Epoch 2637, Loss Train(MSE): 0.1256441039144564, R2 Train: 0.4974235843421744\n",
      "Epoch 2638, Loss Train(MSE): 0.12564354540077843, R2 Train: 0.49742581839688627\n",
      "Epoch 2639, Loss Train(MSE): 0.12564309206901783, R2 Train: 0.4974276317239287\n",
      "Epoch 2640, Loss Train(MSE): 0.12564252846449592, R2 Train: 0.49742988614201633\n",
      "Epoch 2641, Loss Train(MSE): 0.12564200783841878, R2 Train: 0.4974319686463249\n",
      "Epoch 2642, Loss Train(MSE): 0.12564152203973392, R2 Train: 0.49743391184106434\n",
      "Epoch 2643, Loss Train(MSE): 0.12564096172910655, R2 Train: 0.4974361530835738\n",
      "Epoch 2644, Loss Train(MSE): 0.12564047740932374, R2 Train: 0.49743809036270503\n",
      "Epoch 2645, Loss Train(MSE): 0.12563996065367367, R2 Train: 0.49744015738530534\n",
      "Epoch 2646, Loss Train(MSE): 0.12563940358961834, R2 Train: 0.49744238564152665\n",
      "Epoch 2647, Loss Train(MSE): 0.125638954040347, R2 Train: 0.497444183838612\n",
      "Epoch 2648, Loss Train(MSE): 0.12563840779421567, R2 Train: 0.4974463688231373\n",
      "Epoch 2649, Loss Train(MSE): 0.12563787630342743, R2 Train: 0.4974484947862903\n",
      "Epoch 2650, Loss Train(MSE): 0.1256374152474156, R2 Train: 0.49745033901033764\n",
      "Epoch 2651, Loss Train(MSE): 0.12563686323769108, R2 Train: 0.4974525470492357\n",
      "Epoch 2652, Loss Train(MSE): 0.1256363647056303, R2 Train: 0.4974545411774788\n",
      "Epoch 2653, Loss Train(MSE): 0.12563587586768882, R2 Train: 0.49745649652924473\n",
      "Epoch 2654, Loss Train(MSE): 0.1256353269910465, R2 Train: 0.49745869203581405\n",
      "Epoch 2655, Loss Train(MSE): 0.1256348599857038, R2 Train: 0.49746056005718475\n",
      "Epoch 2656, Loss Train(MSE): 0.12563434473330481, R2 Train: 0.49746262106678074\n",
      "Epoch 2657, Loss Train(MSE): 0.12563379894687526, R2 Train: 0.49746480421249895\n",
      "Epoch 2658, Loss Train(MSE): 0.12563336207712172, R2 Train: 0.4974665516915131\n",
      "Epoch 2659, Loss Train(MSE): 0.12563282173863216, R2 Train: 0.49746871304547136\n",
      "Epoch 2660, Loss Train(MSE): 0.1256323022364213, R2 Train: 0.4974707910543148\n",
      "Epoch 2661, Loss Train(MSE): 0.12563184764610152, R2 Train: 0.4974726094155939\n",
      "Epoch 2662, Loss Train(MSE): 0.12563130667647102, R2 Train: 0.49747477329411594\n",
      "Epoch 2663, Loss Train(MSE): 0.12563081570425258, R2 Train: 0.4974767371829897\n",
      "Epoch 2664, Loss Train(MSE): 0.12563033753984693, R2 Train: 0.4974786498406123\n",
      "Epoch 2665, Loss Train(MSE): 0.12562979955753975, R2 Train: 0.497480801769841\n",
      "Epoch 2666, Loss Train(MSE): 0.12562933581696525, R2 Train: 0.497482656732139\n",
      "Epoch 2667, Loss Train(MSE): 0.1256288353181819, R2 Train: 0.49748465872727243\n",
      "Epoch 2668, Loss Train(MSE): 0.1256283002842106, R2 Train: 0.4974867988631576\n",
      "Epoch 2669, Loss Train(MSE): 0.12562786251371685, R2 Train: 0.4974885499451326\n",
      "Epoch 2670, Loss Train(MSE): 0.12562734088502253, R2 Train: 0.4974906364599099\n",
      "Epoch 2671, Loss Train(MSE): 0.12562681998885367, R2 Train: 0.49749272004458533\n",
      "Epoch 2672, Loss Train(MSE): 0.1256263844807264, R2 Train: 0.49749446207709436\n",
      "Epoch 2673, Loss Train(MSE): 0.12562585404782473, R2 Train: 0.4974965838087011\n",
      "Epoch 2674, Loss Train(MSE): 0.12562535770008504, R2 Train: 0.49749856919965985\n",
      "Epoch 2675, Loss Train(MSE): 0.1256249023984515, R2 Train: 0.497500390406194\n",
      "Epoch 2676, Loss Train(MSE): 0.1256243748202384, R2 Train: 0.49750250071904645\n",
      "Epoch 2677, Loss Train(MSE): 0.12562390184245661, R2 Train: 0.49750439263017354\n",
      "Epoch 2678, Loss Train(MSE): 0.12562342787227246, R2 Train: 0.49750628851091017\n",
      "Epoch 2679, Loss Train(MSE): 0.12562290311315588, R2 Train: 0.4975083875473765\n",
      "Epoch 2680, Loss Train(MSE): 0.12562245236002562, R2 Train: 0.49751019055989754\n",
      "Epoch 2681, Loss Train(MSE): 0.12562196081442892, R2 Train: 0.49751215674228433\n",
      "Epoch 2682, Loss Train(MSE): 0.12562143883971588, R2 Train: 0.49751424464113647\n",
      "Epoch 2683, Loss Train(MSE): 0.12562100919811658, R2 Train: 0.49751596320753366\n",
      "Epoch 2684, Loss Train(MSE): 0.1256205011393556, R2 Train: 0.4975179954425776\n",
      "Epoch 2685, Loss Train(MSE): 0.12561998790446643, R2 Train: 0.49752004838213426\n",
      "Epoch 2686, Loss Train(MSE): 0.12561956629423726, R2 Train: 0.49752173482305095\n",
      "Epoch 2687, Loss Train(MSE): 0.1256190486711645, R2 Train: 0.497523805315342\n",
      "Epoch 2688, Loss Train(MSE): 0.12561855533603586, R2 Train: 0.49752577865585657\n",
      "Epoch 2689, Loss Train(MSE): 0.12564022406817474, R2 Train: 0.49743910372730105\n",
      "Epoch 2690, Loss Train(MSE): 0.1256652822305326, R2 Train: 0.49733887107786956\n",
      "Epoch 2691, Loss Train(MSE): 0.12566411772271815, R2 Train: 0.4973435291091274\n",
      "Epoch 2692, Loss Train(MSE): 0.12566292967428083, R2 Train: 0.49734828130287667\n",
      "Epoch 2693, Loss Train(MSE): 0.12566173469745284, R2 Train: 0.49735306121018863\n",
      "Epoch 2694, Loss Train(MSE): 0.1256606004467405, R2 Train: 0.49735759821303804\n",
      "Epoch 2695, Loss Train(MSE): 0.12565937358895335, R2 Train: 0.4973625056441866\n",
      "Epoch 2696, Loss Train(MSE): 0.12565829412521354, R2 Train: 0.49736682349914585\n",
      "Epoch 2697, Loss Train(MSE): 0.12565706770018842, R2 Train: 0.4973717291992463\n",
      "Epoch 2698, Loss Train(MSE): 0.12565597703149697, R2 Train: 0.4973760918740121\n",
      "Epoch 2699, Loss Train(MSE): 0.12565479640971078, R2 Train: 0.4973808143611569\n",
      "Epoch 2700, Loss Train(MSE): 0.1256536692286279, R2 Train: 0.49738532308548844\n",
      "Epoch 2701, Loss Train(MSE): 0.12565254698452594, R2 Train: 0.49738981206189625\n",
      "Epoch 2702, Loss Train(MSE): 0.12565138207218846, R2 Train: 0.4973944717112462\n",
      "Epoch 2703, Loss Train(MSE): 0.12565031903910154, R2 Train: 0.49739872384359385\n",
      "Epoch 2704, Loss Train(MSE): 0.12564913594495364, R2 Train: 0.49740345622018545\n",
      "Epoch 2705, Loss Train(MSE): 0.12564809185952824, R2 Train: 0.497407632561887\n",
      "Epoch 2706, Loss Train(MSE): 0.12564694086037473, R2 Train: 0.4974122365585011\n",
      "Epoch 2707, Loss Train(MSE): 0.12564585494421426, R2 Train: 0.49741658022314295\n",
      "Epoch 2708, Loss Train(MSE): 0.1256447663051936, R2 Train: 0.49742093477922555\n",
      "Epoch 2709, Loss Train(MSE): 0.12564363751036228, R2 Train: 0.49742544995855087\n",
      "Epoch 2710, Loss Train(MSE): 0.12564261192704126, R2 Train: 0.49742955229183494\n",
      "Epoch 2711, Loss Train(MSE): 0.12564146944704904, R2 Train: 0.49743412221180383\n",
      "Epoch 2712, Loss Train(MSE): 0.12564044754439346, R2 Train: 0.49743820982242615\n",
      "Epoch 2713, Loss Train(MSE): 0.12563934594616571, R2 Train: 0.49744261621533714\n",
      "Epoch 2714, Loss Train(MSE): 0.1256382775392215, R2 Train: 0.497446889843114\n",
      "Epoch 2715, Loss Train(MSE): 0.12563724175268012, R2 Train: 0.4974510329892795\n",
      "Epoch 2716, Loss Train(MSE): 0.1256361270278109, R2 Train: 0.4974554918887564\n",
      "Epoch 2717, Loss Train(MSE): 0.12563515580978257, R2 Train: 0.4974593767608697\n",
      "Epoch 2718, Loss Train(MSE): 0.12563405238579922, R2 Train: 0.4974637904568031\n",
      "Epoch 2719, Loss Train(MSE): 0.1256330313561602, R2 Train: 0.49746787457535924\n",
      "Epoch 2720, Loss Train(MSE): 0.12563199623446794, R2 Train: 0.49747201506212824\n",
      "Epoch 2721, Loss Train(MSE): 0.1256309245985959, R2 Train: 0.4974763016056164\n",
      "Epoch 2722, Loss Train(MSE): 0.12562995827055431, R2 Train: 0.49748016691778274\n",
      "Epoch 2723, Loss Train(MSE): 0.12562888001514524, R2 Train: 0.497484479939419\n",
      "Epoch 2724, Loss Train(MSE): 0.12562789377927816, R2 Train: 0.49748842488288736\n",
      "Epoch 2725, Loss Train(MSE): 0.12562686989559665, R2 Train: 0.4974925204176134\n",
      "Epoch 2726, Loss Train(MSE): 0.12562583013530895, R2 Train: 0.4974966794587642\n",
      "Epoch 2727, Loss Train(MSE): 0.12562487721359597, R2 Train: 0.4975004911456161\n",
      "Epoch 2728, Loss Train(MSE): 0.12562382393910687, R2 Train: 0.4975047042435725\n",
      "Epoch 2729, Loss Train(MSE): 0.1256228613261003, R2 Train: 0.49750855469559885\n",
      "Epoch 2730, Loss Train(MSE): 0.1256218579570785, R2 Train: 0.497512568171686\n",
      "Epoch 2731, Loss Train(MSE): 0.12562096301299042, R2 Train: 0.49751614794803833\n",
      "Epoch 2732, Loss Train(MSE): 0.1256205205500899, R2 Train: 0.4975179177996404\n",
      "Epoch 2733, Loss Train(MSE): 0.12561997576723138, R2 Train: 0.49752009693107446\n",
      "Epoch 2734, Loss Train(MSE): 0.12561950837119126, R2 Train: 0.49752196651523495\n",
      "Epoch 2735, Loss Train(MSE): 0.12561901182254082, R2 Train: 0.49752395270983674\n",
      "Epoch 2736, Loss Train(MSE): 0.12561848008361556, R2 Train: 0.49752607966553775\n",
      "Epoch 2737, Loss Train(MSE): 0.12561805135919357, R2 Train: 0.4975277945632257\n",
      "Epoch 2738, Loss Train(MSE): 0.1256175121752623, R2 Train: 0.49752995129895083\n",
      "Epoch 2739, Loss Train(MSE): 0.12561703746973626, R2 Train: 0.49753185012105494\n",
      "Epoch 2740, Loss Train(MSE): 0.12561655731065033, R2 Train: 0.4975337707573987\n",
      "Epoch 2741, Loss Train(MSE): 0.12561602158503998, R2 Train: 0.49753591365984007\n",
      "Epoch 2742, Loss Train(MSE): 0.12561560190052784, R2 Train: 0.49753759239788864\n",
      "Epoch 2743, Loss Train(MSE): 0.1256172979263091, R2 Train: 0.4975308082947636\n",
      "Epoch 2744, Loss Train(MSE): 0.12562408625698532, R2 Train: 0.49750365497205873\n",
      "Epoch 2745, Loss Train(MSE): 0.12562347282308187, R2 Train: 0.49750610870767253\n",
      "Epoch 2746, Loss Train(MSE): 0.12562286051190502, R2 Train: 0.4975085579523799\n",
      "Epoch 2747, Loss Train(MSE): 0.12562224932034527, R2 Train: 0.4975110027186189\n",
      "Epoch 2748, Loss Train(MSE): 0.12562163924530592, R2 Train: 0.4975134430187763\n",
      "Epoch 2749, Loss Train(MSE): 0.1256210302837028, R2 Train: 0.4975158788651888\n",
      "Epoch 2750, Loss Train(MSE): 0.12562042243246452, R2 Train: 0.49751831027014193\n",
      "Epoch 2751, Loss Train(MSE): 0.12561981568853198, R2 Train: 0.4975207372458721\n",
      "Epoch 2752, Loss Train(MSE): 0.12561921004885862, R2 Train: 0.4975231598045655\n",
      "Epoch 2753, Loss Train(MSE): 0.12561860551041015, R2 Train: 0.4975255779583594\n",
      "Epoch 2754, Loss Train(MSE): 0.12561800207016463, R2 Train: 0.4975279917193415\n",
      "Epoch 2755, Loss Train(MSE): 0.12561739972511216, R2 Train: 0.4975304010995514\n",
      "Epoch 2756, Loss Train(MSE): 0.12561679847225513, R2 Train: 0.4975328061109795\n",
      "Epoch 2757, Loss Train(MSE): 0.12561619830860773, R2 Train: 0.49753520676556906\n",
      "Epoch 2758, Loss Train(MSE): 0.1256155992311964, R2 Train: 0.49753760307521444\n",
      "Epoch 2759, Loss Train(MSE): 0.12561500123705918, R2 Train: 0.4975399950517633\n",
      "Epoch 2760, Loss Train(MSE): 0.12561440432324605, R2 Train: 0.4975423827070158\n",
      "Epoch 2761, Loss Train(MSE): 0.1256138084868188, R2 Train: 0.4975447660527248\n",
      "Epoch 2762, Loss Train(MSE): 0.12561321372485065, R2 Train: 0.4975471451005974\n",
      "Epoch 2763, Loss Train(MSE): 0.1256126200344267, R2 Train: 0.49754951986229323\n",
      "Epoch 2764, Loss Train(MSE): 0.12561202741264332, R2 Train: 0.4975518903494267\n",
      "Epoch 2765, Loss Train(MSE): 0.12561143585660856, R2 Train: 0.4975542565735658\n",
      "Epoch 2766, Loss Train(MSE): 0.1256108453634416, R2 Train: 0.49755661854623356\n",
      "Epoch 2767, Loss Train(MSE): 0.12561025593027322, R2 Train: 0.4975589762789071\n",
      "Epoch 2768, Loss Train(MSE): 0.12560966755424519, R2 Train: 0.49756132978301926\n",
      "Epoch 2769, Loss Train(MSE): 0.12560908023251058, R2 Train: 0.4975636790699577\n",
      "Epoch 2770, Loss Train(MSE): 0.12560849396223359, R2 Train: 0.49756602415106566\n",
      "Epoch 2771, Loss Train(MSE): 0.12560790874058939, R2 Train: 0.49756836503764246\n",
      "Epoch 2772, Loss Train(MSE): 0.12560732456476414, R2 Train: 0.4975707017409434\n",
      "Epoch 2773, Loss Train(MSE): 0.12560674143195508, R2 Train: 0.4975730342721797\n",
      "Epoch 2774, Loss Train(MSE): 0.12560615933937005, R2 Train: 0.4975753626425198\n",
      "Epoch 2775, Loss Train(MSE): 0.12560557828422783, R2 Train: 0.4975776868630887\n",
      "Epoch 2776, Loss Train(MSE): 0.12560499826375784, R2 Train: 0.4975800069449686\n",
      "Epoch 2777, Loss Train(MSE): 0.1256044192752003, R2 Train: 0.4975823228991988\n",
      "Epoch 2778, Loss Train(MSE): 0.12560384131580588, R2 Train: 0.4975846347367765\n",
      "Epoch 2779, Loss Train(MSE): 0.1256032643828358, R2 Train: 0.49758694246865676\n",
      "Epoch 2780, Loss Train(MSE): 0.1256026884735619, R2 Train: 0.4975892461057524\n",
      "Epoch 2781, Loss Train(MSE): 0.12560211358526627, R2 Train: 0.49759154565893493\n",
      "Epoch 2782, Loss Train(MSE): 0.12560153971524146, R2 Train: 0.49759384113903415\n",
      "Epoch 2783, Loss Train(MSE): 0.12560096686079023, R2 Train: 0.4975961325568391\n",
      "Epoch 2784, Loss Train(MSE): 0.12560039501922574, R2 Train: 0.49759841992309706\n",
      "Epoch 2785, Loss Train(MSE): 0.12559982418787102, R2 Train: 0.4976007032485159\n",
      "Epoch 2786, Loss Train(MSE): 0.12559925436405966, R2 Train: 0.49760298254376134\n",
      "Epoch 2787, Loss Train(MSE): 0.12559868554513492, R2 Train: 0.4976052578194603\n",
      "Epoch 2788, Loss Train(MSE): 0.12559811772845025, R2 Train: 0.497607529086199\n",
      "Epoch 2789, Loss Train(MSE): 0.12559755091136907, R2 Train: 0.49760979635452374\n",
      "Epoch 2790, Loss Train(MSE): 0.12559698509126463, R2 Train: 0.4976120596349415\n",
      "Epoch 2791, Loss Train(MSE): 0.12559642026552, R2 Train: 0.49761431893791996\n",
      "Epoch 2792, Loss Train(MSE): 0.1255958564315282, R2 Train: 0.49761657427388717\n",
      "Epoch 2793, Loss Train(MSE): 0.1255952935866918, R2 Train: 0.4976188256532328\n",
      "Epoch 2794, Loss Train(MSE): 0.12559473172842306, R2 Train: 0.49762107308630776\n",
      "Epoch 2795, Loss Train(MSE): 0.12559417085414393, R2 Train: 0.4976233165834243\n",
      "Epoch 2796, Loss Train(MSE): 0.12559361096128596, R2 Train: 0.49762555615485615\n",
      "Epoch 2797, Loss Train(MSE): 0.12559305204729013, R2 Train: 0.4976277918108395\n",
      "Epoch 2798, Loss Train(MSE): 0.12559249410960702, R2 Train: 0.49763002356157193\n",
      "Epoch 2799, Loss Train(MSE): 0.12559197691642224, R2 Train: 0.49763209233431105\n",
      "Epoch 2800, Loss Train(MSE): 0.1255915540984451, R2 Train: 0.49763378360621957\n",
      "Epoch 2801, Loss Train(MSE): 0.1255911129276399, R2 Train: 0.49763554828944045\n",
      "Epoch 2802, Loss Train(MSE): 0.12559063298114542, R2 Train: 0.4976374680754183\n",
      "Epoch 2803, Loss Train(MSE): 0.1255902354715714, R2 Train: 0.49763905811371445\n",
      "Epoch 2804, Loss Train(MSE): 0.12558977317736836, R2 Train: 0.49764090729052657\n",
      "Epoch 2805, Loss Train(MSE): 0.1255893022218722, R2 Train: 0.4976427911125112\n",
      "Epoch 2806, Loss Train(MSE): 0.12558891595888313, R2 Train: 0.49764433616446746\n",
      "Epoch 2807, Loss Train(MSE): 0.12558844000272618, R2 Train: 0.4976462399890953\n",
      "Epoch 2808, Loss Train(MSE): 0.12558799305419768, R2 Train: 0.49764802778320927\n",
      "Epoch 2809, Loss Train(MSE): 0.12558758689653915, R2 Train: 0.4976496524138434\n",
      "Epoch 2810, Loss Train(MSE): 0.1255871134175466, R2 Train: 0.49765154632981357\n",
      "Epoch 2811, Loss Train(MSE): 0.1255866894143858, R2 Train: 0.4976532423424568\n",
      "Epoch 2812, Loss Train(MSE): 0.125586264378006, R2 Train: 0.49765494248797604\n",
      "Epoch 2813, Loss Train(MSE): 0.12558579334580852, R2 Train: 0.4976568266167659\n",
      "Epoch 2814, Loss Train(MSE): 0.12558539125493381, R2 Train: 0.49765843498026474\n",
      "Epoch 2815, Loss Train(MSE): 0.12558494832839254, R2 Train: 0.49766020668642985\n",
      "Epoch 2816, Loss Train(MSE): 0.1255844797133712, R2 Train: 0.4976620811465152\n",
      "Epoch 2817, Loss Train(MSE): 0.12558409852941155, R2 Train: 0.4976636058823538\n",
      "Epoch 2818, Loss Train(MSE): 0.12558363867464672, R2 Train: 0.4976654453014131\n",
      "Epoch 2819, Loss Train(MSE): 0.12558318351783157, R2 Train: 0.4976672659286737\n",
      "Epoch 2820, Loss Train(MSE): 0.12558280010207512, R2 Train: 0.4976687995916995\n",
      "Epoch 2821, Loss Train(MSE): 0.1255823352643363, R2 Train: 0.49767065894265483\n",
      "Epoch 2822, Loss Train(MSE): 0.1255818998946982, R2 Train: 0.4976724004212072\n",
      "Epoch 2823, Loss Train(MSE): 0.12558150060545603, R2 Train: 0.4976739975781759\n",
      "Epoch 2824, Loss Train(MSE): 0.12558103811355212, R2 Train: 0.4976758475457915\n",
      "Epoch 2825, Loss Train(MSE): 0.12558062158832992, R2 Train: 0.49767751364668034\n",
      "Epoch 2826, Loss Train(MSE): 0.1255802073274628, R2 Train: 0.4976791706901488\n",
      "Epoch 2827, Loss Train(MSE): 0.12557974715418307, R2 Train: 0.4976810113832677\n",
      "Epoch 2828, Loss Train(MSE): 0.12557934855575414, R2 Train: 0.4976826057769834\n",
      "Epoch 2829, Loss Train(MSE): 0.1255789202009423, R2 Train: 0.4976843191962308\n",
      "Epoch 2830, Loss Train(MSE): 0.1255784623197174, R2 Train: 0.4976861507211304\n",
      "Epoch 2831, Loss Train(MSE): 0.12557808075489799, R2 Train: 0.49768767698040806\n",
      "Epoch 2832, Loss Train(MSE): 0.12557763916030687, R2 Train: 0.4976894433587725\n",
      "Epoch 2833, Loss Train(MSE): 0.12557718354518707, R2 Train: 0.4976912658192517\n",
      "Epoch 2834, Loss Train(MSE): 0.1255768181445548, R2 Train: 0.49769272742178083\n",
      "Epoch 2835, Loss Train(MSE): 0.12557636414147963, R2 Train: 0.4976945434340815\n",
      "Epoch 2836, Loss Train(MSE): 0.12557592436268075, R2 Train: 0.497696302549277\n",
      "Epoch 2837, Loss Train(MSE): 0.12557554707497165, R2 Train: 0.4976978117001134\n",
      "Epoch 2838, Loss Train(MSE): 0.12557509500682876, R2 Train: 0.49769961997268497\n",
      "Epoch 2839, Loss Train(MSE): 0.12557467046464615, R2 Train: 0.4977013181414154\n",
      "Epoch 2840, Loss Train(MSE): 0.12557428163884013, R2 Train: 0.4977028734446395\n",
      "Epoch 2841, Loss Train(MSE): 0.12557383177422565, R2 Train: 0.4977046729030974\n",
      "Epoch 2842, Loss Train(MSE): 0.12557342165248736, R2 Train: 0.49770631339005056\n",
      "Epoch 2843, Loss Train(MSE): 0.12558806580018506, R2 Train: 0.4976477367992598\n",
      "Epoch 2844, Loss Train(MSE): 0.12561789366161813, R2 Train: 0.4975284253535275\n",
      "Epoch 2845, Loss Train(MSE): 0.12561689691830208, R2 Train: 0.4975324123267917\n",
      "Epoch 2846, Loss Train(MSE): 0.12561584063092843, R2 Train: 0.4975366374762863\n",
      "Epoch 2847, Loss Train(MSE): 0.1256148147845738, R2 Train: 0.49754074086170486\n",
      "Epoch 2848, Loss Train(MSE): 0.1256138067391647, R2 Train: 0.4975447730433412\n",
      "Epoch 2849, Loss Train(MSE): 0.12561275066631478, R2 Train: 0.4975489973347409\n",
      "Epoch 2850, Loss Train(MSE): 0.12561179165974928, R2 Train: 0.49755283336100287\n",
      "Epoch 2851, Loss Train(MSE): 0.125610720048376, R2 Train: 0.497557119806496\n",
      "Epoch 2852, Loss Train(MSE): 0.12560977963797532, R2 Train: 0.49756088144809874\n",
      "Epoch 2853, Loss Train(MSE): 0.12560873377635304, R2 Train: 0.49756506489458785\n",
      "Epoch 2854, Loss Train(MSE): 0.12560775940983973, R2 Train: 0.4975689623606411\n",
      "Epoch 2855, Loss Train(MSE): 0.12560676551054514, R2 Train: 0.49757293795781943\n",
      "Epoch 2856, Loss Train(MSE): 0.12560575621187237, R2 Train: 0.49757697515251054\n",
      "Epoch 2857, Loss Train(MSE): 0.12560481495154885, R2 Train: 0.4975807401938046\n",
      "Epoch 2858, Loss Train(MSE): 0.12560377902419412, R2 Train: 0.49758488390322353\n",
      "Epoch 2859, Loss Train(MSE): 0.12560287288064811, R2 Train: 0.49758850847740754\n",
      "Epoch 2860, Loss Train(MSE): 0.12560185558745238, R2 Train: 0.4975925776501905\n",
      "Epoch 2861, Loss Train(MSE): 0.1256009111903653, R2 Train: 0.49759635523853885\n",
      "Epoch 2862, Loss Train(MSE): 0.12559994911765507, R2 Train: 0.4976002035293797\n",
      "Epoch 2863, Loss Train(MSE): 0.1255989656223357, R2 Train: 0.49760413751065724\n",
      "Epoch 2864, Loss Train(MSE): 0.12559805933979526, R2 Train: 0.49760776264081896\n",
      "Epoch 2865, Loss Train(MSE): 0.12559705698247606, R2 Train: 0.49761177207009577\n",
      "Epoch 2866, Loss Train(MSE): 0.12559616523692507, R2 Train: 0.4976153390522997\n",
      "Epoch 2867, Loss Train(MSE): 0.125595192778994, R2 Train: 0.497619228884024\n",
      "Epoch 2868, Loss Train(MSE): 0.1255942589797818, R2 Train: 0.4976229640808728\n",
      "Epoch 2869, Loss Train(MSE): 0.12559334458714558, R2 Train: 0.4976266216514177\n",
      "Epoch 2870, Loss Train(MSE): 0.12559236800675377, R2 Train: 0.4976305279729849\n",
      "Epoch 2871, Loss Train(MSE): 0.12559151215387138, R2 Train: 0.4976339513845145\n",
      "Epoch 2872, Loss Train(MSE): 0.12559054142500145, R2 Train: 0.4976378342999942\n",
      "Epoch 2873, Loss Train(MSE): 0.12558964618531518, R2 Train: 0.4976414152587393\n",
      "Epoch 2874, Loss Train(MSE): 0.12558873314172414, R2 Train: 0.49764506743310344\n",
      "Epoch 2875, Loss Train(MSE): 0.12558779249389684, R2 Train: 0.4976488300244126\n",
      "Epoch 2876, Loss Train(MSE): 0.12558693999036286, R2 Train: 0.4976522400385486\n",
      "Epoch 2877, Loss Train(MSE): 0.1255859909685183, R2 Train: 0.4976560361259268\n",
      "Epoch 2878, Loss Train(MSE): 0.12558512435598573, R2 Train: 0.49765950257605707\n",
      "Epoch 2879, Loss Train(MSE): 0.12558422100967997, R2 Train: 0.4976631159612801\n",
      "Epoch 2880, Loss Train(MSE): 0.12558330658444009, R2 Train: 0.49766677366223966\n",
      "Epoch 2881, Loss Train(MSE): 0.12558246559150033, R2 Train: 0.4976701376339987\n",
      "Epoch 2882, Loss Train(MSE): 0.1255815373989605, R2 Train: 0.497673850404158\n",
      "Epoch 2883, Loss Train(MSE): 0.12558069016162943, R2 Train: 0.4976772393534823\n",
      "Epoch 2884, Loss Train(MSE): 0.12557980426958887, R2 Train: 0.4976807829216445\n",
      "Epoch 2885, Loss Train(MSE): 0.12557890702095428, R2 Train: 0.49768437191618287\n",
      "Epoch 2886, Loss Train(MSE): 0.12557808512252724, R2 Train: 0.497687659509891\n",
      "Epoch 2887, Loss Train(MSE): 0.1255771769311333, R2 Train: 0.4976912922754668\n",
      "Epoch 2888, Loss Train(MSE): 0.12557634045021654, R2 Train: 0.49769463819913384\n",
      "Epoch 2889, Loss Train(MSE): 0.12557547921916332, R2 Train: 0.49769808312334674\n",
      "Epoch 2890, Loss Train(MSE): 0.12557482290335106, R2 Train: 0.49770070838659575\n",
      "Epoch 2891, Loss Train(MSE): 0.12557444838410242, R2 Train: 0.4977022064635903\n",
      "Epoch 2892, Loss Train(MSE): 0.12557397795443362, R2 Train: 0.4977040881822655\n",
      "Epoch 2893, Loss Train(MSE): 0.12557356037975334, R2 Train: 0.49770575848098664\n",
      "Epoch 2894, Loss Train(MSE): 0.12557314368514558, R2 Train: 0.4977074252594177\n",
      "Epoch 2895, Loss Train(MSE): 0.12557267605426814, R2 Train: 0.49770929578292744\n",
      "Epoch 2896, Loss Train(MSE): 0.12557230360425053, R2 Train: 0.49771078558299786\n",
      "Epoch 2897, Loss Train(MSE): 0.1255718462559987, R2 Train: 0.49771261497600516\n",
      "Epoch 2898, Loss Train(MSE): 0.12557141465589153, R2 Train: 0.4977143413764339\n",
      "Epoch 2899, Loss Train(MSE): 0.12557101917857527, R2 Train: 0.4977159232856989\n",
      "Epoch 2900, Loss Train(MSE): 0.12557055589209376, R2 Train: 0.49771777643162496\n",
      "Epoch 2901, Loss Train(MSE): 0.12557016739324964, R2 Train: 0.49771933042700145\n",
      "Epoch 2902, Loss Train(MSE): 0.12556973318581985, R2 Train: 0.4977210672567206\n",
      "Epoch 2903, Loss Train(MSE): 0.12556928510085744, R2 Train: 0.49772285959657026\n",
      "Epoch 2904, Loss Train(MSE): 0.12556891314245183, R2 Train: 0.49772434743019267\n",
      "Epoch 2905, Loss Train(MSE): 0.12556845409849712, R2 Train: 0.4977261836060115\n",
      "Epoch 2906, Loss Train(MSE): 0.12556804718043685, R2 Train: 0.4977278112782526\n",
      "Epoch 2907, Loss Train(MSE): 0.12556763833093648, R2 Train: 0.4977294466762541\n",
      "Epoch 2908, Loss Train(MSE): 0.1255671819149117, R2 Train: 0.4977312723403532\n",
      "Epoch 2909, Loss Train(MSE): 0.12556681473410541, R2 Train: 0.49773274106357834\n",
      "Epoch 2910, Loss Train(MSE): 0.12556637036395096, R2 Train: 0.49773451854419615\n",
      "Epoch 2911, Loss Train(MSE): 0.12556594280513872, R2 Train: 0.4977362287794451\n",
      "Epoch 2912, Loss Train(MSE): 0.12556556138651676, R2 Train: 0.497737754453933\n",
      "Epoch 2913, Loss Train(MSE): 0.1255651090546663, R2 Train: 0.49773956378133477\n",
      "Epoch 2914, Loss Train(MSE): 0.12556471944013287, R2 Train: 0.49774112223946854\n",
      "Epoch 2915, Loss Train(MSE): 0.12556430420538503, R2 Train: 0.4977427831784599\n",
      "Epoch 2916, Loss Train(MSE): 0.1255638544047977, R2 Train: 0.4977445823808092\n",
      "Epoch 2917, Loss Train(MSE): 0.12556350139527458, R2 Train: 0.49774599441890166\n",
      "Epoch 2918, Loss Train(MSE): 0.12556305362863696, R2 Train: 0.4977477854854522\n",
      "Epoch 2919, Loss Train(MSE): 0.12556263955197844, R2 Train: 0.49774944179208624\n",
      "Epoch 2920, Loss Train(MSE): 0.1255622553430397, R2 Train: 0.49775097862784123\n",
      "Epoch 2921, Loss Train(MSE): 0.1255618094794057, R2 Train: 0.4977527620823772\n",
      "Epoch 2922, Loss Train(MSE): 0.1255614303455663, R2 Train: 0.4977542786177348\n",
      "Epoch 2923, Loss Train(MSE): 0.12556101518470736, R2 Train: 0.49775593926117057\n",
      "Epoch 2924, Loss Train(MSE): 0.1255605747054668, R2 Train: 0.4977577011781328\n",
      "Epoch 2925, Loss Train(MSE): 0.12556022333287165, R2 Train: 0.4977591066685134\n",
      "Epoch 2926, Loss Train(MSE): 0.1255597813220469, R2 Train: 0.4977608747118124\n",
      "Epoch 2927, Loss Train(MSE): 0.12555937419576813, R2 Train: 0.4977625032169275\n",
      "Epoch 2928, Loss Train(MSE): 0.12555899338234802, R2 Train: 0.4977640264706079\n",
      "Epoch 2929, Loss Train(MSE): 0.12555855376132585, R2 Train: 0.4977657849546966\n",
      "Epoch 2930, Loss Train(MSE): 0.12555817877911268, R2 Train: 0.49776728488354927\n",
      "Epoch 2931, Loss Train(MSE): 0.12555776968461263, R2 Train: 0.4977689212615495\n",
      "Epoch 2932, Loss Train(MSE): 0.12555733281322773, R2 Train: 0.4977706687470891\n",
      "Epoch 2933, Loss Train(MSE): 0.1255569879768018, R2 Train: 0.49777204809279285\n",
      "Epoch 2934, Loss Train(MSE): 0.12555655207708782, R2 Train: 0.4977737916916487\n",
      "Epoch 2935, Loss Train(MSE): 0.12555614587560718, R2 Train: 0.4977754164975713\n",
      "Epoch 2936, Loss Train(MSE): 0.12555577415880237, R2 Train: 0.4977769033647905\n",
      "Epoch 2937, Loss Train(MSE): 0.12555534056831233, R2 Train: 0.4977786377267507\n",
      "Epoch 2938, Loss Train(MSE): 0.12555496390236148, R2 Train: 0.49778014439055407\n",
      "Epoch 2939, Loss Train(MSE): 0.12555456639363136, R2 Train: 0.4977817344254746\n",
      "Epoch 2940, Loss Train(MSE): 0.12555413508191474, R2 Train: 0.49778345967234106\n",
      "Epoch 2941, Loss Train(MSE): 0.12555378684637927, R2 Train: 0.4977848526144829\n",
      "Epoch 2942, Loss Train(MSE): 0.12555336460612265, R2 Train: 0.4977865415755094\n",
      "Epoch 2943, Loss Train(MSE): 0.1255529537697676, R2 Train: 0.4977881849209296\n",
      "Epoch 2944, Loss Train(MSE): 0.12555259640448002, R2 Train: 0.4977896143820799\n",
      "Epoch 2945, Loss Train(MSE): 0.1255521686449041, R2 Train: 0.4977913254203836\n",
      "Epoch 2946, Loss Train(MSE): 0.12555178491422322, R2 Train: 0.4977928603431071\n",
      "Epoch 2947, Loss Train(MSE): 0.1255514040752846, R2 Train: 0.49779438369886164\n",
      "Epoch 2948, Loss Train(MSE): 0.12555097852106306, R2 Train: 0.49779608591574775\n",
      "Epoch 2949, Loss Train(MSE): 0.12555062085822807, R2 Train: 0.4977975165670877\n",
      "Epoch 2950, Loss Train(MSE): 0.1255502175415767, R2 Train: 0.49779912983369323\n",
      "Epoch 2951, Loss Train(MSE): 0.12554979692250084, R2 Train: 0.4978008123099966\n",
      "Epoch 2952, Loss Train(MSE): 0.1255494587733064, R2 Train: 0.4978021649067744\n",
      "Epoch 2953, Loss Train(MSE): 0.12554903665923264, R2 Train: 0.4978038533630694\n",
      "Epoch 2954, Loss Train(MSE): 0.12554864087977638, R2 Train: 0.4978054364808945\n",
      "Epoch 2955, Loss Train(MSE): 0.12554828141842825, R2 Train: 0.497806874326287\n",
      "Epoch 2956, Loss Train(MSE): 0.12554786144086946, R2 Train: 0.49780855423652215\n",
      "Epoch 2957, Loss Train(MSE): 0.12554748952583988, R2 Train: 0.49781004189664046\n",
      "Epoch 2958, Loss Train(MSE): 0.12554710968821406, R2 Train: 0.49781156124714376\n",
      "Epoch 2959, Loss Train(MSE): 0.12554669182103248, R2 Train: 0.4978132327158701\n",
      "Epoch 2960, Loss Train(MSE): 0.125546342820055, R2 Train: 0.49781462871978\n",
      "Epoch 2961, Loss Train(MSE): 0.12554594351817536, R2 Train: 0.4978162259272986\n",
      "Epoch 2962, Loss Train(MSE): 0.12554553108971678, R2 Train: 0.4978178756411329\n",
      "Epoch 2963, Loss Train(MSE): 0.1255451973454212, R2 Train: 0.49781921061831524\n",
      "Epoch 2964, Loss Train(MSE): 0.1255447827735289, R2 Train: 0.4978208689058844\n",
      "Epoch 2965, Loss Train(MSE): 0.12554439215610347, R2 Train: 0.4978224313755861\n",
      "Epoch 2966, Loss Train(MSE): 0.1255440399931567, R2 Train: 0.49782384002737323\n",
      "Epoch 2967, Loss Train(MSE): 0.12554362746856793, R2 Train: 0.4978254901257283\n",
      "Epoch 2968, Loss Train(MSE): 0.1255432577684969, R2 Train: 0.4978269689260124\n",
      "Epoch 2969, Loss Train(MSE): 0.12554288804448885, R2 Train: 0.4978284478220446\n",
      "Epoch 2970, Loss Train(MSE): 0.12554247754319234, R2 Train: 0.49783008982723065\n",
      "Epoch 2971, Loss Train(MSE): 0.12554212788938796, R2 Train: 0.4978314884424482\n",
      "Epoch 2972, Loss Train(MSE): 0.12554555745588275, R2 Train: 0.497817770176469\n",
      "Epoch 2973, Loss Train(MSE): 0.12555008433734735, R2 Train: 0.4977996626506106\n",
      "Epoch 2974, Loss Train(MSE): 0.12554959774935556, R2 Train: 0.49780160900257775\n",
      "Epoch 2975, Loss Train(MSE): 0.12554911195617727, R2 Train: 0.49780355217529093\n",
      "Epoch 2976, Loss Train(MSE): 0.12554862695591576, R2 Train: 0.49780549217633696\n",
      "Epoch 2977, Loss Train(MSE): 0.12554814274668039, R2 Train: 0.49780742901327846\n",
      "Epoch 2978, Loss Train(MSE): 0.1255476593265866, R2 Train: 0.4978093626936536\n",
      "Epoch 2979, Loss Train(MSE): 0.12554717669375595, R2 Train: 0.4978112932249762\n",
      "Epoch 2980, Loss Train(MSE): 0.12554669484631598, R2 Train: 0.4978132206147361\n",
      "Epoch 2981, Loss Train(MSE): 0.12554621378240022, R2 Train: 0.49781514487039913\n",
      "Epoch 2982, Loss Train(MSE): 0.1255457335001482, R2 Train: 0.4978170659994072\n",
      "Epoch 2983, Loss Train(MSE): 0.1255452539977053, R2 Train: 0.4978189840091788\n",
      "Epoch 2984, Loss Train(MSE): 0.125544775273223, R2 Train: 0.497820898907108\n",
      "Epoch 2985, Loss Train(MSE): 0.12554429732485856, R2 Train: 0.4978228107005658\n",
      "Epoch 2986, Loss Train(MSE): 0.12554382015077506, R2 Train: 0.49782471939689976\n",
      "Epoch 2987, Loss Train(MSE): 0.1255433437491415, R2 Train: 0.497826625003434\n",
      "Epoch 2988, Loss Train(MSE): 0.12554286811813278, R2 Train: 0.49782852752746887\n",
      "Epoch 2989, Loss Train(MSE): 0.1255423932559294, R2 Train: 0.49783042697628244\n",
      "Epoch 2990, Loss Train(MSE): 0.1255419191607178, R2 Train: 0.4978323233571288\n",
      "Epoch 2991, Loss Train(MSE): 0.12554144583069005, R2 Train: 0.4978342166772398\n",
      "Epoch 2992, Loss Train(MSE): 0.12554097326404404, R2 Train: 0.49783610694382385\n",
      "Epoch 2993, Loss Train(MSE): 0.12554050145898327, R2 Train: 0.49783799416406693\n",
      "Epoch 2994, Loss Train(MSE): 0.12554003041371703, R2 Train: 0.4978398783451319\n",
      "Epoch 2995, Loss Train(MSE): 0.12553956012646006, R2 Train: 0.49784175949415976\n",
      "Epoch 2996, Loss Train(MSE): 0.125539090595433, R2 Train: 0.497843637618268\n",
      "Epoch 2997, Loss Train(MSE): 0.12553862181886186, R2 Train: 0.4978455127245526\n",
      "Epoch 2998, Loss Train(MSE): 0.12553815379497835, R2 Train: 0.4978473848200866\n",
      "Epoch 2999, Loss Train(MSE): 0.1255376865220197, R2 Train: 0.49784925391192125\n",
      "Epoch 3000, Loss Train(MSE): 0.12553721999822864, R2 Train: 0.4978511200070854\n",
      "Epoch 3001, Loss Train(MSE): 0.12553675422185348, R2 Train: 0.4978529831125861\n",
      "Epoch 3002, Loss Train(MSE): 0.12553628919114798, R2 Train: 0.4978548432354081\n",
      "Epoch 3003, Loss Train(MSE): 0.12553582490437135, R2 Train: 0.4978567003825146\n",
      "Epoch 3004, Loss Train(MSE): 0.12553536135978827, R2 Train: 0.49785855456084693\n",
      "Epoch 3005, Loss Train(MSE): 0.1255348985556688, R2 Train: 0.4978604057773248\n",
      "Epoch 3006, Loss Train(MSE): 0.12553443649028845, R2 Train: 0.4978622540388462\n",
      "Epoch 3007, Loss Train(MSE): 0.12553397516192807, R2 Train: 0.4978640993522877\n",
      "Epoch 3008, Loss Train(MSE): 0.12553826118921718, R2 Train: 0.4978469552431313\n",
      "Epoch 3009, Loss Train(MSE): 0.12557555329729564, R2 Train: 0.49769778681081744\n",
      "Epoch 3010, Loss Train(MSE): 0.1255745554786708, R2 Train: 0.4977017780853168\n",
      "Epoch 3011, Loss Train(MSE): 0.1255735617701934, R2 Train: 0.49770575291922636\n",
      "Epoch 3012, Loss Train(MSE): 0.12557257214237735, R2 Train: 0.4977097114304906\n",
      "Epoch 3013, Loss Train(MSE): 0.12557158656604273, R2 Train: 0.4977136537358291\n",
      "Epoch 3014, Loss Train(MSE): 0.12557060501231207, R2 Train: 0.4977175799507517\n",
      "Epoch 3015, Loss Train(MSE): 0.12556962745260603, R2 Train: 0.4977214901895759\n",
      "Epoch 3016, Loss Train(MSE): 0.1255686538586395, R2 Train: 0.497725384565442\n",
      "Epoch 3017, Loss Train(MSE): 0.12556768420241796, R2 Train: 0.4977292631903282\n",
      "Epoch 3018, Loss Train(MSE): 0.1255667184562331, R2 Train: 0.4977331261750676\n",
      "Epoch 3019, Loss Train(MSE): 0.12556575659265956, R2 Train: 0.49773697362936176\n",
      "Epoch 3020, Loss Train(MSE): 0.125564798584551, R2 Train: 0.497740805661796\n",
      "Epoch 3021, Loss Train(MSE): 0.1255638444050363, R2 Train: 0.49774462237985484\n",
      "Epoch 3022, Loss Train(MSE): 0.12556289402751614, R2 Train: 0.49774842388993545\n",
      "Epoch 3023, Loss Train(MSE): 0.12556194742565946, R2 Train: 0.49775221029736216\n",
      "Epoch 3024, Loss Train(MSE): 0.12556100457339994, R2 Train: 0.49775598170640023\n",
      "Epoch 3025, Loss Train(MSE): 0.1255600654449324, R2 Train: 0.4977597382202704\n",
      "Epoch 3026, Loss Train(MSE): 0.1255591300147096, R2 Train: 0.49776347994116155\n",
      "Epoch 3027, Loss Train(MSE): 0.12555819825743894, R2 Train: 0.49776720697024424\n",
      "Epoch 3028, Loss Train(MSE): 0.12555727014807896, R2 Train: 0.49777091940768414\n",
      "Epoch 3029, Loss Train(MSE): 0.1255563456618363, R2 Train: 0.4977746173526548\n",
      "Epoch 3030, Loss Train(MSE): 0.12555542477416246, R2 Train: 0.4977783009033502\n",
      "Epoch 3031, Loss Train(MSE): 0.12555450746075067, R2 Train: 0.4977819701569973\n",
      "Epoch 3032, Loss Train(MSE): 0.12555359369753283, R2 Train: 0.4977856252098687\n",
      "Epoch 3033, Loss Train(MSE): 0.12555268346067638, R2 Train: 0.49778926615729446\n",
      "Epoch 3034, Loss Train(MSE): 0.12555184972710154, R2 Train: 0.49779260109159384\n",
      "Epoch 3035, Loss Train(MSE): 0.12555102622511652, R2 Train: 0.4977958950995339\n",
      "Epoch 3036, Loss Train(MSE): 0.12555024440763532, R2 Train: 0.4977990223694587\n",
      "Epoch 3037, Loss Train(MSE): 0.12554939390407976, R2 Train: 0.49780242438368094\n",
      "Epoch 3038, Loss Train(MSE): 0.12554863967320826, R2 Train: 0.497805441307167\n",
      "Epoch 3039, Loss Train(MSE): 0.12554780854080902, R2 Train: 0.4978087658367639\n",
      "Epoch 3040, Loss Train(MSE): 0.12554701327263812, R2 Train: 0.4978119469094475\n",
      "Epoch 3041, Loss Train(MSE): 0.12554623569675463, R2 Train: 0.49781505721298147\n",
      "Epoch 3042, Loss Train(MSE): 0.12554540314573442, R2 Train: 0.4978183874170623\n",
      "Epoch 3043, Loss Train(MSE): 0.1255446711626666, R2 Train: 0.4978213153493336\n",
      "Epoch 3044, Loss Train(MSE): 0.1255438495101581, R2 Train: 0.4978246019593676\n",
      "Epoch 3045, Loss Train(MSE): 0.12554307453267222, R2 Train: 0.49782770186931113\n",
      "Epoch 3046, Loss Train(MSE): 0.12554230793130178, R2 Train: 0.4978307682747929\n",
      "Epoch 3047, Loss Train(MSE): 0.12554149264650347, R2 Train: 0.49783402941398613\n",
      "Epoch 3048, Loss Train(MSE): 0.12554077532389873, R2 Train: 0.49783689870440506\n",
      "Epoch 3049, Loss Train(MSE): 0.12553996956817515, R2 Train: 0.4978401217272994\n",
      "Epoch 3050, Loss Train(MSE): 0.12553920745393227, R2 Train: 0.49784317018427093\n",
      "Epoch 3051, Loss Train(MSE): 0.12553845810846365, R2 Train: 0.4978461675661454\n",
      "Epoch 3052, Loss Train(MSE): 0.12553765944029097, R2 Train: 0.49784936223883614\n",
      "Epoch 3053, Loss Train(MSE): 0.12553694968380757, R2 Train: 0.4978522012647697\n",
      "Epoch 3054, Loss Train(MSE): 0.12553616581028307, R2 Train: 0.49785533675886773\n",
      "Epoch 3055, Loss Train(MSE): 0.1255354096137125, R2 Train: 0.49785836154514995\n",
      "Epoch 3056, Loss Train(MSE): 0.12553468338357154, R2 Train: 0.4978612664657138\n",
      "Epoch 3057, Loss Train(MSE): 0.12553390071670645, R2 Train: 0.4978643971331742\n",
      "Epoch 3058, Loss Train(MSE): 0.12553319189282944, R2 Train: 0.49786723242868225\n",
      "Epoch 3059, Loss Train(MSE): 0.125532683313556, R2 Train: 0.49786926674577603\n",
      "Epoch 3060, Loss Train(MSE): 0.12553229134473567, R2 Train: 0.4978708346210573\n",
      "Epoch 3061, Loss Train(MSE): 0.12553195669583475, R2 Train: 0.497872173216661\n",
      "Epoch 3062, Loss Train(MSE): 0.12553154979943074, R2 Train: 0.49787380080227706\n",
      "Epoch 3063, Loss Train(MSE): 0.12553119454182968, R2 Train: 0.4978752218326813\n",
      "Epoch 3064, Loss Train(MSE): 0.12553082679133484, R2 Train: 0.49787669283466063\n",
      "Epoch 3065, Loss Train(MSE): 0.12553042211524335, R2 Train: 0.4978783115390266\n",
      "Epoch 3066, Loss Train(MSE): 0.12553010235578688, R2 Train: 0.4978795905768525\n",
      "Epoch 3067, Loss Train(MSE): 0.12552970266887742, R2 Train: 0.4978811893244903\n",
      "Epoch 3068, Loss Train(MSE): 0.12552932948255285, R2 Train: 0.4978826820697886\n",
      "Epoch 3069, Loss Train(MSE): 0.1255289853965049, R2 Train: 0.4978840584139804\n",
      "Epoch 3070, Loss Train(MSE): 0.12552858417375856, R2 Train: 0.49788566330496575\n",
      "Epoch 3071, Loss Train(MSE): 0.1255282449585064, R2 Train: 0.4978870201659744\n",
      "Epoch 3072, Loss Train(MSE): 0.12552787039142974, R2 Train: 0.49788851843428106\n",
      "Epoch 3073, Loss Train(MSE): 0.12552747745773685, R2 Train: 0.4978900901690526\n",
      "Epoch 3074, Loss Train(MSE): 0.12552715874145787, R2 Train: 0.49789136503416853\n",
      "Epoch 3075, Loss Train(MSE): 0.1255267608987395, R2 Train: 0.49789295640504205\n",
      "Epoch 3076, Loss Train(MSE): 0.125526400472477, R2 Train: 0.49789439811009195\n",
      "Epoch 3077, Loss Train(MSE): 0.12552605267010394, R2 Train: 0.4978957893195842\n",
      "Epoch 3078, Loss Train(MSE): 0.12552565692459072, R2 Train: 0.4978973723016371\n",
      "Epoch 3079, Loss Train(MSE): 0.12552532790645463, R2 Train: 0.4978986883741815\n",
      "Epoch 3080, Loss Train(MSE): 0.1255249520744432, R2 Train: 0.4979001917022272\n",
      "Epoch 3081, Loss Train(MSE): 0.12552456878692408, R2 Train: 0.49790172485230366\n",
      "Epoch 3082, Loss Train(MSE): 0.1255242492920717, R2 Train: 0.4979030028317132\n",
      "Epoch 3083, Loss Train(MSE): 0.12552385681198255, R2 Train: 0.4979045727520698\n",
      "Epoch 3084, Loss Train(MSE): 0.125523503570358, R2 Train: 0.49790598571856803\n",
      "Epoch 3085, Loss Train(MSE): 0.12552315734396013, R2 Train: 0.49790737062415946\n",
      "Epoch 3086, Loss Train(MSE): 0.12552276689086195, R2 Train: 0.4979089324365522\n",
      "Epoch 3087, Loss Train(MSE): 0.12552244266102575, R2 Train: 0.497910229355897\n",
      "Epoch 3088, Loss Train(MSE): 0.12552207069712132, R2 Train: 0.4979117172115147\n",
      "Epoch 3089, Loss Train(MSE): 0.12552169172003783, R2 Train: 0.4979132331198487\n",
      "Epoch 3090, Loss Train(MSE): 0.12552137651083214, R2 Train: 0.4979144939566714\n",
      "Epoch 3091, Loss Train(MSE): 0.1255209892159854, R2 Train: 0.4979160431360584\n",
      "Epoch 3092, Loss Train(MSE): 0.1255206379827377, R2 Train: 0.49791744806904925\n",
      "Epoch 3093, Loss Train(MSE): 0.1255202982439913, R2 Train: 0.4979188070240348\n",
      "Epoch 3094, Loss Train(MSE): 0.1255199129103878, R2 Train: 0.4979203483584488\n",
      "Epoch 3095, Loss Train(MSE): 0.12551958844765257, R2 Train: 0.4979216462093897\n",
      "Epoch 3096, Loss Train(MSE): 0.12551922511515487, R2 Train: 0.4979230995393805\n",
      "Epoch 3097, Loss Train(MSE): 0.12551884549471498, R2 Train: 0.4979246180211401\n",
      "Epoch 3098, Loss Train(MSE): 0.12551853927091328, R2 Train: 0.4979258429163469\n",
      "Epoch 3099, Loss Train(MSE): 0.12551815699517319, R2 Train: 0.49792737201930726\n",
      "Epoch 3100, Loss Train(MSE): 0.1255178029650063, R2 Train: 0.49792878813997477\n",
      "Epoch 3101, Loss Train(MSE): 0.12551747427148544, R2 Train: 0.49793010291405826\n",
      "Epoch 3102, Loss Train(MSE): 0.12551709389532492, R2 Train: 0.4979316244187003\n",
      "Epoch 3103, Loss Train(MSE): 0.12551676453881888, R2 Train: 0.4979329418447245\n",
      "Epoch 3104, Loss Train(MSE): 0.1255164142569681, R2 Train: 0.4979343429721276\n",
      "Epoch 3105, Loss Train(MSE): 0.12551603575700587, R2 Train: 0.4979358569719765\n",
      "Epoch 3106, Loss Train(MSE): 0.1255157301799178, R2 Train: 0.4979370792803288\n",
      "Epoch 3107, Loss Train(MSE): 0.12551535916962317, R2 Train: 0.4979385633215073\n",
      "Epoch 3108, Loss Train(MSE): 0.12551499789075768, R2 Train: 0.4979400084369693\n",
      "Epoch 3109, Loss Train(MSE): 0.1255146844605064, R2 Train: 0.4979412621579744\n",
      "Epoch 3110, Loss Train(MSE): 0.12551430888858064, R2 Train: 0.4979427644456774\n",
      "Epoch 3111, Loss Train(MSE): 0.1255139703228182, R2 Train: 0.49794411870872723\n",
      "Epoch 3112, Loss Train(MSE): 0.1255136371789638, R2 Train: 0.4979454512841448\n",
      "Epoch 3113, Loss Train(MSE): 0.12551326342672056, R2 Train: 0.49794694629311775\n",
      "Epoch 3114, Loss Train(MSE): 0.12551294673132285, R2 Train: 0.4979482130747086\n",
      "Epoch 3115, Loss Train(MSE): 0.12551259468428386, R2 Train: 0.4979496212628646\n",
      "Epoch 3116, Loss Train(MSE): 0.12551222273023904, R2 Train: 0.49795110907904383\n",
      "Epoch 3117, Loss Train(MSE): 0.12551192708288564, R2 Train: 0.49795229166845745\n",
      "Epoch 3118, Loss Train(MSE): 0.12551155692342866, R2 Train: 0.49795377230628535\n",
      "Epoch 3119, Loss Train(MSE): 0.12551120514212702, R2 Train: 0.4979551794314919\n",
      "Epoch 3120, Loss Train(MSE): 0.12551089292815956, R2 Train: 0.49795642828736175\n",
      "Epoch 3121, Loss Train(MSE): 0.12551052378301752, R2 Train: 0.4979579048679299\n",
      "Epoch 3122, Loss Train(MSE): 0.12551019208673098, R2 Train: 0.4979592316530761\n",
      "Epoch 3123, Loss Train(MSE): 0.1255098626760715, R2 Train: 0.497960549295714\n",
      "Epoch 3124, Loss Train(MSE): 0.12550949527717706, R2 Train: 0.49796201889129177\n",
      "Epoch 3125, Loss Train(MSE): 0.1255091828904227, R2 Train: 0.4979632684383092\n",
      "Epoch 3126, Loss Train(MSE): 0.12550883702880047, R2 Train: 0.4979646518847981\n",
      "Epoch 3127, Loss Train(MSE): 0.12550847135635645, R2 Train: 0.4979661145745742\n",
      "Epoch 3128, Loss Train(MSE): 0.12550817752231558, R2 Train: 0.4979672899107377\n",
      "Epoch 3129, Loss Train(MSE): 0.12550781593747518, R2 Train: 0.4979687362500993\n",
      "Epoch 3130, Loss Train(MSE): 0.12550746563765236, R2 Train: 0.49797013744939056\n",
      "Epoch 3131, Loss Train(MSE): 0.12550716226869443, R2 Train: 0.49797135092522227\n",
      "Epoch 3132, Loss Train(MSE): 0.12550679929550598, R2 Train: 0.49797280281797607\n",
      "Epoch 3133, Loss Train(MSE): 0.1255064666791185, R2 Train: 0.497974133283526\n",
      "Epoch 3134, Loss Train(MSE): 0.12550614841260255, R2 Train: 0.4979754063495898\n",
      "Epoch 3135, Loss Train(MSE): 0.12550578711797228, R2 Train: 0.4979768515281109\n",
      "Epoch 3136, Loss Train(MSE): 0.125505471470848, R2 Train: 0.497978114116608\n",
      "Epoch 3137, Loss Train(MSE): 0.12550513899346702, R2 Train: 0.4979794440261319\n",
      "Epoch 3138, Loss Train(MSE): 0.12550477935910126, R2 Train: 0.49798088256359496\n",
      "Epoch 3139, Loss Train(MSE): 0.12550447998415393, R2 Train: 0.4979820800633843\n",
      "Epoch 3140, Loss Train(MSE): 0.1255041339661195, R2 Train: 0.49798346413552197\n",
      "Epoch 3141, Loss Train(MSE): 0.1255037778835665, R2 Train: 0.49798488846573397\n",
      "Epoch 3142, Loss Train(MSE): 0.12550349026646307, R2 Train: 0.4979860389341477\n",
      "Epoch 3143, Loss Train(MSE): 0.12550313323014353, R2 Train: 0.49798746707942587\n",
      "Epoch 3144, Loss Train(MSE): 0.12550279263568007, R2 Train: 0.49798882945727974\n",
      "Epoch 3145, Loss Train(MSE): 0.12550249222165763, R2 Train: 0.4979900311133695\n",
      "Epoch 3146, Loss Train(MSE): 0.12550213680132777, R2 Train: 0.4979914527946889\n",
      "Epoch 3147, Loss Train(MSE): 0.12550181103674435, R2 Train: 0.4979927558530226\n",
      "Epoch 3148, Loss Train(MSE): 0.1255014984585548, R2 Train: 0.4979940061657808\n",
      "Epoch 3149, Loss Train(MSE): 0.12550114463726453, R2 Train: 0.4979954214509419\n",
      "Epoch 3150, Loss Train(MSE): 0.12550083306001417, R2 Train: 0.49799666775994333\n",
      "Epoch 3151, Loss Train(MSE): 0.125500508935286, R2 Train: 0.497997964258856\n",
      "Epoch 3152, Loss Train(MSE): 0.12550015669644582, R2 Train: 0.4979993732142167\n",
      "Epoch 3153, Loss Train(MSE): 0.1254998586792541, R2 Train: 0.49800056528298364\n",
      "Epoch 3154, Loss Train(MSE): 0.12549952361086486, R2 Train: 0.49800190555654056\n",
      "Epoch 3155, Loss Train(MSE): 0.12549917293823465, R2 Train: 0.4980033082470614\n",
      "Epoch 3156, Loss Train(MSE): 0.1254988878687211, R2 Train: 0.4980044485251156\n",
      "Epoch 3157, Loss Train(MSE): 0.12549854244515954, R2 Train: 0.49800583021936184\n",
      "Epoch 3158, Loss Train(MSE): 0.1254982003502402, R2 Train: 0.4980071985990392\n",
      "Epoch 3159, Loss Train(MSE): 0.12549791356487733, R2 Train: 0.4980083457404907\n",
      "Epoch 3160, Loss Train(MSE): 0.12549756534645312, R2 Train: 0.4980097386141875\n",
      "Epoch 3161, Loss Train(MSE): 0.1254972355355095, R2 Train: 0.49801105785796196\n",
      "Epoch 3162, Loss Train(MSE): 0.1254969390227434, R2 Train: 0.49801224390902643\n",
      "Epoch 3163, Loss Train(MSE): 0.1254965923311936, R2 Train: 0.49801363067522564\n",
      "Epoch 3164, Loss Train(MSE): 0.12549627422549248, R2 Train: 0.4980149030980301\n",
      "Epoch 3165, Loss Train(MSE): 0.125495968541347, R2 Train: 0.49801612583461197\n",
      "Epoch 3166, Loss Train(MSE): 0.1254956233615623, R2 Train: 0.49801750655375077\n",
      "Epoch 3167, Loss Train(MSE): 0.12549531639605166, R2 Train: 0.49801873441579336\n",
      "Epoch 3168, Loss Train(MSE): 0.1254950020833219, R2 Train: 0.4980199916667124\n",
      "Epoch 3169, Loss Train(MSE): 0.12549465840049825, R2 Train: 0.498021366398007\n",
      "Epoch 3170, Loss Train(MSE): 0.12549436202347314, R2 Train: 0.49802255190610745\n",
      "Epoch 3171, Loss Train(MSE): 0.1254940396120454, R2 Train: 0.49802384155181845\n",
      "Epoch 3172, Loss Train(MSE): 0.12549369741167463, R2 Train: 0.4980252103533015\n",
      "Epoch 3173, Loss Train(MSE): 0.1254934110844523, R2 Train: 0.4980263556621908\n",
      "Epoch 3174, Loss Train(MSE): 0.12549308109161533, R2 Train: 0.49802767563353867\n",
      "Epoch 3175, Loss Train(MSE): 0.12549274035947672, R2 Train: 0.49802903856209313\n",
      "Epoch 3176, Loss Train(MSE): 0.12549246355608082, R2 Train: 0.4980301457756767\n",
      "Epoch 3177, Loss Train(MSE): 0.12549212648682892, R2 Train: 0.4980314940526843\n",
      "Epoch 3178, Loss Train(MSE): 0.12549179247543016, R2 Train: 0.49803283009827937\n",
      "Epoch 3179, Loss Train(MSE): 0.1254915141427478, R2 Train: 0.49803394342900875\n",
      "Epoch 3180, Loss Train(MSE): 0.1254911757148268, R2 Train: 0.49803529714069283\n",
      "Epoch 3181, Loss Train(MSE): 0.12549085068556065, R2 Train: 0.4980365972577574\n",
      "Epoch 3182, Loss Train(MSE): 0.1254905657865268, R2 Train: 0.49803773685389285\n",
      "Epoch 3183, Loss Train(MSE): 0.12549022879235822, R2 Train: 0.49803908483056714\n",
      "Epoch 3184, Loss Train(MSE): 0.12548991224768458, R2 Train: 0.4980403510092617\n",
      "Epoch 3185, Loss Train(MSE): 0.1254896212598198, R2 Train: 0.4980415149607208\n",
      "Epoch 3186, Loss Train(MSE): 0.1254892856861122, R2 Train: 0.4980428572555512\n",
      "Epoch 3187, Loss Train(MSE): 0.1255027751848646, R2 Train: 0.4979888992605416\n",
      "Epoch 3188, Loss Train(MSE): 0.12552840143570362, R2 Train: 0.4978863942571855\n",
      "Epoch 3189, Loss Train(MSE): 0.1255275713211594, R2 Train: 0.49788971471536236\n",
      "Epoch 3190, Loss Train(MSE): 0.125526855301377, R2 Train: 0.497892578794492\n",
      "Epoch 3191, Loss Train(MSE): 0.1255260317939091, R2 Train: 0.49789587282436365\n",
      "Epoch 3192, Loss Train(MSE): 0.12552529279692753, R2 Train: 0.49789882881228986\n",
      "Epoch 3193, Loss Train(MSE): 0.125524505516358, R2 Train: 0.49790197793456803\n",
      "Epoch 3194, Loss Train(MSE): 0.1255237419395182, R2 Train: 0.49790503224192717\n",
      "Epoch 3195, Loss Train(MSE): 0.1255229916765705, R2 Train: 0.497908033293718\n",
      "Epoch 3196, Loss Train(MSE): 0.1255222028268854, R2 Train: 0.49791118869245843\n",
      "Epoch 3197, Loss Train(MSE): 0.1255214900884737, R2 Train: 0.49791403964610526\n",
      "Epoch 3198, Loss Train(MSE): 0.12552069127333568, R2 Train: 0.4979172349066573\n",
      "Epoch 3199, Loss Train(MSE): 0.12551998481165128, R2 Train: 0.4979200607533949\n",
      "Epoch 3200, Loss Train(MSE): 0.12551920849504067, R2 Train: 0.4979231660198373\n",
      "Epoch 3201, Loss Train(MSE): 0.12551847440857106, R2 Train: 0.4979261023657158\n",
      "Epoch 3202, Loss Train(MSE): 0.12551773750676767, R2 Train: 0.4979290499729293\n",
      "Epoch 3203, Loss Train(MSE): 0.12551697518487517, R2 Train: 0.4979320992604993\n",
      "Epoch 3204, Loss Train(MSE): 0.1255162781361602, R2 Train: 0.4979348874553592\n",
      "Epoch 3205, Loss Train(MSE): 0.12551550269389497, R2 Train: 0.4979379892244201\n",
      "Epoch 3206, Loss Train(MSE): 0.12551481471835305, R2 Train: 0.4979407411265878\n",
      "Epoch 3207, Loss Train(MSE): 0.125514061163243, R2 Train: 0.49794375534702795\n",
      "Epoch 3208, Loss Train(MSE): 0.12551334283067006, R2 Train: 0.49794662867731976\n",
      "Epoch 3209, Loss Train(MSE): 0.12551263082223793, R2 Train: 0.4979494767110483\n",
      "Epoch 3210, Loss Train(MSE): 0.12551188159636853, R2 Train: 0.4979524736145259\n",
      "Epoch 3211, Loss Train(MSE): 0.12551121151098887, R2 Train: 0.4979551539560445\n",
      "Epoch 3212, Loss Train(MSE): 0.12551045821947823, R2 Train: 0.49795816712208707\n",
      "Epoch 3213, Loss Train(MSE): 0.12550977592690768, R2 Train: 0.4979608962923693\n",
      "Epoch 3214, Loss Train(MSE): 0.12550905584844207, R2 Train: 0.4979637766062317\n",
      "Epoch 3215, Loss Train(MSE): 0.12550834075010206, R2 Train: 0.49796663699959176\n",
      "Epoch 3216, Loss Train(MSE): 0.1255076641097543, R2 Train: 0.49796934356098277\n",
      "Epoch 3217, Loss Train(MSE): 0.1255069260948028, R2 Train: 0.4979722956207888\n",
      "Epoch 3218, Loss Train(MSE): 0.12550627269016693, R2 Train: 0.4979749092393323\n",
      "Epoch 3219, Loss Train(MSE): 0.1255055506874526, R2 Train: 0.49797779725018965\n",
      "Epoch 3220, Loss Train(MSE): 0.12550486270745473, R2 Train: 0.4979805491701811\n",
      "Epoch 3221, Loss Train(MSE): 0.12550418553541062, R2 Train: 0.4979832578583575\n",
      "Epoch 3222, Loss Train(MSE): 0.12550346255698988, R2 Train: 0.4979861497720405\n",
      "Epoch 3223, Loss Train(MSE): 0.12550283049768515, R2 Train: 0.4979886780092594\n",
      "Epoch 3224, Loss Train(MSE): 0.12550211273171857, R2 Train: 0.4979915490731257\n",
      "Epoch 3225, Loss Train(MSE): 0.1255014450017888, R2 Train: 0.49799421999284477\n",
      "Epoch 3226, Loss Train(MSE): 0.12550326740774048, R2 Train: 0.49798693036903807\n",
      "Epoch 3227, Loss Train(MSE): 0.12550802896629576, R2 Train: 0.49796788413481696\n",
      "Epoch 3228, Loss Train(MSE): 0.12550726914554078, R2 Train: 0.4979709234178369\n",
      "Epoch 3229, Loss Train(MSE): 0.12550651193247028, R2 Train: 0.49797395227011887\n",
      "Epoch 3230, Loss Train(MSE): 0.125505757311691, R2 Train: 0.497976970753236\n",
      "Epoch 3231, Loss Train(MSE): 0.1255050052679424, R2 Train: 0.4979799789282304\n",
      "Epoch 3232, Loss Train(MSE): 0.12550425578609503, R2 Train: 0.4979829768556199\n",
      "Epoch 3233, Loss Train(MSE): 0.125503508851149, R2 Train: 0.497985964595404\n",
      "Epoch 3234, Loss Train(MSE): 0.12550276444823266, R2 Train: 0.49798894220706935\n",
      "Epoch 3235, Loss Train(MSE): 0.12550202256260107, R2 Train: 0.49799190974959573\n",
      "Epoch 3236, Loss Train(MSE): 0.12550128317963472, R2 Train: 0.49799486728146114\n",
      "Epoch 3237, Loss Train(MSE): 0.12550054628483773, R2 Train: 0.4979978148606491\n",
      "Epoch 3238, Loss Train(MSE): 0.12549981186383713, R2 Train: 0.4980007525446515\n",
      "Epoch 3239, Loss Train(MSE): 0.12549907990238085, R2 Train: 0.4980036803904766\n",
      "Epoch 3240, Loss Train(MSE): 0.12549835038633678, R2 Train: 0.4980065984546529\n",
      "Epoch 3241, Loss Train(MSE): 0.1254976233016914, R2 Train: 0.4980095067932344\n",
      "Epoch 3242, Loss Train(MSE): 0.12549715003296388, R2 Train: 0.49801139986814447\n",
      "Epoch 3243, Loss Train(MSE): 0.12549674330868726, R2 Train: 0.49801302676525094\n",
      "Epoch 3244, Loss Train(MSE): 0.12549633720729106, R2 Train: 0.49801465117083576\n",
      "Epoch 3245, Loss Train(MSE): 0.12549593172732487, R2 Train: 0.4980162730907005\n",
      "Epoch 3246, Loss Train(MSE): 0.12549552686734353, R2 Train: 0.4980178925306259\n",
      "Epoch 3247, Loss Train(MSE): 0.12549512262590684, R2 Train: 0.49801950949637264\n",
      "Epoch 3248, Loss Train(MSE): 0.12549471900157955, R2 Train: 0.4980211239936818\n",
      "Epoch 3249, Loss Train(MSE): 0.12549431599293145, R2 Train: 0.4980227360282742\n",
      "Epoch 3250, Loss Train(MSE): 0.1254939135985372, R2 Train: 0.4980243456058512\n",
      "Epoch 3251, Loss Train(MSE): 0.1254935118169763, R2 Train: 0.4980259527320948\n",
      "Epoch 3252, Loss Train(MSE): 0.1254931106468332, R2 Train: 0.49802755741266724\n",
      "Epoch 3253, Loss Train(MSE): 0.12549271008669716, R2 Train: 0.4980291596532114\n",
      "Epoch 3254, Loss Train(MSE): 0.1254923101351623, R2 Train: 0.4980307594593508\n",
      "Epoch 3255, Loss Train(MSE): 0.12549191079082747, R2 Train: 0.4980323568366901\n",
      "Epoch 3256, Loss Train(MSE): 0.12549151205229636, R2 Train: 0.49803395179081456\n",
      "Epoch 3257, Loss Train(MSE): 0.12549111391817735, R2 Train: 0.4980355443272906\n",
      "Epoch 3258, Loss Train(MSE): 0.12549071638708353, R2 Train: 0.49803713445166586\n",
      "Epoch 3259, Loss Train(MSE): 0.12549031945763273, R2 Train: 0.4980387221694691\n",
      "Epoch 3260, Loss Train(MSE): 0.12548992312844748, R2 Train: 0.4980403074862101\n",
      "Epoch 3261, Loss Train(MSE): 0.12548952739815478, R2 Train: 0.4980418904073809\n",
      "Epoch 3262, Loss Train(MSE): 0.12548913226538644, R2 Train: 0.49804347093845425\n",
      "Epoch 3263, Loss Train(MSE): 0.12548873772877875, R2 Train: 0.498045049084885\n",
      "Epoch 3264, Loss Train(MSE): 0.12548834378697263, R2 Train: 0.4980466248521095\n",
      "Epoch 3265, Loss Train(MSE): 0.12548795043861352, R2 Train: 0.4980481982455459\n",
      "Epoch 3266, Loss Train(MSE): 0.1254875576823514, R2 Train: 0.4980497692705944\n",
      "Epoch 3267, Loss Train(MSE): 0.12548716551684072, R2 Train: 0.4980513379326371\n",
      "Epoch 3268, Loss Train(MSE): 0.1254867739407404, R2 Train: 0.4980529042370384\n",
      "Epoch 3269, Loss Train(MSE): 0.12548638295271386, R2 Train: 0.49805446818914456\n",
      "Epoch 3270, Loss Train(MSE): 0.12548599255142895, R2 Train: 0.4980560297942842\n",
      "Epoch 3271, Loss Train(MSE): 0.1254856027355579, R2 Train: 0.4980575890577684\n",
      "Epoch 3272, Loss Train(MSE): 0.1254852135037772, R2 Train: 0.4980591459848912\n",
      "Epoch 3273, Loss Train(MSE): 0.12548482485476803, R2 Train: 0.4980607005809279\n",
      "Epoch 3274, Loss Train(MSE): 0.12548443678721558, R2 Train: 0.4980622528511377\n",
      "Epoch 3275, Loss Train(MSE): 0.12548404929980952, R2 Train: 0.4980638028007619\n",
      "Epoch 3276, Loss Train(MSE): 0.12548366239124378, R2 Train: 0.49806535043502487\n",
      "Epoch 3277, Loss Train(MSE): 0.12548327606021661, R2 Train: 0.49806689575913354\n",
      "Epoch 3278, Loss Train(MSE): 0.12548289030543042, R2 Train: 0.49806843877827833\n",
      "Epoch 3279, Loss Train(MSE): 0.12548250512559192, R2 Train: 0.4980699794976323\n",
      "Epoch 3280, Loss Train(MSE): 0.12548212051941202, R2 Train: 0.49807151792235194\n",
      "Epoch 3281, Loss Train(MSE): 0.12548173648560584, R2 Train: 0.4980730540575766\n",
      "Epoch 3282, Loss Train(MSE): 0.12548135302289262, R2 Train: 0.4980745879084295\n",
      "Epoch 3283, Loss Train(MSE): 0.12548097012999582, R2 Train: 0.49807611948001673\n",
      "Epoch 3284, Loss Train(MSE): 0.1254805878056429, R2 Train: 0.49807764877742844\n",
      "Epoch 3285, Loss Train(MSE): 0.12548020604856558, R2 Train: 0.4980791758057377\n",
      "Epoch 3286, Loss Train(MSE): 0.1254798248574996, R2 Train: 0.49808070057000164\n",
      "Epoch 3287, Loss Train(MSE): 0.1254794442311847, R2 Train: 0.4980822230752612\n",
      "Epoch 3288, Loss Train(MSE): 0.12547906416836482, R2 Train: 0.4980837433265407\n",
      "Epoch 3289, Loss Train(MSE): 0.12547868466778786, R2 Train: 0.49808526132884856\n",
      "Epoch 3290, Loss Train(MSE): 0.1254783057282056, R2 Train: 0.49808677708717763\n",
      "Epoch 3291, Loss Train(MSE): 0.12547792734837396, R2 Train: 0.49808829060650417\n",
      "Epoch 3292, Loss Train(MSE): 0.1254775495270529, R2 Train: 0.4980898018917884\n",
      "Epoch 3293, Loss Train(MSE): 0.12547717226300603, R2 Train: 0.4980913109479759\n",
      "Epoch 3294, Loss Train(MSE): 0.12547679555500127, R2 Train: 0.4980928177799949\n",
      "Epoch 3295, Loss Train(MSE): 0.12547647492947453, R2 Train: 0.4980941002821019\n",
      "Epoch 3296, Loss Train(MSE): 0.1254761608165096, R2 Train: 0.49809535673396155\n",
      "Epoch 3297, Loss Train(MSE): 0.12547589125013836, R2 Train: 0.49809643499944656\n",
      "Epoch 3298, Loss Train(MSE): 0.12547556672835644, R2 Train: 0.49809773308657423\n",
      "Epoch 3299, Loss Train(MSE): 0.12547527002471254, R2 Train: 0.49809891990114985\n",
      "Epoch 3300, Loss Train(MSE): 0.1254749854490397, R2 Train: 0.4981000582038412\n",
      "Epoch 3301, Loss Train(MSE): 0.12547466237860674, R2 Train: 0.498101350485573\n",
      "Epoch 3302, Loss Train(MSE): 0.12547438244131165, R2 Train: 0.4981024702347534\n",
      "Epoch 3303, Loss Train(MSE): 0.1254740834759152, R2 Train: 0.49810366609633916\n",
      "Epoch 3304, Loss Train(MSE): 0.125473761841259, R2 Train: 0.498104952634964\n",
      "Epoch 3305, Loss Train(MSE): 0.1254734980419927, R2 Train: 0.4981060078320292\n",
      "Epoch 3306, Loss Train(MSE): 0.12547318529230464, R2 Train: 0.49810725883078144\n",
      "Epoch 3307, Loss Train(MSE): 0.12547287166637383, R2 Train: 0.4981085133345047\n",
      "Epoch 3308, Loss Train(MSE): 0.1254726101996829, R2 Train: 0.4981095592012684\n",
      "Epoch 3309, Loss Train(MSE): 0.12547229081144343, R2 Train: 0.4981108367542263\n",
      "Epoch 3310, Loss Train(MSE): 0.12547199260103697, R2 Train: 0.4981120295958521\n",
      "Epoch 3311, Loss Train(MSE): 0.12547171803816798, R2 Train: 0.4981131278473281\n",
      "Epoch 3312, Loss Train(MSE): 0.12547140004795232, R2 Train: 0.4981143998081907\n",
      "Epoch 3313, Loss Train(MSE): 0.12547111665831284, R2 Train: 0.49811553336674863\n",
      "Epoch 3314, Loss Train(MSE): 0.12547082957232697, R2 Train: 0.4981166817106921\n",
      "Epoch 3315, Loss Train(MSE): 0.12547051296568573, R2 Train: 0.49811794813725707\n",
      "Epoch 3316, Loss Train(MSE): 0.125470243815533, R2 Train: 0.49811902473786795\n",
      "Epoch 3317, Loss Train(MSE): 0.1254699447664673, R2 Train: 0.4981202209341308\n",
      "Epoch 3318, Loss Train(MSE): 0.12546962952925245, R2 Train: 0.4981214818829902\n",
      "Epoch 3319, Loss Train(MSE): 0.12546937405046157, R2 Train: 0.4981225037981537\n",
      "Epoch 3320, Loss Train(MSE): 0.12546906358563661, R2 Train: 0.49812374565745354\n",
      "Epoch 3321, Loss Train(MSE): 0.12546875799169319, R2 Train: 0.49812496803322726\n",
      "Epoch 3322, Loss Train(MSE): 0.12546849904148766, R2 Train: 0.49812600383404937\n",
      "Epoch 3323, Loss Train(MSE): 0.12546818594911083, R2 Train: 0.4981272562035567\n",
      "Epoch 3324, Loss Train(MSE): 0.12546789339005093, R2 Train: 0.49812842643979627\n",
      "Epoch 3325, Loss Train(MSE): 0.12546762362927272, R2 Train: 0.49812950548290913\n",
      "Epoch 3326, Loss Train(MSE): 0.12546731187200538, R2 Train: 0.49813075251197847\n",
      "Epoch 3327, Loss Train(MSE): 0.12546703180968202, R2 Train: 0.4981318727612719\n",
      "Epoch 3328, Loss Train(MSE): 0.12546675175670563, R2 Train: 0.4981329929731775\n",
      "Epoch 3329, Loss Train(MSE): 0.12546644132137869, R2 Train: 0.49813423471448526\n",
      "Epoch 3330, Loss Train(MSE): 0.12546617322976428, R2 Train: 0.4981353070809429\n",
      "Epoch 3331, Loss Train(MSE): 0.12546588339123876, R2 Train: 0.49813646643504494\n",
      "Epoch 3332, Loss Train(MSE): 0.12546557426494698, R2 Train: 0.4981377029402121\n",
      "Epoch 3333, Loss Train(MSE): 0.12546531762984844, R2 Train: 0.4981387294806062\n",
      "Epoch 3334, Loss Train(MSE): 0.12546501850097025, R2 Train: 0.498139925996119\n",
      "Epoch 3335, Loss Train(MSE): 0.12546471156692576, R2 Train: 0.49814115373229695\n",
      "Epoch 3336, Loss Train(MSE): 0.12546446408455614, R2 Train: 0.49814214366177545\n",
      "Epoch 3337, Loss Train(MSE): 0.1254641570105788, R2 Train: 0.49814337195768477\n",
      "Epoch 3338, Loss Train(MSE): 0.12546386097329382, R2 Train: 0.49814455610682473\n",
      "Epoch 3339, Loss Train(MSE): 0.12546360473172113, R2 Train: 0.49814558107311546\n",
      "Epoch 3340, Loss Train(MSE): 0.12546329893546015, R2 Train: 0.4981468042581594\n",
      "Epoch 3341, Loss Train(MSE): 0.1254630133075597, R2 Train: 0.49814794676976115\n",
      "Epoch 3342, Loss Train(MSE): 0.12546274877605934, R2 Train: 0.49814900489576264\n",
      "Epoch 3343, Loss Train(MSE): 0.12546244424546615, R2 Train: 0.4981502230181354\n",
      "Epoch 3344, Loss Train(MSE): 0.12546216855049439, R2 Train: 0.49815132579802246\n",
      "Epoch 3345, Loss Train(MSE): 0.12546189618776646, R2 Train: 0.49815241524893417\n",
      "Epoch 3346, Loss Train(MSE): 0.12546159291102432, R2 Train: 0.4981536283559027\n",
      "Epoch 3347, Loss Train(MSE): 0.1254613266831915, R2 Train: 0.49815469326723405\n",
      "Epoch 3348, Loss Train(MSE): 0.12546104693760363, R2 Train: 0.4981558122495855\n",
      "Epoch 3349, Loss Train(MSE): 0.12546074490312115, R2 Train: 0.4981570203875154\n",
      "Epoch 3350, Loss Train(MSE): 0.12546048768705742, R2 Train: 0.4981580492517703\n",
      "Epoch 3351, Loss Train(MSE): 0.12546020099688143, R2 Train: 0.49815919601247427\n",
      "Epoch 3352, Loss Train(MSE): 0.1254599001932861, R2 Train: 0.4981603992268556\n",
      "Epoch 3353, Loss Train(MSE): 0.1254596515438017, R2 Train: 0.4981613938247932\n",
      "Epoch 3354, Loss Train(MSE): 0.12545935833744368, R2 Train: 0.4981625666502253\n",
      "Epoch 3355, Loss Train(MSE): 0.1254590592210437, R2 Train: 0.4981637631158252\n",
      "Epoch 3356, Loss Train(MSE): 0.12545881776192755, R2 Train: 0.4981647289522898\n",
      "Epoch 3357, Loss Train(MSE): 0.12545851889086473, R2 Train: 0.49816592443654106\n",
      "Epoch 3358, Loss Train(MSE): 0.12545822787896158, R2 Train: 0.4981670884841537\n",
      "Epoch 3359, Loss Train(MSE): 0.12545798034041988, R2 Train: 0.49816807863832047\n",
      "Epoch 3360, Loss Train(MSE): 0.12545768267262208, R2 Train: 0.49816926930951166\n",
      "Epoch 3361, Loss Train(MSE): 0.12545739934289166, R2 Train: 0.4981704026284334\n",
      "Epoch 3362, Loss Train(MSE): 0.12545714613120235, R2 Train: 0.4981714154751906\n",
      "Epoch 3363, Loss Train(MSE): 0.12545684965600784, R2 Train: 0.49817260137596864\n",
      "Epoch 3364, Loss Train(MSE): 0.12545657359553802, R2 Train: 0.4981737056178479\n",
      "Epoch 3365, Loss Train(MSE): 0.1254563151078515, R2 Train: 0.49817473956859404\n",
      "Epoch 3366, Loss Train(MSE): 0.12545601981479199, R2 Train: 0.49817592074083206\n",
      "Epoch 3367, Loss Train(MSE): 0.1254557506198691, R2 Train: 0.4981769975205236\n",
      "Epoch 3368, Loss Train(MSE): 0.12545548724441372, R2 Train: 0.49817805102234514\n",
      "Epoch 3369, Loss Train(MSE): 0.12545519312320907, R2 Train: 0.4981792275071637\n",
      "Epoch 3370, Loss Train(MSE): 0.12545493039910952, R2 Train: 0.49818027840356194\n",
      "Epoch 3371, Loss Train(MSE): 0.1254546625153922, R2 Train: 0.4981813499384312\n",
      "Epoch 3372, Loss Train(MSE): 0.12545436955594552, R2 Train: 0.4981825217762179\n",
      "Epoch 3373, Loss Train(MSE): 0.1254541129167329, R2 Train: 0.49818354833306844\n",
      "Epoch 3374, Loss Train(MSE): 0.12545384089573433, R2 Train: 0.4981846364170627\n",
      "Epoch 3375, Loss Train(MSE): 0.12545354908812686, R2 Train: 0.49818580364749254\n",
      "Epoch 3376, Loss Train(MSE): 0.1254532981564542, R2 Train: 0.4981868073741832\n",
      "Epoch 3377, Loss Train(MSE): 0.12545302236081962, R2 Train: 0.4981879105567215\n",
      "Epoch 3378, Loss Train(MSE): 0.12545273169530607, R2 Train: 0.4981890732187757\n",
      "Epoch 3379, Loss Train(MSE): 0.12545248610222315, R2 Train: 0.4981900555911074\n",
      "Epoch 3380, Loss Train(MSE): 0.1254522068864477, R2 Train: 0.4981911724542092\n",
      "Epoch 3381, Loss Train(MSE): 0.12545231897648934, R2 Train: 0.49819072409404264\n",
      "Epoch 3382, Loss Train(MSE): 0.12548911358918302, R2 Train: 0.4980435456432679\n",
      "Epoch 3383, Loss Train(MSE): 0.12548843388015818, R2 Train: 0.4980462644793673\n",
      "Epoch 3384, Loss Train(MSE): 0.12548774683040254, R2 Train: 0.49804901267838986\n",
      "Epoch 3385, Loss Train(MSE): 0.12548709853652038, R2 Train: 0.4980516058539185\n",
      "Epoch 3386, Loss Train(MSE): 0.12548638990976224, R2 Train: 0.49805444036095103\n",
      "Epoch 3387, Loss Train(MSE): 0.12548577347186965, R2 Train: 0.4980569061125214\n",
      "Epoch 3388, Loss Train(MSE): 0.12548506800255604, R2 Train: 0.4980597279897758\n",
      "Epoch 3389, Loss Train(MSE): 0.12548443341931556, R2 Train: 0.49806226632273776\n",
      "Epoch 3390, Loss Train(MSE): 0.12548375873514897, R2 Train: 0.4980649650594041\n",
      "Epoch 3391, Loss Train(MSE): 0.12548310057839437, R2 Train: 0.4980675976864225\n",
      "Epoch 3392, Loss Train(MSE): 0.1254824593777289, R2 Train: 0.4980701624890844\n",
      "Epoch 3393, Loss Train(MSE): 0.12548177712412506, R2 Train: 0.49807289150349976\n",
      "Epoch 3394, Loss Train(MSE): 0.1254811697922742, R2 Train: 0.4980753208309032\n",
      "Epoch 3395, Loss Train(MSE): 0.12548048398687714, R2 Train: 0.49807806405249144\n",
      "Epoch 3396, Loss Train(MSE): 0.12547986896757202, R2 Train: 0.49808052412971193\n",
      "Epoch 3397, Loss Train(MSE): 0.12547920942212853, R2 Train: 0.49808316231148586\n",
      "Epoch 3398, Loss Train(MSE): 0.12547856849429326, R2 Train: 0.49808572602282697\n",
      "Epoch 3399, Loss Train(MSE): 0.1254779442860247, R2 Train: 0.49808822285590115\n",
      "Epoch 3400, Loss Train(MSE): 0.12547727698623393, R2 Train: 0.4980908920550643\n",
      "Epoch 3401, Loss Train(MSE): 0.12547668845007923, R2 Train: 0.4980932461996831\n",
      "Epoch 3402, Loss Train(MSE): 0.12547602132816635, R2 Train: 0.4980959146873346\n",
      "Epoch 3403, Loss Train(MSE): 0.1254754149582815, R2 Train: 0.49809834016687404\n",
      "Epoch 3404, Loss Train(MSE): 0.12547477978999497, R2 Train: 0.49810088084002013\n",
      "Epoch 3405, Loss Train(MSE): 0.12547414540514054, R2 Train: 0.49810341837943783\n",
      "Epoch 3406, Loss Train(MSE): 0.125473547232122, R2 Train: 0.498105811071512\n",
      "Epoch 3407, Loss Train(MSE): 0.12547289302302217, R2 Train: 0.4981084279079113\n",
      "Epoch 3408, Loss Train(MSE): 0.12547231509855192, R2 Train: 0.49811073960579233\n",
      "Epoch 3409, Loss Train(MSE): 0.1254716742722824, R2 Train: 0.4981133029108704\n",
      "Epoch 3410, Loss Train(MSE): 0.12547106680551107, R2 Train: 0.49811573277795573\n",
      "Epoch 3411, Loss Train(MSE): 0.12547046419764615, R2 Train: 0.4981181432094154\n",
      "Epoch 3412, Loss Train(MSE): 0.12546982681697624, R2 Train: 0.498120692732095\n",
      "Epoch 3413, Loss Train(MSE): 0.1254692626851643, R2 Train: 0.49812294925934275\n",
      "Epoch 3414, Loss Train(MSE): 0.12546862562671493, R2 Train: 0.4981254974931403\n",
      "Epoch 3415, Loss Train(MSE): 0.1254680391839615, R2 Train: 0.49812784326415405\n",
      "Epoch 3416, Loss Train(MSE): 0.12546743728020224, R2 Train: 0.49813025087919105\n",
      "Epoch 3417, Loss Train(MSE): 0.12546681954046374, R2 Train: 0.49813272183814505\n",
      "Epoch 3418, Loss Train(MSE): 0.1254662572117993, R2 Train: 0.4981349711528028\n",
      "Epoch 3419, Loss Train(MSE): 0.12546563202458622, R2 Train: 0.49813747190165514\n",
      "Epoch 3420, Loss Train(MSE): 0.12546506129180457, R2 Train: 0.4981397548327817\n",
      "Epoch 3421, Loss Train(MSE): 0.12546446468614258, R2 Train: 0.49814214125542966\n",
      "Epoch 3422, Loss Train(MSE): 0.1254638613725323, R2 Train: 0.49814455450987083\n",
      "Epoch 3423, Loss Train(MSE): 0.1254633053552493, R2 Train: 0.4981467785790028\n",
      "Epoch 3424, Loss Train(MSE): 0.12546269163783164, R2 Train: 0.49814923344867346\n",
      "Epoch 3425, Loss Train(MSE): 0.1254621316068977, R2 Train: 0.4981514735724092\n",
      "Epoch 3426, Loss Train(MSE): 0.12546154462147757, R2 Train: 0.4981538215140897\n",
      "Epoch 3427, Loss Train(MSE): 0.12546095081895262, R2 Train: 0.49815619672418954\n",
      "Epoch 3428, Loss Train(MSE): 0.1254604053547944, R2 Train: 0.49815837858082235\n",
      "Epoch 3429, Loss Train(MSE): 0.1254598027248059, R2 Train: 0.49816078910077644\n",
      "Epoch 3430, Loss Train(MSE): 0.1254592486756773, R2 Train: 0.49816300529729085\n",
      "Epoch 3431, Loss Train(MSE): 0.12545867537660954, R2 Train: 0.49816529849356184\n",
      "Epoch 3432, Loss Train(MSE): 0.12545808645246304, R2 Train: 0.49816765419014786\n",
      "Epoch 3433, Loss Train(MSE): 0.12545755553213617, R2 Train: 0.49816977787145533\n",
      "Epoch 3434, Loss Train(MSE): 0.1254569636251533, R2 Train: 0.4981721454993868\n",
      "Epoch 3435, Loss Train(MSE): 0.12545641110919678, R2 Train: 0.4981743555632129\n",
      "Epoch 3436, Loss Train(MSE): 0.12545585532134532, R2 Train: 0.49817657871461873\n",
      "Epoch 3437, Loss Train(MSE): 0.1254552698277984, R2 Train: 0.4981789206888064\n",
      "Epoch 3438, Loss Train(MSE): 0.12545475150337185, R2 Train: 0.4981809939865126\n",
      "Epoch 3439, Loss Train(MSE): 0.12545417284144447, R2 Train: 0.4981833086342221\n",
      "Epoch 3440, Loss Train(MSE): 0.12545361786890827, R2 Train: 0.4981855285243669\n",
      "Epoch 3441, Loss Train(MSE): 0.12545316126453854, R2 Train: 0.49818735494184585\n",
      "Epoch 3442, Loss Train(MSE): 0.12545285745024312, R2 Train: 0.4981885701990275\n",
      "Epoch 3443, Loss Train(MSE): 0.12545260364706365, R2 Train: 0.4981895854117454\n",
      "Epoch 3444, Loss Train(MSE): 0.12545231725374648, R2 Train: 0.4981907309850141\n",
      "Epoch 3445, Loss Train(MSE): 0.12545202329921615, R2 Train: 0.4981919068031354\n",
      "Epoch 3446, Loss Train(MSE): 0.12545177850171632, R2 Train: 0.4981928859931347\n",
      "Epoch 3447, Loss Train(MSE): 0.12545147698304657, R2 Train: 0.49819409206781373\n",
      "Epoch 3448, Loss Train(MSE): 0.1254512086411817, R2 Train: 0.4981951654352732\n",
      "Epoch 3449, Loss Train(MSE): 0.125450940552985, R2 Train: 0.49819623778806\n",
      "Epoch 3450, Loss Train(MSE): 0.12545064046109028, R2 Train: 0.4981974381556389\n",
      "Epoch 3451, Loss Train(MSE): 0.12545039697043037, R2 Train: 0.49819841211827853\n",
      "Epoch 3452, Loss Train(MSE): 0.12545010632655826, R2 Train: 0.49819957469376697\n",
      "Epoch 3453, Loss Train(MSE): 0.12544982228223492, R2 Train: 0.4982007108710603\n",
      "Epoch 3454, Loss Train(MSE): 0.1254495735965814, R2 Train: 0.4982017056136744\n",
      "Epoch 3455, Loss Train(MSE): 0.12544927572963788, R2 Train: 0.4982028970814485\n",
      "Epoch 3456, Loss Train(MSE): 0.12544901558158844, R2 Train: 0.49820393767364624\n",
      "Epoch 3457, Loss Train(MSE): 0.12544874525544517, R2 Train: 0.49820501897821934\n",
      "Epoch 3458, Loss Train(MSE): 0.12544844877185693, R2 Train: 0.4982062049125723\n",
      "Epoch 3459, Loss Train(MSE): 0.12544821179885868, R2 Train: 0.4982071528045653\n",
      "Epoch 3460, Loss Train(MSE): 0.1254479205285651, R2 Train: 0.4982083178857396\n",
      "Epoch 3461, Loss Train(MSE): 0.12544764264501707, R2 Train: 0.49820942941993174\n",
      "Epoch 3462, Loss Train(MSE): 0.1254473936523071, R2 Train: 0.49821042539077165\n",
      "Epoch 3463, Loss Train(MSE): 0.1254470993271421, R2 Train: 0.4982116026914316\n",
      "Epoch 3464, Loss Train(MSE): 0.1254468437222418, R2 Train: 0.4982126251110328\n",
      "Epoch 3465, Loss Train(MSE): 0.1254465746443776, R2 Train: 0.49821370142248955\n",
      "Epoch 3466, Loss Train(MSE): 0.12544628166164956, R2 Train: 0.49821487335340175\n",
      "Epoch 3467, Loss Train(MSE): 0.12544604765197637, R2 Train: 0.49821580939209453\n",
      "Epoch 3468, Loss Train(MSE): 0.12544575914892603, R2 Train: 0.4982169634042959\n",
      "Epoch 3469, Loss Train(MSE): 0.12544548391410573, R2 Train: 0.49821806434357707\n",
      "Epoch 3470, Loss Train(MSE): 0.12544523796790577, R2 Train: 0.4982190481283769\n",
      "Epoch 3471, Loss Train(MSE): 0.12544494708089352, R2 Train: 0.49822021167642594\n",
      "Epoch 3472, Loss Train(MSE): 0.12544469259978314, R2 Train: 0.49822122960086745\n",
      "Epoch 3473, Loss Train(MSE): 0.1254444280346549, R2 Train: 0.49822228786138045\n",
      "Epoch 3474, Loss Train(MSE): 0.1254441384514688, R2 Train: 0.49822344619412484\n",
      "Epoch 3475, Loss Train(MSE): 0.12544390407618425, R2 Train: 0.498224383695263\n",
      "Epoch 3476, Loss Train(MSE): 0.12544362151787614, R2 Train: 0.49822551392849546\n",
      "Epoch 3477, Loss Train(MSE): 0.125443345642229, R2 Train: 0.498226617431084\n",
      "Epoch 3478, Loss Train(MSE): 0.125443105882667, R2 Train: 0.498227576469332\n",
      "Epoch 3479, Loss Train(MSE): 0.12544281833601303, R2 Train: 0.49822872665594786\n",
      "Epoch 3480, Loss Train(MSE): 0.1254425617761758, R2 Train: 0.4982297528952968\n",
      "Epoch 3481, Loss Train(MSE): 0.12544230478016638, R2 Train: 0.4982307808793345\n",
      "Epoch 3482, Loss Train(MSE): 0.12544201850085915, R2 Train: 0.4982319259965634\n",
      "Epoch 3483, Loss Train(MSE): 0.12544178064237527, R2 Train: 0.49823287743049893\n",
      "Epoch 3484, Loss Train(MSE): 0.12544150700345288, R2 Train: 0.49823397198618846\n",
      "Epoch 3485, Loss Train(MSE): 0.12544122740607694, R2 Train: 0.49823509037569225\n",
      "Epoch 3486, Loss Train(MSE): 0.12544099677295809, R2 Train: 0.49823601290816766\n",
      "Epoch 3487, Loss Train(MSE): 0.12544071247425348, R2 Train: 0.4982371501029861\n",
      "Epoch 3488, Loss Train(MSE): 0.12544045083656605, R2 Train: 0.4982381966537358\n",
      "Epoch 3489, Loss Train(MSE): 0.12544020427073493, R2 Train: 0.49823918291706026\n",
      "Epoch 3490, Loss Train(MSE): 0.12543992120486858, R2 Train: 0.4982403151805257\n",
      "Epoch 3491, Loss Train(MSE): 0.1254396769438758, R2 Train: 0.49824129222449676\n",
      "Epoch 3492, Loss Train(MSE): 0.12543941500852412, R2 Train: 0.4982423399659035\n",
      "Epoch 3493, Loss Train(MSE): 0.12543913316233163, R2 Train: 0.4982434673506735\n",
      "Epoch 3494, Loss Train(MSE): 0.1254389057075458, R2 Train: 0.49824437716981684\n",
      "Epoch 3495, Loss Train(MSE): 0.12543862895378236, R2 Train: 0.4982454841848706\n",
      "Epoch 3496, Loss Train(MSE): 0.12543835943506235, R2 Train: 0.4982465622597506\n",
      "Epoch 3497, Loss Train(MSE): 0.12543812597118525, R2 Train: 0.498247496115259\n",
      "Epoch 3498, Loss Train(MSE): 0.12543784603252725, R2 Train: 0.498248615869891\n",
      "Epoch 3499, Loss Train(MSE): 0.1254375926418554, R2 Train: 0.49824962943257844\n",
      "Epoch 3500, Loss Train(MSE): 0.12543734500857137, R2 Train: 0.4982506199657145\n",
      "Epoch 3501, Loss Train(MSE): 0.12543706625759582, R2 Train: 0.4982517349696167\n",
      "Epoch 3502, Loss Train(MSE): 0.12543682845340343, R2 Train: 0.4982526861863863\n",
      "Epoch 3503, Loss Train(MSE): 0.12543656717390472, R2 Train: 0.4982537313043811\n",
      "Epoch 3504, Loss Train(MSE): 0.12543628959838216, R2 Train: 0.4982548416064714\n",
      "Epoch 3505, Loss Train(MSE): 0.12543606685064626, R2 Train: 0.49825573259741496\n",
      "Epoch 3506, Loss Train(MSE): 0.1254357924369594, R2 Train: 0.4982568302521624\n",
      "Epoch 3507, Loss Train(MSE): 0.12543552736266259, R2 Train: 0.49825789054934966\n",
      "Epoch 3508, Loss Train(MSE): 0.12543529646347934, R2 Train: 0.49825881414608264\n",
      "Epoch 3509, Loss Train(MSE): 0.12543502072768908, R2 Train: 0.49825991708924366\n",
      "Epoch 3510, Loss Train(MSE): 0.12543477008996634, R2 Train: 0.49826091964013464\n",
      "Epoch 3511, Loss Train(MSE): 0.12544054010079767, R2 Train: 0.49823783959680934\n",
      "Epoch 3512, Loss Train(MSE): 0.125441534787208, R2 Train: 0.49823386085116805\n",
      "Epoch 3513, Loss Train(MSE): 0.12544120797797784, R2 Train: 0.49823516808808865\n",
      "Epoch 3514, Loss Train(MSE): 0.12544088161736508, R2 Train: 0.4982364735305397\n",
      "Epoch 3515, Loss Train(MSE): 0.12544055570446813, R2 Train: 0.4982377771821275\n",
      "Epoch 3516, Loss Train(MSE): 0.12544023023838796, R2 Train: 0.49823907904644815\n",
      "Epoch 3517, Loss Train(MSE): 0.12543990521822798, R2 Train: 0.4982403791270881\n",
      "Epoch 3518, Loss Train(MSE): 0.12543958064309402, R2 Train: 0.49824167742762393\n",
      "Epoch 3519, Loss Train(MSE): 0.1254392565120943, R2 Train: 0.49824297395162276\n",
      "Epoch 3520, Loss Train(MSE): 0.12543893282433946, R2 Train: 0.49824426870264216\n",
      "Epoch 3521, Loss Train(MSE): 0.12543860957894262, R2 Train: 0.4982455616842295\n",
      "Epoch 3522, Loss Train(MSE): 0.12543828677501917, R2 Train: 0.4982468528999233\n",
      "Epoch 3523, Loss Train(MSE): 0.12543796441168703, R2 Train: 0.4982481423532519\n",
      "Epoch 3524, Loss Train(MSE): 0.1254376424880664, R2 Train: 0.49824943004773437\n",
      "Epoch 3525, Loss Train(MSE): 0.12543732100327995, R2 Train: 0.4982507159868802\n",
      "Epoch 3526, Loss Train(MSE): 0.12543699995645258, R2 Train: 0.4982520001741897\n",
      "Epoch 3527, Loss Train(MSE): 0.12543667934671165, R2 Train: 0.4982532826131534\n",
      "Epoch 3528, Loss Train(MSE): 0.12543635917318685, R2 Train: 0.4982545633072526\n",
      "Epoch 3529, Loss Train(MSE): 0.1254360394350101, R2 Train: 0.49825584225995956\n",
      "Epoch 3530, Loss Train(MSE): 0.12543572013131582, R2 Train: 0.4982571194747367\n",
      "Epoch 3531, Loss Train(MSE): 0.1254354012612406, R2 Train: 0.49825839495503765\n",
      "Epoch 3532, Loss Train(MSE): 0.1254350828239235, R2 Train: 0.49825966870430605\n",
      "Epoch 3533, Loss Train(MSE): 0.12543476481850568, R2 Train: 0.49826094072597726\n",
      "Epoch 3534, Loss Train(MSE): 0.1254344472441308, R2 Train: 0.4982622110234768\n",
      "Epoch 3535, Loss Train(MSE): 0.12543413009994467, R2 Train: 0.4982634796002213\n",
      "Epoch 3536, Loss Train(MSE): 0.1254338133850954, R2 Train: 0.4982647464596184\n",
      "Epoch 3537, Loss Train(MSE): 0.1254334970987334, R2 Train: 0.49826601160506645\n",
      "Epoch 3538, Loss Train(MSE): 0.1254331812400113, R2 Train: 0.4982672750399548\n",
      "Epoch 3539, Loss Train(MSE): 0.1254328658080841, R2 Train: 0.4982685367676636\n",
      "Epoch 3540, Loss Train(MSE): 0.12543255080210888, R2 Train: 0.49826979679156447\n",
      "Epoch 3541, Loss Train(MSE): 0.1254322362212451, R2 Train: 0.49827105511501957\n",
      "Epoch 3542, Loss Train(MSE): 0.12543192206465434, R2 Train: 0.49827231174138265\n",
      "Epoch 3543, Loss Train(MSE): 0.12543160833150044, R2 Train: 0.49827356667399825\n",
      "Epoch 3544, Loss Train(MSE): 0.12543129502094952, R2 Train: 0.4982748199162019\n",
      "Epoch 3545, Loss Train(MSE): 0.12543098213216977, R2 Train: 0.4982760714713209\n",
      "Epoch 3546, Loss Train(MSE): 0.12543066966433175, R2 Train: 0.498277321342673\n",
      "Epoch 3547, Loss Train(MSE): 0.12543035761660803, R2 Train: 0.49827856953356786\n",
      "Epoch 3548, Loss Train(MSE): 0.1254300459881735, R2 Train: 0.498279816047306\n",
      "Epoch 3549, Loss Train(MSE): 0.12542973477820513, R2 Train: 0.4982810608871795\n",
      "Epoch 3550, Loss Train(MSE): 0.12542942398588217, R2 Train: 0.49828230405647134\n",
      "Epoch 3551, Loss Train(MSE): 0.1254291136103859, R2 Train: 0.49828354555845644\n",
      "Epoch 3552, Loss Train(MSE): 0.12542880365089984, R2 Train: 0.49828478539640064\n",
      "Epoch 3553, Loss Train(MSE): 0.1254284941066096, R2 Train: 0.49828602357356155\n",
      "Epoch 3554, Loss Train(MSE): 0.125428184976703, R2 Train: 0.498287260093188\n",
      "Epoch 3555, Loss Train(MSE): 0.12542787626036989, R2 Train: 0.49828849495852046\n",
      "Epoch 3556, Loss Train(MSE): 0.12542756795680227, R2 Train: 0.49828972817279094\n",
      "Epoch 3557, Loss Train(MSE): 0.12542726006519436, R2 Train: 0.49829095973922255\n",
      "Epoch 3558, Loss Train(MSE): 0.12542695258474235, R2 Train: 0.4982921896610306\n",
      "Epoch 3559, Loss Train(MSE): 0.1254266455146446, R2 Train: 0.49829341794142157\n",
      "Epoch 3560, Loss Train(MSE): 0.12542633885410154, R2 Train: 0.49829464458359385\n",
      "Epoch 3561, Loss Train(MSE): 0.12542603260231572, R2 Train: 0.49829586959073713\n",
      "Epoch 3562, Loss Train(MSE): 0.1254257267584917, R2 Train: 0.4982970929660332\n",
      "Epoch 3563, Loss Train(MSE): 0.12542542132183618, R2 Train: 0.4982983147126553\n",
      "Epoch 3564, Loss Train(MSE): 0.12542511629155784, R2 Train: 0.49829953483376865\n",
      "Epoch 3565, Loss Train(MSE): 0.12542481166686753, R2 Train: 0.4983007533325299\n",
      "Epoch 3566, Loss Train(MSE): 0.12542450744697808, R2 Train: 0.49830197021208766\n",
      "Epoch 3567, Loss Train(MSE): 0.12542420363110432, R2 Train: 0.4983031854755827\n",
      "Epoch 3568, Loss Train(MSE): 0.12542390021846328, R2 Train: 0.4983043991261469\n",
      "Epoch 3569, Loss Train(MSE): 0.12542359720827373, R2 Train: 0.4983056111669051\n",
      "Epoch 3570, Loss Train(MSE): 0.1254232945997568, R2 Train: 0.49830682160097284\n",
      "Epoch 3571, Loss Train(MSE): 0.12542299239213534, R2 Train: 0.49830803043145866\n",
      "Epoch 3572, Loss Train(MSE): 0.12542269058463446, R2 Train: 0.4983092376614622\n",
      "Epoch 3573, Loss Train(MSE): 0.1254223891764811, R2 Train: 0.4983104432940756\n",
      "Epoch 3574, Loss Train(MSE): 0.12542208816690417, R2 Train: 0.4983116473323833\n",
      "Epoch 3575, Loss Train(MSE): 0.12542178755513472, R2 Train: 0.4983128497794611\n",
      "Epoch 3576, Loss Train(MSE): 0.1254214873404057, R2 Train: 0.4983140506383772\n",
      "Epoch 3577, Loss Train(MSE): 0.12542118752195197, R2 Train: 0.4983152499121921\n",
      "Epoch 3578, Loss Train(MSE): 0.12542088809901047, R2 Train: 0.4983164476039581\n",
      "Epoch 3579, Loss Train(MSE): 0.12542058907082004, R2 Train: 0.4983176437167198\n",
      "Epoch 3580, Loss Train(MSE): 0.12542029043662145, R2 Train: 0.4983188382535142\n",
      "Epoch 3581, Loss Train(MSE): 0.12541999219565747, R2 Train: 0.4983200312173701\n",
      "Epoch 3582, Loss Train(MSE): 0.12541969434717284, R2 Train: 0.4983212226113086\n",
      "Epoch 3583, Loss Train(MSE): 0.12541939689041415, R2 Train: 0.4983224124383434\n",
      "Epoch 3584, Loss Train(MSE): 0.12541909982462984, R2 Train: 0.49832360070148063\n",
      "Epoch 3585, Loss Train(MSE): 0.12541880314907058, R2 Train: 0.49832478740371766\n",
      "Epoch 3586, Loss Train(MSE): 0.12541850686298864, R2 Train: 0.49832597254804545\n",
      "Epoch 3587, Loss Train(MSE): 0.12541821663271177, R2 Train: 0.4983271334691529\n",
      "Epoch 3588, Loss Train(MSE): 0.12541800770439693, R2 Train: 0.4983279691824123\n",
      "Epoch 3589, Loss Train(MSE): 0.12541776301634316, R2 Train: 0.49832894793462734\n",
      "Epoch 3590, Loss Train(MSE): 0.12541751300261597, R2 Train: 0.4983299479895361\n",
      "Epoch 3591, Loss Train(MSE): 0.12541730700119988, R2 Train: 0.4983307719952005\n",
      "Epoch 3592, Loss Train(MSE): 0.125417060916561, R2 Train: 0.49833175633375604\n",
      "Epoch 3593, Loss Train(MSE): 0.12543278826465254, R2 Train: 0.49826884694138984\n",
      "Epoch 3594, Loss Train(MSE): 0.12545183378457175, R2 Train: 0.498192664861713\n",
      "Epoch 3595, Loss Train(MSE): 0.1254512424474367, R2 Train: 0.49819503021025324\n",
      "Epoch 3596, Loss Train(MSE): 0.12545065277285866, R2 Train: 0.49819738890856535\n",
      "Epoch 3597, Loss Train(MSE): 0.12545008924861034, R2 Train: 0.49819964300555863\n",
      "Epoch 3598, Loss Train(MSE): 0.1254494797138846, R2 Train: 0.49820208114446163\n",
      "Epoch 3599, Loss Train(MSE): 0.125448944351944, R2 Train: 0.49820422259222397\n",
      "Epoch 3600, Loss Train(MSE): 0.12544833478781767, R2 Train: 0.49820666084872933\n",
      "Epoch 3601, Loss Train(MSE): 0.1254477875177922, R2 Train: 0.4982088499288312\n",
      "Epoch 3602, Loss Train(MSE): 0.12544720266430115, R2 Train: 0.4982111893427954\n",
      "Epoch 3603, Loss Train(MSE): 0.1254466339514096, R2 Train: 0.49821346419436163\n",
      "Epoch 3604, Loss Train(MSE): 0.1254460785633694, R2 Train: 0.4982156857465224\n",
      "Epoch 3605, Loss Train(MSE): 0.12544548799525765, R2 Train: 0.4982180480189694\n",
      "Epoch 3606, Loss Train(MSE): 0.12544496238022082, R2 Train: 0.4982201504791167\n",
      "Epoch 3607, Loss Train(MSE): 0.12544436873494177, R2 Train: 0.4982225250602329\n",
      "Epoch 3608, Loss Train(MSE): 0.1254438349770591, R2 Train: 0.4982246600917636\n",
      "Epoch 3609, Loss Train(MSE): 0.12544326473537848, R2 Train: 0.4982269410584861\n",
      "Epoch 3610, Loss Train(MSE): 0.12544270767834037, R2 Train: 0.4982291692866385\n",
      "Epoch 3611, Loss Train(MSE): 0.12544216839236258, R2 Train: 0.4982313264305497\n",
      "Epoch 3612, Loss Train(MSE): 0.12544158766884458, R2 Train: 0.49823364932462166\n",
      "Epoch 3613, Loss Train(MSE): 0.12544107960792006, R2 Train: 0.49823568156831977\n",
      "Epoch 3614, Loss Train(MSE): 0.12544050113581642, R2 Train: 0.4982379954567343\n",
      "Epoch 3615, Loss Train(MSE): 0.12543997214914443, R2 Train: 0.4982401114034223\n",
      "Epoch 3616, Loss Train(MSE): 0.12543942398376734, R2 Train: 0.49824230406493064\n",
      "Epoch 3617, Loss Train(MSE): 0.12543887001401885, R2 Train: 0.4982445199439246\n",
      "Epoch 3618, Loss Train(MSE): 0.12543835414589516, R2 Train: 0.49824658341641936\n",
      "Epoch 3619, Loss Train(MSE): 0.12543778618827772, R2 Train: 0.4982488552468891\n",
      "Epoch 3620, Loss Train(MSE): 0.12543728034265625, R2 Train: 0.498250878629375\n",
      "Epoch 3621, Loss Train(MSE): 0.1254367276073881, R2 Train: 0.49825308957044756\n",
      "Epoch 3622, Loss Train(MSE): 0.12543619554986302, R2 Train: 0.49825521780054793\n",
      "Epoch 3623, Loss Train(MSE): 0.1254356761075705, R2 Train: 0.498257295569718\n",
      "Epoch 3624, Loss Train(MSE): 0.12543511831816734, R2 Train: 0.49825952672733065\n",
      "Epoch 3625, Loss Train(MSE): 0.12543463095314114, R2 Train: 0.49826147618743544\n",
      "Epoch 3626, Loss Train(MSE): 0.12543407771717238, R2 Train: 0.49826368913131047\n",
      "Epoch 3627, Loss Train(MSE): 0.12543356299396816, R2 Train: 0.4982657480241274\n",
      "Epoch 3628, Loss Train(MSE): 0.12543304397478042, R2 Train: 0.49826782410087833\n",
      "Epoch 3629, Loss Train(MSE): 0.12543250162226188, R2 Train: 0.49826999351095247\n",
      "Epoch 3630, Loss Train(MSE): 0.12543201700745935, R2 Train: 0.4982719319701626\n",
      "Epoch 3631, Loss Train(MSE): 0.12543147276534553, R2 Train: 0.4982741089386179\n",
      "Epoch 3632, Loss Train(MSE): 0.1254309708553637, R2 Train: 0.4982761165785452\n",
      "Epoch 3633, Loss Train(MSE): 0.12543045622688015, R2 Train: 0.4982781750924794\n",
      "Epoch 3634, Loss Train(MSE): 0.125429925644649, R2 Train: 0.498280297421404\n",
      "Epoch 3635, Loss Train(MSE): 0.1254294462547639, R2 Train: 0.4982822149809444\n",
      "Epoch 3636, Loss Train(MSE): 0.12542891141804888, R2 Train: 0.4982843543278045\n",
      "Epoch 3637, Loss Train(MSE): 0.12542841823558057, R2 Train: 0.4982863270576777\n",
      "Epoch 3638, Loss Train(MSE): 0.12542791155389263, R2 Train: 0.4982883537844295\n",
      "Epoch 3639, Loss Train(MSE): 0.1254273887277618, R2 Train: 0.4982904450889528\n",
      "Epoch 3640, Loss Train(MSE): 0.1254269180566087, R2 Train: 0.49829232777356525\n",
      "Epoch 3641, Loss Train(MSE): 0.12542639232955974, R2 Train: 0.49829443068176105\n",
      "Epoch 3642, Loss Train(MSE): 0.12542590401069958, R2 Train: 0.49829638395720166\n",
      "Epoch 3643, Loss Train(MSE): 0.1254254086334821, R2 Train: 0.49829836546607165\n",
      "Epoch 3644, Loss Train(MSE): 0.1254248897668998, R2 Train: 0.4983004409324008\n",
      "Epoch 3645, Loss Train(MSE): 0.1254244311135273, R2 Train: 0.4983022755458908\n",
      "Epoch 3646, Loss Train(MSE): 0.12542391421352983, R2 Train: 0.4983043431458807\n",
      "Epoch 3647, Loss Train(MSE): 0.12542342710405957, R2 Train: 0.4983062915837617\n",
      "Epoch 3648, Loss Train(MSE): 0.1254229462013824, R2 Train: 0.49830821519447044\n",
      "Epoch 3649, Loss Train(MSE): 0.1254224345888911, R2 Train: 0.4983102616444356\n",
      "Epoch 3650, Loss Train(MSE): 0.12542197740961944, R2 Train: 0.49831209036152224\n",
      "Epoch 3651, Loss Train(MSE): 0.12542147591085992, R2 Train: 0.4983140963565603\n",
      "Epoch 3652, Loss Train(MSE): 0.1254209867226152, R2 Train: 0.4983160531095392\n",
      "Epoch 3653, Loss Train(MSE): 0.1254205231185675, R2 Train: 0.49831790752573\n",
      "Epoch 3654, Loss Train(MSE): 0.12542001990236146, R2 Train: 0.49831992039055417\n",
      "Epoch 3655, Loss Train(MSE): 0.1254195581123884, R2 Train: 0.49832176755044644\n",
      "Epoch 3656, Loss Train(MSE): 0.12541907617081322, R2 Train: 0.49832369531674714\n",
      "Epoch 3657, Loss Train(MSE): 0.1254185816286856, R2 Train: 0.4983256734852576\n",
      "Epoch 3658, Loss Train(MSE): 0.12541813815485972, R2 Train: 0.4983274473805611\n",
      "Epoch 3659, Loss Train(MSE): 0.12541765895519202, R2 Train: 0.4983293641792319\n",
      "Epoch 3660, Loss Train(MSE): 0.12541742212934975, R2 Train: 0.498330311482601\n",
      "Epoch 3661, Loss Train(MSE): 0.12541719417313427, R2 Train: 0.49833122330746293\n",
      "Epoch 3662, Loss Train(MSE): 0.12541693430710404, R2 Train: 0.4983322627715838\n",
      "Epoch 3663, Loss Train(MSE): 0.12541671796706072, R2 Train: 0.4983331281317571\n",
      "Epoch 3664, Loss Train(MSE): 0.12541647137620462, R2 Train: 0.4983341144951815\n",
      "Epoch 3665, Loss Train(MSE): 0.12541621924851393, R2 Train: 0.4983351230059443\n",
      "Epoch 3666, Loss Train(MSE): 0.12541600957969, R2 Train: 0.49833596168123995\n",
      "Epoch 3667, Loss Train(MSE): 0.12541575150639353, R2 Train: 0.4983369939744259\n",
      "Epoch 3668, Loss Train(MSE): 0.12541551911450785, R2 Train: 0.4983379235419686\n",
      "Epoch 3669, Loss Train(MSE): 0.12541529153145625, R2 Train: 0.498338833874175\n",
      "Epoch 3670, Loss Train(MSE): 0.12541503457401693, R2 Train: 0.4983398617039323\n",
      "Epoch 3671, Loss Train(MSE): 0.125414821346503, R2 Train: 0.498340714613988\n",
      "Epoch 3672, Loss Train(MSE): 0.12541457640209022, R2 Train: 0.4983416943916391\n",
      "Epoch 3673, Loss Train(MSE): 0.12541432711902506, R2 Train: 0.49834269152389976\n",
      "Epoch 3674, Loss Train(MSE): 0.1254141193365661, R2 Train: 0.4983435226537356\n",
      "Epoch 3675, Loss Train(MSE): 0.12541386412222894, R2 Train: 0.49834454351108426\n",
      "Epoch 3676, Loss Train(MSE): 0.12541363329695426, R2 Train: 0.49834546681218295\n",
      "Epoch 3677, Loss Train(MSE): 0.1254134088317559, R2 Train: 0.49834636467297644\n",
      "Epoch 3678, Loss Train(MSE): 0.12541315470267916, R2 Train: 0.49834738118928334\n",
      "Epoch 3679, Loss Train(MSE): 0.1254129417919002, R2 Train: 0.49834823283239915\n",
      "Epoch 3680, Loss Train(MSE): 0.125412701169686, R2 Train: 0.498349195321256\n",
      "Epoch 3681, Loss Train(MSE): 0.125412451966711, R2 Train: 0.498350192133156\n",
      "Epoch 3682, Loss Train(MSE): 0.12541224871663514, R2 Train: 0.4983510051334594\n",
      "Epoch 3683, Loss Train(MSE): 0.12541199628368907, R2 Train: 0.4983520148652437\n",
      "Epoch 3684, Loss Train(MSE): 0.12541176432950324, R2 Train: 0.49835294268198704\n",
      "Epoch 3685, Loss Train(MSE): 0.12541154556165587, R2 Train: 0.4983538177533765\n",
      "Epoch 3686, Loss Train(MSE): 0.12541129418498492, R2 Train: 0.4983548232600603\n",
      "Epoch 3687, Loss Train(MSE): 0.1254110789628482, R2 Train: 0.4983556841486072\n",
      "Epoch 3688, Loss Train(MSE): 0.125410845177319, R2 Train: 0.49835661929072395\n",
      "Epoch 3689, Loss Train(MSE): 0.1254105948458664, R2 Train: 0.49835762061653444\n",
      "Epoch 3690, Loss Train(MSE): 0.1254103958495738, R2 Train: 0.49835841660170477\n",
      "Epoch 3691, Loss Train(MSE): 0.12541014753626514, R2 Train: 0.49835940985493943\n",
      "Epoch 3692, Loss Train(MSE): 0.12540991192510761, R2 Train: 0.49836035229956954\n",
      "Epoch 3693, Loss Train(MSE): 0.12540970127212928, R2 Train: 0.4983611949114829\n",
      "Epoch 3694, Loss Train(MSE): 0.12540945257535754, R2 Train: 0.4983621896985698\n",
      "Epoch 3695, Loss Train(MSE): 0.12540923257814376, R2 Train: 0.49836306968742494\n",
      "Epoch 3696, Loss Train(MSE): 0.12540900798473018, R2 Train: 0.4983639680610793\n",
      "Epoch 3697, Loss Train(MSE): 0.1254087603062545, R2 Train: 0.498364958774982\n",
      "Epoch 3698, Loss Train(MSE): 0.125408555441279, R2 Train: 0.49836577823488404\n",
      "Epoch 3699, Loss Train(MSE): 0.1254083173736603, R2 Train: 0.49836673050535885\n",
      "Epoch 3700, Loss Train(MSE): 0.12540807572483792, R2 Train: 0.4983676971006483\n",
      "Epoch 3701, Loss Train(MSE): 0.12540787546325977, R2 Train: 0.49836849814696094\n",
      "Epoch 3702, Loss Train(MSE): 0.1254076293781142, R2 Train: 0.4983694824875432\n",
      "Epoch 3703, Loss Train(MSE): 0.12540740228505856, R2 Train: 0.49837039085976576\n",
      "Epoch 3704, Loss Train(MSE): 0.12540718910243265, R2 Train: 0.4983712435902694\n",
      "Epoch 3705, Loss Train(MSE): 0.12540694401001007, R2 Train: 0.4983722239599597\n",
      "Epoch 3706, Loss Train(MSE): 0.12540673101416586, R2 Train: 0.4983730759433366\n",
      "Epoch 3707, Loss Train(MSE): 0.12540650535433945, R2 Train: 0.4983739785826422\n",
      "Epoch 3708, Loss Train(MSE): 0.1254062612448519, R2 Train: 0.49837495502059237\n",
      "Epoch 3709, Loss Train(MSE): 0.125406061896912, R2 Train: 0.498375752412352\n",
      "Epoch 3710, Loss Train(MSE): 0.1254058241947756, R2 Train: 0.49837670322089755\n",
      "Epoch 3711, Loss Train(MSE): 0.12540558782851308, R2 Train: 0.4983776486859477\n",
      "Epoch 3712, Loss Train(MSE): 0.12540538813645144, R2 Train: 0.49837844745419424\n",
      "Epoch 3713, Loss Train(MSE): 0.12540514556600207, R2 Train: 0.4983794177359917\n",
      "Epoch 3714, Loss Train(MSE): 0.12540492231796063, R2 Train: 0.49838031072815747\n",
      "Epoch 3715, Loss Train(MSE): 0.1254047110915453, R2 Train: 0.4983811556338188\n",
      "Epoch 3716, Loss Train(MSE): 0.12540446948019213, R2 Train: 0.4983821220792315\n",
      "Epoch 3717, Loss Train(MSE): 0.1254042589224577, R2 Train: 0.49838296431016915\n",
      "Epoch 3718, Loss Train(MSE): 0.1254040365758777, R2 Train: 0.4983838536964892\n",
      "Epoch 3719, Loss Train(MSE): 0.12540379591447945, R2 Train: 0.4983848163420822\n",
      "Epoch 3720, Loss Train(MSE): 0.1254035976277204, R2 Train: 0.49838560948911836\n",
      "Epoch 3721, Loss Train(MSE): 0.12540336456684592, R2 Train: 0.4983865417326163\n",
      "Epoch 3722, Loss Train(MSE): 0.12540312907309317, R2 Train: 0.4983874837076273\n",
      "Epoch 3723, Loss Train(MSE): 0.1254029341825552, R2 Train: 0.49838826326977925\n",
      "Epoch 3724, Loss Train(MSE): 0.12540269500954201, R2 Train: 0.49838921996183194\n",
      "Epoch 3725, Loss Train(MSE): 0.12540247130024829, R2 Train: 0.49839011479900686\n",
      "Epoch 3726, Loss Train(MSE): 0.12540226616163577, R2 Train: 0.4983909353534569\n",
      "Epoch 3727, Loss Train(MSE): 0.12540202791630972, R2 Train: 0.4983918883347611\n",
      "Epoch 3728, Loss Train(MSE): 0.1254018155919305, R2 Train: 0.498392737632278\n",
      "Epoch 3729, Loss Train(MSE): 0.12540160059195124, R2 Train: 0.49839359763219504\n",
      "Epoch 3730, Loss Train(MSE): 0.1254013632657584, R2 Train: 0.4983945469369664\n",
      "Epoch 3731, Loss Train(MSE): 0.1254011619347213, R2 Train: 0.49839535226111475\n",
      "Epoch 3732, Loss Train(MSE): 0.12540093745234948, R2 Train: 0.4983962501906021\n",
      "Epoch 3733, Loss Train(MSE): 0.12540070103689438, R2 Train: 0.4983971958524225\n",
      "Epoch 3734, Loss Train(MSE): 0.1254005103154316, R2 Train: 0.49839795873827364\n",
      "Epoch 3735, Loss Train(MSE): 0.1254002767220691, R2 Train: 0.4983988931117236\n",
      "Epoch 3736, Loss Train(MSE): 0.1254000485846096, R2 Train: 0.4983998056615616\n",
      "Epoch 3737, Loss Train(MSE): 0.12539985333685402, R2 Train: 0.4984005866525839\n",
      "Epoch 3738, Loss Train(MSE): 0.1253996183495504, R2 Train: 0.4984015266017984\n",
      "Epoch 3739, Loss Train(MSE): 0.12539940038622527, R2 Train: 0.4984023984550989\n",
      "Epoch 3740, Loss Train(MSE): 0.12539919644426106, R2 Train: 0.49840321422295575\n",
      "Epoch 3741, Loss Train(MSE): 0.1253989623472388, R2 Train: 0.4984041506110448\n",
      "Epoch 3742, Loss Train(MSE): 0.12539875419219276, R2 Train: 0.49840498323122895\n",
      "Epoch 3743, Loss Train(MSE): 0.1253985419100589, R2 Train: 0.49840583235976443\n",
      "Epoch 3744, Loss Train(MSE): 0.12539830869544247, R2 Train: 0.49840676521823013\n",
      "Epoch 3745, Loss Train(MSE): 0.12539810999007897, R2 Train: 0.4984075600396841\n",
      "Epoch 3746, Loss Train(MSE): 0.1253978897147665, R2 Train: 0.49840844114093397\n",
      "Epoch 3747, Loss Train(MSE): 0.12539765737482125, R2 Train: 0.498409370500715\n",
      "Epoch 3748, Loss Train(MSE): 0.1253974677676519, R2 Train: 0.4984101289293924\n",
      "Epoch 3749, Loss Train(MSE): 0.12539723983924905, R2 Train: 0.4984110406430038\n",
      "Epoch 3750, Loss Train(MSE): 0.12539701266991535, R2 Train: 0.4984119493203386\n",
      "Epoch 3751, Loss Train(MSE): 0.12539682320220064, R2 Train: 0.49841270719119746\n",
      "Epoch 3752, Loss Train(MSE): 0.12539659223498423, R2 Train: 0.4984136310600631\n",
      "Epoch 3753, Loss Train(MSE): 0.12539637377457977, R2 Train: 0.49841450490168093\n",
      "Epoch 3754, Loss Train(MSE): 0.12539617702582384, R2 Train: 0.49841529189670464\n",
      "Epoch 3755, Loss Train(MSE): 0.1253959469144269, R2 Train: 0.4984162123422924\n",
      "Epoch 3756, Loss Train(MSE): 0.12539573682769933, R2 Train: 0.49841705268920267\n",
      "Epoch 3757, Loss Train(MSE): 0.12539553312223944, R2 Train: 0.4984178675110422\n",
      "Epoch 3758, Loss Train(MSE): 0.12539530385938902, R2 Train: 0.49841878456244393\n",
      "Epoch 3759, Loss Train(MSE): 0.1253951018177052, R2 Train: 0.49841959272917924\n",
      "Epoch 3760, Loss Train(MSE): 0.12539489147344623, R2 Train: 0.49842043410621506\n",
      "Epoch 3761, Loss Train(MSE): 0.12539466305199504, R2 Train: 0.49842134779201985\n",
      "Epoch 3762, Loss Train(MSE): 0.12539446873320503, R2 Train: 0.49842212506717987\n",
      "Epoch 3763, Loss Train(MSE): 0.12539425206175042, R2 Train: 0.4984229917529983\n",
      "Epoch 3764, Loss Train(MSE): 0.12539402447467368, R2 Train: 0.49842390210130527\n",
      "Epoch 3765, Loss Train(MSE): 0.1253938375629785, R2 Train: 0.498424649748086\n",
      "Epoch 3766, Loss Train(MSE): 0.12539361486975784, R2 Train: 0.49842554052096866\n",
      "Epoch 3767, Loss Train(MSE): 0.12539339027216356, R2 Train: 0.49842643891134575\n",
      "Epoch 3768, Loss Train(MSE): 0.12539320612859525, R2 Train: 0.498427175485619\n",
      "Epoch 3769, Loss Train(MSE): 0.1253929798523037, R2 Train: 0.49842808059078525\n",
      "Epoch 3770, Loss Train(MSE): 0.1253927623233817, R2 Train: 0.4984289507064732\n",
      "Epoch 3771, Loss Train(MSE): 0.1253925724807918, R2 Train: 0.4984297100768328\n",
      "Epoch 3772, Loss Train(MSE): 0.12539234702174581, R2 Train: 0.49843061191301674\n",
      "Epoch 3773, Loss Train(MSE): 0.1253921362601126, R2 Train: 0.49843145495954955\n",
      "Epoch 3774, Loss Train(MSE): 0.12539194100993367, R2 Train: 0.4984322359602653\n",
      "Epoch 3775, Loss Train(MSE): 0.12539171636150928, R2 Train: 0.4984331345539629\n",
      "Epoch 3776, Loss Train(MSE): 0.12539151207170365, R2 Train: 0.4984339517131854\n",
      "Epoch 3777, Loss Train(MSE): 0.12539131169960757, R2 Train: 0.4984347532015697\n",
      "Epoch 3778, Loss Train(MSE): 0.12539108785529046, R2 Train: 0.49843564857883815\n",
      "Epoch 3779, Loss Train(MSE): 0.125390889747654, R2 Train: 0.498436441009384\n",
      "Epoch 3780, Loss Train(MSE): 0.1253906845336676, R2 Train: 0.49843726186532955\n",
      "Epoch 3781, Loss Train(MSE): 0.12539046148705013, R2 Train: 0.4984381540517995\n",
      "Epoch 3782, Loss Train(MSE): 0.1253902692776105, R2 Train: 0.49843892288955804\n",
      "Epoch 3783, Loss Train(MSE): 0.12539005949622822, R2 Train: 0.4984397620150871\n",
      "Epoch 3784, Loss Train(MSE): 0.1253898372410071, R2 Train: 0.49844065103597157\n",
      "Epoch 3785, Loss Train(MSE): 0.1253896506513636, R2 Train: 0.4984413973945456\n",
      "Epoch 3786, Loss Train(MSE): 0.12538943657165785, R2 Train: 0.4984422537133686\n",
      "Epoch 3787, Loss Train(MSE): 0.12538921510163145, R2 Train: 0.4984431395934742\n",
      "Epoch 3788, Loss Train(MSE): 0.12538903385884384, R2 Train: 0.49844386456462464\n",
      "Epoch 3789, Loss Train(MSE): 0.1253888157445726, R2 Train: 0.49844473702170955\n",
      "Epoch 3790, Loss Train(MSE): 0.1253885967317907, R2 Train: 0.4984456130728372\n",
      "Epoch 3791, Loss Train(MSE): 0.12538841720872426, R2 Train: 0.49844633116510295\n",
      "Epoch 3792, Loss Train(MSE): 0.12538819697384637, R2 Train: 0.4984472121046145\n",
      "Epoch 3793, Loss Train(MSE): 0.12538798303020518, R2 Train: 0.4984480678791793\n",
      "Epoch 3794, Loss Train(MSE): 0.12538779973637715, R2 Train: 0.4984488010544914\n",
      "Epoch 3795, Loss Train(MSE): 0.12538758027156272, R2 Train: 0.49844967891374914\n",
      "Epoch 3796, Loss Train(MSE): 0.12538737113639944, R2 Train: 0.4984505154544022\n",
      "Epoch 3797, Loss Train(MSE): 0.12538718432363471, R2 Train: 0.49845126270546114\n",
      "Epoch 3798, Loss Train(MSE): 0.12538696562301485, R2 Train: 0.4984521375079406\n",
      "Epoch 3799, Loss Train(MSE): 0.12538676104076787, R2 Train: 0.4984529558369285\n",
      "Epoch 3800, Loss Train(MSE): 0.12538657095592395, R2 Train: 0.4984537161763042\n",
      "Epoch 3801, Loss Train(MSE): 0.12538635301372128, R2 Train: 0.4984545879451149\n",
      "Epoch 3802, Loss Train(MSE): 0.125386152733829, R2 Train: 0.498455389064684\n",
      "Epoch 3803, Loss Train(MSE): 0.12538595961889407, R2 Train: 0.49845616152442374\n",
      "Epoch 3804, Loss Train(MSE): 0.12538574242942052, R2 Train: 0.4984570302823179\n",
      "Epoch 3805, Loss Train(MSE): 0.12538554620622272, R2 Train: 0.4984578151751091\n",
      "Epoch 3806, Loss Train(MSE): 0.1253853502984109, R2 Train: 0.49845859880635635\n",
      "Epoch 3807, Loss Train(MSE): 0.12538513385606553, R2 Train: 0.49845946457573787\n",
      "Epoch 3808, Loss Train(MSE): 0.12538494144870713, R2 Train: 0.4984602342051715\n",
      "Epoch 3809, Loss Train(MSE): 0.12538474298055186, R2 Train: 0.4984610280777926\n",
      "Epoch 3810, Loss Train(MSE): 0.1253845272798189, R2 Train: 0.49846189088072435\n",
      "Epoch 3811, Loss Train(MSE): 0.1253843384521553, R2 Train: 0.4984626461913788\n",
      "Epoch 3812, Loss Train(MSE): 0.12538413765160095, R2 Train: 0.4984634493935962\n",
      "Epoch 3813, Loss Train(MSE): 0.1253839226870479, R2 Train: 0.4984643092518084\n",
      "Epoch 3814, Loss Train(MSE): 0.12538373720755291, R2 Train: 0.49846505116978834\n",
      "Epoch 3815, Loss Train(MSE): 0.12538353429804372, R2 Train: 0.4984658628078251\n",
      "Epoch 3816, Loss Train(MSE): 0.12538332006431946, R2 Train: 0.49846671974272216\n",
      "Epoch 3817, Loss Train(MSE): 0.12538313770599485, R2 Train: 0.4984674491760206\n",
      "Epoch 3818, Loss Train(MSE): 0.12538293290656277, R2 Train: 0.4984682683737489\n",
      "Epoch 3819, Loss Train(MSE): 0.1253827193983956, R2 Train: 0.4984691224064176\n",
      "Epoch 3820, Loss Train(MSE): 0.12538885144306572, R2 Train: 0.4984445942277371\n",
      "Epoch 3821, Loss Train(MSE): 0.1254151206279261, R2 Train: 0.49833951748829564\n",
      "Epoch 3822, Loss Train(MSE): 0.12541460570858848, R2 Train: 0.4983415771656461\n",
      "Epoch 3823, Loss Train(MSE): 0.12541412513526026, R2 Train: 0.49834349945895895\n",
      "Epoch 3824, Loss Train(MSE): 0.12541359443262243, R2 Train: 0.4983456222695103\n",
      "Epoch 3825, Loss Train(MSE): 0.1254131361773205, R2 Train: 0.498347455290718\n",
      "Epoch 3826, Loss Train(MSE): 0.12541260947361071, R2 Train: 0.49834956210555714\n",
      "Epoch 3827, Loss Train(MSE): 0.12541213413560562, R2 Train: 0.4983514634575775\n",
      "Epoch 3828, Loss Train(MSE): 0.12541637360192676, R2 Train: 0.498334505592293\n",
      "Epoch 3829, Loss Train(MSE): 0.12541775415982, R2 Train: 0.49832898336071996\n",
      "Epoch 3830, Loss Train(MSE): 0.1254172060445504, R2 Train: 0.4983311758217984\n",
      "Epoch 3831, Loss Train(MSE): 0.1254166596245528, R2 Train: 0.49833336150178875\n",
      "Epoch 3832, Loss Train(MSE): 0.12541611489069746, R2 Train: 0.4983355404372102\n",
      "Epoch 3833, Loss Train(MSE): 0.1254155718339266, R2 Train: 0.4983377126642936\n",
      "Epoch 3834, Loss Train(MSE): 0.1254150304452539, R2 Train: 0.4983398782189844\n",
      "Epoch 3835, Loss Train(MSE): 0.12541449071576366, R2 Train: 0.49834203713694536\n",
      "Epoch 3836, Loss Train(MSE): 0.12541395263660998, R2 Train: 0.49834418945356007\n",
      "Epoch 3837, Loss Train(MSE): 0.12541341619901625, R2 Train: 0.498346335203935\n",
      "Epoch 3838, Loss Train(MSE): 0.12541288139427412, R2 Train: 0.49834847442290353\n",
      "Epoch 3839, Loss Train(MSE): 0.12541234821374325, R2 Train: 0.498350607145027\n",
      "Epoch 3840, Loss Train(MSE): 0.12541181664885018, R2 Train: 0.4983527334045993\n",
      "Epoch 3841, Loss Train(MSE): 0.1254112866910879, R2 Train: 0.49835485323564843\n",
      "Epoch 3842, Loss Train(MSE): 0.12541075833201515, R2 Train: 0.4983569666719394\n",
      "Epoch 3843, Loss Train(MSE): 0.12541023156325556, R2 Train: 0.49835907374697774\n",
      "Epoch 3844, Loss Train(MSE): 0.12540970637649737, R2 Train: 0.4983611744940105\n",
      "Epoch 3845, Loss Train(MSE): 0.12540918276349228, R2 Train: 0.4983632689460309\n",
      "Epoch 3846, Loss Train(MSE): 0.12540866071605525, R2 Train: 0.498365357135779\n",
      "Epoch 3847, Loss Train(MSE): 0.1254081402260637, R2 Train: 0.4983674390957452\n",
      "Epoch 3848, Loss Train(MSE): 0.1254076212854567, R2 Train: 0.49836951485817316\n",
      "Epoch 3849, Loss Train(MSE): 0.1254071038862348, R2 Train: 0.4983715844550608\n",
      "Epoch 3850, Loss Train(MSE): 0.12540658802045876, R2 Train: 0.49837364791816496\n",
      "Epoch 3851, Loss Train(MSE): 0.1254060736802497, R2 Train: 0.4983757052790012\n",
      "Epoch 3852, Loss Train(MSE): 0.1254055608577879, R2 Train: 0.4983777565688484\n",
      "Epoch 3853, Loss Train(MSE): 0.12540504954531256, R2 Train: 0.49837980181874975\n",
      "Epoch 3854, Loss Train(MSE): 0.125404539735121, R2 Train: 0.498381841059516\n",
      "Epoch 3855, Loss Train(MSE): 0.12540403141956816, R2 Train: 0.49838387432172737\n",
      "Epoch 3856, Loss Train(MSE): 0.12540352459106624, R2 Train: 0.49838590163573504\n",
      "Epoch 3857, Loss Train(MSE): 0.1254030192420837, R2 Train: 0.49838792303166524\n",
      "Epoch 3858, Loss Train(MSE): 0.12540251536514507, R2 Train: 0.4983899385394197\n",
      "Epoch 3859, Loss Train(MSE): 0.12540201295283038, R2 Train: 0.4983919481886785\n",
      "Epoch 3860, Loss Train(MSE): 0.12540151199777436, R2 Train: 0.4983939520089026\n",
      "Epoch 3861, Loss Train(MSE): 0.12540101249266614, R2 Train: 0.49839595002933545\n",
      "Epoch 3862, Loss Train(MSE): 0.1254005144302486, R2 Train: 0.4983979422790056\n",
      "Epoch 3863, Loss Train(MSE): 0.125400017803318, R2 Train: 0.498399928786728\n",
      "Epoch 3864, Loss Train(MSE): 0.12539952260472323, R2 Train: 0.4984019095811071\n",
      "Epoch 3865, Loss Train(MSE): 0.12539902882736556, R2 Train: 0.49840388469053776\n",
      "Epoch 3866, Loss Train(MSE): 0.12539853646419782, R2 Train: 0.4984058541432087\n",
      "Epoch 3867, Loss Train(MSE): 0.12539804550822428, R2 Train: 0.4984078179671029\n",
      "Epoch 3868, Loss Train(MSE): 0.12539755595249993, R2 Train: 0.49840977619000026\n",
      "Epoch 3869, Loss Train(MSE): 0.12539706779012985, R2 Train: 0.4984117288394806\n",
      "Epoch 3870, Loss Train(MSE): 0.12539658101426912, R2 Train: 0.4984136759429235\n",
      "Epoch 3871, Loss Train(MSE): 0.125396095618122, R2 Train: 0.49841561752751196\n",
      "Epoch 3872, Loss Train(MSE): 0.12539561159494167, R2 Train: 0.4984175536202333\n",
      "Epoch 3873, Loss Train(MSE): 0.12539512893802957, R2 Train: 0.49841948424788174\n",
      "Epoch 3874, Loss Train(MSE): 0.1253946476407352, R2 Train: 0.49842140943705915\n",
      "Epoch 3875, Loss Train(MSE): 0.12539416769645545, R2 Train: 0.4984233292141782\n",
      "Epoch 3876, Loss Train(MSE): 0.1253936890986342, R2 Train: 0.49842524360546325\n",
      "Epoch 3877, Loss Train(MSE): 0.12539321184076194, R2 Train: 0.49842715263695225\n",
      "Epoch 3878, Loss Train(MSE): 0.1253927359163753, R2 Train: 0.49842905633449885\n",
      "Epoch 3879, Loss Train(MSE): 0.12539226131905654, R2 Train: 0.49843095472377386\n",
      "Epoch 3880, Loss Train(MSE): 0.12539178804243334, R2 Train: 0.49843284783026665\n",
      "Epoch 3881, Loss Train(MSE): 0.12539131608017806, R2 Train: 0.49843473567928775\n",
      "Epoch 3882, Loss Train(MSE): 0.12539084542600767, R2 Train: 0.4984366182959693\n",
      "Epoch 3883, Loss Train(MSE): 0.12539037607368295, R2 Train: 0.4984384957052682\n",
      "Epoch 3884, Loss Train(MSE): 0.12538990801700844, R2 Train: 0.4984403679319662\n",
      "Epoch 3885, Loss Train(MSE): 0.1253894412498319, R2 Train: 0.4984422350006724\n",
      "Epoch 3886, Loss Train(MSE): 0.12538897576604377, R2 Train: 0.4984440969358249\n",
      "Epoch 3887, Loss Train(MSE): 0.125388511559577, R2 Train: 0.498445953761692\n",
      "Epoch 3888, Loss Train(MSE): 0.1253880486244066, R2 Train: 0.49844780550237355\n",
      "Epoch 3889, Loss Train(MSE): 0.12538758695454907, R2 Train: 0.4984496521818037\n",
      "Epoch 3890, Loss Train(MSE): 0.12538712654406234, R2 Train: 0.49845149382375065\n",
      "Epoch 3891, Loss Train(MSE): 0.12538666738704504, R2 Train: 0.49845333045181983\n",
      "Epoch 3892, Loss Train(MSE): 0.12538636842758819, R2 Train: 0.49845452628964726\n",
      "Epoch 3893, Loss Train(MSE): 0.12538611127647503, R2 Train: 0.4984555548940999\n",
      "Epoch 3894, Loss Train(MSE): 0.12538585444774042, R2 Train: 0.4984565822090383\n",
      "Epoch 3895, Loss Train(MSE): 0.12538559794076481, R2 Train: 0.49845760823694074\n",
      "Epoch 3896, Loss Train(MSE): 0.12538534175493066, R2 Train: 0.49845863298027737\n",
      "Epoch 3897, Loss Train(MSE): 0.125385085889622, R2 Train: 0.498459656441512\n",
      "Epoch 3898, Loss Train(MSE): 0.12538483034422485, R2 Train: 0.4984606786231006\n",
      "Epoch 3899, Loss Train(MSE): 0.1253845751181268, R2 Train: 0.49846169952749275\n",
      "Epoch 3900, Loss Train(MSE): 0.12538432021071746, R2 Train: 0.49846271915713014\n",
      "Epoch 3901, Loss Train(MSE): 0.125384065621388, R2 Train: 0.49846373751444795\n",
      "Epoch 3902, Loss Train(MSE): 0.1253838113495315, R2 Train: 0.498464754601874\n",
      "Epoch 3903, Loss Train(MSE): 0.12538355739454263, R2 Train: 0.4984657704218295\n",
      "Epoch 3904, Loss Train(MSE): 0.1253833037558179, R2 Train: 0.49846678497672836\n",
      "Epoch 3905, Loss Train(MSE): 0.1253830504327556, R2 Train: 0.4984677982689776\n",
      "Epoch 3906, Loss Train(MSE): 0.12538279742475564, R2 Train: 0.49846881030097745\n",
      "Epoch 3907, Loss Train(MSE): 0.12538254473121963, R2 Train: 0.4984698210751215\n",
      "Epoch 3908, Loss Train(MSE): 0.125382292351551, R2 Train: 0.49847083059379604\n",
      "Epoch 3909, Loss Train(MSE): 0.1253820402851548, R2 Train: 0.49847183885938084\n",
      "Epoch 3910, Loss Train(MSE): 0.1253817885314378, R2 Train: 0.4984728458742488\n",
      "Epoch 3911, Loss Train(MSE): 0.1253815370898085, R2 Train: 0.49847385164076596\n",
      "Epoch 3912, Loss Train(MSE): 0.12538128595967696, R2 Train: 0.49847485616129217\n",
      "Epoch 3913, Loss Train(MSE): 0.12538103514045498, R2 Train: 0.49847585943818007\n",
      "Epoch 3914, Loss Train(MSE): 0.12538081027812434, R2 Train: 0.49847675888750265\n",
      "Epoch 3915, Loss Train(MSE): 0.12538061246859963, R2 Train: 0.49847755012560147\n",
      "Epoch 3916, Loss Train(MSE): 0.12538042080212106, R2 Train: 0.4984783167915158\n",
      "Epoch 3917, Loss Train(MSE): 0.1253802038188918, R2 Train: 0.49847918472443276\n",
      "Epoch 3918, Loss Train(MSE): 0.12538001906196486, R2 Train: 0.49847992375214056\n",
      "Epoch 3919, Loss Train(MSE): 0.1253798157121674, R2 Train: 0.49848073715133046\n",
      "Epoch 3920, Loss Train(MSE): 0.12537959956146033, R2 Train: 0.4984816017541587\n",
      "Epoch 3921, Loss Train(MSE): 0.12537942746881253, R2 Train: 0.49848229012474987\n",
      "Epoch 3922, Loss Train(MSE): 0.12537921281221, R2 Train: 0.49848314875115995\n",
      "Epoch 3923, Loss Train(MSE): 0.12537900825153206, R2 Train: 0.49848396699387176\n",
      "Epoch 3924, Loss Train(MSE): 0.12537882690071198, R2 Train: 0.4984846923971521\n",
      "Epoch 3925, Loss Train(MSE): 0.12537861205384015, R2 Train: 0.4984855517846394\n",
      "Epoch 3926, Loss Train(MSE): 0.1253784196947865, R2 Train: 0.498486321220854\n",
      "Epoch 3927, Loss Train(MSE): 0.12537822748200475, R2 Train: 0.498487090071981\n",
      "Epoch 3928, Loss Train(MSE): 0.12537801344812008, R2 Train: 0.4984879462075197\n",
      "Epoch 3929, Loss Train(MSE): 0.12537783291997215, R2 Train: 0.4984886683201114\n",
      "Epoch 3930, Loss Train(MSE): 0.1253776302046912, R2 Train: 0.4984894791812352\n",
      "Epoch 3931, Loss Train(MSE): 0.12537741709964909, R2 Train: 0.49849033160140366\n",
      "Epoch 3932, Loss Train(MSE): 0.12537724778209375, R2 Train: 0.498491008871625\n",
      "Epoch 3933, Loss Train(MSE): 0.12537703502198846, R2 Train: 0.49849185991204614\n",
      "Epoch 3934, Loss Train(MSE): 0.12537683331061136, R2 Train: 0.49849266675755455\n",
      "Epoch 3935, Loss Train(MSE): 0.12537665391081051, R2 Train: 0.49849338435675794\n",
      "Epoch 3936, Loss Train(MSE): 0.1253764419450574, R2 Train: 0.4984942322197704\n",
      "Epoch 3937, Loss Train(MSE): 0.12537625127332108, R2 Train: 0.49849499490671567\n",
      "Epoch 3938, Loss Train(MSE): 0.1253760621345544, R2 Train: 0.4984957514617824\n",
      "Epoch 3939, Loss Train(MSE): 0.12537585095599468, R2 Train: 0.49849659617602127\n",
      "Epoch 3940, Loss Train(MSE): 0.1253756709765864, R2 Train: 0.49849731609365444\n",
      "Epoch 3941, Loss Train(MSE): 0.12537547243561745, R2 Train: 0.4984981102575302\n",
      "Epoch 3942, Loss Train(MSE): 0.1253752620372216, R2 Train: 0.4984989518511136\n",
      "Epoch 3943, Loss Train(MSE): 0.12537509240940484, R2 Train: 0.49849963036238065\n",
      "Epoch 3944, Loss Train(MSE): 0.12537488479661096, R2 Train: 0.49850046081355615\n",
      "Epoch 3945, Loss Train(MSE): 0.12537468237538985, R2 Train: 0.4985012704984406\n",
      "Epoch 3946, Loss Train(MSE): 0.1253745083485847, R2 Train: 0.49850196660566115\n",
      "Epoch 3947, Loss Train(MSE): 0.12537429917350534, R2 Train: 0.4985028033059786\n",
      "Epoch 3948, Loss Train(MSE): 0.12537410670988436, R2 Train: 0.49850357316046257\n",
      "Epoch 3949, Loss Train(MSE): 0.12537392398960429, R2 Train: 0.49850430404158286\n",
      "Epoch 3950, Loss Train(MSE): 0.1253737155775326, R2 Train: 0.4985051376898696\n",
      "Epoch 3951, Loss Train(MSE): 0.12537353274595622, R2 Train: 0.4985058690161751\n",
      "Epoch 3952, Loss Train(MSE): 0.1253733416478495, R2 Train: 0.498506633408602\n",
      "Epoch 3953, Loss Train(MSE): 0.125373133992181, R2 Train: 0.498507464031276\n",
      "Epoch 3954, Loss Train(MSE): 0.1253729604732288, R2 Train: 0.4985081581070848\n",
      "Epoch 3955, Loss Train(MSE): 0.1253727613069819, R2 Train: 0.49850895477207235\n",
      "Epoch 3956, Loss Train(MSE): 0.12537255488148666, R2 Train: 0.49850978047405337\n",
      "Epoch 3957, Loss Train(MSE): 0.1253723893938402, R2 Train: 0.49851044242463916\n",
      "Epoch 3958, Loss Train(MSE): 0.12537218292493066, R2 Train: 0.49851126830027737\n",
      "Epoch 3959, Loss Train(MSE): 0.12537198544869727, R2 Train: 0.49851205820521094\n",
      "Epoch 3960, Loss Train(MSE): 0.12537181224156838, R2 Train: 0.4985127510337265\n",
      "Epoch 3961, Loss Train(MSE): 0.12537160651294063, R2 Train: 0.4985135739482375\n",
      "Epoch 3962, Loss Train(MSE): 0.12537141768066423, R2 Train: 0.49851432927734307\n",
      "Epoch 3963, Loss Train(MSE): 0.12537123705003506, R2 Train: 0.4985150517998598\n",
      "Epoch 3964, Loss Train(MSE): 0.1253710320554731, R2 Train: 0.49851587177810763\n",
      "Epoch 3965, Loss Train(MSE): 0.12537085156757755, R2 Train: 0.4985165937296898\n",
      "Epoch 3966, Loss Train(MSE): 0.12537066380385944, R2 Train: 0.4985173447845622\n",
      "Epoch 3967, Loss Train(MSE): 0.1253704595372529, R2 Train: 0.4985181618509884\n",
      "Epoch 3968, Loss Train(MSE): 0.12537028709977827, R2 Train: 0.49851885160088694\n",
      "Epoch 3969, Loss Train(MSE): 0.12537009248792058, R2 Train: 0.4985196300483177\n",
      "Epoch 3970, Loss Train(MSE): 0.1253698889432622, R2 Train: 0.4985204442269512\n",
      "Epoch 3971, Loss Train(MSE): 0.12536972426775433, R2 Train: 0.49852110292898266\n",
      "Epoch 3972, Loss Train(MSE): 0.1253695230873505, R2 Train: 0.498521907650598\n",
      "Epoch 3973, Loss Train(MSE): 0.12536932534120623, R2 Train: 0.49852269863517507\n",
      "Epoch 3974, Loss Train(MSE): 0.12536915797381432, R2 Train: 0.49852336810474274\n",
      "Epoch 3975, Loss Train(MSE): 0.1253689555629166, R2 Train: 0.4985241777483336\n",
      "Epoch 3976, Loss Train(MSE): 0.12536876525975785, R2 Train: 0.4985249389609686\n",
      "Epoch 3977, Loss Train(MSE): 0.12536859162940328, R2 Train: 0.4985256334823869\n",
      "Epoch 3978, Loss Train(MSE): 0.12536838992578622, R2 Train: 0.4985264402968551\n",
      "Epoch 3979, Loss Train(MSE): 0.12536820678975596, R2 Train: 0.49852717284097614\n",
      "Epoch 3980, Loss Train(MSE): 0.125368027163791, R2 Train: 0.498527891344836\n",
      "Epoch 3981, Loss Train(MSE): 0.12536782616178585, R2 Train: 0.4985286953528566\n",
      "Epoch 3982, Loss Train(MSE): 0.1253676499221751, R2 Train: 0.49852940031129955\n",
      "Epoch 3983, Loss Train(MSE): 0.12536746456294137, R2 Train: 0.4985301417482345\n",
      "Epoch 3984, Loss Train(MSE): 0.12536726425697187, R2 Train: 0.49853094297211253\n",
      "Epoch 3985, Loss Train(MSE): 0.12536709464811988, R2 Train: 0.4985316214075205\n",
      "Epoch 3986, Loss Train(MSE): 0.12536690381304463, R2 Train: 0.49853238474782147\n",
      "Epoch 3987, Loss Train(MSE): 0.12536670419762483, R2 Train: 0.4985331832095007\n",
      "Epoch 3988, Loss Train(MSE): 0.12536654095882155, R2 Train: 0.4985338361647138\n",
      "Epoch 3989, Loss Train(MSE): 0.12536634490051182, R2 Train: 0.4985346203979527\n",
      "Epoch 3990, Loss Train(MSE): 0.12536614849277106, R2 Train: 0.4985354060289158\n",
      "Epoch 3991, Loss Train(MSE): 0.12536598631864174, R2 Train: 0.49853605472543305\n",
      "Epoch 3992, Loss Train(MSE): 0.12536578778865734, R2 Train: 0.49853684884537064\n",
      "Epoch 3993, Loss Train(MSE): 0.12536559747155346, R2 Train: 0.49853761011378617\n",
      "Epoch 3994, Loss Train(MSE): 0.1253654303413358, R2 Train: 0.49853827863465683\n",
      "Epoch 3995, Loss Train(MSE): 0.1253652324885019, R2 Train: 0.4985390700459924\n",
      "Epoch 3996, Loss Train(MSE): 0.12536504801258413, R2 Train: 0.49853980794966346\n",
      "Epoch 3997, Loss Train(MSE): 0.12536487616793535, R2 Train: 0.4985404953282586\n",
      "Epoch 3998, Loss Train(MSE): 0.1253646789870622, R2 Train: 0.49854128405175124\n",
      "Epoch 3999, Loss Train(MSE): 0.12536450010751454, R2 Train: 0.49854199956994183\n",
      "Epoch 4000, Loss Train(MSE): 0.1253643237855769, R2 Train: 0.49854270485769236\n",
      "Epoch 4001, Loss Train(MSE): 0.1253641272715559, R2 Train: 0.49854349091377637\n",
      "Epoch 4002, Loss Train(MSE): 0.1253639537481087, R2 Train: 0.49854418500756525\n",
      "Epoch 4003, Loss Train(MSE): 0.1253637731815951, R2 Train: 0.49854490727361955\n",
      "Epoch 4004, Loss Train(MSE): 0.125363577329397, R2 Train: 0.49854569068241195\n",
      "Epoch 4005, Loss Train(MSE): 0.12536340892624023, R2 Train: 0.49854636429503907\n",
      "Epoch 4006, Loss Train(MSE): 0.12536322434351804, R2 Train: 0.49854710262592783\n",
      "Epoch 4007, Loss Train(MSE): 0.12536302914819097, R2 Train: 0.4985478834072361\n",
      "Epoch 4008, Loss Train(MSE): 0.12536286563388996, R2 Train: 0.49854853746444017\n",
      "Epoch 4009, Loss Train(MSE): 0.12536267725906267, R2 Train: 0.49854929096374934\n",
      "Epoch 4010, Loss Train(MSE): 0.1253624827157306, R2 Train: 0.49855006913707756\n",
      "Epoch 4011, Loss Train(MSE): 0.1253623238631429, R2 Train: 0.49855070454742845\n",
      "Epoch 4012, Loss Train(MSE): 0.12536213191613044, R2 Train: 0.49855147233547825\n",
      "Epoch 4013, Loss Train(MSE): 0.12536193982499572, R2 Train: 0.49855224070001714\n",
      "Epoch 4014, Loss Train(MSE): 0.1253617817983844, R2 Train: 0.49855287280646243\n",
      "Epoch 4015, Loss Train(MSE): 0.1253615882811245, R2 Train: 0.49855364687550197\n",
      "Epoch 4016, Loss Train(MSE): 0.12536140061990886, R2 Train: 0.49855439752036457\n",
      "Epoch 4017, Loss Train(MSE): 0.1253612392419537, R2 Train: 0.49855504303218523\n",
      "Epoch 4018, Loss Train(MSE): 0.12536104636477313, R2 Train: 0.49855581454090747\n",
      "Epoch 4019, Loss Train(MSE): 0.1253608629160091, R2 Train: 0.49855654833596363\n",
      "Epoch 4020, Loss Train(MSE): 0.12536069839721234, R2 Train: 0.49855720641115064\n",
      "Epoch 4021, Loss Train(MSE): 0.12536050615548336, R2 Train: 0.49855797537806656\n",
      "Epoch 4022, Loss Train(MSE): 0.12536032670572833, R2 Train: 0.4985586931770867\n",
      "Epoch 4023, Loss Train(MSE): 0.12536015925266747, R2 Train: 0.4985593629893301\n",
      "Epoch 4024, Loss Train(MSE): 0.12535996764183066, R2 Train: 0.49856012943267736\n",
      "Epoch 4025, Loss Train(MSE): 0.12535979198159164, R2 Train: 0.49856083207363344\n",
      "Epoch 4026, Loss Train(MSE): 0.12535962179699245, R2 Train: 0.4985615128120302\n",
      "Epoch 4027, Loss Train(MSE): 0.1253594308125554, R2 Train: 0.4985622767497784\n",
      "Epoch 4028, Loss Train(MSE): 0.125359258736215, R2 Train: 0.49856296505514\n",
      "Epoch 4029, Loss Train(MSE): 0.12535908601902318, R2 Train: 0.4985636559239073\n",
      "Epoch 4030, Loss Train(MSE): 0.1253588956565589, R2 Train: 0.49856441737376445\n",
      "Epoch 4031, Loss Train(MSE): 0.1253587269623035, R2 Train: 0.49856509215078604\n",
      "Epoch 4032, Loss Train(MSE): 0.12535855190775452, R2 Train: 0.4985657923689819\n",
      "Epoch 4033, Loss Train(MSE): 0.12535836216289983, R2 Train: 0.49856655134840067\n",
      "Epoch 4034, Loss Train(MSE): 0.12535819665264863, R2 Train: 0.49856721338940546\n",
      "Epoch 4035, Loss Train(MSE): 0.12535801945233638, R2 Train: 0.4985679221906545\n",
      "Epoch 4036, Loss Train(MSE): 0.12535783032079081, R2 Train: 0.49856867871683674\n",
      "Epoch 4037, Loss Train(MSE): 0.12535766780012678, R2 Train: 0.4985693287994929\n",
      "Epoch 4038, Loss Train(MSE): 0.1253574886420705, R2 Train: 0.498570045431718\n",
      "Epoch 4039, Loss Train(MSE): 0.1253573001195949, R2 Train: 0.4985707995216204\n",
      "Epoch 4040, Loss Train(MSE): 0.12535714039769674, R2 Train: 0.49857143840921303\n",
      "Epoch 4041, Loss Train(MSE): 0.12535695946640707, R2 Train: 0.4985721621343717\n",
      "Epoch 4042, Loss Train(MSE): 0.1253567715488219, R2 Train: 0.49857291380471236\n",
      "Epoch 4043, Loss Train(MSE): 0.12535661443839827, R2 Train: 0.49857354224640693\n",
      "Epoch 4044, Loss Train(MSE): 0.12535643191494134, R2 Train: 0.49857427234023466\n",
      "Epoch 4045, Loss Train(MSE): 0.1253562445981259, R2 Train: 0.49857502160749645\n",
      "Epoch 4046, Loss Train(MSE): 0.1253560899153497, R2 Train: 0.49857564033860124\n",
      "Epoch 4047, Loss Train(MSE): 0.12535590597741048, R2 Train: 0.4985763760903581\n",
      "Epoch 4048, Loss Train(MSE): 0.12535571925730118, R2 Train: 0.49857712297079526\n",
      "Epoch 4049, Loss Train(MSE): 0.12535556682174662, R2 Train: 0.49857773271301353\n",
      "Epoch 4050, Loss Train(MSE): 0.12535538164369034, R2 Train: 0.49857847342523864\n",
      "Epoch 4051, Loss Train(MSE): 0.12535519600288905, R2 Train: 0.4985792159884438\n",
      "Epoch 4052, Loss Train(MSE): 0.1253550446637579, R2 Train: 0.49857982134496837\n",
      "Epoch 4053, Loss Train(MSE): 0.12535485888451756, R2 Train: 0.4985805644619298\n",
      "Epoch 4054, Loss Train(MSE): 0.1253546753248059, R2 Train: 0.4985812987007764\n",
      "Epoch 4055, Loss Train(MSE): 0.12535452290246515, R2 Train: 0.4985819083901394\n",
      "Epoch 4056, Loss Train(MSE): 0.1253543377099905, R2 Train: 0.498582649160038\n",
      "Epoch 4057, Loss Train(MSE): 0.125354156058479, R2 Train: 0.498583375766084\n",
      "Epoch 4058, Loss Train(MSE): 0.1253540027199498, R2 Train: 0.49858398912020085\n",
      "Epoch 4059, Loss Train(MSE): 0.12535381811036503, R2 Train: 0.4985847275585399\n",
      "Epoch 4060, Loss Train(MSE): 0.1253536381973612, R2 Train: 0.4985854472105552\n",
      "Epoch 4061, Loss Train(MSE): 0.12535348410654326, R2 Train: 0.49858606357382695\n",
      "Epoch 4062, Loss Train(MSE): 0.12535330007602485, R2 Train: 0.4985867996959006\n",
      "Epoch 4063, Loss Train(MSE): 0.1253531217349752, R2 Train: 0.4985875130600992\n",
      "Epoch 4064, Loss Train(MSE): 0.12535296705270316, R2 Train: 0.49858813178918737\n",
      "Epoch 4065, Loss Train(MSE): 0.12535278359747862, R2 Train: 0.49858886561008553\n",
      "Epoch 4066, Loss Train(MSE): 0.1253526066649115, R2 Train: 0.49858957334035403\n",
      "Epoch 4067, Loss Train(MSE): 0.1253524515490104, R2 Train: 0.4985901938039584\n",
      "Epoch 4068, Loss Train(MSE): 0.1253581947921995, R2 Train: 0.498567220831202\n",
      "Epoch 4069, Loss Train(MSE): 0.1253826885198094, R2 Train: 0.49846924592076236\n",
      "Epoch 4070, Loss Train(MSE): 0.125382282397718, R2 Train: 0.49847087040912796\n",
      "Epoch 4071, Loss Train(MSE): 0.12538182438057507, R2 Train: 0.4984727024776997\n",
      "Epoch 4072, Loss Train(MSE): 0.12538141966775215, R2 Train: 0.4984743213289914\n",
      "Epoch 4073, Loss Train(MSE): 0.12538097390363717, R2 Train: 0.4984761043854513\n",
      "Epoch 4074, Loss Train(MSE): 0.12538055397575568, R2 Train: 0.4984777840969773\n",
      "Epoch 4075, Loss Train(MSE): 0.1253801287818614, R2 Train: 0.49847948487255445\n",
      "Epoch 4076, Loss Train(MSE): 0.12537969335487748, R2 Train: 0.4984812265804901\n",
      "Epoch 4077, Loss Train(MSE): 0.12537928895295747, R2 Train: 0.4984828441881701\n",
      "Epoch 4078, Loss Train(MSE): 0.12537884157968088, R2 Train: 0.4984846336812765\n",
      "Epoch 4079, Loss Train(MSE): 0.12537845062183048, R2 Train: 0.4984861975126781\n",
      "Epoch 4080, Loss Train(MSE): 0.1253780099115244, R2 Train: 0.4984879603539024\n",
      "Epoch 4081, Loss Train(MSE): 0.12537760246495416, R2 Train: 0.49848959014018335\n",
      "Epoch 4082, Loss Train(MSE): 0.12537718338045115, R2 Train: 0.4984912664781954\n",
      "Epoch 4083, Loss Train(MSE): 0.12537675918811914, R2 Train: 0.49849296324752346\n",
      "Epoch 4084, Loss Train(MSE): 0.12537636192781465, R2 Train: 0.4984945522887414\n",
      "Epoch 4085, Loss Train(MSE): 0.1253759247536444, R2 Train: 0.4984963009854224\n",
      "Epoch 4086, Loss Train(MSE): 0.12537554157391656, R2 Train: 0.49849783370433376\n",
      "Epoch 4087, Loss Train(MSE): 0.1253751111319154, R2 Train: 0.4984995554723384\n",
      "Epoch 4088, Loss Train(MSE): 0.12537471029338645, R2 Train: 0.4985011588264542\n",
      "Epoch 4089, Loss Train(MSE): 0.125374302441856, R2 Train: 0.49850279023257604\n",
      "Epoch 4090, Loss Train(MSE): 0.12537388371242142, R2 Train: 0.4985044651503143\n",
      "Epoch 4091, Loss Train(MSE): 0.1253734986281951, R2 Train: 0.4985060054872196\n",
      "Epoch 4092, Loss Train(MSE): 0.12537307123438954, R2 Train: 0.49850771506244185\n",
      "Epoch 4093, Loss Train(MSE): 0.12537269027302386, R2 Train: 0.49850923890790455\n",
      "Epoch 4094, Loss Train(MSE): 0.1253722749407563, R2 Train: 0.49851090023697475\n",
      "Epoch 4095, Loss Train(MSE): 0.1253718752464659, R2 Train: 0.4985124990141364\n",
      "Epoch 4096, Loss Train(MSE): 0.12537148338511744, R2 Train: 0.4985140664595302\n",
      "Epoch 4097, Loss Train(MSE): 0.12537106474884047, R2 Train: 0.49851574100463814\n",
      "Epoch 4098, Loss Train(MSE): 0.12537069651533234, R2 Train: 0.49851721393867066\n",
      "Epoch 4099, Loss Train(MSE): 0.12537027850726587, R2 Train: 0.4985188859709365\n",
      "Epoch 4100, Loss Train(MSE): 0.12536989459229725, R2 Train: 0.498520421630811\n",
      "Epoch 4101, Loss Train(MSE): 0.12536949886421528, R2 Train: 0.4985220045431389\n",
      "Epoch 4102, Loss Train(MSE): 0.12536909523114814, R2 Train: 0.49852361907540743\n",
      "Epoch 4103, Loss Train(MSE): 0.12536872377637523, R2 Train: 0.4985251048944991\n",
      "Epoch 4104, Loss Train(MSE): 0.12536831230931172, R2 Train: 0.4985267507627531\n",
      "Epoch 4105, Loss Train(MSE): 0.12536794120633415, R2 Train: 0.4985282351746634\n",
      "Epoch 4106, Loss Train(MSE): 0.1253675442469158, R2 Train: 0.49852982301233684\n",
      "Epoch 4107, Loss Train(MSE): 0.12536715269564613, R2 Train: 0.49853138921741547\n",
      "Epoch 4108, Loss Train(MSE): 0.12536678061433884, R2 Train: 0.49853287754264464\n",
      "Epoch 4109, Loss Train(MSE): 0.12536637550215624, R2 Train: 0.49853449799137506\n",
      "Epoch 4110, Loss Train(MSE): 0.12536601438559455, R2 Train: 0.4985359424576218\n",
      "Epoch 4111, Loss Train(MSE): 0.12536561870178373, R2 Train: 0.4985375251928651\n",
      "Epoch 4112, Loss Train(MSE): 0.12536523645027067, R2 Train: 0.4985390541989173\n",
      "Epoch 4113, Loss Train(MSE): 0.1253648662108212, R2 Train: 0.49854053515671515\n",
      "Epoch 4114, Loss Train(MSE): 0.1253644672748841, R2 Train: 0.4985421309004636\n",
      "Epoch 4115, Loss Train(MSE): 0.12536411345399653, R2 Train: 0.49854354618401386\n",
      "Epoch 4116, Loss Train(MSE): 0.12536372143052812, R2 Train: 0.4985451142778875\n",
      "Epoch 4117, Loss Train(MSE): 0.1253633458293155, R2 Train: 0.49854661668273803\n",
      "Epoch 4118, Loss Train(MSE): 0.12536297977990402, R2 Train: 0.49854808088038394\n",
      "Epoch 4119, Loss Train(MSE): 0.12536258684868348, R2 Train: 0.49854965260526607\n",
      "Epoch 4120, Loss Train(MSE): 0.1253622377610208, R2 Train: 0.49855104895591684\n",
      "Epoch 4121, Loss Train(MSE): 0.12536185166632502, R2 Train: 0.4985525933346999\n",
      "Epoch 4122, Loss Train(MSE): 0.12536148019212734, R2 Train: 0.49855407923149064\n",
      "Epoch 4123, Loss Train(MSE): 0.12536112056651377, R2 Train: 0.4985555177339449\n",
      "Epoch 4124, Loss Train(MSE): 0.12536073347523075, R2 Train: 0.498557066099077\n",
      "Epoch 4125, Loss Train(MSE): 0.1253603866804532, R2 Train: 0.49855845327818715\n",
      "Epoch 4126, Loss Train(MSE): 0.12536000867223993, R2 Train: 0.4985599653110403\n",
      "Epoch 4127, Loss Train(MSE): 0.12535963892187804, R2 Train: 0.49856144431248783\n",
      "Epoch 4128, Loss Train(MSE): 0.1253592878448823, R2 Train: 0.49856284862047084\n",
      "Epoch 4129, Loss Train(MSE): 0.12535890643517178, R2 Train: 0.4985643742593129\n",
      "Epoch 4130, Loss Train(MSE): 0.12535855960920142, R2 Train: 0.4985657615631943\n",
      "Epoch 4131, Loss Train(MSE): 0.12535819173974477, R2 Train: 0.4985672330410209\n",
      "Epoch 4132, Loss Train(MSE): 0.1253578214244093, R2 Train: 0.4985687143023628\n",
      "Epoch 4133, Loss Train(MSE): 0.12535748091709767, R2 Train: 0.4985700763316093\n",
      "Epoch 4134, Loss Train(MSE): 0.12535710503669278, R2 Train: 0.49857157985322886\n",
      "Epoch 4135, Loss Train(MSE): 0.1253567559661808, R2 Train: 0.49857297613527685\n",
      "Epoch 4136, Loss Train(MSE): 0.12535640018732197, R2 Train: 0.4985743992507121\n",
      "Epoch 4137, Loss Train(MSE): 0.1253560276304219, R2 Train: 0.49857588947831244\n",
      "Epoch 4138, Loss Train(MSE): 0.1253556986792492, R2 Train: 0.4985772052830032\n",
      "Epoch 4139, Loss Train(MSE): 0.12535532865995844, R2 Train: 0.49857868536016625\n",
      "Epoch 4140, Loss Train(MSE): 0.12535497534265647, R2 Train: 0.49858009862937414\n",
      "Epoch 4141, Loss Train(MSE): 0.12535463340449807, R2 Train: 0.49858146638200773\n",
      "Epoch 4142, Loss Train(MSE): 0.12535426614300418, R2 Train: 0.49858293542798326\n",
      "Epoch 4143, Loss Train(MSE): 0.1253539313823045, R2 Train: 0.498584274470782\n",
      "Epoch 4144, Loss Train(MSE): 0.12535357661723395, R2 Train: 0.4985856935310642\n",
      "Epoch 4145, Loss Train(MSE): 0.12535321704207714, R2 Train: 0.49858713183169145\n",
      "Epoch 4146, Loss Train(MSE): 0.12535294859217186, R2 Train: 0.49858820563131256\n",
      "Epoch 4147, Loss Train(MSE): 0.1253527572518253, R2 Train: 0.49858897099269883\n",
      "Epoch 4148, Loss Train(MSE): 0.1253525847216048, R2 Train: 0.49858966111358083\n",
      "Epoch 4149, Loss Train(MSE): 0.12535241623154877, R2 Train: 0.4985903350738049\n",
      "Epoch 4150, Loss Train(MSE): 0.12535222561932155, R2 Train: 0.4985910975227138\n",
      "Epoch 4151, Loss Train(MSE): 0.12535206743633645, R2 Train: 0.4985917302546542\n",
      "Epoch 4152, Loss Train(MSE): 0.12535188577558987, R2 Train: 0.49859245689764053\n",
      "Epoch 4153, Loss Train(MSE): 0.12535170089914136, R2 Train: 0.4985931964034346\n",
      "Epoch 4154, Loss Train(MSE): 0.1253515466541992, R2 Train: 0.4985938133832032\n",
      "Epoch 4155, Loss Train(MSE): 0.12535135718122487, R2 Train: 0.49859457127510054\n",
      "Epoch 4156, Loss Train(MSE): 0.12535118618630448, R2 Train: 0.4985952552547821\n",
      "Epoch 4157, Loss Train(MSE): 0.1253510192202242, R2 Train: 0.49859592311910317\n",
      "Epoch 4158, Loss Train(MSE): 0.12535083045770667, R2 Train: 0.4985966781691733\n",
      "Epoch 4159, Loss Train(MSE): 0.12535067298611627, R2 Train: 0.4985973080555349\n",
      "Epoch 4160, Loss Train(MSE): 0.12535049364691556, R2 Train: 0.4985980254123378\n",
      "Epoch 4161, Loss Train(MSE): 0.12535030932178556, R2 Train: 0.49859876271285775\n",
      "Epoch 4162, Loss Train(MSE): 0.1253501575431235, R2 Train: 0.49859936982750597\n",
      "Epoch 4163, Loss Train(MSE): 0.12534996989265038, R2 Train: 0.4986001204293985\n",
      "Epoch 4164, Loss Train(MSE): 0.1253497986487718, R2 Train: 0.4986008054049128\n",
      "Epoch 4165, Loss Train(MSE): 0.12534963492368018, R2 Train: 0.4986014603052793\n",
      "Epoch 4166, Loss Train(MSE): 0.1253494479668252, R2 Train: 0.4986022081326992\n",
      "Epoch 4167, Loss Train(MSE): 0.12534928946139534, R2 Train: 0.49860284215441864\n",
      "Epoch 4168, Loss Train(MSE): 0.12534911412296998, R2 Train: 0.4986035435081201\n",
      "Epoch 4169, Loss Train(MSE): 0.12534892862091943, R2 Train: 0.4986042855163223\n",
      "Epoch 4170, Loss Train(MSE): 0.12534878097149532, R2 Train: 0.4986048761140187\n",
      "Epoch 4171, Loss Train(MSE): 0.12534859510074264, R2 Train: 0.4986056195970294\n",
      "Epoch 4172, Loss Train(MSE): 0.12534842191748385, R2 Train: 0.4986063123300646\n",
      "Epoch 4173, Loss Train(MSE): 0.1253482630597108, R2 Train: 0.49860694776115677\n",
      "Epoch 4174, Loss Train(MSE): 0.12534807786650862, R2 Train: 0.49860768853396553\n",
      "Epoch 4175, Loss Train(MSE): 0.12534791667387735, R2 Train: 0.4986083333044906\n",
      "Epoch 4176, Loss Train(MSE): 0.125347746926655, R2 Train: 0.49860901229337995\n",
      "Epoch 4177, Loss Train(MSE): 0.12534756240484363, R2 Train: 0.4986097503806255\n",
      "Epoch 4178, Loss Train(MSE): 0.12534741288051757, R2 Train: 0.49861034847792973\n",
      "Epoch 4179, Loss Train(MSE): 0.12534723255707209, R2 Train: 0.49861106977171166\n",
      "Epoch 4180, Loss Train(MSE): 0.12534705583663516, R2 Train: 0.49861177665345935\n",
      "Epoch 4181, Loss Train(MSE): 0.12534931693039933, R2 Train: 0.4986027322784027\n",
      "Epoch 4182, Loss Train(MSE): 0.1253527357630673, R2 Train: 0.4985890569477308\n",
      "Epoch 4183, Loss Train(MSE): 0.12535251851604304, R2 Train: 0.49858992593582785\n",
      "Epoch 4184, Loss Train(MSE): 0.1253523015183179, R2 Train: 0.49859079392672845\n",
      "Epoch 4185, Loss Train(MSE): 0.12535208476947185, R2 Train: 0.4985916609221126\n",
      "Epoch 4186, Loss Train(MSE): 0.12535186826908604, R2 Train: 0.49859252692365585\n",
      "Epoch 4187, Loss Train(MSE): 0.12535165201674242, R2 Train: 0.4985933919330303\n",
      "Epoch 4188, Loss Train(MSE): 0.12535143601202395, R2 Train: 0.4985942559519042\n",
      "Epoch 4189, Loss Train(MSE): 0.1253512202545145, R2 Train: 0.498595118981942\n",
      "Epoch 4190, Loss Train(MSE): 0.125351004743799, R2 Train: 0.49859598102480396\n",
      "Epoch 4191, Loss Train(MSE): 0.1253507894794632, R2 Train: 0.49859684208214716\n",
      "Epoch 4192, Loss Train(MSE): 0.12535057446109385, R2 Train: 0.4985977021556246\n",
      "Epoch 4193, Loss Train(MSE): 0.12535035968827857, R2 Train: 0.49859856124688573\n",
      "Epoch 4194, Loss Train(MSE): 0.125350145160606, R2 Train: 0.498599419357576\n",
      "Epoch 4195, Loss Train(MSE): 0.12534993087766566, R2 Train: 0.49860027648933736\n",
      "Epoch 4196, Loss Train(MSE): 0.12534971683904803, R2 Train: 0.4986011326438079\n",
      "Epoch 4197, Loss Train(MSE): 0.12534950304434447, R2 Train: 0.4986019878226221\n",
      "Epoch 4198, Loss Train(MSE): 0.12534928949314736, R2 Train: 0.49860284202741056\n",
      "Epoch 4199, Loss Train(MSE): 0.12534907618504984, R2 Train: 0.49860369525980064\n",
      "Epoch 4200, Loss Train(MSE): 0.1253488631196462, R2 Train: 0.49860454752141525\n",
      "Epoch 4201, Loss Train(MSE): 0.12534865029653133, R2 Train: 0.49860539881387467\n",
      "Epoch 4202, Loss Train(MSE): 0.1253484377153013, R2 Train: 0.4986062491387948\n",
      "Epoch 4203, Loss Train(MSE): 0.125348225375553, R2 Train: 0.49860709849778795\n",
      "Epoch 4204, Loss Train(MSE): 0.1253480132768842, R2 Train: 0.49860794689246324\n",
      "Epoch 4205, Loss Train(MSE): 0.12534780141889362, R2 Train: 0.4986087943244255\n",
      "Epoch 4206, Loss Train(MSE): 0.12534758980118083, R2 Train: 0.4986096407952767\n",
      "Epoch 4207, Loss Train(MSE): 0.12534737842334634, R2 Train: 0.49861048630661464\n",
      "Epoch 4208, Loss Train(MSE): 0.1253471672849915, R2 Train: 0.49861133086003395\n",
      "Epoch 4209, Loss Train(MSE): 0.12534695638571866, R2 Train: 0.49861217445712536\n",
      "Epoch 4210, Loss Train(MSE): 0.12534674572513094, R2 Train: 0.49861301709947625\n",
      "Epoch 4211, Loss Train(MSE): 0.12534653530283238, R2 Train: 0.4986138587886705\n",
      "Epoch 4212, Loss Train(MSE): 0.12534632511842797, R2 Train: 0.4986146995262881\n",
      "Epoch 4213, Loss Train(MSE): 0.12534611517152353, R2 Train: 0.4986155393139059\n",
      "Epoch 4214, Loss Train(MSE): 0.12534590546172572, R2 Train: 0.4986163781530971\n",
      "Epoch 4215, Loss Train(MSE): 0.1253456959886422, R2 Train: 0.4986172160454312\n",
      "Epoch 4216, Loss Train(MSE): 0.12534548675188137, R2 Train: 0.4986180529924745\n",
      "Epoch 4217, Loss Train(MSE): 0.12534527775105253, R2 Train: 0.4986188889957899\n",
      "Epoch 4218, Loss Train(MSE): 0.12534506898576586, R2 Train: 0.49861972405693655\n",
      "Epoch 4219, Loss Train(MSE): 0.12534486045563253, R2 Train: 0.49862055817746986\n",
      "Epoch 4220, Loss Train(MSE): 0.1253446521602644, R2 Train: 0.49862139135894235\n",
      "Epoch 4221, Loss Train(MSE): 0.1253444440992742, R2 Train: 0.4986222236029032\n",
      "Epoch 4222, Loss Train(MSE): 0.12534423627227567, R2 Train: 0.4986230549108973\n",
      "Epoch 4223, Loss Train(MSE): 0.12534402867888322, R2 Train: 0.4986238852844671\n",
      "Epoch 4224, Loss Train(MSE): 0.12534382131871227, R2 Train: 0.4986247147251509\n",
      "Epoch 4225, Loss Train(MSE): 0.12534361419137893, R2 Train: 0.49862554323448427\n",
      "Epoch 4226, Loss Train(MSE): 0.1253434072965004, R2 Train: 0.4986263708139984\n",
      "Epoch 4227, Loss Train(MSE): 0.12534320063369447, R2 Train: 0.4986271974652221\n",
      "Epoch 4228, Loss Train(MSE): 0.12534299420257983, R2 Train: 0.4986280231896807\n",
      "Epoch 4229, Loss Train(MSE): 0.12534278800277618, R2 Train: 0.4986288479888953\n",
      "Epoch 4230, Loss Train(MSE): 0.1253425820339039, R2 Train: 0.49862967186438445\n",
      "Epoch 4231, Loss Train(MSE): 0.12534237629558417, R2 Train: 0.4986304948176633\n",
      "Epoch 4232, Loss Train(MSE): 0.1253421707874392, R2 Train: 0.4986313168502432\n",
      "Epoch 4233, Loss Train(MSE): 0.12534196550909182, R2 Train: 0.4986321379636327\n",
      "Epoch 4234, Loss Train(MSE): 0.1253417604601658, R2 Train: 0.49863295815933684\n",
      "Epoch 4235, Loss Train(MSE): 0.12534155564028565, R2 Train: 0.4986337774388574\n",
      "Epoch 4236, Loss Train(MSE): 0.12534135104907693, R2 Train: 0.4986345958036923\n",
      "Epoch 4237, Loss Train(MSE): 0.12534114668616567, R2 Train: 0.49863541325533733\n",
      "Epoch 4238, Loss Train(MSE): 0.12534094255117909, R2 Train: 0.49863622979528366\n",
      "Epoch 4239, Loss Train(MSE): 0.12534073864374487, R2 Train: 0.4986370454250205\n",
      "Epoch 4240, Loss Train(MSE): 0.12534053496349176, R2 Train: 0.49863786014603295\n",
      "Epoch 4241, Loss Train(MSE): 0.1253403315100492, R2 Train: 0.4986386739598032\n",
      "Epoch 4242, Loss Train(MSE): 0.1253401282830476, R2 Train: 0.49863948686780957\n",
      "Epoch 4243, Loss Train(MSE): 0.1253399252821179, R2 Train: 0.4986402988715284\n",
      "Epoch 4244, Loss Train(MSE): 0.12533972250689204, R2 Train: 0.49864110997243183\n",
      "Epoch 4245, Loss Train(MSE): 0.1253395199570028, R2 Train: 0.49864192017198883\n",
      "Epoch 4246, Loss Train(MSE): 0.12533931763208359, R2 Train: 0.49864272947166566\n",
      "Epoch 4247, Loss Train(MSE): 0.12533911553176877, R2 Train: 0.4986435378729249\n",
      "Epoch 4248, Loss Train(MSE): 0.12533891365569336, R2 Train: 0.49864434537722657\n",
      "Epoch 4249, Loss Train(MSE): 0.12533871200349334, R2 Train: 0.49864515198602666\n",
      "Epoch 4250, Loss Train(MSE): 0.12533851057480536, R2 Train: 0.4986459577007786\n",
      "Epoch 4251, Loss Train(MSE): 0.12533830936926682, R2 Train: 0.4986467625229327\n",
      "Epoch 4252, Loss Train(MSE): 0.12533810838651613, R2 Train: 0.4986475664539355\n",
      "Epoch 4253, Loss Train(MSE): 0.1253379076261921, R2 Train: 0.4986483694952316\n",
      "Epoch 4254, Loss Train(MSE): 0.12533770708793476, R2 Train: 0.49864917164826095\n",
      "Epoch 4255, Loss Train(MSE): 0.1253375067713846, R2 Train: 0.49864997291446156\n",
      "Epoch 4256, Loss Train(MSE): 0.12533730667618304, R2 Train: 0.49865077329526786\n",
      "Epoch 4257, Loss Train(MSE): 0.1253371068019722, R2 Train: 0.49865157279211125\n",
      "Epoch 4258, Loss Train(MSE): 0.125336907148395, R2 Train: 0.49865237140642005\n",
      "Epoch 4259, Loss Train(MSE): 0.12533670771509514, R2 Train: 0.49865316913961943\n",
      "Epoch 4260, Loss Train(MSE): 0.12533650850171715, R2 Train: 0.4986539659931314\n",
      "Epoch 4261, Loss Train(MSE): 0.1253363095079062, R2 Train: 0.4986547619683752\n",
      "Epoch 4262, Loss Train(MSE): 0.12533611073330833, R2 Train: 0.4986555570667667\n",
      "Epoch 4263, Loss Train(MSE): 0.12533591217757017, R2 Train: 0.4986563512897193\n",
      "Epoch 4264, Loss Train(MSE): 0.12533571384033942, R2 Train: 0.4986571446386423\n",
      "Epoch 4265, Loss Train(MSE): 0.1253355157212642, R2 Train: 0.4986579371149432\n",
      "Epoch 4266, Loss Train(MSE): 0.12533531781999369, R2 Train: 0.49865872872002526\n",
      "Epoch 4267, Loss Train(MSE): 0.12533512013617754, R2 Train: 0.49865951945528986\n",
      "Epoch 4268, Loss Train(MSE): 0.1253349226694664, R2 Train: 0.49866030932213445\n",
      "Epoch 4269, Loss Train(MSE): 0.12533472541951138, R2 Train: 0.4986610983219545\n",
      "Epoch 4270, Loss Train(MSE): 0.12533452838596468, R2 Train: 0.4986618864561413\n",
      "Epoch 4271, Loss Train(MSE): 0.125334331568479, R2 Train: 0.49866267372608397\n",
      "Epoch 4272, Loss Train(MSE): 0.12533413496670792, R2 Train: 0.4986634601331683\n",
      "Epoch 4273, Loss Train(MSE): 0.12533393858030561, R2 Train: 0.49866424567877754\n",
      "Epoch 4274, Loss Train(MSE): 0.12533374240892717, R2 Train: 0.49866503036429133\n",
      "Epoch 4275, Loss Train(MSE): 0.12533354645222827, R2 Train: 0.4986658141910869\n",
      "Epoch 4276, Loss Train(MSE): 0.12533335727310974, R2 Train: 0.49866657090756106\n",
      "Epoch 4277, Loss Train(MSE): 0.12533321633543631, R2 Train: 0.49866713465825474\n",
      "Epoch 4278, Loss Train(MSE): 0.12533305669106432, R2 Train: 0.49866777323574274\n",
      "Epoch 4279, Loss Train(MSE): 0.1253328906780247, R2 Train: 0.4986684372879012\n",
      "Epoch 4280, Loss Train(MSE): 0.12533275279618072, R2 Train: 0.4986689888152771\n",
      "Epoch 4281, Loss Train(MSE): 0.12533259095551766, R2 Train: 0.49866963617792937\n",
      "Epoch 4282, Loss Train(MSE): 0.12533242545124326, R2 Train: 0.49867029819502695\n",
      "Epoch 4283, Loss Train(MSE): 0.12533229045977207, R2 Train: 0.49867083816091173\n",
      "Epoch 4284, Loss Train(MSE): 0.1253321265831998, R2 Train: 0.49867149366720076\n",
      "Epoch 4285, Loss Train(MSE): 0.12533196265217975, R2 Train: 0.498672149391281\n",
      "Epoch 4286, Loss Train(MSE): 0.12533182825143968, R2 Train: 0.4986726869942413\n",
      "Epoch 4287, Loss Train(MSE): 0.12533166354897982, R2 Train: 0.49867334580408074\n",
      "Epoch 4288, Loss Train(MSE): 0.12533150234697443, R2 Train: 0.49867399061210227\n",
      "Epoch 4289, Loss Train(MSE): 0.1253313660639667, R2 Train: 0.49867453574413323\n",
      "Epoch 4290, Loss Train(MSE): 0.1253312018619126, R2 Train: 0.4986751925523496\n",
      "Epoch 4291, Loss Train(MSE): 0.12533104323003733, R2 Train: 0.4986758270798507\n",
      "Epoch 4292, Loss Train(MSE): 0.12533090521873283, R2 Train: 0.4986763791250687\n",
      "Epoch 4293, Loss Train(MSE): 0.1253307415138269, R2 Train: 0.4986770339446924\n",
      "Epoch 4294, Loss Train(MSE): 0.12533058529596086, R2 Train: 0.49867765881615655\n",
      "Epoch 4295, Loss Train(MSE): 0.12533044570762994, R2 Train: 0.49867821716948024\n",
      "Epoch 4296, Loss Train(MSE): 0.1253302824966578, R2 Train: 0.49867887001336875\n",
      "Epoch 4297, Loss Train(MSE): 0.1253301285393959, R2 Train: 0.4986794858424164\n",
      "Epoch 4298, Loss Train(MSE): 0.1253299875226549, R2 Train: 0.4986800499093804\n",
      "Epoch 4299, Loss Train(MSE): 0.12532982480244484, R2 Train: 0.49868070079022064\n",
      "Epoch 4300, Loss Train(MSE): 0.12532967295505076, R2 Train: 0.49868130817979694\n",
      "Epoch 4301, Loss Train(MSE): 0.12532953065590766, R2 Train: 0.4986818773763694\n",
      "Epoch 4302, Loss Train(MSE): 0.1253293684233295, R2 Train: 0.498682526306682\n",
      "Epoch 4303, Loss Train(MSE): 0.12532921853768955, R2 Train: 0.4986831258492418\n",
      "Epoch 4304, Loss Train(MSE): 0.12532907509958888, R2 Train: 0.4986836996016445\n",
      "Epoch 4305, Loss Train(MSE): 0.12532891335155327, R2 Train: 0.4986843465937869\n",
      "Epoch 4306, Loss Train(MSE): 0.1253287652821313, R2 Train: 0.4986849388714748\n",
      "Epoch 4307, Loss Train(MSE): 0.12532862084599816, R2 Train: 0.49868551661600735\n",
      "Epoch 4308, Loss Train(MSE): 0.12532845957945563, R2 Train: 0.4986861616821775\n",
      "Epoch 4309, Loss Train(MSE): 0.12532831318324855, R2 Train: 0.4986867472670058\n",
      "Epoch 4310, Loss Train(MSE): 0.12532816788753162, R2 Train: 0.4986873284498735\n",
      "Epoch 4311, Loss Train(MSE): 0.125328007099472, R2 Train: 0.498687971602112\n",
      "Epoch 4312, Loss Train(MSE): 0.12532786223596623, R2 Train: 0.4986885510561351\n",
      "Epoch 4313, Loss Train(MSE): 0.12532771621668035, R2 Train: 0.4986891351332786\n",
      "Epoch 4314, Loss Train(MSE): 0.1253275559041317, R2 Train: 0.4986897763834732\n",
      "Epoch 4315, Loss Train(MSE): 0.12532741243526074, R2 Train: 0.498690350258957\n",
      "Epoch 4316, Loss Train(MSE): 0.1253272658260281, R2 Train: 0.4986909366958876\n",
      "Epoch 4317, Loss Train(MSE): 0.12532710598605618, R2 Train: 0.4986915760557753\n",
      "Epoch 4318, Loss Train(MSE): 0.12532696377615876, R2 Train: 0.49869214489536495\n",
      "Epoch 4319, Loss Train(MSE): 0.1253268167082498, R2 Train: 0.4986927331670008\n",
      "Epoch 4320, Loss Train(MSE): 0.12532665733795725, R2 Train: 0.498693370648171\n",
      "Epoch 4321, Loss Train(MSE): 0.12532651625373603, R2 Train: 0.4986939349850559\n",
      "Epoch 4322, Loss Train(MSE): 0.1253263688561094, R2 Train: 0.4986945245755624\n",
      "Epoch 4323, Loss Train(MSE): 0.12532620995263505, R2 Train: 0.4986951601894598\n",
      "Epoch 4324, Loss Train(MSE): 0.1253260698631166, R2 Train: 0.49869572054753364\n",
      "Epoch 4325, Loss Train(MSE): 0.1253259222624582, R2 Train: 0.4986963109501672\n",
      "Epoch 4326, Loss Train(MSE): 0.12532576382297642, R2 Train: 0.49869694470809434\n",
      "Epoch 4327, Loss Train(MSE): 0.1253256245994717, R2 Train: 0.49869750160211324\n",
      "Epoch 4328, Loss Train(MSE): 0.12532547692023335, R2 Train: 0.4986980923190666\n",
      "Epoch 4329, Loss Train(MSE): 0.12532531894195312, R2 Train: 0.49869872423218753\n",
      "Epoch 4330, Loss Train(MSE): 0.12532518045801871, R2 Train: 0.49869927816792514\n",
      "Epoch 4331, Loss Train(MSE): 0.12532503282245586, R2 Train: 0.4986998687101766\n",
      "Epoch 4332, Loss Train(MSE): 0.12532487530262038, R2 Train: 0.4987004987895185\n",
      "Epoch 4333, Loss Train(MSE): 0.12532473743402053, R2 Train: 0.49870105026391787\n",
      "Epoch 4334, Loss Train(MSE): 0.12532458996222898, R2 Train: 0.4987016401510841\n",
      "Epoch 4335, Loss Train(MSE): 0.1253244328981149, R2 Train: 0.4987022684075404\n",
      "Epoch 4336, Loss Train(MSE): 0.1253242955227842, R2 Train: 0.49870281790886317\n",
      "Epoch 4337, Loss Train(MSE): 0.12532414833273686, R2 Train: 0.49870340666905255\n",
      "Epoch 4338, Loss Train(MSE): 0.12532399172165368, R2 Train: 0.4987040331133853\n",
      "Epoch 4339, Loss Train(MSE): 0.12532385471966045, R2 Train: 0.4987045811213582\n",
      "Epoch 4340, Loss Train(MSE): 0.12533709232148427, R2 Train: 0.49865163071406293\n",
      "Epoch 4341, Loss Train(MSE): 0.12535232168317284, R2 Train: 0.49859071326730864\n",
      "Epoch 4342, Loss Train(MSE): 0.1253519689101865, R2 Train: 0.49859212435925404\n",
      "Epoch 4343, Loss Train(MSE): 0.1253515871853325, R2 Train: 0.49859365125866995\n",
      "Epoch 4344, Loss Train(MSE): 0.12535122108936542, R2 Train: 0.49859511564253833\n",
      "Epoch 4345, Loss Train(MSE): 0.125350857025551, R2 Train: 0.49859657189779605\n",
      "Epoch 4346, Loss Train(MSE): 0.1253504773780061, R2 Train: 0.49859809048797565\n",
      "Epoch 4347, Loss Train(MSE): 0.12535013115638957, R2 Train: 0.4985994753744417\n",
      "Epoch 4348, Loss Train(MSE): 0.12534974434366067, R2 Train: 0.4986010226253573\n",
      "Epoch 4349, Loss Train(MSE): 0.12534940299942968, R2 Train: 0.4986023880022813\n",
      "Epoch 4350, Loss Train(MSE): 0.12534902509598422, R2 Train: 0.4986038996160631\n",
      "Epoch 4351, Loss Train(MSE): 0.1253486694001939, R2 Train: 0.4986053223992244\n",
      "Epoch 4352, Loss Train(MSE): 0.1253483100200907, R2 Train: 0.4986067599196372\n",
      "Epoch 4353, Loss Train(MSE): 0.12534793976462974, R2 Train: 0.49860824094148104\n",
      "Epoch 4354, Loss Train(MSE): 0.1253475990711665, R2 Train: 0.498609603715334\n",
      "Epoch 4355, Loss Train(MSE): 0.1253472205444908, R2 Train: 0.4986111178220368\n",
      "Epoch 4356, Loss Train(MSE): 0.12534688578872016, R2 Train: 0.49861245684511935\n",
      "Epoch 4357, Loss Train(MSE): 0.1253465159650461, R2 Train: 0.49861393613981564\n",
      "Epoch 4358, Loss Train(MSE): 0.12534616590787592, R2 Train: 0.4986153363684963\n",
      "Epoch 4359, Loss Train(MSE): 0.12534581540024392, R2 Train: 0.4986167383990243\n",
      "Epoch 4360, Loss Train(MSE): 0.12534544985257315, R2 Train: 0.4986182005897074\n",
      "Epoch 4361, Loss Train(MSE): 0.12534511880771168, R2 Train: 0.4986195247691533\n",
      "Epoch 4362, Loss Train(MSE): 0.12534474824668088, R2 Train: 0.4986210070132765\n",
      "Epoch 4363, Loss Train(MSE): 0.12534441555676115, R2 Train: 0.4986223377729554\n",
      "Epoch 4364, Loss Train(MSE): 0.1253440577853279, R2 Train: 0.4986237688586884\n",
      "Epoch 4365, Loss Train(MSE): 0.12534370891744914, R2 Train: 0.49862516433020343\n",
      "Epoch 4366, Loss Train(MSE): 0.12534337118997643, R2 Train: 0.49862651524009427\n",
      "Epoch 4367, Loss Train(MSE): 0.12534300618846253, R2 Train: 0.4986279752461499\n",
      "Epoch 4368, Loss Train(MSE): 0.12534268827562473, R2 Train: 0.4986292468975011\n",
      "Epoch 4369, Loss Train(MSE): 0.12534232556043265, R2 Train: 0.4986306977582694\n",
      "Epoch 4370, Loss Train(MSE): 0.1253419908200796, R2 Train: 0.4986320367196816\n",
      "Epoch 4371, Loss Train(MSE): 0.12534164869624395, R2 Train: 0.4986334052150242\n",
      "Epoch 4372, Loss Train(MSE): 0.12534129696927618, R2 Train: 0.4986348121228953\n",
      "Epoch 4373, Loss Train(MSE): 0.12534097555733675, R2 Train: 0.498636097770653\n",
      "Epoch 4374, Loss Train(MSE): 0.12534061800991325, R2 Train: 0.498637527960347\n",
      "Epoch 4375, Loss Train(MSE): 0.12534029485303708, R2 Train: 0.4986388205878517\n",
      "Epoch 4376, Loss Train(MSE): 0.12533995062144898, R2 Train: 0.4986401975142041\n",
      "Epoch 4377, Loss Train(MSE): 0.12533960987647053, R2 Train: 0.4986415604941179\n",
      "Epoch 4378, Loss Train(MSE): 0.12533928686147092, R2 Train: 0.49864285255411633\n",
      "Epoch 4379, Loss Train(MSE): 0.12533893452140918, R2 Train: 0.4986442619143633\n",
      "Epoch 4380, Loss Train(MSE): 0.12533862062372497, R2 Train: 0.4986455175051001\n",
      "Epoch 4381, Loss Train(MSE): 0.12533827636254666, R2 Train: 0.49864689454981337\n",
      "Epoch 4382, Loss Train(MSE): 0.12533794430891937, R2 Train: 0.4986482227643225\n",
      "Epoch 4383, Loss Train(MSE): 0.12533762173901877, R2 Train: 0.4986495130439249\n",
      "Epoch 4384, Loss Train(MSE): 0.12533727446811688, R2 Train: 0.4986509021275325\n",
      "Epoch 4385, Loss Train(MSE): 0.12533696760959348, R2 Train: 0.4986521295616261\n",
      "Epoch 4386, Loss Train(MSE): 0.1253366253019711, R2 Train: 0.4986534987921156\n",
      "Epoch 4387, Loss Train(MSE): 0.1253362997516729, R2 Train: 0.4986548009933084\n",
      "Epoch 4388, Loss Train(MSE): 0.1253359795814765, R2 Train: 0.49865608167409403\n",
      "Epoch 4389, Loss Train(MSE): 0.12533563724673877, R2 Train: 0.49865745101304493\n",
      "Epoch 4390, Loss Train(MSE): 0.1253353353068311, R2 Train: 0.49865865877267557\n",
      "Epoch 4391, Loss Train(MSE): 0.12533499684521862, R2 Train: 0.4986600126191255\n",
      "Epoch 4392, Loss Train(MSE): 0.12533467570816292, R2 Train: 0.4986612971673483\n",
      "Epoch 4393, Loss Train(MSE): 0.12533435980296873, R2 Train: 0.4986625607881251\n",
      "Epoch 4394, Loss Train(MSE): 0.1253340222763571, R2 Train: 0.4986639108945716\n",
      "Epoch 4395, Loss Train(MSE): 0.1253337232294849, R2 Train: 0.49866510708206035\n",
      "Epoch 4396, Loss Train(MSE): 0.12533339041974725, R2 Train: 0.498666438321011\n",
      "Epoch 4397, Loss Train(MSE): 0.12533307169934377, R2 Train: 0.49866771320262493\n",
      "Epoch 4398, Loss Train(MSE): 0.12533276183917316, R2 Train: 0.49866895264330735\n",
      "Epoch 4399, Loss Train(MSE): 0.12533242899737246, R2 Train: 0.49867028401051017\n",
      "Epoch 4400, Loss Train(MSE): 0.1253321309086309, R2 Train: 0.4986714763654764\n",
      "Epoch 4401, Loss Train(MSE): 0.1253318054739388, R2 Train: 0.4986727781042448\n",
      "Epoch 4402, Loss Train(MSE): 0.12533148726288107, R2 Train: 0.4986740509484757\n",
      "Epoch 4403, Loss Train(MSE): 0.1253311851463056, R2 Train: 0.49867525941477764\n",
      "Epoch 4404, Loss Train(MSE): 0.12533085687050208, R2 Train: 0.4986765725179917\n",
      "Epoch 4405, Loss Train(MSE): 0.12533055789159064, R2 Train: 0.4986777684336374\n",
      "Epoch 4406, Loss Train(MSE): 0.12533024147611949, R2 Train: 0.49867903409552206\n",
      "Epoch 4407, Loss Train(MSE): 0.12532992195238662, R2 Train: 0.4986803121904535\n",
      "Epoch 4408, Loss Train(MSE): 0.12532962920016216, R2 Train: 0.49868148319935135\n",
      "Epoch 4409, Loss Train(MSE): 0.12532930537583464, R2 Train: 0.4986827784966614\n",
      "Epoch 4410, Loss Train(MSE): 0.12532900374119169, R2 Train: 0.49868398503523326\n",
      "Epoch 4411, Loss Train(MSE): 0.12532869791363555, R2 Train: 0.4986852083454578\n",
      "Epoch 4412, Loss Train(MSE): 0.1253283767687361, R2 Train: 0.4986864929250556\n",
      "Epoch 4413, Loss Train(MSE): 0.1253280921203713, R2 Train: 0.4986876315185148\n",
      "Epoch 4414, Loss Train(MSE): 0.12532777404908432, R2 Train: 0.4986889038036627\n",
      "Epoch 4415, Loss Train(MSE): 0.12532746815745616, R2 Train: 0.4986901273701754\n",
      "Epoch 4416, Loss Train(MSE): 0.12532717432879742, R2 Train: 0.4986913026848103\n",
      "Epoch 4417, Loss Train(MSE): 0.12532685745890113, R2 Train: 0.4986925701643955\n",
      "Epoch 4418, Loss Train(MSE): 0.125326567298257, R2 Train: 0.498693730806972\n",
      "Epoch 4419, Loss Train(MSE): 0.12532626236834923, R2 Train: 0.49869495052660306\n",
      "Epoch 4420, Loss Train(MSE): 0.12532595060598994, R2 Train: 0.4986961975760402\n",
      "Epoch 4421, Loss Train(MSE): 0.12532567020666002, R2 Train: 0.4986973191733599\n",
      "Epoch 4422, Loss Train(MSE): 0.12532535750756804, R2 Train: 0.49869856996972783\n",
      "Epoch 4423, Loss Train(MSE): 0.1253250602712362, R2 Train: 0.49869975891505525\n",
      "Epoch 4424, Loss Train(MSE): 0.12532476986669722, R2 Train: 0.49870092053321113\n",
      "Epoch 4425, Loss Train(MSE): 0.12532445967938471, R2 Train: 0.49870216128246114\n",
      "Epoch 4426, Loss Train(MSE): 0.12532417615332275, R2 Train: 0.498703295386709\n",
      "Epoch 4427, Loss Train(MSE): 0.1253239465892039, R2 Train: 0.4987042136431844\n",
      "Epoch 4428, Loss Train(MSE): 0.12532378586700726, R2 Train: 0.49870485653197094\n",
      "Epoch 4429, Loss Train(MSE): 0.12532365575116822, R2 Train: 0.49870537699532713\n",
      "Epoch 4430, Loss Train(MSE): 0.1253234933959862, R2 Train: 0.4987060264160552\n",
      "Epoch 4431, Loss Train(MSE): 0.12532334370073567, R2 Train: 0.49870662519705733\n",
      "Epoch 4432, Loss Train(MSE): 0.12532320347622816, R2 Train: 0.49870718609508735\n",
      "Epoch 4433, Loss Train(MSE): 0.12532304168211728, R2 Train: 0.4987078332715309\n",
      "Epoch 4434, Loss Train(MSE): 0.12532290273886232, R2 Train: 0.4987083890445507\n",
      "Epoch 4435, Loss Train(MSE): 0.12532275267331364, R2 Train: 0.49870898930674545\n",
      "Epoch 4436, Loss Train(MSE): 0.12532259143540644, R2 Train: 0.4987096342583742\n",
      "Epoch 4437, Loss Train(MSE): 0.12532246297381192, R2 Train: 0.49871014810475234\n",
      "Epoch 4438, Loss Train(MSE): 0.12532230333036107, R2 Train: 0.49871078667855573\n",
      "Epoch 4439, Loss Train(MSE): 0.12532215125169502, R2 Train: 0.4987113949932199\n",
      "Epoch 4440, Loss Train(MSE): 0.12532201578214705, R2 Train: 0.4987119368714118\n",
      "Epoch 4441, Loss Train(MSE): 0.12532185541600405, R2 Train: 0.4987125783359838\n",
      "Epoch 4442, Loss Train(MSE): 0.12532171348979454, R2 Train: 0.49871314604082184\n",
      "Epoch 4443, Loss Train(MSE): 0.12532156876065517, R2 Train: 0.4987137249573793\n",
      "Epoch 4444, Loss Train(MSE): 0.125321408938785, R2 Train: 0.49871436424486004\n",
      "Epoch 4445, Loss Train(MSE): 0.12532127690551395, R2 Train: 0.4987148923779442\n",
      "Epoch 4446, Loss Train(MSE): 0.12532112316941013, R2 Train: 0.4987155073223595\n",
      "Epoch 4447, Loss Train(MSE): 0.12532096742481638, R2 Train: 0.4987161303007345\n",
      "Epoch 4448, Loss Train(MSE): 0.12532083794649576, R2 Train: 0.49871664821401696\n",
      "Epoch 4449, Loss Train(MSE): 0.12532067897798002, R2 Train: 0.4987172840880799\n",
      "Epoch 4450, Loss Train(MSE): 0.1253205328128521, R2 Train: 0.4987178687485916\n",
      "Epoch 4451, Loss Train(MSE): 0.1253203946306075, R2 Train: 0.49871842147756995\n",
      "Epoch 4452, Loss Train(MSE): 0.12532023619493332, R2 Train: 0.4987190552202667\n",
      "Epoch 4453, Loss Train(MSE): 0.12532009936007588, R2 Train: 0.4987196025596965\n",
      "Epoch 4454, Loss Train(MSE): 0.12531995271649773, R2 Train: 0.49872018913400906\n",
      "Epoch 4455, Loss Train(MSE): 0.12531979480926664, R2 Train: 0.49872082076293345\n",
      "Epoch 4456, Loss Train(MSE): 0.1253196670596291, R2 Train: 0.4987213317614836\n",
      "Epoch 4457, Loss Train(MSE): 0.12531951219327248, R2 Train: 0.4987219512269101\n",
      "Epoch 4458, Loss Train(MSE): 0.1253193605979798, R2 Train: 0.4987225576080808\n",
      "Epoch 4459, Loss Train(MSE): 0.1253192301100367, R2 Train: 0.4987230795598532\n",
      "Epoch 4460, Loss Train(MSE): 0.12531907303174558, R2 Train: 0.4987237078730177\n",
      "Epoch 4461, Loss Train(MSE): 0.12531893022921967, R2 Train: 0.49872427908312134\n",
      "Epoch 4462, Loss Train(MSE): 0.12531879180111677, R2 Train: 0.4987248327955329\n",
      "Epoch 4463, Loss Train(MSE): 0.12531863524048795, R2 Train: 0.4987254590380482\n",
      "Epoch 4464, Loss Train(MSE): 0.12531850099534939, R2 Train: 0.49872599601860246\n",
      "Epoch 4465, Loss Train(MSE): 0.12531835485623302, R2 Train: 0.4987265805750679\n",
      "Epoch 4466, Loss Train(MSE): 0.12531819880911244, R2 Train: 0.49872720476355026\n",
      "Epoch 4467, Loss Train(MSE): 0.1253180728898778, R2 Train: 0.49872770844048875\n",
      "Epoch 4468, Loss Train(MSE): 0.12531791926509891, R2 Train: 0.49872832293960434\n",
      "Epoch 4469, Loss Train(MSE): 0.12531776938667397, R2 Train: 0.49872892245330414\n",
      "Epoch 4470, Loss Train(MSE): 0.125317640240914, R2 Train: 0.49872943903634404\n",
      "Epoch 4471, Loss Train(MSE): 0.12531748499969547, R2 Train: 0.4987300600012181\n",
      "Epoch 4472, Loss Train(MSE): 0.12531734317438714, R2 Train: 0.49873062730245143\n",
      "Epoch 4473, Loss Train(MSE): 0.12531720680646508, R2 Train: 0.4987311727741397\n",
      "Epoch 4474, Loss Train(MSE): 0.12531705206857277, R2 Train: 0.4987317917257089\n",
      "Epoch 4475, Loss Train(MSE): 0.12531691807396414, R2 Train: 0.49873232770414344\n",
      "Epoch 4476, Loss Train(MSE): 0.1253167747004052, R2 Train: 0.4987329011983792\n",
      "Epoch 4477, Loss Train(MSE): 0.12531662046191042, R2 Train: 0.49873351815235833\n",
      "Epoch 4478, Loss Train(MSE): 0.12531649407924939, R2 Train: 0.49873402368300246\n",
      "Epoch 4479, Loss Train(MSE): 0.12531634391300667, R2 Train: 0.49873462434797333\n",
      "Epoch 4480, Loss Train(MSE): 0.12531619347692605, R2 Train: 0.4987352260922958\n",
      "Epoch 4481, Loss Train(MSE): 0.12531606787170915, R2 Train: 0.4987357285131634\n",
      "Epoch 4482, Loss Train(MSE): 0.12531591441734571, R2 Train: 0.49873634233061714\n",
      "Epoch 4483, Loss Train(MSE): 0.12531577133883534, R2 Train: 0.49873691464465864\n",
      "Epoch 4484, Loss Train(MSE): 0.12531563918652755, R2 Train: 0.4987374432538898\n",
      "Epoch 4485, Loss Train(MSE): 0.12531548622193164, R2 Train: 0.4987380551122734\n",
      "Epoch 4486, Loss Train(MSE): 0.12531535029074434, R2 Train: 0.49873859883702265\n",
      "Epoch 4487, Loss Train(MSE): 0.12531521179601535, R2 Train: 0.4987391528159386\n",
      "Epoch 4488, Loss Train(MSE): 0.12531505931746628, R2 Train: 0.4987397627301349\n",
      "Epoch 4489, Loss Train(MSE): 0.12531493032680455, R2 Train: 0.4987402786927818\n",
      "Epoch 4490, Loss Train(MSE): 0.1253147856909595, R2 Train: 0.498740857236162\n",
      "Epoch 4491, Loss Train(MSE): 0.12531463369479354, R2 Train: 0.49874146522082585\n",
      "Epoch 4492, Loss Train(MSE): 0.12531451144124992, R2 Train: 0.49874195423500034\n",
      "Epoch 4493, Loss Train(MSE): 0.12531436086228726, R2 Train: 0.498742556550851\n",
      "Epoch 4494, Loss Train(MSE): 0.12531421444580004, R2 Train: 0.49874314221679983\n",
      "Epoch 4495, Loss Train(MSE): 0.12531408852267528, R2 Train: 0.49874364590929887\n",
      "Epoch 4496, Loss Train(MSE): 0.12531393728439372, R2 Train: 0.49874425086242513\n",
      "Epoch 4497, Loss Train(MSE): 0.1253137973727885, R2 Train: 0.498744810508846\n",
      "Epoch 4498, Loss Train(MSE): 0.1253136657305852, R2 Train: 0.4987453370776592\n",
      "Epoch 4499, Loss Train(MSE): 0.12531351496570525, R2 Train: 0.498745940137179\n",
      "Epoch 4500, Loss Train(MSE): 0.12531338136341216, R2 Train: 0.49874647454635135\n",
      "Epoch 4501, Loss Train(MSE): 0.12531324419249046, R2 Train: 0.49874702323003817\n",
      "Epoch 4502, Loss Train(MSE): 0.12531309389753645, R2 Train: 0.4987476244098542\n",
      "Epoch 4503, Loss Train(MSE): 0.12531296641217968, R2 Train: 0.4987481343512813\n",
      "Epoch 4504, Loss Train(MSE): 0.12531282389978254, R2 Train: 0.4987487044008698\n",
      "Epoch 4505, Loss Train(MSE): 0.1253126740713303, R2 Train: 0.49874930371467885\n",
      "Epoch 4506, Loss Train(MSE): 0.12531255251367318, R2 Train: 0.4987497899453073\n",
      "Epoch 4507, Loss Train(MSE): 0.1253124048439798, R2 Train: 0.49875038062408084\n",
      "Epoch 4508, Loss Train(MSE): 0.12531225904132748, R2 Train: 0.4987509638346901\n",
      "Epoch 4509, Loss Train(MSE): 0.12531213609577085, R2 Train: 0.4987514556169166\n",
      "Epoch 4510, Loss Train(MSE): 0.1253119870006969, R2 Train: 0.4987520519972124\n",
      "Epoch 4511, Loss Train(MSE): 0.12531184691388775, R2 Train: 0.498752612344449\n",
      "Epoch 4512, Loss Train(MSE): 0.1253117190152227, R2 Train: 0.4987531239391092\n",
      "Epoch 4513, Loss Train(MSE): 0.12531157037825577, R2 Train: 0.4987537184869769\n",
      "Epoch 4514, Loss Train(MSE): 0.12531143582528215, R2 Train: 0.4987542566988714\n",
      "Epoch 4515, Loss Train(MSE): 0.12531130315063838, R2 Train: 0.4987547873974465\n",
      "Epoch 4516, Loss Train(MSE): 0.12531115496852482, R2 Train: 0.4987553801259007\n",
      "Epoch 4517, Loss Train(MSE): 0.12531102577033926, R2 Train: 0.49875589691864297\n",
      "Epoch 4518, Loss Train(MSE): 0.1253108884939558, R2 Train: 0.4987564460241768\n",
      "Epoch 4519, Loss Train(MSE): 0.12531074076348866, R2 Train: 0.49875703694604534\n",
      "Epoch 4520, Loss Train(MSE): 0.12531061674395386, R2 Train: 0.4987575330241846\n",
      "Epoch 4521, Loss Train(MSE): 0.12531047503722798, R2 Train: 0.4987580998510881\n",
      "Epoch 4522, Loss Train(MSE): 0.12531032775524614, R2 Train: 0.49875868897901543\n",
      "Epoch 4523, Loss Train(MSE): 0.12531020874108545, R2 Train: 0.4987591650356582\n",
      "Epoch 4524, Loss Train(MSE): 0.12531006277262005, R2 Train: 0.4987597489095198\n",
      "Epoch 4525, Loss Train(MSE): 0.12530991943593098, R2 Train: 0.4987603222562761\n",
      "Epoch 4526, Loss Train(MSE): 0.12530979825352956, R2 Train: 0.49876080698588177\n",
      "Epoch 4527, Loss Train(MSE): 0.12530965167712288, R2 Train: 0.4987613932915085\n",
      "Epoch 4528, Loss Train(MSE): 0.12530951315682826, R2 Train: 0.498761947372687\n",
      "Epoch 4529, Loss Train(MSE): 0.12530938789457471, R2 Train: 0.49876244842170114\n",
      "Epoch 4530, Loss Train(MSE): 0.1253092417589027, R2 Train: 0.4987630329643892\n",
      "Epoch 4531, Loss Train(MSE): 0.12530910788828037, R2 Train: 0.4987635684468785\n",
      "Epoch 4532, Loss Train(MSE): 0.12530897870838217, R2 Train: 0.4987640851664713\n",
      "Epoch 4533, Loss Train(MSE): 0.1253088330104353, R2 Train: 0.4987646679582588\n",
      "Epoch 4534, Loss Train(MSE): 0.1253087036254633, R2 Train: 0.49876518549814675\n",
      "Epoch 4535, Loss Train(MSE): 0.12530857068748924, R2 Train: 0.49876571725004304\n",
      "Epoch 4536, Loss Train(MSE): 0.12530842542429976, R2 Train: 0.49876629830280095\n",
      "Epoch 4537, Loss Train(MSE): 0.1253083003636114, R2 Train: 0.4987667985455544\n",
      "Epoch 4538, Loss Train(MSE): 0.12530816382453552, R2 Train: 0.4987673447018579\n",
      "Epoch 4539, Loss Train(MSE): 0.12530801899317665, R2 Train: 0.4987679240272934\n",
      "Epoch 4540, Loss Train(MSE): 0.12530789809801612, R2 Train: 0.4987684076079355\n",
      "Epoch 4541, Loss Train(MSE): 0.1253077581122608, R2 Train: 0.4987689675509568\n",
      "Epoch 4542, Loss Train(MSE): 0.1253076137098456, R2 Train: 0.49876954516061756\n",
      "Epoch 4543, Loss Train(MSE): 0.12530749682402467, R2 Train: 0.4987700127039013\n",
      "Epoch 4544, Loss Train(MSE): 0.12530735354350248, R2 Train: 0.4987705858259901\n",
      "Epoch 4545, Loss Train(MSE): 0.12530721227803882, R2 Train: 0.49877115088784474\n",
      "Epoch 4546, Loss Train(MSE): 0.1253070938237355, R2 Train: 0.498771624705058\n",
      "Epoch 4547, Loss Train(MSE): 0.1253069500967301, R2 Train: 0.4987721996130796\n",
      "Epoch 4548, Loss Train(MSE): 0.12530681267601207, R2 Train: 0.4987727492959517\n",
      "Epoch 4549, Loss Train(MSE): 0.12530669108504033, R2 Train: 0.49877323565983867\n",
      "Epoch 4550, Loss Train(MSE): 0.12530654777989916, R2 Train: 0.49877380888040335\n",
      "Epoch 4551, Loss Train(MSE): 0.12530641405358275, R2 Train: 0.498774343785669\n",
      "Epoch 4552, Loss Train(MSE): 0.1253062894721488, R2 Train: 0.49877484211140477\n",
      "Epoch 4553, Loss Train(MSE): 0.12530614658611752, R2 Train: 0.4987754136555299\n",
      "Epoch 4554, Loss Train(MSE): 0.12530601640628497, R2 Train: 0.4987759343748601\n",
      "Epoch 4555, Loss Train(MSE): 0.12530588897822276, R2 Train: 0.49877644408710897\n",
      "Epoch 4556, Loss Train(MSE): 0.12530574650858362, R2 Train: 0.49877701396566554\n",
      "Epoch 4557, Loss Train(MSE): 0.12530561972970314, R2 Train: 0.49877752108118745\n",
      "Epoch 4558, Loss Train(MSE): 0.1253054895965132, R2 Train: 0.49877804161394723\n",
      "Epoch 4559, Loss Train(MSE): 0.12530534754058437, R2 Train: 0.49877860983766253\n",
      "Epoch 4560, Loss Train(MSE): 0.12530522401947133, R2 Train: 0.49877910392211466\n",
      "Epoch 4561, Loss Train(MSE): 0.12530509132035875, R2 Train: 0.498779634718565\n",
      "Epoch 4562, Loss Train(MSE): 0.1253049496754936, R2 Train: 0.4987802012980256\n",
      "Epoch 4563, Loss Train(MSE): 0.1253048292712718, R2 Train: 0.49878068291491284\n",
      "Epoch 4564, Loss Train(MSE): 0.12530469414318374, R2 Train: 0.49878122342726505\n",
      "Epoch 4565, Loss Train(MSE): 0.12530455290677012, R2 Train: 0.49878178837291953\n",
      "Epoch 4566, Loss Train(MSE): 0.12530443548083423, R2 Train: 0.4987822580766631\n",
      "Epoch 4567, Loss Train(MSE): 0.1253042980584965, R2 Train: 0.498782807766014\n",
      "Epoch 4568, Loss Train(MSE): 0.12530415722795615, R2 Train: 0.4987833710881754\n",
      "Epoch 4569, Loss Train(MSE): 0.12530404264393455, R2 Train: 0.4987838294242618\n",
      "Epoch 4570, Loss Train(MSE): 0.12530390305988784, R2 Train: 0.49878438776044864\n",
      "Epoch 4571, Loss Train(MSE): 0.1253037640693003, R2 Train: 0.4987849437227988\n",
      "Epoch 4572, Loss Train(MSE): 0.12530364931831028, R2 Train: 0.49878540272675886\n",
      "Epoch 4573, Loss Train(MSE): 0.1253035091275527, R2 Train: 0.4987859634897892\n",
      "Epoch 4574, Loss Train(MSE): 0.1253033728424014, R2 Train: 0.4987865086303944\n",
      "Epoch 4575, Loss Train(MSE): 0.12530325606035037, R2 Train: 0.49878697575859854\n",
      "Epoch 4576, Loss Train(MSE): 0.1253031162691407, R2 Train: 0.4987875349234372\n",
      "Epoch 4577, Loss Train(MSE): 0.12530298255809208, R2 Train: 0.4987880697676317\n",
      "Epoch 4578, Loss Train(MSE): 0.12530354391300272, R2 Train: 0.4987858243479891\n",
      "Epoch 4579, Loss Train(MSE): 0.1253081918425037, R2 Train: 0.49876723262998524\n",
      "Epoch 4580, Loss Train(MSE): 0.12530802221423873, R2 Train: 0.4987679111430451\n",
      "Epoch 4581, Loss Train(MSE): 0.12530785276122544, R2 Train: 0.49876858895509824\n",
      "Epoch 4582, Loss Train(MSE): 0.12530768348318805, R2 Train: 0.4987692660672478\n",
      "Epoch 4583, Loss Train(MSE): 0.12530751437985135, R2 Train: 0.4987699424805946\n",
      "Epoch 4584, Loss Train(MSE): 0.125307345450941, R2 Train: 0.498770618196236\n",
      "Epoch 4585, Loss Train(MSE): 0.1253071766961832, R2 Train: 0.49877129321526725\n",
      "Epoch 4586, Loss Train(MSE): 0.12530700811530482, R2 Train: 0.4987719675387807\n",
      "Epoch 4587, Loss Train(MSE): 0.1253068397080336, R2 Train: 0.49877264116786557\n",
      "Epoch 4588, Loss Train(MSE): 0.1253066714740978, R2 Train: 0.4987733141036088\n",
      "Epoch 4589, Loss Train(MSE): 0.1253065034132264, R2 Train: 0.49877398634709436\n",
      "Epoch 4590, Loss Train(MSE): 0.12530633552514905, R2 Train: 0.4987746578994038\n",
      "Epoch 4591, Loss Train(MSE): 0.1253061678095961, R2 Train: 0.49877532876161557\n",
      "Epoch 4592, Loss Train(MSE): 0.12530600026629857, R2 Train: 0.49877599893480573\n",
      "Epoch 4593, Loss Train(MSE): 0.12530583289498806, R2 Train: 0.49877666842004775\n",
      "Epoch 4594, Loss Train(MSE): 0.12530566569539694, R2 Train: 0.4987773372184122\n",
      "Epoch 4595, Loss Train(MSE): 0.1253054986672582, R2 Train: 0.49877800533096717\n",
      "Epoch 4596, Loss Train(MSE): 0.12530533181030545, R2 Train: 0.4987786727587782\n",
      "Epoch 4597, Loss Train(MSE): 0.12530516512427303, R2 Train: 0.49877933950290787\n",
      "Epoch 4598, Loss Train(MSE): 0.1253049986088958, R2 Train: 0.4987800055644168\n",
      "Epoch 4599, Loss Train(MSE): 0.12530483226390937, R2 Train: 0.4987806709443625\n",
      "Epoch 4600, Loss Train(MSE): 0.12530466608905, R2 Train: 0.4987813356438\n",
      "Epoch 4601, Loss Train(MSE): 0.12530450008405453, R2 Train: 0.4987819996637819\n",
      "Epoch 4602, Loss Train(MSE): 0.1253043342486605, R2 Train: 0.498782663005358\n",
      "Epoch 4603, Loss Train(MSE): 0.12530416858260593, R2 Train: 0.4987833256695763\n",
      "Epoch 4604, Loss Train(MSE): 0.12530400308562972, R2 Train: 0.4987839876574811\n",
      "Epoch 4605, Loss Train(MSE): 0.1253038377574712, R2 Train: 0.4987846489701152\n",
      "Epoch 4606, Loss Train(MSE): 0.12530367259787045, R2 Train: 0.4987853096085182\n",
      "Epoch 4607, Loss Train(MSE): 0.12530350760656794, R2 Train: 0.4987859695737282\n",
      "Epoch 4608, Loss Train(MSE): 0.1253033427833051, R2 Train: 0.4987866288667796\n",
      "Epoch 4609, Loss Train(MSE): 0.1253031781278238, R2 Train: 0.4987872874887048\n",
      "Epoch 4610, Loss Train(MSE): 0.1253030136398664, R2 Train: 0.4987879454405344\n",
      "Epoch 4611, Loss Train(MSE): 0.12530284931917607, R2 Train: 0.4987886027232957\n",
      "Epoch 4612, Loss Train(MSE): 0.12530268516549653, R2 Train: 0.4987892593380139\n",
      "Epoch 4613, Loss Train(MSE): 0.12530252117857207, R2 Train: 0.4987899152857117\n",
      "Epoch 4614, Loss Train(MSE): 0.12530235735814754, R2 Train: 0.49879057056740983\n",
      "Epoch 4615, Loss Train(MSE): 0.12530219370396856, R2 Train: 0.4987912251841258\n",
      "Epoch 4616, Loss Train(MSE): 0.12530203021578118, R2 Train: 0.4987918791368753\n",
      "Epoch 4617, Loss Train(MSE): 0.1253018668933321, R2 Train: 0.49879253242667165\n",
      "Epoch 4618, Loss Train(MSE): 0.12530170373636854, R2 Train: 0.49879318505452586\n",
      "Epoch 4619, Loss Train(MSE): 0.1253015407446385, R2 Train: 0.498793837021446\n",
      "Epoch 4620, Loss Train(MSE): 0.1253013779178904, R2 Train: 0.4987944883284384\n",
      "Epoch 4621, Loss Train(MSE): 0.12530121525587326, R2 Train: 0.49879513897650696\n",
      "Epoch 4622, Loss Train(MSE): 0.1253010527583367, R2 Train: 0.4987957889666532\n",
      "Epoch 4623, Loss Train(MSE): 0.12530089042503098, R2 Train: 0.4987964382998761\n",
      "Epoch 4624, Loss Train(MSE): 0.12530072825570684, R2 Train: 0.4987970869771726\n",
      "Epoch 4625, Loss Train(MSE): 0.12530056625011562, R2 Train: 0.4987977349995375\n",
      "Epoch 4626, Loss Train(MSE): 0.12530040440800932, R2 Train: 0.4987983823679627\n",
      "Epoch 4627, Loss Train(MSE): 0.12530024272914037, R2 Train: 0.49879902908343854\n",
      "Epoch 4628, Loss Train(MSE): 0.12530008121326186, R2 Train: 0.49879967514695256\n",
      "Epoch 4629, Loss Train(MSE): 0.12529991986012742, R2 Train: 0.4988003205594903\n",
      "Epoch 4630, Loss Train(MSE): 0.1252997586694912, R2 Train: 0.4988009653220352\n",
      "Epoch 4631, Loss Train(MSE): 0.12529959764110798, R2 Train: 0.49880160943556806\n",
      "Epoch 4632, Loss Train(MSE): 0.125299436774733, R2 Train: 0.49880225290106806\n",
      "Epoch 4633, Loss Train(MSE): 0.12530111718845577, R2 Train: 0.4987955312461769\n",
      "Epoch 4634, Loss Train(MSE): 0.12532586119668704, R2 Train: 0.49869655521325185\n",
      "Epoch 4635, Loss Train(MSE): 0.12532550872510978, R2 Train: 0.4986979650995609\n",
      "Epoch 4636, Loss Train(MSE): 0.12532515716557835, R2 Train: 0.4986993713376866\n",
      "Epoch 4637, Loss Train(MSE): 0.12532480651394204, R2 Train: 0.49870077394423185\n",
      "Epoch 4638, Loss Train(MSE): 0.1253244567660782, R2 Train: 0.4987021729356872\n",
      "Epoch 4639, Loss Train(MSE): 0.1253241079178921, R2 Train: 0.49870356832843155\n",
      "Epoch 4640, Loss Train(MSE): 0.12532375996531658, R2 Train: 0.49870496013873367\n",
      "Epoch 4641, Loss Train(MSE): 0.12532341290431193, R2 Train: 0.4987063483827523\n",
      "Epoch 4642, Loss Train(MSE): 0.12532306673086538, R2 Train: 0.4987077330765385\n",
      "Epoch 4643, Loss Train(MSE): 0.12532272144099116, R2 Train: 0.49870911423603537\n",
      "Epoch 4644, Loss Train(MSE): 0.12532237703073015, R2 Train: 0.4987104918770794\n",
      "Epoch 4645, Loss Train(MSE): 0.12532203349614954, R2 Train: 0.49871186601540185\n",
      "Epoch 4646, Loss Train(MSE): 0.12532169083334269, R2 Train: 0.49871323666662926\n",
      "Epoch 4647, Loss Train(MSE): 0.12532134903842898, R2 Train: 0.4987146038462841\n",
      "Epoch 4648, Loss Train(MSE): 0.12532100810755337, R2 Train: 0.49871596756978653\n",
      "Epoch 4649, Loss Train(MSE): 0.1253206680368863, R2 Train: 0.4987173278524548\n",
      "Epoch 4650, Loss Train(MSE): 0.1253203288226235, R2 Train: 0.498718684709506\n",
      "Epoch 4651, Loss Train(MSE): 0.12531999046098574, R2 Train: 0.49872003815605703\n",
      "Epoch 4652, Loss Train(MSE): 0.12531965294821845, R2 Train: 0.4987213882071262\n",
      "Epoch 4653, Loss Train(MSE): 0.12531931628059187, R2 Train: 0.4987227348776325\n",
      "Epoch 4654, Loss Train(MSE): 0.12531898045440037, R2 Train: 0.4987240781823985\n",
      "Epoch 4655, Loss Train(MSE): 0.1253186454659626, R2 Train: 0.49872541813614957\n",
      "Epoch 4656, Loss Train(MSE): 0.1253183113116211, R2 Train: 0.4987267547535156\n",
      "Epoch 4657, Loss Train(MSE): 0.1253179779877422, R2 Train: 0.49872808804903124\n",
      "Epoch 4658, Loss Train(MSE): 0.12531764549071567, R2 Train: 0.49872941803713733\n",
      "Epoch 4659, Loss Train(MSE): 0.12531731381695466, R2 Train: 0.4987307447321814\n",
      "Epoch 4660, Loss Train(MSE): 0.12531698296289548, R2 Train: 0.4987320681484181\n",
      "Epoch 4661, Loss Train(MSE): 0.12531665292499725, R2 Train: 0.498733388300011\n",
      "Epoch 4662, Loss Train(MSE): 0.12531632369974186, R2 Train: 0.4987347052010326\n",
      "Epoch 4663, Loss Train(MSE): 0.12531599528363377, R2 Train: 0.4987360188654649\n",
      "Epoch 4664, Loss Train(MSE): 0.12531566767319968, R2 Train: 0.49873732930720127\n",
      "Epoch 4665, Loss Train(MSE): 0.12531534086498855, R2 Train: 0.4987386365400458\n",
      "Epoch 4666, Loss Train(MSE): 0.12531501485557114, R2 Train: 0.4987399405777154\n",
      "Epoch 4667, Loss Train(MSE): 0.12531468964154016, R2 Train: 0.49874124143383936\n",
      "Epoch 4668, Loss Train(MSE): 0.1253143652195097, R2 Train: 0.4987425391219612\n",
      "Epoch 4669, Loss Train(MSE): 0.1253140415861154, R2 Train: 0.4987438336555384\n",
      "Epoch 4670, Loss Train(MSE): 0.12531371873801403, R2 Train: 0.4987451250479439\n",
      "Epoch 4671, Loss Train(MSE): 0.12531339667188338, R2 Train: 0.4987464133124665\n",
      "Epoch 4672, Loss Train(MSE): 0.12531307538442218, R2 Train: 0.49874769846231126\n",
      "Epoch 4673, Loss Train(MSE): 0.12531275487234975, R2 Train: 0.498748980510601\n",
      "Epoch 4674, Loss Train(MSE): 0.12531243513240609, R2 Train: 0.49875025947037566\n",
      "Epoch 4675, Loss Train(MSE): 0.12531211616135116, R2 Train: 0.4987515353545954\n",
      "Epoch 4676, Loss Train(MSE): 0.1253117979559655, R2 Train: 0.498752808176138\n",
      "Epoch 4677, Loss Train(MSE): 0.12531148051304938, R2 Train: 0.49875407794780247\n",
      "Epoch 4678, Loss Train(MSE): 0.12531116382942306, R2 Train: 0.49875534468230776\n",
      "Epoch 4679, Loss Train(MSE): 0.12531084790192631, R2 Train: 0.49875660839229474\n",
      "Epoch 4680, Loss Train(MSE): 0.12531053272741854, R2 Train: 0.4987578690903258\n",
      "Epoch 4681, Loss Train(MSE): 0.12531021830277836, R2 Train: 0.49875912678888656\n",
      "Epoch 4682, Loss Train(MSE): 0.12530990462490366, R2 Train: 0.4987603815003854\n",
      "Epoch 4683, Loss Train(MSE): 0.12530959169071132, R2 Train: 0.4987616332371547\n",
      "Epoch 4684, Loss Train(MSE): 0.12530927949713705, R2 Train: 0.4987628820114518\n",
      "Epoch 4685, Loss Train(MSE): 0.12530899280495755, R2 Train: 0.4987640287801698\n",
      "Epoch 4686, Loss Train(MSE): 0.1253087107574263, R2 Train: 0.4987651569702948\n",
      "Epoch 4687, Loss Train(MSE): 0.12530844352698026, R2 Train: 0.498766225892079\n",
      "Epoch 4688, Loss Train(MSE): 0.12530815243168564, R2 Train: 0.49876739027325745\n",
      "Epoch 4689, Loss Train(MSE): 0.12530788919193997, R2 Train: 0.4987684432322401\n",
      "Epoch 4690, Loss Train(MSE): 0.12530760734090898, R2 Train: 0.49876957063636407\n",
      "Epoch 4691, Loss Train(MSE): 0.12530732696836952, R2 Train: 0.4987706921265219\n",
      "Epoch 4692, Loss Train(MSE): 0.12530706489744292, R2 Train: 0.4987717404102283\n",
      "Epoch 4693, Loss Train(MSE): 0.1253067775878224, R2 Train: 0.49877288964871036\n",
      "Epoch 4694, Loss Train(MSE): 0.12530651484863353, R2 Train: 0.49877394060546587\n",
      "Epoch 4695, Loss Train(MSE): 0.1253062392342132, R2 Train: 0.49877504306314724\n",
      "Epoch 4696, Loss Train(MSE): 0.12530595900167402, R2 Train: 0.49877616399330393\n",
      "Epoch 4697, Loss Train(MSE): 0.12530570346706485, R2 Train: 0.4987771861317406\n",
      "Epoch 4698, Loss Train(MSE): 0.12530541985302482, R2 Train: 0.4987783205879007\n",
      "Epoch 4699, Loss Train(MSE): 0.12530515612399398, R2 Train: 0.49877937550402407\n",
      "Epoch 4700, Loss Train(MSE): 0.12530488808167853, R2 Train: 0.4987804476732859\n",
      "Epoch 4701, Loss Train(MSE): 0.12530460669448243, R2 Train: 0.4987815732220703\n",
      "Epoch 4702, Loss Train(MSE): 0.12530435870963916, R2 Train: 0.49878256516144337\n",
      "Epoch 4703, Loss Train(MSE): 0.12530407886281097, R2 Train: 0.4987836845487561\n",
      "Epoch 4704, Loss Train(MSE): 0.12530381278727118, R2 Train: 0.4987847488509153\n",
      "Epoch 4705, Loss Train(MSE): 0.1253035535237581, R2 Train: 0.4987857859049676\n",
      "Epoch 4706, Loss Train(MSE): 0.12530327569299077, R2 Train: 0.49878689722803693\n",
      "Epoch 4707, Loss Train(MSE): 0.1253030243013132, R2 Train: 0.49878790279474716\n",
      "Epoch 4708, Loss Train(MSE): 0.1253027542045672, R2 Train: 0.49878898318173115\n",
      "Epoch 4709, Loss Train(MSE): 0.12530248441257685, R2 Train: 0.4987900623496926\n",
      "Epoch 4710, Loss Train(MSE): 0.12530223515306774, R2 Train: 0.49879105938772905\n",
      "Epoch 4711, Loss Train(MSE): 0.12530196079637337, R2 Train: 0.4987921568145065\n",
      "Epoch 4712, Loss Train(MSE): 0.12530170466793217, R2 Train: 0.49879318132827133\n",
      "Epoch 4713, Loss Train(MSE): 0.12530144550938685, R2 Train: 0.4987942179624526\n",
      "Epoch 4714, Loss Train(MSE): 0.12530117324704287, R2 Train: 0.4987953070118285\n",
      "Epoch 4715, Loss Train(MSE): 0.1253009300919536, R2 Train: 0.49879627963218565\n",
      "Epoch 4716, Loss Train(MSE): 0.12530066167313653, R2 Train: 0.4987973533074539\n",
      "Epoch 4717, Loss Train(MSE): 0.12530039960057474, R2 Train: 0.49879840159770106\n",
      "Epoch 4718, Loss Train(MSE): 0.12530015245015722, R2 Train: 0.49879939019937114\n",
      "Epoch 4719, Loss Train(MSE): 0.12529988353452906, R2 Train: 0.49880046586188376\n",
      "Epoch 4720, Loss Train(MSE): 0.12529963347748832, R2 Train: 0.4988014660900467\n",
      "Epoch 4721, Loss Train(MSE): 0.12529937794298185, R2 Train: 0.4988024882280726\n",
      "Epoch 4722, Loss Train(MSE): 0.1252991110454727, R2 Train: 0.4988035558181092\n",
      "Epoch 4723, Loss Train(MSE): 0.1252988723532593, R2 Train: 0.49880451058696285\n",
      "Epoch 4724, Loss Train(MSE): 0.12529860903656945, R2 Train: 0.4988055638537222\n",
      "Epoch 4725, Loss Train(MSE): 0.12529835095524494, R2 Train: 0.49880659617902023\n",
      "Epoch 4726, Loss Train(MSE): 0.12529810929701415, R2 Train: 0.4988075628119434\n",
      "Epoch 4727, Loss Train(MSE): 0.12529784562550833, R2 Train: 0.4988086174979667\n",
      "Epoch 4728, Loss Train(MSE): 0.1252976859966831, R2 Train: 0.49880925601326764\n",
      "Epoch 4729, Loss Train(MSE): 0.12529756092836686, R2 Train: 0.49880975628653257\n",
      "Epoch 4730, Loss Train(MSE): 0.12529742229228225, R2 Train: 0.498810310830871\n",
      "Epoch 4731, Loss Train(MSE): 0.12529730722050259, R2 Train: 0.49881077111798966\n",
      "Epoch 4732, Loss Train(MSE): 0.12529717437957427, R2 Train: 0.4988113024817029\n",
      "Epoch 4733, Loss Train(MSE): 0.125297038662652, R2 Train: 0.49881184534939205\n",
      "Epoch 4734, Loss Train(MSE): 0.1252969269119887, R2 Train: 0.49881229235204516\n",
      "Epoch 4735, Loss Train(MSE): 0.12529678897099739, R2 Train: 0.49881284411601046\n",
      "Epoch 4736, Loss Train(MSE): 0.12529666149338062, R2 Train: 0.49881335402647753\n",
      "Epoch 4737, Loss Train(MSE): 0.1252965422168536, R2 Train: 0.4988138311325856\n",
      "Epoch 4738, Loss Train(MSE): 0.1252964047101695, R2 Train: 0.498814381159322\n",
      "Epoch 4739, Loss Train(MSE): 0.12529628526895745, R2 Train: 0.4988148589241702\n",
      "Epoch 4740, Loss Train(MSE): 0.1252961586644035, R2 Train: 0.498815365342386\n",
      "Epoch 4741, Loss Train(MSE): 0.12529602158865366, R2 Train: 0.49881591364538536\n",
      "Epoch 4742, Loss Train(MSE): 0.12529590998411885, R2 Train: 0.4988163600635246\n",
      "Epoch 4743, Loss Train(MSE): 0.12529577624628072, R2 Train: 0.49881689501487714\n",
      "Epoch 4744, Loss Train(MSE): 0.12529564388763592, R2 Train: 0.4988174244494563\n",
      "Epoch 4745, Loss Train(MSE): 0.12529553133851898, R2 Train: 0.4988178746459241\n",
      "Epoch 4746, Loss Train(MSE): 0.12529539493913852, R2 Train: 0.4988184202434459\n",
      "Epoch 4747, Loss Train(MSE): 0.12529527017852723, R2 Train: 0.49881891928589106\n",
      "Epoch 4748, Loss Train(MSE): 0.12529515072721345, R2 Train: 0.4988193970911462\n",
      "Epoch 4749, Loss Train(MSE): 0.12529501475048563, R2 Train: 0.4988199409980575\n",
      "Epoch 4750, Loss Train(MSE): 0.1252948973956001, R2 Train: 0.4988204104175996\n",
      "Epoch 4751, Loss Train(MSE): 0.12529477122960286, R2 Train: 0.49882091508158855\n",
      "Epoch 4752, Loss Train(MSE): 0.12529463567233168, R2 Train: 0.4988214573106733\n",
      "Epoch 4753, Loss Train(MSE): 0.12529452553385806, R2 Train: 0.49882189786456776\n",
      "Epoch 4754, Loss Train(MSE): 0.12529439283776994, R2 Train: 0.49882242864892024\n",
      "Epoch 4755, Loss Train(MSE): 0.12529426185216996, R2 Train: 0.49882295259132015\n",
      "Epoch 4756, Loss Train(MSE): 0.1252941504278687, R2 Train: 0.4988233982885252\n",
      "Epoch 4757, Loss Train(MSE): 0.1252940155292409, R2 Train: 0.49882393788303636\n",
      "Epoch 4758, Loss Train(MSE): 0.1252938915365256, R2 Train: 0.49882443385389763\n",
      "Epoch 4759, Loss Train(MSE): 0.1252937737985025, R2 Train: 0.49882490480598995\n",
      "Epoch 4760, Loss Train(MSE): 0.12529363931148466, R2 Train: 0.4988254427540614\n",
      "Epoch 4761, Loss Train(MSE): 0.12529352212932235, R2 Train: 0.4988259114827106\n",
      "Epoch 4762, Loss Train(MSE): 0.12529339825536326, R2 Train: 0.49882640697854697\n",
      "Epoch 4763, Loss Train(MSE): 0.12529326417692346, R2 Train: 0.49882694329230615\n",
      "Epoch 4764, Loss Train(MSE): 0.12529315362580892, R2 Train: 0.4988273854967643\n",
      "Epoch 4765, Loss Train(MSE): 0.12529302379094093, R2 Train: 0.4988279048362363\n",
      "Epoch 4766, Loss Train(MSE): 0.12529289231455637, R2 Train: 0.4988284307417745\n",
      "Epoch 4767, Loss Train(MSE): 0.12529278382018386, R2 Train: 0.4988288647192646\n",
      "Epoch 4768, Loss Train(MSE): 0.12529265038358375, R2 Train: 0.498829398465665\n",
      "Epoch 4769, Loss Train(MSE): 0.12529252532892593, R2 Train: 0.4988298986842963\n",
      "Epoch 4770, Loss Train(MSE): 0.12529241107617428, R2 Train: 0.4988303556953029\n",
      "Epoch 4771, Loss Train(MSE): 0.1252922780407081, R2 Train: 0.49883088783716756\n",
      "Epoch 4772, Loss Train(MSE): 0.12529215923484602, R2 Train: 0.49883136306061593\n",
      "Epoch 4773, Loss Train(MSE): 0.12529203939233008, R2 Train: 0.4988318424306797\n",
      "Epoch 4774, Loss Train(MSE): 0.12529190675511795, R2 Train: 0.4988323729795282\n",
      "Epoch 4775, Loss Train(MSE): 0.12529179402779045, R2 Train: 0.4988328238888382\n",
      "Epoch 4776, Loss Train(MSE): 0.12529166876151765, R2 Train: 0.4988333249539294\n",
      "Epoch 4777, Loss Train(MSE): 0.12529153651972127, R2 Train: 0.4988338539211149\n",
      "Epoch 4778, Loss Train(MSE): 0.12529142970329352, R2 Train: 0.49883428118682593\n",
      "Epoch 4779, Loss Train(MSE): 0.12529129917670626, R2 Train: 0.498834803293175\n",
      "Epoch 4780, Loss Train(MSE): 0.1252911713428993, R2 Train: 0.4988353146284028\n",
      "Epoch 4781, Loss Train(MSE): 0.12529106223752265, R2 Train: 0.4988357510499094\n",
      "Epoch 4782, Loss Train(MSE): 0.12529093061723853, R2 Train: 0.49883627753104587\n",
      "Epoch 4783, Loss Train(MSE): 0.1252908085021685, R2 Train: 0.498836765991326\n",
      "Epoch 4784, Loss Train(MSE): 0.12529069432227988, R2 Train: 0.4988372227108805\n",
      "Epoch 4785, Loss Train(MSE): 0.12529056309044254, R2 Train: 0.49883774763822986\n",
      "Epoch 4786, Loss Train(MSE): 0.12529044653256208, R2 Train: 0.4988382138697517\n",
      "Epoch 4787, Loss Train(MSE): 0.12529032743566293, R2 Train: 0.4988386902573483\n",
      "Epoch 4788, Loss Train(MSE): 0.12529019658957224, R2 Train: 0.498839213641711\n",
      "Epoch 4789, Loss Train(MSE): 0.1252900854298167, R2 Train: 0.4988396582807332\n",
      "Epoch 4790, Loss Train(MSE): 0.12528996157098235, R2 Train: 0.4988401537160706\n",
      "Epoch 4791, Loss Train(MSE): 0.12528983110797604, R2 Train: 0.49884067556809586\n",
      "Epoch 4792, Loss Train(MSE): 0.1252897251897237, R2 Train: 0.4988410992411052\n",
      "Epoch 4793, Loss Train(MSE): 0.12528959672164208, R2 Train: 0.49884161311343167\n",
      "Epoch 4794, Loss Train(MSE): 0.12528946971527136, R2 Train: 0.4988421211389146\n",
      "Epoch 4795, Loss Train(MSE): 0.12528936272845137, R2 Train: 0.4988425490861945\n",
      "Epoch 4796, Loss Train(MSE): 0.12528923286790894, R2 Train: 0.49884306852836424\n",
      "Epoch 4797, Loss Train(MSE): 0.12528911092674797, R2 Train: 0.49884355629300814\n",
      "Epoch 4798, Loss Train(MSE): 0.1252889995010036, R2 Train: 0.49884400199598555\n",
      "Epoch 4799, Loss Train(MSE): 0.12528887001700872, R2 Train: 0.4988445199319651\n",
      "Epoch 4800, Loss Train(MSE): 0.12528875299007686, R2 Train: 0.49884498803969257\n",
      "Epoch 4801, Loss Train(MSE): 0.12528863727258788, R2 Train: 0.4988454509096485\n",
      "Epoch 4802, Loss Train(MSE): 0.12528850816260417, R2 Train: 0.4988459673495833\n",
      "Epoch 4803, Loss Train(MSE): 0.1252883959012318, R2 Train: 0.49884641639507277\n",
      "Epoch 4804, Loss Train(MSE): 0.12528827603691828, R2 Train: 0.4988468958523269\n",
      "Epoch 4805, Loss Train(MSE): 0.1252881472984439, R2 Train: 0.4988474108062244\n",
      "Epoch 4806, Loss Train(MSE): 0.12528803965623578, R2 Train: 0.4988478413750569\n",
      "Epoch 4807, Loss Train(MSE): 0.12528791578779405, R2 Train: 0.4988483368488238\n",
      "Epoch 4808, Loss Train(MSE): 0.1252877874183612, R2 Train: 0.49884885032655524\n",
      "Epoch 4809, Loss Train(MSE): 0.1252876842511599, R2 Train: 0.49884926299536037\n",
      "Epoch 4810, Loss Train(MSE): 0.1252875565190977, R2 Train: 0.4988497739236092\n",
      "Epoch 4811, Loss Train(MSE): 0.12528743219471541, R2 Train: 0.49885027122113834\n",
      "Epoch 4812, Loss Train(MSE): 0.12528732600080597, R2 Train: 0.4988506959967761\n",
      "Epoch 4813, Loss Train(MSE): 0.12528719821214382, R2 Train: 0.4988512071514247\n",
      "Epoch 4814, Loss Train(MSE): 0.12528707820438048, R2 Train: 0.4988516871824781\n",
      "Epoch 4815, Loss Train(MSE): 0.12528696829969557, R2 Train: 0.4988521268012177\n",
      "Epoch 4816, Loss Train(MSE): 0.1252868408740149, R2 Train: 0.4988526365039404\n",
      "Epoch 4817, Loss Train(MSE): 0.12528672504385674, R2 Train: 0.49885309982457304\n",
      "Epoch 4818, Loss Train(MSE): 0.12528661156387866, R2 Train: 0.49885355374448537\n",
      "Epoch 4819, Loss Train(MSE): 0.1252864844988242, R2 Train: 0.49885406200470317\n",
      "Epoch 4820, Loss Train(MSE): 0.12528637270937665, R2 Train: 0.4988545091624934\n",
      "Epoch 4821, Loss Train(MSE): 0.12528625578751454, R2 Train: 0.49885497684994184\n",
      "Epoch 4822, Loss Train(MSE): 0.12528612908076203, R2 Train: 0.49885548367695187\n",
      "Epoch 4823, Loss Train(MSE): 0.12528602119721635, R2 Train: 0.4988559152111346\n",
      "Epoch 4824, Loss Train(MSE): 0.12528590096483852, R2 Train: 0.4988563961406459\n",
      "Epoch 4825, Loss Train(MSE): 0.1252857746140941, R2 Train: 0.4988569015436236\n",
      "Epoch 4826, Loss Train(MSE): 0.12528567050369468, R2 Train: 0.49885731798522126\n",
      "Epoch 4827, Loss Train(MSE): 0.1252855470901605, R2 Train: 0.49885781163935805\n",
      "Epoch 4828, Loss Train(MSE): 0.1252854217801616, R2 Train: 0.49885831287935356\n",
      "Epoch 4829, Loss Train(MSE): 0.1252853199358721, R2 Train: 0.49885872025651157\n",
      "Epoch 4830, Loss Train(MSE): 0.12528519414575723, R2 Train: 0.4988592234169711\n",
      "Epoch 4831, Loss Train(MSE): 0.12528507246682796, R2 Train: 0.49885971013268815\n",
      "Epoch 4832, Loss Train(MSE): 0.12528496757829635, R2 Train: 0.4988601296868146\n",
      "Epoch 4833, Loss Train(MSE): 0.12528484213855645, R2 Train: 0.4988606314457742\n",
      "Epoch 4834, Loss Train(MSE): 0.12528472396263585, R2 Train: 0.4988611041494566\n",
      "Epoch 4835, Loss Train(MSE): 0.1252846161546323, R2 Train: 0.4988615353814708\n",
      "Epoch 4836, Loss Train(MSE): 0.12528449106307452, R2 Train: 0.49886203574770194\n",
      "Epoch 4837, Loss Train(MSE): 0.12528437626404737, R2 Train: 0.4988624949438105\n",
      "Epoch 4838, Loss Train(MSE): 0.12528426565943732, R2 Train: 0.49886293736225074\n",
      "Epoch 4839, Loss Train(MSE): 0.12528414091389675, R2 Train: 0.498863436344413\n",
      "Epoch 4840, Loss Train(MSE): 0.12528402936756336, R2 Train: 0.49886388252974656\n",
      "Epoch 4841, Loss Train(MSE): 0.12528391608733724, R2 Train: 0.498864335650651\n",
      "Epoch 4842, Loss Train(MSE): 0.12528379168567635, R2 Train: 0.4988648332572946\n",
      "Epoch 4843, Loss Train(MSE): 0.1252836832697228, R2 Train: 0.4988652669211088\n",
      "Epoch 4844, Loss Train(MSE): 0.12528356743302466, R2 Train: 0.4988657302679014\n",
      "Epoch 4845, Loss Train(MSE): 0.12528344337313294, R2 Train: 0.49886622650746826\n",
      "Epoch 4846, Loss Train(MSE): 0.12528333796710162, R2 Train: 0.49886664813159354\n",
      "Epoch 4847, Loss Train(MSE): 0.1252832196912581, R2 Train: 0.49886712123496757\n",
      "Epoch 4848, Loss Train(MSE): 0.12528309597105125, R2 Train: 0.498867616115795\n",
      "Epoch 4849, Loss Train(MSE): 0.12528299345631244, R2 Train: 0.49886802617475023\n",
      "Epoch 4850, Loss Train(MSE): 0.12528287285686032, R2 Train: 0.4988685085725587\n",
      "Epoch 4851, Loss Train(MSE): 0.12528274947428003, R2 Train: 0.4988690021028799\n",
      "Epoch 4852, Loss Train(MSE): 0.12528264973400355, R2 Train: 0.4988694010639858\n",
      "Epoch 4853, Loss Train(MSE): 0.12528252692471734, R2 Train: 0.49886989230113066\n",
      "Epoch 4854, Loss Train(MSE): 0.125282405944274, R2 Train: 0.498870376222904\n",
      "Epoch 4855, Loss Train(MSE): 0.1252823047288142, R2 Train: 0.49887078108474325\n",
      "Epoch 4856, Loss Train(MSE): 0.12528218187845633, R2 Train: 0.4988712724861747\n",
      "Epoch 4857, Loss Train(MSE): 0.12528206355307947, R2 Train: 0.4988717457876821\n",
      "Epoch 4858, Loss Train(MSE): 0.12528196024252988, R2 Train: 0.49887215902988047\n",
      "Epoch 4859, Loss Train(MSE): 0.12528183772474263, R2 Train: 0.49887264910102946\n",
      "Epoch 4860, Loss Train(MSE): 0.12528172194168263, R2 Train: 0.49887311223326947\n",
      "Epoch 4861, Loss Train(MSE): 0.125281616645826, R2 Train: 0.49887353341669605\n",
      "Epoch 4862, Loss Train(MSE): 0.12528149445863684, R2 Train: 0.4988740221654526\n",
      "Epoch 4863, Loss Train(MSE): 0.12528138110685175, R2 Train: 0.498874475572593\n",
      "Epoch 4864, Loss Train(MSE): 0.1252812739337981, R2 Train: 0.49887490426480763\n",
      "Epoch 4865, Loss Train(MSE): 0.12528115207525833, R2 Train: 0.49887539169896666\n",
      "Epoch 4866, Loss Train(MSE): 0.12528104104538776, R2 Train: 0.49887583581844896\n",
      "Epoch 4867, Loss Train(MSE): 0.12528093210159977, R2 Train: 0.49887627159360093\n",
      "Epoch 4868, Loss Train(MSE): 0.12528081056978413, R2 Train: 0.4988767577208635\n",
      "Epoch 4869, Loss Train(MSE): 0.12528070175412356, R2 Train: 0.49887719298350575\n",
      "Epoch 4870, Loss Train(MSE): 0.12528059114444162, R2 Train: 0.4988776354222335\n",
      "Epoch 4871, Loss Train(MSE): 0.12528046993744785, R2 Train: 0.4988781202502086\n",
      "Epoch 4872, Loss Train(MSE): 0.1252803632299234, R2 Train: 0.49887854708030643\n",
      "Epoch 4873, Loss Train(MSE): 0.12528025105759022, R2 Train: 0.4988789957696391\n",
      "Epoch 4874, Loss Train(MSE): 0.1252801301735385, R2 Train: 0.498879479305846\n",
      "Epoch 4875, Loss Train(MSE): 0.12528002546968223, R2 Train: 0.4988798981212711\n",
      "Epoch 4876, Loss Train(MSE): 0.12527991183636686, R2 Train: 0.49888035265453257\n",
      "Epoch 4877, Loss Train(MSE): 0.12527979127339978, R2 Train: 0.4988808349064009\n",
      "Epoch 4878, Loss Train(MSE): 0.12527968847032522, R2 Train: 0.4988812461186991\n",
      "Epoch 4879, Loss Train(MSE): 0.12527957347614693, R2 Train: 0.49888170609541227\n",
      "Epoch 4880, Loss Train(MSE): 0.1252794532324286, R2 Train: 0.49888218707028564\n",
      "Epoch 4881, Loss Train(MSE): 0.12527935222880698, R2 Train: 0.49888259108477206\n",
      "Epoch 4882, Loss Train(MSE): 0.12527923597235863, R2 Train: 0.4988830561105655\n",
      "Epoch 4883, Loss Train(MSE): 0.12527911604607467, R2 Train: 0.49888353581570133\n",
      "Epoch 4884, Loss Train(MSE): 0.12527901674211125, R2 Train: 0.498883933031555\n",
      "Epoch 4885, Loss Train(MSE): 0.12527889932048203, R2 Train: 0.4988844027180719\n",
      "Epoch 4886, Loss Train(MSE): 0.12527877970983908, R2 Train: 0.49888488116064367\n",
      "Epoch 4887, Loss Train(MSE): 0.12527868200725, R2 Train: 0.49888527197099997\n",
      "Epoch 4888, Loss Train(MSE): 0.1252785635160483, R2 Train: 0.4988857459358068\n",
      "Epoch 4889, Loss Train(MSE): 0.12527844458326817, R2 Train: 0.4988862216669273\n",
      "Epoch 4890, Loss Train(MSE): 0.12527834765669274, R2 Train: 0.49888660937322904\n",
      "Epoch 4891, Loss Train(MSE): 0.12527822854427503, R2 Train: 0.49888708582289987\n",
      "Epoch 4892, Loss Train(MSE): 0.12527811111968024, R2 Train: 0.49888755552127906\n",
      "Epoch 4893, Loss Train(MSE): 0.1252780132126389, R2 Train: 0.4988879471494444\n",
      "Epoch 4894, Loss Train(MSE): 0.12527789441145504, R2 Train: 0.49888842235417985\n",
      "Epoch 4895, Loss Train(MSE): 0.12527777840014495, R2 Train: 0.4988888863994202\n",
      "Epoch 4896, Loss Train(MSE): 0.1252776796049342, R2 Train: 0.49888928158026324\n",
      "Epoch 4897, Loss Train(MSE): 0.12527756111326027, R2 Train: 0.4988897555469589\n",
      "Epoch 4898, Loss Train(MSE): 0.12527744642177022, R2 Train: 0.49889021431291913\n",
      "Epoch 4899, Loss Train(MSE): 0.12527734682927882, R2 Train: 0.4988906126828847\n",
      "Epoch 4900, Loss Train(MSE): 0.12527722864541033, R2 Train: 0.4988910854183587\n",
      "Epoch 4901, Loss Train(MSE): 0.12527711518169027, R2 Train: 0.4988915392732389\n",
      "Epoch 4902, Loss Train(MSE): 0.12527701488142, R2 Train: 0.49889194047431995\n",
      "Epoch 4903, Loss Train(MSE): 0.12527689700367156, R2 Train: 0.49889241198531376\n",
      "Epoch 4904, Loss Train(MSE): 0.1252767846770649, R2 Train: 0.4988928612917404\n",
      "Epoch 4905, Loss Train(MSE): 0.12527668375715129, R2 Train: 0.49889326497139486\n",
      "Epoch 4906, Loss Train(MSE): 0.12527656618385613, R2 Train: 0.4988937352645755\n",
      "Epoch 4907, Loss Train(MSE): 0.12527645490507908, R2 Train: 0.4988941803796837\n",
      "Epoch 4908, Loss Train(MSE): 0.12527635345231156, R2 Train: 0.4988945861907538\n",
      "Epoch 4909, Loss Train(MSE): 0.1252762361818216, R2 Train: 0.4988950552727136\n",
      "Epoch 4910, Loss Train(MSE): 0.12527612586294248, R2 Train: 0.49889549654823007\n",
      "Epoch 4911, Loss Train(MSE): 0.1252760239627845, R2 Train: 0.498895904148862\n",
      "Epoch 4912, Loss Train(MSE): 0.1252759069934696, R2 Train: 0.4988963720261216\n",
      "Epoch 4913, Loss Train(MSE): 0.1252757975478892, R2 Train: 0.4988968098084432\n",
      "Epoch 4914, Loss Train(MSE): 0.1252756952844976, R2 Train: 0.49889721886200955\n",
      "Epoch 4915, Loss Train(MSE): 0.12527557861474545, R2 Train: 0.4988976855410182\n",
      "Epoch 4916, Loss Train(MSE): 0.125275469957177, R2 Train: 0.498898120171292\n",
      "Epoch 4917, Loss Train(MSE): 0.1252753674134214, R2 Train: 0.4988985303463144\n",
      "Epoch 4918, Loss Train(MSE): 0.1252752510416373, R2 Train: 0.49889899583345076\n",
      "Epoch 4919, Loss Train(MSE): 0.12527514308808724, R2 Train: 0.49889942764765105\n",
      "Epoch 4920, Loss Train(MSE): 0.12527504034556902, R2 Train: 0.4988998386177239\n",
      "Epoch 4921, Loss Train(MSE): 0.1252749242701754, R2 Train: 0.49890030291929843\n",
      "Epoch 4922, Loss Train(MSE): 0.12527481693792406, R2 Train: 0.4989007322483038\n",
      "Epoch 4923, Loss Train(MSE): 0.12527471407699506, R2 Train: 0.49890114369201977\n",
      "Epoch 4924, Loss Train(MSE): 0.1252745982964313, R2 Train: 0.49890160681427476\n",
      "Epoch 4925, Loss Train(MSE): 0.1252744915040144, R2 Train: 0.4989020339839424\n",
      "Epoch 4926, Loss Train(MSE): 0.12527438860379506, R2 Train: 0.4989024455848198\n",
      "Epoch 4927, Loss Train(MSE): 0.12527427311651726, R2 Train: 0.49890290753393096\n",
      "Epoch 4928, Loss Train(MSE): 0.12527416678370723, R2 Train: 0.4989033328651711\n",
      "Epoch 4929, Loss Train(MSE): 0.12527406392210488, R2 Train: 0.49890374431158047\n",
      "Epoch 4930, Loss Train(MSE): 0.12527394872658545, R2 Train: 0.4989042050936582\n",
      "Epoch 4931, Loss Train(MSE): 0.12527384277437348, R2 Train: 0.4989046289025061\n",
      "Epoch 4932, Loss Train(MSE): 0.12527374002810002, R2 Train: 0.49890503988759993\n",
      "Epoch 4933, Loss Train(MSE): 0.1252736251228274, R2 Train: 0.49890549950869045\n",
      "Epoch 4934, Loss Train(MSE): 0.1252735194734054, R2 Train: 0.4989059221063784\n",
      "Epoch 4935, Loss Train(MSE): 0.1252734169179946, R2 Train: 0.4989063323280216\n",
      "Epoch 4936, Loss Train(MSE): 0.12527330230147327, R2 Train: 0.49890679079410694\n",
      "Epoch 4937, Loss Train(MSE): 0.12527319687821642, R2 Train: 0.4989072124871343\n",
      "Epoch 4938, Loss Train(MSE): 0.12527309458804134, R2 Train: 0.49890762164783464\n",
      "Epoch 4939, Loss Train(MSE): 0.12527298025879113, R2 Train: 0.49890807896483547\n",
      "Epoch 4940, Loss Train(MSE): 0.12527287498624054, R2 Train: 0.49890850005503784\n",
      "Epoch 4941, Loss Train(MSE): 0.1252727730345303, R2 Train: 0.4989089078618788\n",
      "Epoch 4942, Loss Train(MSE): 0.12527265899108656, R2 Train: 0.4989093640356538\n",
      "Epoch 4943, Loss Train(MSE): 0.1252725537949322, R2 Train: 0.49890978482027115\n",
      "Epoch 4944, Loss Train(MSE): 0.12527245225378866, R2 Train: 0.49891019098484535\n",
      "Epoch 4945, Loss Train(MSE): 0.12527233849470162, R2 Train: 0.4989106460211935\n",
      "Epoch 4946, Loss Train(MSE): 0.1252722333017659, R2 Train: 0.4989110667929364\n",
      "Epoch 4947, Loss Train(MSE): 0.12527213224217992, R2 Train: 0.4989114710312803\n",
      "Epoch 4948, Loss Train(MSE): 0.1252720187660148, R2 Train: 0.49891192493594083\n",
      "Epoch 4949, Loss Train(MSE): 0.12527191350423564, R2 Train: 0.49891234598305745\n",
      "Epoch 4950, Loss Train(MSE): 0.1252718129961034, R2 Train: 0.4989127480155864\n",
      "Epoch 4951, Loss Train(MSE): 0.12527169980143993, R2 Train: 0.4989132007942403\n",
      "Epoch 4952, Loss Train(MSE): 0.12527159439985497, R2 Train: 0.4989136224005801\n",
      "Epoch 4953, Loss Train(MSE): 0.12527874312077159, R2 Train: 0.49888502751691366\n",
      "Epoch 4954, Loss Train(MSE): 0.12529612890810357, R2 Train: 0.4988154843675857\n",
      "Epoch 4955, Loss Train(MSE): 0.12529586421643604, R2 Train: 0.49881654313425583\n",
      "Epoch 4956, Loss Train(MSE): 0.12529559374381294, R2 Train: 0.49881762502474825\n",
      "Epoch 4957, Loss Train(MSE): 0.1252953193733666, R2 Train: 0.4988187225065336\n",
      "Epoch 4958, Loss Train(MSE): 0.12529506133136922, R2 Train: 0.4988197546745231\n",
      "Epoch 4959, Loss Train(MSE): 0.1252947772736836, R2 Train: 0.4988208909052656\n",
      "Epoch 4960, Loss Train(MSE): 0.12529453155770054, R2 Train: 0.49882187376919784\n",
      "Epoch 4961, Loss Train(MSE): 0.12529424911568512, R2 Train: 0.4988230035372595\n",
      "Epoch 4962, Loss Train(MSE): 0.12529399320192325, R2 Train: 0.498824027192307\n",
      "Epoch 4963, Loss Train(MSE): 0.1252937236424964, R2 Train: 0.49882510543001435\n",
      "Epoch 4964, Loss Train(MSE): 0.12529345739428466, R2 Train: 0.49882617042286137\n",
      "Epoch 4965, Loss Train(MSE): 0.12529320082875195, R2 Train: 0.4988271966849922\n",
      "Epoch 4966, Loss Train(MSE): 0.1252929241122502, R2 Train: 0.49882830355099916\n",
      "Epoch 4967, Loss Train(MSE): 0.12529268064943858, R2 Train: 0.4988292774022457\n",
      "Epoch 4968, Loss Train(MSE): 0.1252924033849243, R2 Train: 0.49883038646030275\n",
      "Epoch 4969, Loss Train(MSE): 0.1252921530766064, R2 Train: 0.4988313876935744\n",
      "Epoch 4970, Loss Train(MSE): 0.12529188727892568, R2 Train: 0.4988324508842973\n",
      "Epoch 4971, Loss Train(MSE): 0.12529162602441626, R2 Train: 0.49883349590233494\n",
      "Epoch 4972, Loss Train(MSE): 0.12529137374446803, R2 Train: 0.4988345050221279\n",
      "Epoch 4973, Loss Train(MSE): 0.12529110142054492, R2 Train: 0.4988355943178203\n",
      "Epoch 4974, Loss Train(MSE): 0.12529086275774737, R2 Train: 0.49883654896901053\n",
      "Epoch 4975, Loss Train(MSE): 0.12529059060217665, R2 Train: 0.4988376375912934\n",
      "Epoch 4976, Loss Train(MSE): 0.12529034298352973, R2 Train: 0.4988386280658811\n",
      "Epoch 4977, Loss Train(MSE): 0.12529008355493235, R2 Train: 0.4988396657802706\n",
      "Epoch 4978, Loss Train(MSE): 0.12528982441940129, R2 Train: 0.49884070232239486\n",
      "Epoch 4979, Loss Train(MSE): 0.12528957899554571, R2 Train: 0.49884168401781714\n",
      "Epoch 4980, Loss Train(MSE): 0.12528931042052394, R2 Train: 0.49884275831790426\n",
      "Epoch 4981, Loss Train(MSE): 0.12528907475631976, R2 Train: 0.49884370097472097\n",
      "Epoch 4982, Loss Train(MSE): 0.12528880970808365, R2 Train: 0.4988447611676654\n",
      "Epoch 4983, Loss Train(MSE): 0.12528856210093658, R2 Train: 0.49884575159625366\n",
      "Epoch 4984, Loss Train(MSE): 0.12528831142567595, R2 Train: 0.4988467542972962\n",
      "Epoch 4985, Loss Train(MSE): 0.12528805176933458, R2 Train: 0.4988477929226617\n",
      "Epoch 4986, Loss Train(MSE): 0.12528781555140414, R2 Train: 0.49884873779438343\n",
      "Epoch 4987, Loss Train(MSE): 0.12528755179643117, R2 Train: 0.49884979281427533\n",
      "Epoch 4988, Loss Train(MSE): 0.1252873140534731, R2 Train: 0.4988507437861076\n",
      "Epoch 4989, Loss Train(MSE): 0.12528705964597278, R2 Train: 0.4988517614161089\n",
      "Epoch 4990, Loss Train(MSE): 0.12528680945513304, R2 Train: 0.49885276217946783\n",
      "Epoch 4991, Loss Train(MSE): 0.12528656984852432, R2 Train: 0.4988537206059027\n",
      "Epoch 4992, Loss Train(MSE): 0.1252863094741351, R2 Train: 0.4988547621034596\n",
      "Epoch 4993, Loss Train(MSE): 0.12528608006490438, R2 Train: 0.4988556797403825\n",
      "Epoch 4994, Loss Train(MSE): 0.1252858233152757, R2 Train: 0.49885670673889715\n",
      "Epoch 4995, Loss Train(MSE): 0.1252855810787715, R2 Train: 0.498857675684914\n",
      "Epoch 4996, Loss Train(MSE): 0.12528533945614448, R2 Train: 0.4988586421754221\n",
      "Epoch 4997, Loss Train(MSE): 0.12528508430130864, R2 Train: 0.49885966279476546\n",
      "Epoch 4998, Loss Train(MSE): 0.12528485787655616, R2 Train: 0.49886056849377536\n",
      "Epoch 4999, Loss Train(MSE): 0.12528460205634667, R2 Train: 0.4988615917746133\n",
      "Epoch 5000, Loss Train(MSE): 0.12528436625677378, R2 Train: 0.4988625349729049\n",
      "Epoch 5001, Loss Train(MSE): 0.12528412400182612, R2 Train: 0.4988635039926955\n",
      "Epoch 5002, Loss Train(MSE): 0.1252838749291792, R2 Train: 0.4988645002832832\n",
      "Epoch 5003, Loss Train(MSE): 0.12528364817599258, R2 Train: 0.4988654072960297\n",
      "Epoch 5004, Loss Train(MSE): 0.12528339555214613, R2 Train: 0.4988664177914155\n",
      "Epoch 5005, Loss Train(MSE): 0.1252831647958842, R2 Train: 0.4988673408164632\n",
      "Epoch 5006, Loss Train(MSE): 0.12528292317272394, R2 Train: 0.49886830730910425\n",
      "Epoch 5007, Loss Train(MSE): 0.12528267880605526, R2 Train: 0.49886928477577897\n",
      "Epoch 5008, Loss Train(MSE): 0.12528245297279245, R2 Train: 0.4988701881088302\n",
      "Epoch 5009, Loss Train(MSE): 0.125282203472352, R2 Train: 0.498871186110592\n",
      "Epoch 5010, Loss Train(MSE): 0.12528197642005276, R2 Train: 0.49887209431978896\n",
      "Epoch 5011, Loss Train(MSE): 0.12528173664273032, R2 Train: 0.4988730534290787\n",
      "Epoch 5012, Loss Train(MSE): 0.1252814956593663, R2 Train: 0.4988740173625348\n",
      "Epoch 5013, Loss Train(MSE): 0.12528127194499222, R2 Train: 0.4988749122200311\n",
      "Epoch 5014, Loss Train(MSE): 0.12528102549738376, R2 Train: 0.49887589801046495\n",
      "Epoch 5015, Loss Train(MSE): 0.12528080086182153, R2 Train: 0.4988767965527139\n",
      "Epoch 5016, Loss Train(MSE): 0.12528056409630103, R2 Train: 0.49887774361479587\n",
      "Epoch 5017, Loss Train(MSE): 0.12528032522498933, R2 Train: 0.4988786991000427\n",
      "Epoch 5018, Loss Train(MSE): 0.12528010478101687, R2 Train: 0.49887958087593254\n",
      "Epoch 5019, Loss Train(MSE): 0.12527986131794913, R2 Train: 0.4988805547282035\n",
      "Epoch 5020, Loss Train(MSE): 0.12527963786196747, R2 Train: 0.4988814485521301\n",
      "Epoch 5021, Loss Train(MSE): 0.1252794052280105, R2 Train: 0.49888237908795796\n",
      "Epoch 5022, Loss Train(MSE): 0.1252791672468993, R2 Train: 0.4988833310124028\n",
      "Epoch 5023, Loss Train(MSE): 0.12527895117924417, R2 Train: 0.4988841952830233\n",
      "Epoch 5024, Loss Train(MSE): 0.12527871063461368, R2 Train: 0.49888515746154527\n",
      "Epoch 5025, Loss Train(MSE): 0.125278487169164, R2 Train: 0.49888605132334396\n",
      "Epoch 5026, Loss Train(MSE): 0.12527893110106939, R2 Train: 0.49888427559572246\n",
      "Epoch 5027, Loss Train(MSE): 0.12528296547508616, R2 Train: 0.49886813809965536\n",
      "Epoch 5028, Loss Train(MSE): 0.12528270684615134, R2 Train: 0.49886917261539465\n",
      "Epoch 5029, Loss Train(MSE): 0.1252824487604149, R2 Train: 0.49887020495834045\n",
      "Epoch 5030, Loss Train(MSE): 0.12528219121591977, R2 Train: 0.4988712351363209\n",
      "Epoch 5031, Loss Train(MSE): 0.12528193421071906, R2 Train: 0.49887226315712374\n",
      "Epoch 5032, Loss Train(MSE): 0.1252816777428764, R2 Train: 0.49887328902849437\n",
      "Epoch 5033, Loss Train(MSE): 0.12528142181046542, R2 Train: 0.4988743127581383\n",
      "Epoch 5034, Loss Train(MSE): 0.12528116641156997, R2 Train: 0.4988753343537201\n",
      "Epoch 5035, Loss Train(MSE): 0.12528091154428397, R2 Train: 0.4988763538228641\n",
      "Epoch 5036, Loss Train(MSE): 0.12528065720671133, R2 Train: 0.49887737117315467\n",
      "Epoch 5037, Loss Train(MSE): 0.12528040339696586, R2 Train: 0.49887838641213655\n",
      "Epoch 5038, Loss Train(MSE): 0.1252801501131712, R2 Train: 0.4988793995473152\n",
      "Epoch 5039, Loss Train(MSE): 0.1252798973534609, R2 Train: 0.4988804105861564\n",
      "Epoch 5040, Loss Train(MSE): 0.1252796451159781, R2 Train: 0.49888141953608756\n",
      "Epoch 5041, Loss Train(MSE): 0.12527939339887564, R2 Train: 0.4988824264044974\n",
      "Epoch 5042, Loss Train(MSE): 0.12527914220031586, R2 Train: 0.49888343119873657\n",
      "Epoch 5043, Loss Train(MSE): 0.12527889151847077, R2 Train: 0.4988844339261169\n",
      "Epoch 5044, Loss Train(MSE): 0.12527864135152167, R2 Train: 0.4988854345939133\n",
      "Epoch 5045, Loss Train(MSE): 0.12527839169765936, R2 Train: 0.49888643320936255\n",
      "Epoch 5046, Loss Train(MSE): 0.1252781425550839, R2 Train: 0.4988874297796644\n",
      "Epoch 5047, Loss Train(MSE): 0.12527789392200456, R2 Train: 0.49888842431198177\n",
      "Epoch 5048, Loss Train(MSE): 0.1252776457966399, R2 Train: 0.49888941681344035\n",
      "Epoch 5049, Loss Train(MSE): 0.12527739817721753, R2 Train: 0.4988904072911299\n",
      "Epoch 5050, Loss Train(MSE): 0.12527715106197418, R2 Train: 0.4988913957521033\n",
      "Epoch 5051, Loss Train(MSE): 0.12527690444915543, R2 Train: 0.4988923822033783\n",
      "Epoch 5052, Loss Train(MSE): 0.125276658337016, R2 Train: 0.49889336665193595\n",
      "Epoch 5053, Loss Train(MSE): 0.1252764127238194, R2 Train: 0.4988943491047224\n",
      "Epoch 5054, Loss Train(MSE): 0.12527616760783788, R2 Train: 0.49889532956864846\n",
      "Epoch 5055, Loss Train(MSE): 0.1252759229873526, R2 Train: 0.4988963080505896\n",
      "Epoch 5056, Loss Train(MSE): 0.12527576755690148, R2 Train: 0.4988969297723941\n",
      "Epoch 5057, Loss Train(MSE): 0.1252756290298314, R2 Train: 0.49889748388067445\n",
      "Epoch 5058, Loss Train(MSE): 0.12527549063321095, R2 Train: 0.4988980374671562\n",
      "Epoch 5059, Loss Train(MSE): 0.1252753523668584, R2 Train: 0.49889859053256636\n",
      "Epoch 5060, Loss Train(MSE): 0.1252752142305925, R2 Train: 0.49889914307762995\n",
      "Epoch 5061, Loss Train(MSE): 0.12527507622423226, R2 Train: 0.498899695103071\n",
      "Epoch 5062, Loss Train(MSE): 0.12527493834759693, R2 Train: 0.4989002466096123\n",
      "Epoch 5063, Loss Train(MSE): 0.12527480060050622, R2 Train: 0.4989007975979751\n",
      "Epoch 5064, Loss Train(MSE): 0.12527466298278023, R2 Train: 0.4989013480688791\n",
      "Epoch 5065, Loss Train(MSE): 0.12527452549423926, R2 Train: 0.49890189802304297\n",
      "Epoch 5066, Loss Train(MSE): 0.12527438813470404, R2 Train: 0.49890244746118384\n",
      "Epoch 5067, Loss Train(MSE): 0.1252742509039957, R2 Train: 0.49890299638401725\n",
      "Epoch 5068, Loss Train(MSE): 0.1252741138019355, R2 Train: 0.49890354479225796\n",
      "Epoch 5069, Loss Train(MSE): 0.12527397682834523, R2 Train: 0.49890409268661906\n",
      "Epoch 5070, Loss Train(MSE): 0.125273839983047, R2 Train: 0.498904640067812\n",
      "Epoch 5071, Loss Train(MSE): 0.12527370326586315, R2 Train: 0.4989051869365474\n",
      "Epoch 5072, Loss Train(MSE): 0.1252735666766165, R2 Train: 0.49890573329353405\n",
      "Epoch 5073, Loss Train(MSE): 0.12527343021513007, R2 Train: 0.4989062791394797\n",
      "Epoch 5074, Loss Train(MSE): 0.12527329388122724, R2 Train: 0.49890682447509105\n",
      "Epoch 5075, Loss Train(MSE): 0.12527315767473182, R2 Train: 0.4989073693010727\n",
      "Epoch 5076, Loss Train(MSE): 0.12527302159546783, R2 Train: 0.4989079136181287\n",
      "Epoch 5077, Loss Train(MSE): 0.12527288564325972, R2 Train: 0.4989084574269611\n",
      "Epoch 5078, Loss Train(MSE): 0.1252727498179322, R2 Train: 0.49890900072827116\n",
      "Epoch 5079, Loss Train(MSE): 0.12527261411931032, R2 Train: 0.49890954352275874\n",
      "Epoch 5080, Loss Train(MSE): 0.1252724785472195, R2 Train: 0.49891008581112195\n",
      "Epoch 5081, Loss Train(MSE): 0.1252723431014855, R2 Train: 0.498910627594058\n",
      "Epoch 5082, Loss Train(MSE): 0.1252722077819343, R2 Train: 0.49891116887226283\n",
      "Epoch 5083, Loss Train(MSE): 0.1252720725883923, R2 Train: 0.49891170964643083\n",
      "Epoch 5084, Loss Train(MSE): 0.12527193752068622, R2 Train: 0.49891224991725514\n",
      "Epoch 5085, Loss Train(MSE): 0.12527180257864307, R2 Train: 0.49891278968542774\n",
      "Epoch 5086, Loss Train(MSE): 0.12527166776209017, R2 Train: 0.4989133289516393\n",
      "Epoch 5087, Loss Train(MSE): 0.12527153307085526, R2 Train: 0.498913867716579\n",
      "Epoch 5088, Loss Train(MSE): 0.12527139850476626, R2 Train: 0.49891440598093495\n",
      "Epoch 5089, Loss Train(MSE): 0.12527126406365158, R2 Train: 0.4989149437453937\n",
      "Epoch 5090, Loss Train(MSE): 0.12527112974733975, R2 Train: 0.498915481010641\n",
      "Epoch 5091, Loss Train(MSE): 0.12527099555565985, R2 Train: 0.4989160177773606\n",
      "Epoch 5092, Loss Train(MSE): 0.12527086148844105, R2 Train: 0.4989165540462358\n",
      "Epoch 5093, Loss Train(MSE): 0.12527072754551294, R2 Train: 0.49891708981794824\n",
      "Epoch 5094, Loss Train(MSE): 0.12527059372670551, R2 Train: 0.49891762509317794\n",
      "Epoch 5095, Loss Train(MSE): 0.12527046003184897, R2 Train: 0.49891815987260413\n",
      "Epoch 5096, Loss Train(MSE): 0.12527032646077382, R2 Train: 0.4989186941569047\n",
      "Epoch 5097, Loss Train(MSE): 0.12527019301331094, R2 Train: 0.49891922794675625\n",
      "Epoch 5098, Loss Train(MSE): 0.1252700596892915, R2 Train: 0.498919761242834\n",
      "Epoch 5099, Loss Train(MSE): 0.12526992648854698, R2 Train: 0.49892029404581206\n",
      "Epoch 5100, Loss Train(MSE): 0.12526979341090919, R2 Train: 0.49892082635636326\n",
      "Epoch 5101, Loss Train(MSE): 0.12526966045621024, R2 Train: 0.49892135817515904\n",
      "Epoch 5102, Loss Train(MSE): 0.12526952762428256, R2 Train: 0.49892188950286975\n",
      "Epoch 5103, Loss Train(MSE): 0.1252693949149589, R2 Train: 0.49892242034016443\n",
      "Epoch 5104, Loss Train(MSE): 0.12526926232807223, R2 Train: 0.4989229506877111\n",
      "Epoch 5105, Loss Train(MSE): 0.12526912986345598, R2 Train: 0.4989234805461761\n",
      "Epoch 5106, Loss Train(MSE): 0.12526899752094373, R2 Train: 0.4989240099162251\n",
      "Epoch 5107, Loss Train(MSE): 0.12526886530036951, R2 Train: 0.49892453879852194\n",
      "Epoch 5108, Loss Train(MSE): 0.12526873320156756, R2 Train: 0.49892506719372975\n",
      "Epoch 5109, Loss Train(MSE): 0.1252686012243725, R2 Train: 0.49892559510251\n",
      "Epoch 5110, Loss Train(MSE): 0.12526846936861913, R2 Train: 0.4989261225255235\n",
      "Epoch 5111, Loss Train(MSE): 0.12526833763414272, R2 Train: 0.49892664946342913\n",
      "Epoch 5112, Loss Train(MSE): 0.12526820602077868, R2 Train: 0.4989271759168853\n",
      "Epoch 5113, Loss Train(MSE): 0.12526807452836286, R2 Train: 0.49892770188654856\n",
      "Epoch 5114, Loss Train(MSE): 0.12526794315673137, R2 Train: 0.4989282273730745\n",
      "Epoch 5115, Loss Train(MSE): 0.1252678119057206, R2 Train: 0.49892875237711765\n",
      "Epoch 5116, Loss Train(MSE): 0.12526768077516712, R2 Train: 0.49892927689933153\n",
      "Epoch 5117, Loss Train(MSE): 0.1252675497649081, R2 Train: 0.49892980094036765\n",
      "Epoch 5118, Loss Train(MSE): 0.12526741887478074, R2 Train: 0.49893032450087704\n",
      "Epoch 5119, Loss Train(MSE): 0.12526728810462268, R2 Train: 0.4989308475815093\n",
      "Epoch 5120, Loss Train(MSE): 0.12526715745427172, R2 Train: 0.4989313701829131\n",
      "Epoch 5121, Loss Train(MSE): 0.12526702692356614, R2 Train: 0.4989318923057354\n",
      "Epoch 5122, Loss Train(MSE): 0.12526689651234446, R2 Train: 0.49893241395062216\n",
      "Epoch 5123, Loss Train(MSE): 0.12526676622044533, R2 Train: 0.4989329351182187\n",
      "Epoch 5124, Loss Train(MSE): 0.12526663604770782, R2 Train: 0.4989334558091687\n",
      "Epoch 5125, Loss Train(MSE): 0.1252665059939715, R2 Train: 0.49893397602411405\n",
      "Epoch 5126, Loss Train(MSE): 0.12526637605907578, R2 Train: 0.49893449576369686\n",
      "Epoch 5127, Loss Train(MSE): 0.12526624624286078, R2 Train: 0.49893501502855686\n",
      "Epoch 5128, Loss Train(MSE): 0.1252661165451667, R2 Train: 0.4989355338193332\n",
      "Epoch 5129, Loss Train(MSE): 0.12526598696583402, R2 Train: 0.4989360521366639\n",
      "Epoch 5130, Loss Train(MSE): 0.12526585750470362, R2 Train: 0.4989365699811855\n",
      "Epoch 5131, Loss Train(MSE): 0.12526572816161663, R2 Train: 0.4989370873535335\n",
      "Epoch 5132, Loss Train(MSE): 0.12526559893641437, R2 Train: 0.4989376042543425\n",
      "Epoch 5133, Loss Train(MSE): 0.1252654698289386, R2 Train: 0.4989381206842456\n",
      "Epoch 5134, Loss Train(MSE): 0.12526534083903132, R2 Train: 0.4989386366438747\n",
      "Epoch 5135, Loss Train(MSE): 0.12526521196653478, R2 Train: 0.49893915213386086\n",
      "Epoch 5136, Loss Train(MSE): 0.12526508321129146, R2 Train: 0.4989396671548342\n",
      "Epoch 5137, Loss Train(MSE): 0.12526495457314427, R2 Train: 0.49894018170742294\n",
      "Epoch 5138, Loss Train(MSE): 0.1252648260519363, R2 Train: 0.49894069579225475\n",
      "Epoch 5139, Loss Train(MSE): 0.12526469764751103, R2 Train: 0.4989412094099559\n",
      "Epoch 5140, Loss Train(MSE): 0.12526456935971206, R2 Train: 0.49894172256115177\n",
      "Epoch 5141, Loss Train(MSE): 0.12526444118838342, R2 Train: 0.4989422352464663\n",
      "Epoch 5142, Loss Train(MSE): 0.12526431313336928, R2 Train: 0.4989427474665229\n",
      "Epoch 5143, Loss Train(MSE): 0.12526418519451427, R2 Train: 0.49894325922194294\n",
      "Epoch 5144, Loss Train(MSE): 0.1252640573716632, R2 Train: 0.49894377051334715\n",
      "Epoch 5145, Loss Train(MSE): 0.12526392966466113, R2 Train: 0.4989442813413555\n",
      "Epoch 5146, Loss Train(MSE): 0.12526380207335341, R2 Train: 0.49894479170658634\n",
      "Epoch 5147, Loss Train(MSE): 0.12526370505145779, R2 Train: 0.49894517979416886\n",
      "Epoch 5148, Loss Train(MSE): 0.12526359615542393, R2 Train: 0.4989456153783043\n",
      "Epoch 5149, Loss Train(MSE): 0.125263499732303, R2 Train: 0.498946001070788\n",
      "Epoch 5150, Loss Train(MSE): 0.12526340003237263, R2 Train: 0.4989463998705095\n",
      "Epoch 5151, Loss Train(MSE): 0.12526329142142298, R2 Train: 0.4989468343143081\n",
      "Epoch 5152, Loss Train(MSE): 0.1252631980486184, R2 Train: 0.49894720780552637\n",
      "Epoch 5153, Loss Train(MSE): 0.12526309577295774, R2 Train: 0.498947616908169\n",
      "Epoch 5154, Loss Train(MSE): 0.1252629874454085, R2 Train: 0.49894805021836597\n",
      "Epoch 5155, Loss Train(MSE): 0.125262897019586, R2 Train: 0.49894841192165595\n",
      "Epoch 5156, Loss Train(MSE): 0.12526279226903347, R2 Train: 0.4989488309238661\n",
      "Epoch 5157, Loss Train(MSE): 0.12526268422322115, R2 Train: 0.4989492631071154\n",
      "Epoch 5158, Loss Train(MSE): 0.1252625966425202, R2 Train: 0.49894961342991917\n",
      "Epoch 5159, Loss Train(MSE): 0.12526248951646982, R2 Train: 0.49895004193412074\n",
      "Epoch 5160, Loss Train(MSE): 0.12526238356052205, R2 Train: 0.4989504657579118\n",
      "Epoch 5161, Loss Train(MSE): 0.1252622951033838, R2 Train: 0.4989508195864648\n",
      "Epoch 5162, Loss Train(MSE): 0.12526218750162163, R2 Train: 0.4989512499935135\n",
      "Epoch 5163, Loss Train(MSE): 0.1252620842836397, R2 Train: 0.49895166286544124\n",
      "Epoch 5164, Loss Train(MSE): 0.12526199355440457, R2 Train: 0.49895202578238174\n",
      "Epoch 5165, Loss Train(MSE): 0.12526188623028806, R2 Train: 0.4989524550788478\n",
      "Epoch 5166, Loss Train(MSE): 0.12526178565177679, R2 Train: 0.49895285739289286\n",
      "Epoch 5167, Loss Train(MSE): 0.12526169274654364, R2 Train: 0.49895322901382544\n",
      "Epoch 5168, Loss Train(MSE): 0.12526158569847662, R2 Train: 0.4989536572060935\n",
      "Epoch 5169, Loss Train(MSE): 0.12526148766234296, R2 Train: 0.49895404935062815\n",
      "Epoch 5170, Loss Train(MSE): 0.125261392675836, R2 Train: 0.498954429296656\n",
      "Epoch 5171, Loss Train(MSE): 0.12526128590224103, R2 Train: 0.4989548563910359\n",
      "Epoch 5172, Loss Train(MSE): 0.1252611903127738, R2 Train: 0.4989552387489048\n",
      "Epoch 5173, Loss Train(MSE): 0.1252610933383623, R2 Train: 0.4989556266465508\n",
      "Epoch 5174, Loss Train(MSE): 0.12526098683768036, R2 Train: 0.49895605264927856\n",
      "Epoch 5175, Loss Train(MSE): 0.12526089360053017, R2 Train: 0.49895642559787934\n",
      "Epoch 5176, Loss Train(MSE): 0.12526079473024815, R2 Train: 0.4989568210790074\n",
      "Epoch 5177, Loss Train(MSE): 0.12526068850093833, R2 Train: 0.4989572459962467\n",
      "Epoch 5178, Loss Train(MSE): 0.1252605975230979, R2 Train: 0.4989576099076084\n",
      "Epoch 5179, Loss Train(MSE): 0.12526049684766324, R2 Train: 0.498958012609347\n",
      "Epoch 5180, Loss Train(MSE): 0.1252603908882023, R2 Train: 0.49895843644719085\n",
      "Epoch 5181, Loss Train(MSE): 0.12526030207798727, R2 Train: 0.4989587916880509\n",
      "Epoch 5182, Loss Train(MSE): 0.12526019968682053, R2 Train: 0.49895920125271787\n",
      "Epoch 5183, Loss Train(MSE): 0.1252600939957027, R2 Train: 0.4989596240171892\n",
      "Epoch 5184, Loss Train(MSE): 0.12526000726273245, R2 Train: 0.4989599709490702\n",
      "Epoch 5185, Loss Train(MSE): 0.12525990324397565, R2 Train: 0.4989603870240974\n",
      "Epoch 5186, Loss Train(MSE): 0.1252597981193421, R2 Train: 0.4989608075226316\n",
      "Epoch 5187, Loss Train(MSE): 0.1252597127742303, R2 Train: 0.49896114890307885\n",
      "Epoch 5188, Loss Train(MSE): 0.1252596075064384, R2 Train: 0.49896156997424645\n",
      "Epoch 5189, Loss Train(MSE): 0.1252595043682206, R2 Train: 0.49896198252711765\n",
      "Epoch 5190, Loss Train(MSE): 0.1252594174829349, R2 Train: 0.4989623300682604\n",
      "Epoch 5191, Loss Train(MSE): 0.12525931247978217, R2 Train: 0.4989627500808713\n",
      "Epoch 5192, Loss Train(MSE): 0.12525921124055217, R2 Train: 0.4989631550377913\n",
      "Epoch 5193, Loss Train(MSE): 0.12525912290034166, R2 Train: 0.49896350839863335\n",
      "Epoch 5194, Loss Train(MSE): 0.12525901816038024, R2 Train: 0.49896392735847905\n",
      "Epoch 5195, Loss Train(MSE): 0.12525891873395184, R2 Train: 0.49896432506419264\n",
      "Epoch 5196, Loss Train(MSE): 0.1252588290228475, R2 Train: 0.49896468390861004\n",
      "Epoch 5197, Loss Train(MSE): 0.12525872454464568, R2 Train: 0.4989651018214173\n",
      "Epoch 5198, Loss Train(MSE): 0.1252586268460567, R2 Train: 0.49896549261577317\n",
      "Epoch 5199, Loss Train(MSE): 0.1252585358468886, R2 Train: 0.49896585661244564\n",
      "Epoch 5200, Loss Train(MSE): 0.12525843162903058, R2 Train: 0.4989662734838777\n",
      "Epoch 5201, Loss Train(MSE): 0.12525833557452531, R2 Train: 0.49896665770189874\n",
      "Epoch 5202, Loss Train(MSE): 0.12525824336893981, R2 Train: 0.49896702652424074\n",
      "Epoch 5203, Loss Train(MSE): 0.12525813941002537, R2 Train: 0.49896744235989854\n",
      "Epoch 5204, Loss Train(MSE): 0.12525804491703774, R2 Train: 0.49896782033184905\n",
      "Epoch 5205, Loss Train(MSE): 0.125257951585514, R2 Train: 0.498968193657944\n",
      "Epoch 5206, Loss Train(MSE): 0.12525784788415836, R2 Train: 0.49896860846336655\n",
      "Epoch 5207, Loss Train(MSE): 0.1252577548712946, R2 Train: 0.49896898051482164\n",
      "Epoch 5208, Loss Train(MSE): 0.12525766049316134, R2 Train: 0.49896935802735465\n",
      "Epoch 5209, Loss Train(MSE): 0.1252575570479947, R2 Train: 0.49896977180802116\n",
      "Epoch 5210, Loss Train(MSE): 0.12525746543501717, R2 Train: 0.4989701382599313\n",
      "Epoch 5211, Loss Train(MSE): 0.12525737008846866, R2 Train: 0.4989705196461254\n",
      "Epoch 5212, Loss Train(MSE): 0.1252572668981362, R2 Train: 0.49897093240745516\n",
      "Epoch 5213, Loss Train(MSE): 0.1252571766059468, R2 Train: 0.49897129357621284\n",
      "Epoch 5214, Loss Train(MSE): 0.12525708036805888, R2 Train: 0.4989716785277645\n",
      "Epoch 5215, Loss Train(MSE): 0.12525697743122025, R2 Train: 0.498972090275119\n",
      "Epoch 5216, Loss Train(MSE): 0.1252568883818445, R2 Train: 0.49897244647262196\n",
      "Epoch 5217, Loss Train(MSE): 0.12525679132859038, R2 Train: 0.49897283468563847\n",
      "Epoch 5218, Loss Train(MSE): 0.12525668864391976, R2 Train: 0.498973245424321\n",
      "Epoch 5219, Loss Train(MSE): 0.1252566007604909, R2 Train: 0.49897359695803645\n",
      "Epoch 5220, Loss Train(MSE): 0.12525650296675644, R2 Train: 0.4989739881329742\n",
      "Epoch 5221, Loss Train(MSE): 0.12525640053294193, R2 Train: 0.4989743978682323\n",
      "Epoch 5222, Loss Train(MSE): 0.12525631373968543, R2 Train: 0.49897474504125827\n",
      "Epoch 5223, Loss Train(MSE): 0.12525621527928454, R2 Train: 0.4989751388828618\n",
      "Epoch 5224, Loss Train(MSE): 0.12525611309502824, R2 Train: 0.49897554761988705\n",
      "Epoch 5225, Loss Train(MSE): 0.1252560273172463, R2 Train: 0.4989758907310148\n",
      "Epoch 5226, Loss Train(MSE): 0.12525592826293583, R2 Train: 0.4989762869482567\n",
      "Epoch 5227, Loss Train(MSE): 0.1252558263269536, R2 Train: 0.49897669469218564\n",
      "Epoch 5228, Loss Train(MSE): 0.12525574149101032, R2 Train: 0.4989770340359587\n",
      "Epoch 5229, Loss Train(MSE): 0.12525564191450472, R2 Train: 0.4989774323419811\n",
      "Epoch 5230, Loss Train(MSE): 0.1252555402255257, R2 Train: 0.49897783909789717\n",
      "Epoch 5231, Loss Train(MSE): 0.12525545625883205, R2 Train: 0.4989781749646718\n",
      "Epoch 5232, Loss Train(MSE): 0.12525535623081818, R2 Train: 0.49897857507672727\n",
      "Epoch 5233, Loss Train(MSE): 0.1252552547875848, R2 Train: 0.49897898084966075\n",
      "Epoch 5234, Loss Train(MSE): 0.12525517161858418, R2 Train: 0.49897931352566327\n",
      "Epoch 5235, Loss Train(MSE): 0.12525507120873514, R2 Train: 0.49897971516505946\n",
      "Epoch 5236, Loss Train(MSE): 0.12525497001000285, R2 Train: 0.4989801199599886\n",
      "Epoch 5237, Loss Train(MSE): 0.1252548875681565, R2 Train: 0.498980449727374\n",
      "Epoch 5238, Loss Train(MSE): 0.12525478684514613, R2 Train: 0.4989808526194155\n",
      "Epoch 5239, Loss Train(MSE): 0.12525468605374368, R2 Train: 0.4989812557850253\n",
      "Epoch 5240, Loss Train(MSE): 0.125254603941353, R2 Train: 0.49898158423458805\n",
      "Epoch 5241, Loss Train(MSE): 0.12525450312903727, R2 Train: 0.4989819874838509\n",
      "Epoch 5242, Loss Train(MSE): 0.12525440300191662, R2 Train: 0.4989823879923335\n",
      "Epoch 5243, Loss Train(MSE): 0.125254320636384, R2 Train: 0.498982717454464\n",
      "Epoch 5244, Loss Train(MSE): 0.12525422006550618, R2 Train: 0.49898311973797527\n",
      "Epoch 5245, Loss Train(MSE): 0.12525412053439022, R2 Train: 0.4989835178624391\n",
      "Epoch 5246, Loss Train(MSE): 0.12525403798217188, R2 Train: 0.49898384807131246\n",
      "Epoch 5247, Loss Train(MSE): 0.12525393765153048, R2 Train: 0.4989842493938781\n",
      "Epoch 5248, Loss Train(MSE): 0.12525383864911374, R2 Train: 0.49898464540354504\n",
      "Epoch 5249, Loss Train(MSE): 0.12525375597571184, R2 Train: 0.49898497609715264\n",
      "Epoch 5250, Loss Train(MSE): 0.12525365588411755, R2 Train: 0.4989853764635298\n",
      "Epoch 5251, Loss Train(MSE): 0.12525355734405258, R2 Train: 0.4989857706237897\n",
      "Epoch 5252, Loss Train(MSE): 0.12525347461402847, R2 Train: 0.4989861015438861\n",
      "Epoch 5253, Loss Train(MSE): 0.1252533747603039, R2 Train: 0.4989865009587844\n",
      "Epoch 5254, Loss Train(MSE): 0.12525327661718805, R2 Train: 0.4989868935312478\n",
      "Epoch 5255, Loss Train(MSE): 0.12525319389417522, R2 Train: 0.4989872244232991\n",
      "Epoch 5256, Loss Train(MSE): 0.1252530942771547, R2 Train: 0.4989876228913812\n",
      "Epoch 5257, Loss Train(MSE): 0.1252529964665172, R2 Train: 0.49898801413393123\n",
      "Epoch 5258, Loss Train(MSE): 0.1252529138132339, R2 Train: 0.49898834474706444\n",
      "Epoch 5259, Loss Train(MSE): 0.1252528144317634, R2 Train: 0.49898874227294643\n",
      "Epoch 5260, Loss Train(MSE): 0.12525271689005255, R2 Train: 0.4989891324397898\n",
      "Epoch 5261, Loss Train(MSE): 0.12525263436831438, R2 Train: 0.4989894625267425\n",
      "Epoch 5262, Loss Train(MSE): 0.12525253522125124, R2 Train: 0.49898985911499505\n",
      "Epoch 5263, Loss Train(MSE): 0.12525243788582155, R2 Train: 0.4989902484567138\n",
      "Epoch 5264, Loss Train(MSE): 0.12525235555655398, R2 Train: 0.4989905777737841\n",
      "Epoch 5265, Loss Train(MSE): 0.12525225664276676, R2 Train: 0.49899097342893295\n",
      "Epoch 5266, Loss Train(MSE): 0.1252521594518669, R2 Train: 0.4989913621925324\n",
      "Epoch 5267, Loss Train(MSE): 0.12525207737511718, R2 Train: 0.4989916904995313\n",
      "Epoch 5268, Loss Train(MSE): 0.12525197869348545, R2 Train: 0.4989920852260582\n",
      "Epoch 5269, Loss Train(MSE): 0.12525188158624573, R2 Train: 0.4989924736550171\n",
      "Epoch 5270, Loss Train(MSE): 0.12525179982119508, R2 Train: 0.4989928007152197\n",
      "Epoch 5271, Loss Train(MSE): 0.1252517013706093, R2 Train: 0.49899319451756285\n",
      "Epoch 5272, Loss Train(MSE): 0.12525160428702972, R2 Train: 0.4989935828518811\n",
      "Epoch 5273, Loss Train(MSE): 0.12525152289200503, R2 Train: 0.4989939084319799\n",
      "Epoch 5274, Loss Train(MSE): 0.12525142467136638, R2 Train: 0.4989943013145345\n",
      "Epoch 5275, Loss Train(MSE): 0.12525132755230484, R2 Train: 0.49899468979078065\n",
      "Epoch 5276, Loss Train(MSE): 0.12525124658479037, R2 Train: 0.49899501366083854\n",
      "Epoch 5277, Loss Train(MSE): 0.12525114859301048, R2 Train: 0.4989954056279581\n",
      "Epoch 5278, Loss Train(MSE): 0.1252510513801709, R2 Train: 0.4989957944793164\n",
      "Epoch 5279, Loss Train(MSE): 0.12525097089681958, R2 Train: 0.4989961164127217\n",
      "Epoch 5280, Loss Train(MSE): 0.1252508731328206, R2 Train: 0.4989965074687176\n",
      "Epoch 5281, Loss Train(MSE): 0.12525077576874166, R2 Train: 0.49899689692503335\n",
      "Epoch 5282, Loss Train(MSE): 0.12525069582538656, R2 Train: 0.49899721669845376\n",
      "Epoch 5283, Loss Train(MSE): 0.12525059828810078, R2 Train: 0.49899760684759686\n",
      "Epoch 5284, Loss Train(MSE): 0.12525050084297432, R2 Train: 0.49899799662810274\n",
      "Epoch 5285, Loss Train(MSE): 0.12525042124035057, R2 Train: 0.4989983150385977\n",
      "Epoch 5286, Loss Train(MSE): 0.1252503240633388, R2 Train: 0.4989987037466448\n",
      "Epoch 5287, Loss Train(MSE): 0.12525022684334697, R2 Train: 0.49899909262661213\n",
      "Epoch 5288, Loss Train(MSE): 0.1252501469096129, R2 Train: 0.49899941236154843\n",
      "Epoch 5289, Loss Train(MSE): 0.12525005044854465, R2 Train: 0.4989997982058214\n",
      "Epoch 5290, Loss Train(MSE): 0.1252499534526455, R2 Train: 0.499000186189418\n",
      "Epoch 5291, Loss Train(MSE): 0.12524987313338842, R2 Train: 0.4990005074664463\n",
      "Epoch 5292, Loss Train(MSE): 0.12524977744109847, R2 Train: 0.4990008902356061\n",
      "Epoch 5293, Loss Train(MSE): 0.12524968066825967, R2 Train: 0.4990012773269613\n",
      "Epoch 5294, Loss Train(MSE): 0.1252495999098498, R2 Train: 0.4990016003606008\n",
      "Epoch 5295, Loss Train(MSE): 0.1252495050384037, R2 Train: 0.49900197984638517\n",
      "Epoch 5296, Loss Train(MSE): 0.12524940848760255, R2 Train: 0.4990023660495898\n",
      "Epoch 5297, Loss Train(MSE): 0.1252493272371824, R2 Train: 0.4990026910512704\n",
      "Epoch 5298, Loss Train(MSE): 0.12524923323788675, R2 Train: 0.499003067048453\n",
      "Epoch 5299, Loss Train(MSE): 0.1252491369081099, R2 Train: 0.4990034523675604\n",
      "Epoch 5300, Loss Train(MSE): 0.12524905511358395, R2 Train: 0.4990037795456642\n",
      "Epoch 5301, Loss Train(MSE): 0.12524896203699673, R2 Train: 0.4990041518520131\n",
      "Epoch 5302, Loss Train(MSE): 0.12524886592724016, R2 Train: 0.49900453629103936\n",
      "Epoch 5303, Loss Train(MSE): 0.1252487835372645, R2 Train: 0.49900486585094195\n",
      "Epoch 5304, Loss Train(MSE): 0.12525079965184516, R2 Train: 0.49899680139261937\n",
      "Epoch 5305, Loss Train(MSE): 0.1252717052444538, R2 Train: 0.4989131790221848\n",
      "Epoch 5306, Loss Train(MSE): 0.1252714877416872, R2 Train: 0.4989140490332512\n",
      "Epoch 5307, Loss Train(MSE): 0.12527124916208426, R2 Train: 0.49891500335166294\n",
      "Epoch 5308, Loss Train(MSE): 0.12527102305434243, R2 Train: 0.4989159077826303\n",
      "Epoch 5309, Loss Train(MSE): 0.12527079526005522, R2 Train: 0.49891681895977913\n",
      "Epoch 5310, Loss Train(MSE): 0.125270560434786, R2 Train: 0.49891775826085605\n",
      "Epoch 5311, Loss Train(MSE): 0.12527034351892527, R2 Train: 0.4989186259242989\n",
      "Epoch 5312, Loss Train(MSE): 0.125270102531158, R2 Train: 0.498919589875368\n",
      "Epoch 5313, Loss Train(MSE): 0.12526989129347113, R2 Train: 0.4989204348261155\n",
      "Epoch 5314, Loss Train(MSE): 0.12526965413359478, R2 Train: 0.49892138346562087\n",
      "Epoch 5315, Loss Train(MSE): 0.1252694337791514, R2 Train: 0.4989222648833944\n",
      "Epoch 5316, Loss Train(MSE): 0.1252692078480032, R2 Train: 0.49892316860798724\n",
      "Epoch 5317, Loss Train(MSE): 0.1252689782725465, R2 Train: 0.49892408690981405\n",
      "Epoch 5318, Loss Train(MSE): 0.1252687636558405, R2 Train: 0.498924945376638\n",
      "Epoch 5319, Loss Train(MSE): 0.12526852686787573, R2 Train: 0.49892589252849706\n",
      "Epoch 5320, Loss Train(MSE): 0.12526831946635583, R2 Train: 0.4989267221345767\n",
      "Epoch 5321, Loss Train(MSE): 0.1252680859150524, R2 Train: 0.4989276563397904\n",
      "Epoch 5322, Loss Train(MSE): 0.12526786891706376, R2 Train: 0.49892852433174495\n",
      "Epoch 5323, Loss Train(MSE): 0.12526764700896975, R2 Train: 0.498929411964121\n",
      "Epoch 5324, Loss Train(MSE): 0.12526742031806037, R2 Train: 0.4989303187277585\n",
      "Epoch 5325, Loss Train(MSE): 0.12526721013192943, R2 Train: 0.4989311594722823\n",
      "Epoch 5326, Loss Train(MSE): 0.12526697741068013, R2 Train: 0.4989320903572795\n",
      "Epoch 5327, Loss Train(MSE): 0.12526677154655944, R2 Train: 0.49893291381376226\n",
      "Epoch 5328, Loss Train(MSE): 0.12526654367348583, R2 Train: 0.4989338253060567\n",
      "Epoch 5329, Loss Train(MSE): 0.12526632776342853, R2 Train: 0.4989346889462859\n",
      "Epoch 5330, Loss Train(MSE): 0.12526611192075127, R2 Train: 0.49893555231699493\n",
      "Epoch 5331, Loss Train(MSE): 0.1252658858756509, R2 Train: 0.4989364564973964\n",
      "Epoch 5332, Loss Train(MSE): 0.12526568213557246, R2 Train: 0.49893727145771016\n",
      "Epoch 5333, Loss Train(MSE): 0.12526545335399575, R2 Train: 0.498938186584017\n",
      "Epoch 5334, Loss Train(MSE): 0.1252652468515848, R2 Train: 0.4989390125936608\n",
      "Epoch 5335, Loss Train(MSE): 0.1252650266136238, R2 Train: 0.49893989354550483\n",
      "Epoch 5336, Loss Train(MSE): 0.1252648096443426, R2 Train: 0.49894076142262955\n",
      "Epoch 5337, Loss Train(MSE): 0.1252646017982066, R2 Train: 0.49894159280717365\n",
      "Epoch 5338, Loss Train(MSE): 0.12526437578415564, R2 Train: 0.49894249686337744\n",
      "Epoch 5339, Loss Train(MSE): 0.12526417742278212, R2 Train: 0.4989432903088715\n",
      "Epoch 5340, Loss Train(MSE): 0.1252639539476353, R2 Train: 0.49894418420945885\n",
      "Epoch 5341, Loss Train(MSE): 0.12526374480254232, R2 Train: 0.4989450207898307\n",
      "Epoch 5342, Loss Train(MSE): 0.12526353399482534, R2 Train: 0.4989458640206986\n",
      "Epoch 5343, Loss Train(MSE): 0.1252633139887238, R2 Train: 0.49894674404510475\n",
      "Epoch 5344, Loss Train(MSE): 0.12526311591008127, R2 Train: 0.4989475363596749\n",
      "Epoch 5345, Loss Train(MSE): 0.1252628936300713, R2 Train: 0.49894842547971485\n",
      "Epoch 5346, Loss Train(MSE): 0.12526269104970653, R2 Train: 0.4989492358011739\n",
      "Epoch 5347, Loss Train(MSE): 0.12526247843625304, R2 Train: 0.4989500862549878\n",
      "Epoch 5348, Loss Train(MSE): 0.12526226469723245, R2 Train: 0.4989509412110702\n",
      "Epoch 5349, Loss Train(MSE): 0.12526206507104495, R2 Train: 0.4989517397158202\n",
      "Epoch 5350, Loss Train(MSE): 0.12526184541579022, R2 Train: 0.4989526183368391\n",
      "Epoch 5351, Loss Train(MSE): 0.12526164823986036, R2 Train: 0.49895340704055857\n",
      "Epoch 5352, Loss Train(MSE): 0.1252614348804378, R2 Train: 0.49895426047824876\n",
      "Epoch 5353, Loss Train(MSE): 0.1252612262618021, R2 Train: 0.49895509495279156\n",
      "Epoch 5354, Loss Train(MSE): 0.12526102613546578, R2 Train: 0.4989558954581369\n",
      "Epoch 5355, Loss Train(MSE): 0.1252608090482636, R2 Train: 0.49895676380694565\n",
      "Epoch 5356, Loss Train(MSE): 0.12526061615880477, R2 Train: 0.49895753536478094\n",
      "Epoch 5357, Loss Train(MSE): 0.1252604030739654, R2 Train: 0.49895838770413836\n",
      "Epoch 5358, Loss Train(MSE): 0.12526019847080755, R2 Train: 0.4989592061167698\n",
      "Epoch 5359, Loss Train(MSE): 0.12525999885299385, R2 Train: 0.4989600045880246\n",
      "Epoch 5360, Loss Train(MSE): 0.12525978427890294, R2 Train: 0.49896086288438823\n",
      "Epoch 5361, Loss Train(MSE): 0.12525959459870029, R2 Train: 0.49896162160519886\n",
      "Epoch 5362, Loss Train(MSE): 0.12525938277123513, R2 Train: 0.49896246891505946\n",
      "Epoch 5363, Loss Train(MSE): 0.12525918111888001, R2 Train: 0.49896327552447994\n",
      "Epoch 5364, Loss Train(MSE): 0.12525898298096858, R2 Train: 0.4989640680761257\n",
      "Epoch 5365, Loss Train(MSE): 0.12525877086673778, R2 Train: 0.49896491653304886\n",
      "Epoch 5366, Loss Train(MSE): 0.12525858335781226, R2 Train: 0.49896566656875097\n",
      "Epoch 5367, Loss Train(MSE): 0.12525837373414375, R2 Train: 0.498966505063425\n",
      "Epoch 5368, Loss Train(MSE): 0.12525817400665776, R2 Train: 0.49896730397336897\n",
      "Epoch 5369, Loss Train(MSE): 0.12525797828410887, R2 Train: 0.49896808686356453\n",
      "Epoch 5370, Loss Train(MSE): 0.12525776857810977, R2 Train: 0.4989689256875609\n",
      "Epoch 5371, Loss Train(MSE): 0.12525758224026884, R2 Train: 0.49896967103892464\n",
      "Epoch 5372, Loss Train(MSE): 0.12525737573178597, R2 Train: 0.4989704970728561\n",
      "Epoch 5373, Loss Train(MSE): 0.1252571769405487, R2 Train: 0.49897129223780523\n",
      "Epoch 5374, Loss Train(MSE): 0.1252569845342193, R2 Train: 0.4989720618631228\n",
      "Epoch 5375, Loss Train(MSE): 0.12525677718638215, R2 Train: 0.4989728912544714\n",
      "Epoch 5376, Loss Train(MSE): 0.12525659105583095, R2 Train: 0.4989736357766762\n",
      "Epoch 5377, Loss Train(MSE): 0.12525638854016904, R2 Train: 0.4989744458393238\n",
      "Epoch 5378, Loss Train(MSE): 0.125256189732505, R2 Train: 0.49897524106998004\n",
      "Epoch 5379, Loss Train(MSE): 0.1252560015099102, R2 Train: 0.4989759939603592\n",
      "Epoch 5380, Loss Train(MSE): 0.12525579647166257, R2 Train: 0.4989768141133497\n",
      "Epoch 5381, Loss Train(MSE): 0.12525560961967327, R2 Train: 0.4989775615213069\n",
      "Epoch 5382, Loss Train(MSE): 0.12525541194194106, R2 Train: 0.49897835223223574\n",
      "Epoch 5383, Loss Train(MSE): 0.12525521219980854, R2 Train: 0.49897915120076586\n",
      "Epoch 5384, Loss Train(MSE): 0.12525502899633142, R2 Train: 0.4989798840146743\n",
      "Epoch 5385, Loss Train(MSE): 0.1252548262205402, R2 Train: 0.49898069511783916\n",
      "Epoch 5386, Loss Train(MSE): 0.12525463775217624, R2 Train: 0.49898144899129504\n",
      "Epoch 5387, Loss Train(MSE): 0.1252544457261332, R2 Train: 0.4989822170954672\n",
      "Epoch 5388, Loss Train(MSE): 0.12525424431740945, R2 Train: 0.4989830227303622\n",
      "Epoch 5389, Loss Train(MSE): 0.12525406666164532, R2 Train: 0.4989837333534187\n",
      "Epoch 5390, Loss Train(MSE): 0.1252538662446522, R2 Train: 0.4989845350213912\n",
      "Epoch 5391, Loss Train(MSE): 0.1252536753412757, R2 Train: 0.49898529863489716\n",
      "Epoch 5392, Loss Train(MSE): 0.12525348970659653, R2 Train: 0.4989860411736139\n",
      "Epoch 5393, Loss Train(MSE): 0.1252532904870741, R2 Train: 0.49898683805170363\n",
      "Epoch 5394, Loss Train(MSE): 0.1252531097506937, R2 Train: 0.4989875609972252\n",
      "Epoch 5395, Loss Train(MSE): 0.12525291632360658, R2 Train: 0.4989883347055737\n",
      "Epoch 5396, Loss Train(MSE): 0.12525272215305924, R2 Train: 0.49898911138776303\n",
      "Epoch 5397, Loss Train(MSE): 0.1252525436653373, R2 Train: 0.49898982533865077\n",
      "Epoch 5398, Loss Train(MSE): 0.12525234659139248, R2 Train: 0.4989906136344301\n",
      "Epoch 5399, Loss Train(MSE): 0.12525216196328423, R2 Train: 0.4989913521468631\n",
      "Epoch 5400, Loss Train(MSE): 0.1252519762619885, R2 Train: 0.498992094952046\n",
      "Epoch 5401, Loss Train(MSE): 0.12525178048493354, R2 Train: 0.4989928780602658\n",
      "Epoch 5402, Loss Train(MSE): 0.12525160497447932, R2 Train: 0.4989935801020827\n",
      "Epoch 5403, Loss Train(MSE): 0.12525141245698732, R2 Train: 0.49899435017205074\n",
      "Epoch 5404, Loss Train(MSE): 0.12525122319693532, R2 Train: 0.4989951072122587\n",
      "Epoch 5405, Loss Train(MSE): 0.1252510458884142, R2 Train: 0.49899581644634317\n",
      "Epoch 5406, Loss Train(MSE): 0.12525085218906987, R2 Train: 0.49899659124372053\n",
      "Epoch 5407, Loss Train(MSE): 0.1252506714552285, R2 Train: 0.498997314179086\n",
      "Epoch 5408, Loss Train(MSE): 0.12525048787828813, R2 Train: 0.4989980484868475\n",
      "Epoch 5409, Loss Train(MSE): 0.12525029543506078, R2 Train: 0.4989988182597569\n",
      "Epoch 5410, Loss Train(MSE): 0.1252501228236839, R2 Train: 0.49899950870526444\n",
      "Epoch 5411, Loss Train(MSE): 0.12524993335588538, R2 Train: 0.49900026657645846\n",
      "Epoch 5412, Loss Train(MSE): 0.12524974670366046, R2 Train: 0.49900101318535817\n",
      "Epoch 5413, Loss Train(MSE): 0.1252495726923003, R2 Train: 0.4990017092307988\n",
      "Epoch 5414, Loss Train(MSE): 0.12524938226198748, R2 Train: 0.4990024709520501\n",
      "Epoch 5415, Loss Train(MSE): 0.1252492031720817, R2 Train: 0.49900318731167315\n",
      "Epoch 5416, Loss Train(MSE): 0.12524902378827793, R2 Train: 0.4990039048468883\n",
      "Epoch 5417, Loss Train(MSE): 0.12524883457514055, R2 Train: 0.4990046616994378\n",
      "Epoch 5418, Loss Train(MSE): 0.1252487253597545, R2 Train: 0.49900509856098196\n",
      "Epoch 5419, Loss Train(MSE): 0.12524862964862835, R2 Train: 0.4990054814054866\n",
      "Epoch 5420, Loss Train(MSE): 0.12524853182672696, R2 Train: 0.49900587269309216\n",
      "Epoch 5421, Loss Train(MSE): 0.12524845129474488, R2 Train: 0.4990061948210205\n",
      "Epoch 5422, Loss Train(MSE): 0.12524835184124994, R2 Train: 0.49900659263500025\n",
      "Epoch 5423, Loss Train(MSE): 0.12524825996386055, R2 Train: 0.4990069601445578\n",
      "Epoch 5424, Loss Train(MSE): 0.1252481739324573, R2 Train: 0.4990073042701708\n",
      "Epoch 5425, Loss Train(MSE): 0.12524807475013472, R2 Train: 0.4990077009994611\n",
      "Epoch 5426, Loss Train(MSE): 0.12524798869032513, R2 Train: 0.49900804523869946\n",
      "Epoch 5427, Loss Train(MSE): 0.12524789728369864, R2 Train: 0.4990084108652054\n",
      "Epoch 5428, Loss Train(MSE): 0.12524779837072564, R2 Train: 0.49900880651709745\n",
      "Epoch 5429, Loss Train(MSE): 0.12524771800327494, R2 Train: 0.4990091279869002\n",
      "Epoch 5430, Loss Train(MSE): 0.12524762134394868, R2 Train: 0.4990095146242053\n",
      "Epoch 5431, Loss Train(MSE): 0.12524752600586253, R2 Train: 0.4990098959765499\n",
      "Epoch 5432, Loss Train(MSE): 0.12524744458893847, R2 Train: 0.49901022164424613\n",
      "Epoch 5433, Loss Train(MSE): 0.12524734609925778, R2 Train: 0.4990106156029689\n",
      "Epoch 5434, Loss Train(MSE): 0.12524725630316166, R2 Train: 0.49901097478735335\n",
      "Epoch 5435, Loss Train(MSE): 0.12524716977980513, R2 Train: 0.4990113208807795\n",
      "Epoch 5436, Loss Train(MSE): 0.12524707155498055, R2 Train: 0.4990117137800778\n",
      "Epoch 5437, Loss Train(MSE): 0.12524698717967256, R2 Train: 0.49901205128130977\n",
      "Epoch 5438, Loss Train(MSE): 0.12524689566847702, R2 Train: 0.4990124173260919\n",
      "Epoch 5439, Loss Train(MSE): 0.12524679770676858, R2 Train: 0.4990128091729257\n",
      "Epoch 5440, Loss Train(MSE): 0.12524671863267425, R2 Train: 0.499013125469303\n",
      "Epoch 5441, Loss Train(MSE): 0.12524662225064015, R2 Train: 0.4990135109974394\n",
      "Epoch 5442, Loss Train(MSE): 0.12524652814561163, R2 Train: 0.4990138874175535\n",
      "Epoch 5443, Loss Train(MSE): 0.12524644706085694, R2 Train: 0.49901421175657223\n",
      "Epoch 5444, Loss Train(MSE): 0.12524634951278693, R2 Train: 0.4990146019488523\n",
      "Epoch 5445, Loss Train(MSE): 0.12524626056682703, R2 Train: 0.4990149577326919\n",
      "Epoch 5446, Loss Train(MSE): 0.12524617474946456, R2 Train: 0.49901530100214175\n",
      "Epoch 5447, Loss Train(MSE): 0.12524607746022173, R2 Train: 0.49901569015911307\n",
      "Epoch 5448, Loss Train(MSE): 0.12524599355756982, R2 Train: 0.4990160257697207\n",
      "Epoch 5449, Loss Train(MSE): 0.1252459031208692, R2 Train: 0.4990163875165232\n",
      "Epoch 5450, Loss Train(MSE): 0.12524580608879124, R2 Train: 0.49901677564483504\n",
      "Epoch 5451, Loss Train(MSE): 0.12524572711523496, R2 Train: 0.49901709153906015\n",
      "Epoch 5452, Loss Train(MSE): 0.12524563217094958, R2 Train: 0.49901747131620167\n",
      "Epoch 5453, Loss Train(MSE): 0.12524553811429762, R2 Train: 0.49901784754280953\n",
      "Epoch 5454, Loss Train(MSE): 0.12524545851427546, R2 Train: 0.49901816594289816\n",
      "Epoch 5455, Loss Train(MSE): 0.12524536188661803, R2 Train: 0.4990185524535279\n",
      "Epoch 5456, Loss Train(MSE): 0.12524527262471138, R2 Train: 0.49901890950115446\n",
      "Epoch 5457, Loss Train(MSE): 0.12524518864771506, R2 Train: 0.4990192454091398\n",
      "Epoch 5458, Loss Train(MSE): 0.12524509227312478, R2 Train: 0.4990196309075009\n",
      "Epoch 5459, Loss Train(MSE): 0.12524500769537178, R2 Train: 0.4990199692185129\n",
      "Epoch 5460, Loss Train(MSE): 0.12524491944961147, R2 Train: 0.4990203222015541\n",
      "Epoch 5461, Loss Train(MSE): 0.1252448233264985, R2 Train: 0.499020706694006\n",
      "Epoch 5462, Loss Train(MSE): 0.1252447433237815, R2 Train: 0.499021026704874\n",
      "Epoch 5463, Loss Train(MSE): 0.1252446509160234, R2 Train: 0.49902139633590636\n",
      "Epoch 5464, Loss Train(MSE): 0.1252445557857114, R2 Train: 0.49902177685715443\n",
      "Epoch 5465, Loss Train(MSE): 0.12524447876176528, R2 Train: 0.49902208495293887\n",
      "Epoch 5466, Loss Train(MSE): 0.1252443830342628, R2 Train: 0.4990224678629488\n",
      "Epoch 5467, Loss Train(MSE): 0.12524429235202722, R2 Train: 0.4990228305918911\n",
      "Epoch 5468, Loss Train(MSE): 0.12524421128946592, R2 Train: 0.49902315484213633\n",
      "Epoch 5469, Loss Train(MSE): 0.12524411580952222, R2 Train: 0.49902353676191114\n",
      "Epoch 5470, Loss Train(MSE): 0.12524402946968402, R2 Train: 0.4990238821212639\n",
      "Epoch 5471, Loss Train(MSE): 0.12524394447190754, R2 Train: 0.49902422211236985\n",
      "Epoch 5472, Loss Train(MSE): 0.12524384923800064, R2 Train: 0.49902460304799745\n",
      "Epoch 5473, Loss Train(MSE): 0.12524376713628446, R2 Train: 0.49902493145486215\n",
      "Epoch 5474, Loss Train(MSE): 0.12524367830531707, R2 Train: 0.4990252867787317\n",
      "Epoch 5475, Loss Train(MSE): 0.12524358331594396, R2 Train: 0.49902566673622417\n",
      "Epoch 5476, Loss Train(MSE): 0.12524350534945858, R2 Train: 0.4990259786021657\n",
      "Epoch 5477, Loss Train(MSE): 0.125243412785968, R2 Train: 0.49902634885612795\n",
      "Epoch 5478, Loss Train(MSE): 0.12524331963783109, R2 Train: 0.49902672144867566\n",
      "Epoch 5479, Loss Train(MSE): 0.1252432425061767, R2 Train: 0.49902702997529325\n",
      "Epoch 5480, Loss Train(MSE): 0.12524314790166133, R2 Train: 0.4990274083933547\n",
      "Epoch 5481, Loss Train(MSE): 0.12524305877081346, R2 Train: 0.49902776491674616\n",
      "Epoch 5482, Loss Train(MSE): 0.1252429780211945, R2 Train: 0.499028087915222\n",
      "Epoch 5483, Loss Train(MSE): 0.12524288365750835, R2 Train: 0.4990284653699666\n",
      "Epoch 5484, Loss Train(MSE): 0.12524279844427733, R2 Train: 0.4990288062228907\n",
      "Epoch 5485, Loss Train(MSE): 0.12524271417420887, R2 Train: 0.4990291433031645\n",
      "Epoch 5486, Loss Train(MSE): 0.12524262004991166, R2 Train: 0.49902951980035337\n",
      "Epoch 5487, Loss Train(MSE): 0.12524253865594415, R2 Train: 0.4990298453762234\n",
      "Epoch 5488, Loss Train(MSE): 0.1252424509616483, R2 Train: 0.4990301961534068\n",
      "Epoch 5489, Loss Train(MSE): 0.12524235707531683, R2 Train: 0.49903057169873266\n",
      "Epoch 5490, Loss Train(MSE): 0.1252422794035602, R2 Train: 0.4990308823857592\n",
      "Epoch 5491, Loss Train(MSE): 0.12524218837998366, R2 Train: 0.49903124648006536\n",
      "Epoch 5492, Loss Train(MSE): 0.1252420954837816, R2 Train: 0.4990316180648736\n",
      "Epoch 5493, Loss Train(MSE): 0.12524201992912248, R2 Train: 0.49903192028351007\n",
      "Epoch 5494, Loss Train(MSE): 0.12524192641747595, R2 Train: 0.4990322943300962\n",
      "Epoch 5495, Loss Train(MSE): 0.12524183713397605, R2 Train: 0.4990326514640958\n",
      "Epoch 5496, Loss Train(MSE): 0.12524175835633958, R2 Train: 0.4990329665746417\n",
      "Epoch 5497, Loss Train(MSE): 0.1252416650791517, R2 Train: 0.49903333968339325\n",
      "Epoch 5498, Loss Train(MSE): 0.12524157931431654, R2 Train: 0.49903368274273385\n",
      "Epoch 5499, Loss Train(MSE): 0.12524149740569535, R2 Train: 0.4990340103772186\n",
      "Epoch 5500, Loss Train(MSE): 0.1252414043616008, R2 Train: 0.4990343825535968\n",
      "Epoch 5501, Loss Train(MSE): 0.12524132202263322, R2 Train: 0.49903471190946713\n",
      "Epoch 5502, Loss Train(MSE): 0.1252412370738034, R2 Train: 0.49903505170478635\n",
      "Epoch 5503, Loss Train(MSE): 0.12524114426145308, R2 Train: 0.4990354229541877\n",
      "Epoch 5504, Loss Train(MSE): 0.12524106525677875, R2 Train: 0.499035738972885\n",
      "Epoch 5505, Loss Train(MSE): 0.12524097735731693, R2 Train: 0.4990360905707323\n",
      "Epoch 5506, Loss Train(MSE): 0.12524088477537723, R2 Train: 0.4990364608984911\n",
      "Epoch 5507, Loss Train(MSE): 0.12524080901462842, R2 Train: 0.4990367639414863\n",
      "Epoch 5508, Loss Train(MSE): 0.12524071825292768, R2 Train: 0.4990371269882893\n",
      "Epoch 5509, Loss Train(MSE): 0.12524062722391305, R2 Train: 0.4990374911043478\n",
      "Epoch 5510, Loss Train(MSE): 0.1252405519683805, R2 Train: 0.49903779212647803\n",
      "Epoch 5511, Loss Train(MSE): 0.12524045974942494, R2 Train: 0.49903816100230025\n",
      "Epoch 5512, Loss Train(MSE): 0.12524037186445716, R2 Train: 0.49903851254217135\n",
      "Epoch 5513, Loss Train(MSE): 0.12524029384355712, R2 Train: 0.49903882462577154\n",
      "Epoch 5514, Loss Train(MSE): 0.12524020185172613, R2 Train: 0.4990391925930955\n",
      "Epoch 5515, Loss Train(MSE): 0.1252401170232224, R2 Train: 0.49903953190711037\n",
      "Epoch 5516, Loss Train(MSE): 0.12524003632261813, R2 Train: 0.4990398547095275\n",
      "Epoch 5517, Loss Train(MSE): 0.12523994455663082, R2 Train: 0.49904022177347673\n",
      "Epoch 5518, Loss Train(MSE): 0.1252398626981593, R2 Train: 0.49904054920736285\n",
      "Epoch 5519, Loss Train(MSE): 0.12523977940238476, R2 Train: 0.49904088239046096\n",
      "Epoch 5520, Loss Train(MSE): 0.12523968786097475, R2 Train: 0.499041248556101\n",
      "Epoch 5521, Loss Train(MSE): 0.12523960888723898, R2 Train: 0.4990415644510441\n",
      "Epoch 5522, Loss Train(MSE): 0.12523952307971395, R2 Train: 0.4990419076811442\n",
      "Epoch 5523, Loss Train(MSE): 0.1252394317616293, R2 Train: 0.4990422729534828\n",
      "Epoch 5524, Loss Train(MSE): 0.1252393555884527, R2 Train: 0.4990425776461892\n",
      "Epoch 5525, Loss Train(MSE): 0.12523926735149787, R2 Train: 0.4990429305940085\n",
      "Epoch 5526, Loss Train(MSE): 0.1252391762555008, R2 Train: 0.4990432949779968\n",
      "Epoch 5527, Loss Train(MSE): 0.1252391027998115, R2 Train: 0.49904358880075395\n",
      "Epoch 5528, Loss Train(MSE): 0.1252390122146633, R2 Train: 0.4990439511413468\n",
      "Epoch 5529, Loss Train(MSE): 0.12523892345289375, R2 Train: 0.499044306188425\n",
      "Epoch 5530, Loss Train(MSE): 0.12523994040288597, R2 Train: 0.4990402383884561\n",
      "Epoch 5531, Loss Train(MSE): 0.12524322250948536, R2 Train: 0.49902710996205857\n",
      "Epoch 5532, Loss Train(MSE): 0.12524311264021037, R2 Train: 0.4990275494391585\n",
      "Epoch 5533, Loss Train(MSE): 0.12524300286420645, R2 Train: 0.4990279885431742\n",
      "Epoch 5534, Loss Train(MSE): 0.12524289318135337, R2 Train: 0.4990284272745865\n",
      "Epoch 5535, Loss Train(MSE): 0.12524278359153154, R2 Train: 0.49902886563387383\n",
      "Epoch 5536, Loss Train(MSE): 0.12524267409462145, R2 Train: 0.4990293036215142\n",
      "Epoch 5537, Loss Train(MSE): 0.12524256469050382, R2 Train: 0.4990297412379847\n",
      "Epoch 5538, Loss Train(MSE): 0.12524245537905965, R2 Train: 0.4990301784837614\n",
      "Epoch 5539, Loss Train(MSE): 0.12524234616017024, R2 Train: 0.49903061535931903\n",
      "Epoch 5540, Loss Train(MSE): 0.12524223703371704, R2 Train: 0.49903105186513186\n",
      "Epoch 5541, Loss Train(MSE): 0.12524212799958187, R2 Train: 0.4990314880016725\n",
      "Epoch 5542, Loss Train(MSE): 0.12524201905764665, R2 Train: 0.4990319237694134\n",
      "Epoch 5543, Loss Train(MSE): 0.12524191020779363, R2 Train: 0.4990323591688255\n",
      "Epoch 5544, Loss Train(MSE): 0.12524180144990532, R2 Train: 0.4990327942003787\n",
      "Epoch 5545, Loss Train(MSE): 0.1252416927838644, R2 Train: 0.4990332288645424\n",
      "Epoch 5546, Loss Train(MSE): 0.12524158420955392, R2 Train: 0.4990336631617843\n",
      "Epoch 5547, Loss Train(MSE): 0.12524147572685695, R2 Train: 0.4990340970925722\n",
      "Epoch 5548, Loss Train(MSE): 0.12524136733565702, R2 Train: 0.4990345306573719\n",
      "Epoch 5549, Loss Train(MSE): 0.12524125903583774, R2 Train: 0.499034963856649\n",
      "Epoch 5550, Loss Train(MSE): 0.1252411508272831, R2 Train: 0.4990353966908676\n",
      "Epoch 5551, Loss Train(MSE): 0.12524104270987718, R2 Train: 0.4990358291604913\n",
      "Epoch 5552, Loss Train(MSE): 0.12524093468350433, R2 Train: 0.49903626126598266\n",
      "Epoch 5553, Loss Train(MSE): 0.12524082674804918, R2 Train: 0.4990366930078033\n",
      "Epoch 5554, Loss Train(MSE): 0.12524071890339666, R2 Train: 0.49903712438641334\n",
      "Epoch 5555, Loss Train(MSE): 0.12524061114943172, R2 Train: 0.4990375554022731\n",
      "Epoch 5556, Loss Train(MSE): 0.12524050348603977, R2 Train: 0.49903798605584093\n",
      "Epoch 5557, Loss Train(MSE): 0.12524039591310626, R2 Train: 0.49903841634757495\n",
      "Epoch 5558, Loss Train(MSE): 0.12524028843051696, R2 Train: 0.49903884627793216\n",
      "Epoch 5559, Loss Train(MSE): 0.12524018103815787, R2 Train: 0.4990392758473685\n",
      "Epoch 5560, Loss Train(MSE): 0.12524007373591517, R2 Train: 0.4990397050563393\n",
      "Epoch 5561, Loss Train(MSE): 0.12523996652367536, R2 Train: 0.49904013390529856\n",
      "Epoch 5562, Loss Train(MSE): 0.12523985940132504, R2 Train: 0.49904056239469985\n",
      "Epoch 5563, Loss Train(MSE): 0.1252397523687511, R2 Train: 0.4990409905249956\n",
      "Epoch 5564, Loss Train(MSE): 0.12523964542584068, R2 Train: 0.4990414182966373\n",
      "Epoch 5565, Loss Train(MSE): 0.12523953857248105, R2 Train: 0.4990418457100758\n",
      "Epoch 5566, Loss Train(MSE): 0.12523943180855976, R2 Train: 0.49904227276576096\n",
      "Epoch 5567, Loss Train(MSE): 0.1252393251339646, R2 Train: 0.4990426994641416\n",
      "Epoch 5568, Loss Train(MSE): 0.12523921854858358, R2 Train: 0.4990431258056657\n",
      "Epoch 5569, Loss Train(MSE): 0.1252391120523048, R2 Train: 0.49904355179078075\n",
      "Epoch 5570, Loss Train(MSE): 0.12523900564501675, R2 Train: 0.499043977419933\n",
      "Epoch 5571, Loss Train(MSE): 0.12523889932660803, R2 Train: 0.49904440269356787\n",
      "Epoch 5572, Loss Train(MSE): 0.1252387930969675, R2 Train: 0.49904482761213\n",
      "Epoch 5573, Loss Train(MSE): 0.12523868695598417, R2 Train: 0.4990452521760633\n",
      "Epoch 5574, Loss Train(MSE): 0.1252385809035474, R2 Train: 0.49904567638581043\n",
      "Epoch 5575, Loss Train(MSE): 0.12523847493954657, R2 Train: 0.49904610024181373\n",
      "Epoch 5576, Loss Train(MSE): 0.12523836906387148, R2 Train: 0.4990465237445141\n",
      "Epoch 5577, Loss Train(MSE): 0.12523826327641197, R2 Train: 0.49904694689435214\n",
      "Epoch 5578, Loss Train(MSE): 0.12523815757705814, R2 Train: 0.49904736969176744\n",
      "Epoch 5579, Loss Train(MSE): 0.12523805196570031, R2 Train: 0.49904779213719874\n",
      "Epoch 5580, Loss Train(MSE): 0.1252379464422291, R2 Train: 0.4990482142310836\n",
      "Epoch 5581, Loss Train(MSE): 0.12523784100653512, R2 Train: 0.4990486359738595\n",
      "Epoch 5582, Loss Train(MSE): 0.12523773565850937, R2 Train: 0.4990490573659625\n",
      "Epoch 5583, Loss Train(MSE): 0.125237630398043, R2 Train: 0.499049478407828\n",
      "Epoch 5584, Loss Train(MSE): 0.12523752522502737, R2 Train: 0.4990498990998905\n",
      "Epoch 5585, Loss Train(MSE): 0.12523742013935402, R2 Train: 0.49905031944258393\n",
      "Epoch 5586, Loss Train(MSE): 0.12523731514091474, R2 Train: 0.49905073943634104\n",
      "Epoch 5587, Loss Train(MSE): 0.12523721022960144, R2 Train: 0.49905115908159425\n",
      "Epoch 5588, Loss Train(MSE): 0.12523710540530633, R2 Train: 0.4990515783787747\n",
      "Epoch 5589, Loss Train(MSE): 0.12523700066792176, R2 Train: 0.49905199732831296\n",
      "Epoch 5590, Loss Train(MSE): 0.12523689601734028, R2 Train: 0.49905241593063887\n",
      "Epoch 5591, Loss Train(MSE): 0.1252367914534547, R2 Train: 0.4990528341861812\n",
      "Epoch 5592, Loss Train(MSE): 0.1252366869761579, R2 Train: 0.49905325209536844\n",
      "Epoch 5593, Loss Train(MSE): 0.12523658258534304, R2 Train: 0.49905366965862785\n",
      "Epoch 5594, Loss Train(MSE): 0.1252364782809036, R2 Train: 0.49905408687638564\n",
      "Epoch 5595, Loss Train(MSE): 0.12523637406273305, R2 Train: 0.4990545037490678\n",
      "Epoch 5596, Loss Train(MSE): 0.12523626993072512, R2 Train: 0.4990549202770995\n",
      "Epoch 5597, Loss Train(MSE): 0.1252361658847738, R2 Train: 0.49905533646090483\n",
      "Epoch 5598, Loss Train(MSE): 0.12523606192477316, R2 Train: 0.4990557523009074\n",
      "Epoch 5599, Loss Train(MSE): 0.12523595805061763, R2 Train: 0.4990561677975295\n",
      "Epoch 5600, Loss Train(MSE): 0.12523585426220166, R2 Train: 0.4990565829511934\n",
      "Epoch 5601, Loss Train(MSE): 0.12523575055941996, R2 Train: 0.49905699776232015\n",
      "Epoch 5602, Loss Train(MSE): 0.12523564694216752, R2 Train: 0.49905741223132993\n",
      "Epoch 5603, Loss Train(MSE): 0.12523554341033935, R2 Train: 0.4990578263586426\n",
      "Epoch 5604, Loss Train(MSE): 0.1252354399638308, R2 Train: 0.4990582401446768\n",
      "Epoch 5605, Loss Train(MSE): 0.12523533660253727, R2 Train: 0.4990586535898509\n",
      "Epoch 5606, Loss Train(MSE): 0.12523523332635447, R2 Train: 0.4990590666945821\n",
      "Epoch 5607, Loss Train(MSE): 0.12523513013517834, R2 Train: 0.49905947945928664\n",
      "Epoch 5608, Loss Train(MSE): 0.1252350270289048, R2 Train: 0.4990598918843808\n",
      "Epoch 5609, Loss Train(MSE): 0.12523492400743014, R2 Train: 0.49906030397027945\n",
      "Epoch 5610, Loss Train(MSE): 0.12523482107065076, R2 Train: 0.49906071571739696\n",
      "Epoch 5611, Loss Train(MSE): 0.12523471821846327, R2 Train: 0.4990611271261469\n",
      "Epoch 5612, Loss Train(MSE): 0.12523461545076445, R2 Train: 0.4990615381969422\n",
      "Epoch 5613, Loss Train(MSE): 0.1252345127674512, R2 Train: 0.4990619489301952\n",
      "Epoch 5614, Loss Train(MSE): 0.12523441016842082, R2 Train: 0.4990623593263167\n",
      "Epoch 5615, Loss Train(MSE): 0.12523430765357055, R2 Train: 0.4990627693857178\n",
      "Epoch 5616, Loss Train(MSE): 0.12523420522279793, R2 Train: 0.49906317910880826\n",
      "Epoch 5617, Loss Train(MSE): 0.12523410287600067, R2 Train: 0.4990635884959973\n",
      "Epoch 5618, Loss Train(MSE): 0.12523400061307663, R2 Train: 0.4990639975476935\n",
      "Epoch 5619, Loss Train(MSE): 0.12523389843392388, R2 Train: 0.4990644062643045\n",
      "Epoch 5620, Loss Train(MSE): 0.12523379633844067, R2 Train: 0.4990648146462373\n",
      "Epoch 5621, Loss Train(MSE): 0.12523369432652542, R2 Train: 0.4990652226938983\n",
      "Epoch 5622, Loss Train(MSE): 0.1252335923980768, R2 Train: 0.4990656304076928\n",
      "Epoch 5623, Loss Train(MSE): 0.12523349055299343, R2 Train: 0.4990660377880263\n",
      "Epoch 5624, Loss Train(MSE): 0.12523338879117443, R2 Train: 0.49906644483530227\n",
      "Epoch 5625, Loss Train(MSE): 0.12523328711251885, R2 Train: 0.4990668515499246\n",
      "Epoch 5626, Loss Train(MSE): 0.125233185516926, R2 Train: 0.499067257932296\n",
      "Epoch 5627, Loss Train(MSE): 0.12523308400429542, R2 Train: 0.4990676639828183\n",
      "Epoch 5628, Loss Train(MSE): 0.1252329825745267, R2 Train: 0.4990680697018932\n",
      "Epoch 5629, Loss Train(MSE): 0.12523288122751977, R2 Train: 0.4990684750899209\n",
      "Epoch 5630, Loss Train(MSE): 0.12523277996317453, R2 Train: 0.4990688801473019\n",
      "Epoch 5631, Loss Train(MSE): 0.12523267878139127, R2 Train: 0.4990692848744349\n",
      "Epoch 5632, Loss Train(MSE): 0.12523257768207027, R2 Train: 0.4990696892717189\n",
      "Epoch 5633, Loss Train(MSE): 0.12523247666511209, R2 Train: 0.49907009333955166\n",
      "Epoch 5634, Loss Train(MSE): 0.12523237573041748, R2 Train: 0.49907049707833007\n",
      "Epoch 5635, Loss Train(MSE): 0.12523227487788724, R2 Train: 0.49907090048845104\n",
      "Epoch 5636, Loss Train(MSE): 0.12523217410742246, R2 Train: 0.49907130357031015\n",
      "Epoch 5637, Loss Train(MSE): 0.12523207341892437, R2 Train: 0.49907170632430253\n",
      "Epoch 5638, Loss Train(MSE): 0.12523197281229428, R2 Train: 0.49907210875082286\n",
      "Epoch 5639, Loss Train(MSE): 0.12523187228743388, R2 Train: 0.4990725108502645\n",
      "Epoch 5640, Loss Train(MSE): 0.12523177184424475, R2 Train: 0.499072912623021\n",
      "Epoch 5641, Loss Train(MSE): 0.1252316714826289, R2 Train: 0.4990733140694844\n",
      "Epoch 5642, Loss Train(MSE): 0.1252315712024883, R2 Train: 0.49907371519004684\n",
      "Epoch 5643, Loss Train(MSE): 0.12523147100372525, R2 Train: 0.499074115985099\n",
      "Epoch 5644, Loss Train(MSE): 0.12523137088624212, R2 Train: 0.49907451645503154\n",
      "Epoch 5645, Loss Train(MSE): 0.12523127084994146, R2 Train: 0.4990749166002342\n",
      "Epoch 5646, Loss Train(MSE): 0.12523117089472602, R2 Train: 0.4990753164210959\n",
      "Epoch 5647, Loss Train(MSE): 0.12523107102049869, R2 Train: 0.49907571591800526\n",
      "Epoch 5648, Loss Train(MSE): 0.12523097122716254, R2 Train: 0.49907611509134986\n",
      "Epoch 5649, Loss Train(MSE): 0.12523087151462076, R2 Train: 0.499076513941517\n",
      "Epoch 5650, Loss Train(MSE): 0.1252307718827768, R2 Train: 0.49907691246889285\n",
      "Epoch 5651, Loss Train(MSE): 0.1252306723315341, R2 Train: 0.49907731067386363\n",
      "Epoch 5652, Loss Train(MSE): 0.12523057286079647, R2 Train: 0.4990777085568141\n",
      "Epoch 5653, Loss Train(MSE): 0.1252304734704678, R2 Train: 0.49907810611812875\n",
      "Epoch 5654, Loss Train(MSE): 0.12523037416045207, R2 Train: 0.4990785033581917\n",
      "Epoch 5655, Loss Train(MSE): 0.12523027493065353, R2 Train: 0.4990789002773859\n",
      "Epoch 5656, Loss Train(MSE): 0.12523017578097648, R2 Train: 0.4990792968760941\n",
      "Epoch 5657, Loss Train(MSE): 0.12523007671132544, R2 Train: 0.49907969315469825\n",
      "Epoch 5658, Loss Train(MSE): 0.12522997772160518, R2 Train: 0.49908008911357926\n",
      "Epoch 5659, Loss Train(MSE): 0.12522987881172049, R2 Train: 0.49908048475311806\n",
      "Epoch 5660, Loss Train(MSE): 0.1252297799815764, R2 Train: 0.49908088007369444\n",
      "Epoch 5661, Loss Train(MSE): 0.125229681231078, R2 Train: 0.499081275075688\n",
      "Epoch 5662, Loss Train(MSE): 0.1252295825601307, R2 Train: 0.4990816697594772\n",
      "Epoch 5663, Loss Train(MSE): 0.1252294839686399, R2 Train: 0.4990820641254404\n",
      "Epoch 5664, Loss Train(MSE): 0.12522938545651127, R2 Train: 0.4990824581739549\n",
      "Epoch 5665, Loss Train(MSE): 0.12522928856538545, R2 Train: 0.4990828457384582\n",
      "Epoch 5666, Loss Train(MSE): 0.12522921949356933, R2 Train: 0.4990831220257227\n",
      "Epoch 5667, Loss Train(MSE): 0.125229138555695, R2 Train: 0.49908344577721997\n",
      "Epoch 5668, Loss Train(MSE): 0.12522905614105542, R2 Train: 0.4990837754357783\n",
      "Epoch 5669, Loss Train(MSE): 0.1252289863472074, R2 Train: 0.49908405461117045\n",
      "Epoch 5670, Loss Train(MSE): 0.1252289064324407, R2 Train: 0.4990843742702372\n",
      "Epoch 5671, Loss Train(MSE): 0.1252288241924005, R2 Train: 0.499084703230398\n",
      "Epoch 5672, Loss Train(MSE): 0.12522875363771552, R2 Train: 0.49908498544913793\n",
      "Epoch 5673, Loss Train(MSE): 0.12522867478374702, R2 Train: 0.4990853008650119\n",
      "Epoch 5674, Loss Train(MSE): 0.1252285927175805, R2 Train: 0.499085629129678\n",
      "Epoch 5675, Loss Train(MSE): 0.12522852136378354, R2 Train: 0.49908591454486584\n",
      "Epoch 5676, Loss Train(MSE): 0.12522844360778237, R2 Train: 0.4990862255688705\n",
      "Epoch 5677, Loss Train(MSE): 0.12522836171476992, R2 Train: 0.4990865531409203\n",
      "Epoch 5678, Loss Train(MSE): 0.12522828952410908, R2 Train: 0.49908684190356367\n",
      "Epoch 5679, Loss Train(MSE): 0.12522821290272967, R2 Train: 0.4990871483890813\n",
      "Epoch 5680, Loss Train(MSE): 0.1252281311821576, R2 Train: 0.49908747527136965\n",
      "Epoch 5681, Loss Train(MSE): 0.12522805811739776, R2 Train: 0.49908776753040895\n",
      "Epoch 5682, Loss Train(MSE): 0.12522798266678617, R2 Train: 0.4990880693328553\n",
      "Epoch 5683, Loss Train(MSE): 0.1252279011179466, R2 Train: 0.4990883955282136\n",
      "Epoch 5684, Loss Train(MSE): 0.12522782714236283, R2 Train: 0.4990886914305487\n",
      "Epoch 5685, Loss Train(MSE): 0.1252277528981631, R2 Train: 0.4990889884073476\n",
      "Epoch 5686, Loss Train(MSE): 0.12522767152035397, R2 Train: 0.49908931391858413\n",
      "Epoch 5687, Loss Train(MSE): 0.12522759659772534, R2 Train: 0.49908961360909865\n",
      "Epoch 5688, Loss Train(MSE): 0.1252304384425978, R2 Train: 0.49907824622960884\n",
      "Epoch 5689, Loss Train(MSE): 0.1252489512683789, R2 Train: 0.4990041949264844\n",
      "Epoch 5690, Loss Train(MSE): 0.12524875998799487, R2 Train: 0.49900496004802053\n",
      "Epoch 5691, Loss Train(MSE): 0.1252485637651414, R2 Train: 0.4990057449394344\n",
      "Epoch 5692, Loss Train(MSE): 0.12524836489027938, R2 Train: 0.4990065404388825\n",
      "Epoch 5693, Loss Train(MSE): 0.1252481779813341, R2 Train: 0.49900728807466355\n",
      "Epoch 5694, Loss Train(MSE): 0.12524797213483602, R2 Train: 0.49900811146065593\n",
      "Epoch 5695, Loss Train(MSE): 0.12524779322431712, R2 Train: 0.49900882710273153\n",
      "Epoch 5696, Loss Train(MSE): 0.1252475890134366, R2 Train: 0.4990096439462536\n",
      "Epoch 5697, Loss Train(MSE): 0.1252474021927902, R2 Train: 0.49901039122883917\n",
      "Epoch 5698, Loss Train(MSE): 0.1252472075750066, R2 Train: 0.4990111696999736\n",
      "Epoch 5699, Loss Train(MSE): 0.12524701276186903, R2 Train: 0.4990119489525239\n",
      "Epoch 5700, Loss Train(MSE): 0.12524682780570864, R2 Train: 0.4990126887771654\n",
      "Epoch 5701, Loss Train(MSE): 0.1252466253065703, R2 Train: 0.4990134987737188\n",
      "Epoch 5702, Loss Train(MSE): 0.12524644933509738, R2 Train: 0.4990142026596105\n",
      "Epoch 5703, Loss Train(MSE): 0.12524624812192145, R2 Train: 0.4990150075123142\n",
      "Epoch 5704, Loss Train(MSE): 0.1252460638599178, R2 Train: 0.4990157445603288\n",
      "Epoch 5705, Loss Train(MSE): 0.1252458725715232, R2 Train: 0.4990165097139072\n",
      "Epoch 5706, Loss Train(MSE): 0.12524567994241473, R2 Train: 0.4990172802303411\n",
      "Epoch 5707, Loss Train(MSE): 0.12524549864213502, R2 Train: 0.4990180054314599\n",
      "Epoch 5708, Loss Train(MSE): 0.12524529939098172, R2 Train: 0.4990188024360731\n",
      "Epoch 5709, Loss Train(MSE): 0.12524512453024342, R2 Train: 0.49901950187902633\n",
      "Epoch 5710, Loss Train(MSE): 0.12524492797182538, R2 Train: 0.4990202881126985\n",
      "Epoch 5711, Loss Train(MSE): 0.12524474446262615, R2 Train: 0.4990210221494954\n",
      "Epoch 5712, Loss Train(MSE): 0.12524455814028662, R2 Train: 0.4990217674388535\n",
      "Epoch 5713, Loss Train(MSE): 0.12524436591152233, R2 Train: 0.4990225363539107\n",
      "Epoch 5714, Loss Train(MSE): 0.12524418988368746, R2 Train: 0.49902324046525015\n",
      "Epoch 5715, Loss Train(MSE): 0.1252439937854094, R2 Train: 0.4990240248583624\n",
      "Epoch 5716, Loss Train(MSE): 0.12524381829870473, R2 Train: 0.49902472680518106\n",
      "Epoch 5717, Loss Train(MSE): 0.12524362796777713, R2 Train: 0.49902548812889147\n",
      "Epoch 5718, Loss Train(MSE): 0.12524344349596275, R2 Train: 0.499026226016149\n",
      "Epoch 5719, Loss Train(MSE): 0.12524326369309693, R2 Train: 0.49902694522761226\n",
      "Epoch 5720, Loss Train(MSE): 0.1252430701702383, R2 Train: 0.49902771931904677\n",
      "Epoch 5721, Loss Train(MSE): 0.12524290094922166, R2 Train: 0.49902839620311334\n",
      "Epoch 5722, Loss Train(MSE): 0.1252427079127351, R2 Train: 0.49902916834905964\n",
      "Epoch 5723, Loss Train(MSE): 0.1252425301503446, R2 Train: 0.49902987939862165\n",
      "Epoch 5724, Loss Train(MSE): 0.12524234753953775, R2 Train: 0.499030609841849\n",
      "Epoch 5725, Loss Train(MSE): 0.12524216047554848, R2 Train: 0.4990313580978061\n",
      "Epoch 5726, Loss Train(MSE): 0.12524198866648445, R2 Train: 0.4990320453340622\n",
      "Epoch 5727, Loss Train(MSE): 0.12524179778524192, R2 Train: 0.49903280885903234\n",
      "Epoch 5728, Loss Train(MSE): 0.12524162576430126, R2 Train: 0.499033496942795\n",
      "Epoch 5729, Loss Train(MSE): 0.1252414412353809, R2 Train: 0.49903423505847644\n",
      "Epoch 5730, Loss Train(MSE): 0.12524125967292649, R2 Train: 0.49903496130829406\n",
      "Epoch 5731, Loss Train(MSE): 0.1252410861559138, R2 Train: 0.49903565537634476\n",
      "Epoch 5732, Loss Train(MSE): 0.12524089738578956, R2 Train: 0.4990364104568418\n",
      "Epoch 5733, Loss Train(MSE): 0.1252407301713857, R2 Train: 0.4990370793144572\n",
      "Epoch 5734, Loss Train(MSE): 0.12524054458343198, R2 Train: 0.4990378216662721\n",
      "Epoch 5735, Loss Train(MSE): 0.12524036759784066, R2 Train: 0.49903852960863737\n",
      "Epoch 5736, Loss Train(MSE): 0.1252401932225932, R2 Train: 0.4990392271096272\n",
      "Epoch 5737, Loss Train(MSE): 0.125240006520769, R2 Train: 0.499039973916924\n",
      "Epoch 5738, Loss Train(MSE): 0.1252398432098861, R2 Train: 0.4990406271604556\n",
      "Epoch 5739, Loss Train(MSE): 0.1252396573923049, R2 Train: 0.49904137043078045\n",
      "Epoch 5740, Loss Train(MSE): 0.12523948409042002, R2 Train: 0.49904206363831993\n",
      "Epoch 5741, Loss Train(MSE): 0.12523930967732497, R2 Train: 0.49904276129070013\n",
      "Epoch 5742, Loss Train(MSE): 0.1252391263334218, R2 Train: 0.4990434946663128\n",
      "Epoch 5743, Loss Train(MSE): 0.12523896336517612, R2 Train: 0.4990441465392955\n",
      "Epoch 5744, Loss Train(MSE): 0.12523877946084896, R2 Train: 0.49904488215660414\n",
      "Epoch 5745, Loss Train(MSE): 0.1252386089398703, R2 Train: 0.4990455642405188\n",
      "Epoch 5746, Loss Train(MSE): 0.12523843532117065, R2 Train: 0.4990462587153174\n",
      "Epoch 5747, Loss Train(MSE): 0.1252382545510256, R2 Train: 0.49904698179589757\n",
      "Epoch 5748, Loss Train(MSE): 0.12523809255740112, R2 Train: 0.4990476297703955\n",
      "Epoch 5749, Loss Train(MSE): 0.12523791062425613, R2 Train: 0.4990483575029755\n",
      "Epoch 5750, Loss Train(MSE): 0.12523774205185526, R2 Train: 0.499049031792579\n",
      "Epoch 5751, Loss Train(MSE): 0.12523756999123198, R2 Train: 0.4990497200350721\n",
      "Epoch 5752, Loss Train(MSE): 0.12523739097153427, R2 Train: 0.4990504361138629\n",
      "Epoch 5753, Loss Train(MSE): 0.12523723070795967, R2 Train: 0.49905107716816133\n",
      "Epoch 5754, Loss Train(MSE): 0.12523705070724173, R2 Train: 0.4990517971710331\n",
      "Epoch 5755, Loss Train(MSE): 0.12523688327939994, R2 Train: 0.49905246688240024\n",
      "Epoch 5756, Loss Train(MSE): 0.12523671351417026, R2 Train: 0.49905314594331895\n",
      "Epoch 5757, Loss Train(MSE): 0.12523653544958718, R2 Train: 0.49905385820165127\n",
      "Epoch 5758, Loss Train(MSE): 0.1252363776454301, R2 Train: 0.49905448941827957\n",
      "Epoch 5759, Loss Train(MSE): 0.12523619953948611, R2 Train: 0.49905520184205554\n",
      "Epoch 5760, Loss Train(MSE): 0.12523603247952014, R2 Train: 0.49905587008191943\n",
      "Epoch 5761, Loss Train(MSE): 0.12523586572153847, R2 Train: 0.4990565371138461\n",
      "Epoch 5762, Loss Train(MSE): 0.125235688761414, R2 Train: 0.499057244954344\n",
      "Epoch 5763, Loss Train(MSE): 0.1252355323100209, R2 Train: 0.49905787075991637\n",
      "Epoch 5764, Loss Train(MSE): 0.12523535697063268, R2 Train: 0.4990585721174693\n",
      "Epoch 5765, Loss Train(MSE): 0.12523518956508262, R2 Train: 0.49905924173966953\n",
      "Epoch 5766, Loss Train(MSE): 0.12523502646467954, R2 Train: 0.4990598941412818\n",
      "Epoch 5767, Loss Train(MSE): 0.12523485134071882, R2 Train: 0.4990605946371247\n",
      "Epoch 5768, Loss Train(MSE): 0.1252346939858598, R2 Train: 0.49906122405656084\n",
      "Epoch 5769, Loss Train(MSE): 0.1252345228242693, R2 Train: 0.4990619087029228\n",
      "Epoch 5770, Loss Train(MSE): 0.12523435434740277, R2 Train: 0.49906258261038894\n",
      "Epoch 5771, Loss Train(MSE): 0.12523419556902474, R2 Train: 0.49906321772390105\n",
      "Epoch 5772, Loss Train(MSE): 0.12523402224626765, R2 Train: 0.4990639110149294\n",
      "Epoch 5773, Loss Train(MSE): 0.12523386327928343, R2 Train: 0.4990645468828663\n",
      "Epoch 5774, Loss Train(MSE): 0.12523369694389366, R2 Train: 0.49906521222442535\n",
      "Epoch 5775, Loss Train(MSE): 0.12523352669463908, R2 Train: 0.4990658932214437\n",
      "Epoch 5776, Loss Train(MSE): 0.12523337287974373, R2 Train: 0.4990665084810251\n",
      "Epoch 5777, Loss Train(MSE): 0.1252332013241923, R2 Train: 0.4990671947032308\n",
      "Epoch 5778, Loss Train(MSE): 0.1252330400605264, R2 Train: 0.49906783975789437\n",
      "Epoch 5779, Loss Train(MSE): 0.1252328791772723, R2 Train: 0.4990684832909108\n",
      "Epoch 5780, Loss Train(MSE): 0.12523270869086905, R2 Train: 0.4990691652365238\n",
      "Epoch 5781, Loss Train(MSE): 0.12523255605670106, R2 Train: 0.4990697757731958\n",
      "Epoch 5782, Loss Train(MSE): 0.12523238843979242, R2 Train: 0.4990704462408303\n",
      "Epoch 5783, Loss Train(MSE): 0.12523222425237096, R2 Train: 0.49907110299051616\n",
      "Epoch 5784, Loss Train(MSE): 0.1252320693911821, R2 Train: 0.49907172243527165\n",
      "Epoch 5785, Loss Train(MSE): 0.12523190061899567, R2 Train: 0.49907239752401733\n",
      "Epoch 5786, Loss Train(MSE): 0.12523174456282096, R2 Train: 0.49907302174871615\n",
      "Epoch 5787, Loss Train(MSE): 0.12523158343211516, R2 Train: 0.4990736662715394\n",
      "Epoch 5788, Loss Train(MSE): 0.1252314156972069, R2 Train: 0.49907433721117245\n",
      "Epoch 5789, Loss Train(MSE): 0.12523126743274413, R2 Train: 0.4990749302690235\n",
      "Epoch 5790, Loss Train(MSE): 0.12523110035157842, R2 Train: 0.49907559859368633\n",
      "Epoch 5791, Loss Train(MSE): 0.12523094028202308, R2 Train: 0.49907623887190766\n",
      "Epoch 5792, Loss Train(MSE): 0.12523078617417724, R2 Train: 0.49907685530329104\n",
      "Epoch 5793, Loss Train(MSE): 0.125230620102779, R2 Train: 0.499077519588884\n",
      "Epoch 5794, Loss Train(MSE): 0.12523046735138, R2 Train: 0.49907813059448003\n",
      "Epoch 5795, Loss Train(MSE): 0.12523030773395716, R2 Train: 0.49907876906417137\n",
      "Epoch 5796, Loss Train(MSE): 0.12523014304702343, R2 Train: 0.4990794278119063\n",
      "Epoch 5797, Loss Train(MSE): 0.12522999651285233, R2 Train: 0.4990800139485907\n",
      "Epoch 5798, Loss Train(MSE): 0.12522983207467092, R2 Train: 0.4990806717013163\n",
      "Epoch 5799, Loss Train(MSE): 0.125229674246697, R2 Train: 0.49908130301321196\n",
      "Epoch 5800, Loss Train(MSE): 0.1252295226303245, R2 Train: 0.499081909478702\n",
      "Epoch 5801, Loss Train(MSE): 0.12522935918068742, R2 Train: 0.4990825632772503\n",
      "Epoch 5802, Loss Train(MSE): 0.12522920789701875, R2 Train: 0.499083168411925\n",
      "Epoch 5803, Loss Train(MSE): 0.12522905149398675, R2 Train: 0.499083794024053\n",
      "Epoch 5804, Loss Train(MSE): 0.12522888902189833, R2 Train: 0.4990844439124067\n",
      "Epoch 5805, Loss Train(MSE): 0.12522874397365233, R2 Train: 0.4990850241053907\n",
      "Epoch 5806, Loss Train(MSE): 0.12522858307403392, R2 Train: 0.4990856677038643\n",
      "Epoch 5807, Loss Train(MSE): 0.12522842577025078, R2 Train: 0.49908629691899686\n",
      "Epoch 5808, Loss Train(MSE): 0.12522827823013816, R2 Train: 0.49908688707944737\n",
      "Epoch 5809, Loss Train(MSE): 0.12522811732648764, R2 Train: 0.49908753069404943\n",
      "Epoch 5810, Loss Train(MSE): 0.12522796583023368, R2 Train: 0.49908813667906526\n",
      "Epoch 5811, Loss Train(MSE): 0.1252278141914016, R2 Train: 0.49908874323439356\n",
      "Epoch 5812, Loss Train(MSE): 0.1252276542372946, R2 Train: 0.4990893830508216\n",
      "Epoch 5813, Loss Train(MSE): 0.12522750825387835, R2 Train: 0.4990899669844866\n",
      "Epoch 5814, Loss Train(MSE): 0.1252274058513602, R2 Train: 0.49909037659455924\n",
      "Epoch 5815, Loss Train(MSE): 0.12522732221166838, R2 Train: 0.4990907111533265\n",
      "Epoch 5816, Loss Train(MSE): 0.12522725490224568, R2 Train: 0.4990909803910173\n",
      "Epoch 5817, Loss Train(MSE): 0.1252271708243247, R2 Train: 0.49909131670270124\n",
      "Epoch 5818, Loss Train(MSE): 0.12522709167966398, R2 Train: 0.4990916332813441\n",
      "Epoch 5819, Loss Train(MSE): 0.12522702021985566, R2 Train: 0.4990919191205774\n",
      "Epoch 5820, Loss Train(MSE): 0.12522693635128007, R2 Train: 0.4990922545948797\n",
      "Epoch 5821, Loss Train(MSE): 0.1252268616074522, R2 Train: 0.4990925535701912\n",
      "Epoch 5822, Loss Train(MSE): 0.12522678608955945, R2 Train: 0.4990928556417622\n",
      "Epoch 5823, Loss Train(MSE): 0.12522670242906295, R2 Train: 0.4990931902837482\n",
      "Epoch 5824, Loss Train(MSE): 0.12522663199304926, R2 Train: 0.49909347202780296\n",
      "Epoch 5825, Loss Train(MSE): 0.12522655250821665, R2 Train: 0.4990937899671334\n",
      "Epoch 5826, Loss Train(MSE): 0.1252264690879535, R2 Train: 0.49909412364818595\n",
      "Epoch 5827, Loss Train(MSE): 0.12522640279854172, R2 Train: 0.4990943888058331\n",
      "Epoch 5828, Loss Train(MSE): 0.1252263194653963, R2 Train: 0.49909472213841477\n",
      "Epoch 5829, Loss Train(MSE): 0.12522624024307163, R2 Train: 0.49909503902771346\n",
      "Epoch 5830, Loss Train(MSE): 0.12522617009374268, R2 Train: 0.4990953196250293\n",
      "Epoch 5831, Loss Train(MSE): 0.1252260869655505, R2 Train: 0.49909565213779805\n",
      "Epoch 5832, Loss Train(MSE): 0.12522601185091065, R2 Train: 0.4990959525963574\n",
      "Epoch 5833, Loss Train(MSE): 0.12522593793009998, R2 Train: 0.4990962482796001\n",
      "Epoch 5834, Loss Train(MSE): 0.1252258550056469, R2 Train: 0.4990965799774124\n",
      "Epoch 5835, Loss Train(MSE): 0.12522578390956457, R2 Train: 0.4990968643617417\n",
      "Epoch 5836, Loss Train(MSE): 0.12522570630460292, R2 Train: 0.4990971747815883\n",
      "Epoch 5837, Loss Train(MSE): 0.12522562358268924, R2 Train: 0.49909750566924305\n",
      "Epoch 5838, Loss Train(MSE): 0.12522555641714836, R2 Train: 0.49909777433140656\n",
      "Epoch 5839, Loss Train(MSE): 0.12522547521427638, R2 Train: 0.4990980991428945\n",
      "Epoch 5840, Loss Train(MSE): 0.12522539501099936, R2 Train: 0.49909841995600257\n",
      "Epoch 5841, Loss Train(MSE): 0.12522532705222067, R2 Train: 0.4990986917911173\n",
      "Epoch 5842, Loss Train(MSE): 0.12522524464907145, R2 Train: 0.4990990214037142\n",
      "Epoch 5843, Loss Train(MSE): 0.12522516827373348, R2 Train: 0.4990993269050661\n",
      "Epoch 5844, Loss Train(MSE): 0.12522509681694358, R2 Train: 0.4990996127322257\n",
      "Epoch 5845, Loss Train(MSE): 0.1252250146133725, R2 Train: 0.49909994154651005\n",
      "Epoch 5846, Loss Train(MSE): 0.1252249419805553, R2 Train: 0.49910023207777876\n",
      "Epoch 5847, Loss Train(MSE): 0.1252248671094485, R2 Train: 0.499100531562206\n",
      "Epoch 5848, Loss Train(MSE): 0.12522478510430404, R2 Train: 0.49910085958278383\n",
      "Epoch 5849, Loss Train(MSE): 0.12522471612965116, R2 Train: 0.49910113548139534\n",
      "Epoch 5850, Loss Train(MSE): 0.1252246379268799, R2 Train: 0.4991014482924804\n",
      "Epoch 5851, Loss Train(MSE): 0.12522455611902394, R2 Train: 0.4991017755239042\n",
      "Epoch 5852, Loss Train(MSE): 0.12522449071922664, R2 Train: 0.4991020371230934\n",
      "Epoch 5853, Loss Train(MSE): 0.12522440926641523, R2 Train: 0.49910236293433907\n",
      "Epoch 5854, Loss Train(MSE): 0.12522433078457576, R2 Train: 0.49910267686169696\n",
      "Epoch 5855, Loss Train(MSE): 0.12522426261559713, R2 Train: 0.4991029495376115\n",
      "Epoch 5856, Loss Train(MSE): 0.12522418111836583, R2 Train: 0.49910327552653666\n",
      "Epoch 5857, Loss Train(MSE): 0.12522410611560655, R2 Train: 0.4991035755375738\n",
      "Epoch 5858, Loss Train(MSE): 0.1252240347897977, R2 Train: 0.49910386084080915\n",
      "Epoch 5859, Loss Train(MSE): 0.12522395348704582, R2 Train: 0.4991041860518167\n",
      "Epoch 5860, Loss Train(MSE): 0.12522388188250005, R2 Train: 0.4991044724699998\n",
      "Epoch 5861, Loss Train(MSE): 0.12522380747909054, R2 Train: 0.49910477008363785\n",
      "Epoch 5862, Loss Train(MSE): 0.12522372636972454, R2 Train: 0.49910509452110186\n",
      "Epoch 5863, Loss Train(MSE): 0.1252236580835272, R2 Train: 0.4991053676658912\n",
      "Epoch 5864, Loss Train(MSE): 0.12522358068076356, R2 Train: 0.49910567727694577\n",
      "Epoch 5865, Loss Train(MSE): 0.12522349976370234, R2 Train: 0.49910600094519064\n",
      "Epoch 5866, Loss Train(MSE): 0.12522343471697667, R2 Train: 0.49910626113209333\n",
      "Epoch 5867, Loss Train(MSE): 0.12522335439213517, R2 Train: 0.49910658243145933\n",
      "Epoch 5868, Loss Train(MSE): 0.1252232762276128, R2 Train: 0.49910689508954875\n",
      "Epoch 5869, Loss Train(MSE): 0.12522320921802005, R2 Train: 0.4991071631279198\n",
      "Epoch 5870, Loss Train(MSE): 0.12522312860385698, R2 Train: 0.4991074855845721\n",
      "Epoch 5871, Loss Train(MSE): 0.1252230535894426, R2 Train: 0.4991077856422296\n",
      "Epoch 5872, Loss Train(MSE): 0.1252229837446973, R2 Train: 0.49910806502121075\n",
      "Epoch 5873, Loss Train(MSE): 0.12522290332017083, R2 Train: 0.4991083867193167\n",
      "Epoch 5874, Loss Train(MSE): 0.12522283137928467, R2 Train: 0.49910867448286134\n",
      "Epoch 5875, Loss Train(MSE): 0.12522275877440986, R2 Train: 0.49910896490236056\n",
      "Epoch 5876, Loss Train(MSE): 0.12522267853848024, R2 Train: 0.49910928584607905\n",
      "Epoch 5877, Loss Train(MSE): 0.12522260959548787, R2 Train: 0.49910956161804854\n",
      "Epoch 5878, Loss Train(MSE): 0.12522253430457825, R2 Train: 0.499109862781687\n",
      "Epoch 5879, Loss Train(MSE): 0.12522245425621728, R2 Train: 0.4991101829751309\n",
      "Epoch 5880, Loss Train(MSE): 0.12522238823641743, R2 Train: 0.4991104470543303\n",
      "Epoch 5881, Loss Train(MSE): 0.12522231033265135, R2 Train: 0.4991107586693946\n",
      "Epoch 5882, Loss Train(MSE): 0.12522223116726766, R2 Train: 0.49911107533092935\n",
      "Epoch 5883, Loss Train(MSE): 0.12522216660240926, R2 Train: 0.49911133359036297\n",
      "Epoch 5884, Loss Train(MSE): 0.12522208684960276, R2 Train: 0.49911165260158896\n",
      "Epoch 5885, Loss Train(MSE): 0.12522201052407236, R2 Train: 0.49911195790371055\n",
      "Epoch 5886, Loss Train(MSE): 0.12522194342737586, R2 Train: 0.49911222629049656\n",
      "Epoch 5887, Loss Train(MSE): 0.12522186385960052, R2 Train: 0.4991125445615979\n",
      "Epoch 5888, Loss Train(MSE): 0.12522179030138372, R2 Train: 0.4991128387944651\n",
      "Epoch 5889, Loss Train(MSE): 0.1252217207439064, R2 Train: 0.49911311702437444\n",
      "Epoch 5890, Loss Train(MSE): 0.1252216413601724, R2 Train: 0.49911343455931045\n",
      "Epoch 5891, Loss Train(MSE): 0.12522157049762234, R2 Train: 0.49911371800951065\n",
      "Epoch 5892, Loss Train(MSE): 0.12522149854954448, R2 Train: 0.4991140058018221\n",
      "Epoch 5893, Loss Train(MSE): 0.12522141934887276, R2 Train: 0.49911432260450894\n",
      "Epoch 5894, Loss Train(MSE): 0.12522135111122396, R2 Train: 0.4991145955551042\n",
      "Epoch 5895, Loss Train(MSE): 0.12522127684185996, R2 Train: 0.49911489263256015\n",
      "Epoch 5896, Loss Train(MSE): 0.12522119782328192, R2 Train: 0.4991152087068723\n",
      "Epoch 5897, Loss Train(MSE): 0.12522113214063924, R2 Train: 0.49911547143744306\n",
      "Epoch 5898, Loss Train(MSE): 0.1252210556184486, R2 Train: 0.4991157775262056\n",
      "Epoch 5899, Loss Train(MSE): 0.125220976781006, R2 Train: 0.499116092875976\n",
      "Epoch 5900, Loss Train(MSE): 0.12522091358433324, R2 Train: 0.49911634566266705\n",
      "Epoch 5901, Loss Train(MSE): 0.12522083487693153, R2 Train: 0.4991166604922739\n",
      "Epoch 5902, Loss Train(MSE): 0.12522075849854739, R2 Train: 0.49911696600581046\n",
      "Epoch 5903, Loss Train(MSE): 0.12522069316058118, R2 Train: 0.4991172273576753\n",
      "Epoch 5904, Loss Train(MSE): 0.12522061460872055, R2 Train: 0.4991175415651178\n",
      "Epoch 5905, Loss Train(MSE): 0.1252205406411217, R2 Train: 0.4991178374355132\n",
      "Epoch 5906, Loss Train(MSE): 0.12522047319096266, R2 Train: 0.49911810723614936\n",
      "Epoch 5907, Loss Train(MSE): 0.12522039481787298, R2 Train: 0.4991184207285081\n",
      "Epoch 5908, Loss Train(MSE): 0.1252203231940039, R2 Train: 0.49911870722398444\n",
      "Epoch 5909, Loss Train(MSE): 0.1252202536973232, R2 Train: 0.4991189852107072\n",
      "Epoch 5910, Loss Train(MSE): 0.12522017550208078, R2 Train: 0.49911929799167687\n",
      "Epoch 5911, Loss Train(MSE): 0.1252201061557086, R2 Train: 0.49911957537716556\n",
      "Epoch 5912, Loss Train(MSE): 0.12522003467736892, R2 Train: 0.4991198612905243\n",
      "Epoch 5913, Loss Train(MSE): 0.12521995665905986, R2 Train: 0.4991201733637606\n",
      "Epoch 5914, Loss Train(MSE): 0.12521988952476396, R2 Train: 0.4991204419009442\n",
      "Epoch 5915, Loss Train(MSE): 0.12521981612882976, R2 Train: 0.49912073548468094\n",
      "Epoch 5916, Loss Train(MSE): 0.12521973828654964, R2 Train: 0.49912104685380143\n",
      "Epoch 5917, Loss Train(MSE): 0.12521967329971123, R2 Train: 0.4991213068011551\n",
      "Epoch 5918, Loss Train(MSE): 0.12521959804945895, R2 Train: 0.4991216078021642\n",
      "Epoch 5919, Loss Train(MSE): 0.12521952038231263, R2 Train: 0.4991219184707495\n",
      "Epoch 5920, Loss Train(MSE): 0.1252194574791049, R2 Train: 0.4991221700835804\n",
      "Epoch 5921, Loss Train(MSE): 0.1252193804370326, R2 Train: 0.4991224782518696\n",
      "Epoch 5922, Loss Train(MSE): 0.12521930433033787, R2 Train: 0.4991227826786485\n",
      "Epoch 5923, Loss Train(MSE): 0.12521924067423862, R2 Train: 0.4991230373030455\n",
      "Epoch 5924, Loss Train(MSE): 0.1252191632833683, R2 Train: 0.4991233468665268\n",
      "Epoch 5925, Loss Train(MSE): 0.1252190891925877, R2 Train: 0.4991236432296492\n",
      "Epoch 5926, Loss Train(MSE): 0.12521902381035727, R2 Train: 0.49912390475857094\n",
      "Epoch 5927, Loss Train(MSE): 0.12521894659241214, R2 Train: 0.4991242136303514\n",
      "Epoch 5928, Loss Train(MSE): 0.1252188744555354, R2 Train: 0.4991245021778584\n",
      "Epoch 5929, Loss Train(MSE): 0.1252188074078878, R2 Train: 0.4991247703684488\n",
      "Epoch 5930, Loss Train(MSE): 0.12521873036200393, R2 Train: 0.4991250785519843\n",
      "Epoch 5931, Loss Train(MSE): 0.12521866011777955, R2 Train: 0.4991253595288818\n",
      "Epoch 5932, Loss Train(MSE): 0.12521859146468287, R2 Train: 0.49912563414126854\n",
      "Epoch 5933, Loss Train(MSE): 0.1252185145900053, R2 Train: 0.4991259416399788\n",
      "Epoch 5934, Loss Train(MSE): 0.12521844617793096, R2 Train: 0.49912621528827616\n",
      "Epoch 5935, Loss Train(MSE): 0.12521837597861682, R2 Train: 0.4991264960855327\n",
      "Epoch 5936, Loss Train(MSE): 0.12521829927429898, R2 Train: 0.4991268029028041\n",
      "Epoch 5937, Loss Train(MSE): 0.12521823263461235, R2 Train: 0.4991270694615506\n",
      "Epoch 5938, Loss Train(MSE): 0.1252181609475847, R2 Train: 0.4991273562096612\n",
      "Epoch 5939, Loss Train(MSE): 0.1252180844127887, R2 Train: 0.4991276623488452\n",
      "Epoch 5940, Loss Train(MSE): 0.12521801948645803, R2 Train: 0.4991279220541679\n",
      "Epoch 5941, Loss Train(MSE): 0.1252179463695026, R2 Train: 0.49912821452198963\n",
      "Epoch 5942, Loss Train(MSE): 0.1252178700033987, R2 Train: 0.4991285199864052\n",
      "Epoch 5943, Loss Train(MSE): 0.12521780673211397, R2 Train: 0.49912877307154413\n",
      "Epoch 5944, Loss Train(MSE): 0.1252177322423067, R2 Train: 0.4991290710307732\n",
      "Epoch 5945, Loss Train(MSE): 0.12521765604407353, R2 Train: 0.49912937582370587\n",
      "Epoch 5946, Loss Train(MSE): 0.1252175943702374, R2 Train: 0.49912962251905035\n",
      "Epoch 5947, Loss Train(MSE): 0.12521751856395347, R2 Train: 0.49912992574418613\n",
      "Epoch 5948, Loss Train(MSE): 0.125217443671549, R2 Train: 0.499130225313804\n",
      "Epoch 5949, Loss Train(MSE): 0.12521738125996973, R2 Train: 0.4991304749601211\n",
      "Epoch 5950, Loss Train(MSE): 0.12521730532675, R2 Train: 0.49913077869300004\n",
      "Epoch 5951, Loss Train(MSE): 0.12521723197297005, R2 Train: 0.4991310721081198\n",
      "Epoch 5952, Loss Train(MSE): 0.12521716830187118, R2 Train: 0.49913132679251526\n",
      "Epoch 5953, Loss Train(MSE): 0.12521709253449534, R2 Train: 0.49913162986201864\n",
      "Epoch 5954, Loss Train(MSE): 0.12521702066336635, R2 Train: 0.4991319173465346\n",
      "Epoch 5955, Loss Train(MSE): 0.12521695578752784, R2 Train: 0.49913217684988864\n",
      "Epoch 5956, Loss Train(MSE): 0.12521688018520194, R2 Train: 0.49913247925919224\n",
      "Epoch 5957, Loss Train(MSE): 0.12521680974143368, R2 Train: 0.4991327610342653\n",
      "Epoch 5958, Loss Train(MSE): 0.12521674371496333, R2 Train: 0.4991330251401467\n",
      "Epoch 5959, Loss Train(MSE): 0.12521666827690106, R2 Train: 0.49913332689239576\n",
      "Epoch 5960, Loss Train(MSE): 0.12521659920587844, R2 Train: 0.49913360317648625\n",
      "Epoch 5961, Loss Train(MSE): 0.12521653208221992, R2 Train: 0.49913387167112033\n",
      "Epoch 5962, Loss Train(MSE): 0.12521645680764273, R2 Train: 0.49913417276942906\n",
      "Epoch 5963, Loss Train(MSE): 0.12521638905541724, R2 Train: 0.49913444377833105\n",
      "Epoch 5964, Loss Train(MSE): 0.12521632088735865, R2 Train: 0.4991347164505654\n",
      "Epoch 5965, Loss Train(MSE): 0.1252162457754954, R2 Train: 0.4991350168980184\n",
      "Epoch 5966, Loss Train(MSE): 0.1252161792887771, R2 Train: 0.4991352828448916\n",
      "Epoch 5967, Loss Train(MSE): 0.12521611012845865, R2 Train: 0.4991355594861654\n",
      "Epoch 5968, Loss Train(MSE): 0.12521603517854546, R2 Train: 0.49913585928581816\n",
      "Epoch 5969, Loss Train(MSE): 0.12521596990469497, R2 Train: 0.4991361203812201\n",
      "Epoch 5970, Loss Train(MSE): 0.12521589980361691, R2 Train: 0.49913640078553234\n",
      "Epoch 5971, Loss Train(MSE): 0.12521582501489734, R2 Train: 0.49913669994041066\n",
      "Epoch 5972, Loss Train(MSE): 0.12521576090191786, R2 Train: 0.4991369563923286\n",
      "Epoch 5973, Loss Train(MSE): 0.12521568991094836, R2 Train: 0.49913724035620655\n",
      "Epoch 5974, Loss Train(MSE): 0.12521561528267297, R2 Train: 0.4991375388693081\n",
      "Epoch 5975, Loss Train(MSE): 0.12521555227920236, R2 Train: 0.49913779088319055\n",
      "Epoch 5976, Loss Train(MSE): 0.1252154804485851, R2 Train: 0.4991380782056596\n",
      "Epoch 5977, Loss Train(MSE): 0.12521540598001166, R2 Train: 0.49913837607995337\n",
      "Epoch 5978, Loss Train(MSE): 0.12521534403531479, R2 Train: 0.49913862385874086\n",
      "Epoch 5979, Loss Train(MSE): 0.1252152714146766, R2 Train: 0.49913891434129365\n",
      "Epoch 5980, Loss Train(MSE): 0.12521519710506973, R2 Train: 0.4991392115797211\n",
      "Epoch 5981, Loss Train(MSE): 0.12521513616903093, R2 Train: 0.49913945532387627\n",
      "Epoch 5982, Loss Train(MSE): 0.12521506280738912, R2 Train: 0.49913974877044354\n",
      "Epoch 5983, Loss Train(MSE): 0.1252149886567462, R2 Train: 0.49914004537301515\n",
      "Epoch 5984, Loss Train(MSE): 0.1252149286780297, R2 Train: 0.49914028528788124\n",
      "Epoch 5985, Loss Train(MSE): 0.1252148546196249, R2 Train: 0.4991405815215004\n",
      "Epoch 5986, Loss Train(MSE): 0.125214781429429, R2 Train: 0.49914087428228404\n",
      "Epoch 5987, Loss Train(MSE): 0.12521472075623935, R2 Train: 0.4991411169750426\n",
      "Epoch 5988, Loss Train(MSE): 0.1252146468549865, R2 Train: 0.49914141258005396\n",
      "Epoch 5989, Loss Train(MSE): 0.1252145745765188, R2 Train: 0.4991417016939248\n",
      "Epoch 5990, Loss Train(MSE): 0.1252145132565005, R2 Train: 0.49914194697399805\n",
      "Epoch 5991, Loss Train(MSE): 0.1252144395116874, R2 Train: 0.4991422419532504\n",
      "Epoch 5992, Loss Train(MSE): 0.12521436809682368, R2 Train: 0.4991425276127053\n",
      "Epoch 5993, Loss Train(MSE): 0.12521430617703597, R2 Train: 0.49914277529185613\n",
      "Epoch 5994, Loss Train(MSE): 0.12521423258795708, R2 Train: 0.4991430696481717\n",
      "Epoch 5995, Loss Train(MSE): 0.12521416198916047, R2 Train: 0.49914335204335814\n",
      "Epoch 5996, Loss Train(MSE): 0.12521409951608453, R2 Train: 0.4991436019356619\n",
      "Epoch 5997, Loss Train(MSE): 0.12521402608204066, R2 Train: 0.49914389567183737\n",
      "Epoch 5998, Loss Train(MSE): 0.1252139562523547, R2 Train: 0.49914417499058117\n",
      "Epoch 5999, Loss Train(MSE): 0.12521389327190074, R2 Train: 0.499144426912397\n",
      "Epoch 6000, Loss Train(MSE): 0.12521381999219905, R2 Train: 0.4991447200312038\n",
      "Epoch 6001, Loss Train(MSE): 0.1252137508852405, R2 Train: 0.499144996459038\n",
      "Epoch 6002, Loss Train(MSE): 0.1252136874427544, R2 Train: 0.4991452502289824\n",
      "Epoch 6003, Loss Train(MSE): 0.1252136143167084, R2 Train: 0.49914554273316636\n",
      "Epoch 6004, Loss Train(MSE): 0.12521354588666045, R2 Train: 0.4991458164533582\n",
      "Epoch 6005, Loss Train(MSE): 0.12521348202693067, R2 Train: 0.49914607189227733\n",
      "Epoch 6006, Loss Train(MSE): 0.12521340905385994, R2 Train: 0.49914636378456023\n",
      "Epoch 6007, Loss Train(MSE): 0.12521334125546546, R2 Train: 0.49914663497813816\n",
      "Epoch 6008, Loss Train(MSE): 0.12521327702272958, R2 Train: 0.4991468919090817\n",
      "Epoch 6009, Loss Train(MSE): 0.12521320420195992, R2 Train: 0.49914718319216034\n",
      "Epoch 6010, Loss Train(MSE): 0.12521313699051473, R2 Train: 0.4991474520379411\n",
      "Epoch 6011, Loss Train(MSE): 0.12521307242846605, R2 Train: 0.4991477102861358\n",
      "Epoch 6012, Loss Train(MSE): 0.12521299975932917, R2 Train: 0.4991480009626833\n",
      "Epoch 6013, Loss Train(MSE): 0.12521293309067544, R2 Train: 0.49914826763729825\n",
      "Epoch 6014, Loss Train(MSE): 0.1252128682424696, R2 Train: 0.4991485270301216\n",
      "Epoch 6015, Loss Train(MSE): 0.12521279572430316, R2 Train: 0.49914881710278736\n",
      "Epoch 6016, Loss Train(MSE): 0.1252127295548229, R2 Train: 0.4991490817807084\n",
      "Epoch 6017, Loss Train(MSE): 0.12521266446308393, R2 Train: 0.49914934214766427\n",
      "Epoch 6018, Loss Train(MSE): 0.1252125920952315, R2 Train: 0.499149631619074\n",
      "Epoch 6019, Loss Train(MSE): 0.12521252638184013, R2 Train: 0.49914989447263947\n",
      "Epoch 6020, Loss Train(MSE): 0.12521246108866727, R2 Train: 0.49915015564533094\n",
      "Epoch 6021, Loss Train(MSE): 0.12521238887047803, R2 Train: 0.4991504445180879\n",
      "Epoch 6022, Loss Train(MSE): 0.12521232357061798, R2 Train: 0.49915070571752806\n",
      "Epoch 6023, Loss Train(MSE): 0.12521225811759146, R2 Train: 0.49915096752963417\n",
      "Epoch 6024, Loss Train(MSE): 0.1252121860484204, R2 Train: 0.49915125580631836\n",
      "Epoch 6025, Loss Train(MSE): 0.12521212112005492, R2 Train: 0.4991515155197803\n",
      "Epoch 6026, Loss Train(MSE): 0.1252120555482423, R2 Train: 0.4991517778070308\n",
      "Epoch 6027, Loss Train(MSE): 0.12521198362745012, R2 Train: 0.4991520654901995\n",
      "Epoch 6028, Loss Train(MSE): 0.125211919029057, R2 Train: 0.49915232388377195\n",
      "Epoch 6029, Loss Train(MSE): 0.12521185337901924, R2 Train: 0.499152586483923\n",
      "Epoch 6030, Loss Train(MSE): 0.12521178160597196, R2 Train: 0.49915287357611215\n",
      "Epoch 6031, Loss Train(MSE): 0.12521171729653768, R2 Train: 0.49915313081384927\n",
      "Epoch 6032, Loss Train(MSE): 0.12521165160833514, R2 Train: 0.49915339356665944\n",
      "Epoch 6033, Loss Train(MSE): 0.12521157998240434, R2 Train: 0.49915368007038263\n",
      "Epoch 6034, Loss Train(MSE): 0.1252115159214178, R2 Train: 0.4991539363143288\n",
      "Epoch 6035, Loss Train(MSE): 0.125211450234616, R2 Train: 0.499154199061536\n",
      "Epoch 6036, Loss Train(MSE): 0.12521137875517868, R2 Train: 0.4991544849792853\n",
      "Epoch 6037, Loss Train(MSE): 0.1252113149026252, R2 Train: 0.49915474038949925\n",
      "Epoch 6038, Loss Train(MSE): 0.12521124925630084, R2 Train: 0.49915500297479665\n",
      "Epoch 6039, Loss Train(MSE): 0.1252111779227393, R2 Train: 0.49915528830904277\n",
      "Epoch 6040, Loss Train(MSE): 0.12521111423909514, R2 Train: 0.4991555430436194\n",
      "Epoch 6041, Loss Train(MSE): 0.1252110486718417, R2 Train: 0.49915580531263315\n",
      "Epoch 6042, Loss Train(MSE): 0.1252109774835435, R2 Train: 0.49915609006582595\n",
      "Epoch 6043, Loss Train(MSE): 0.12521091392976974, R2 Train: 0.49915634428092104\n",
      "Epoch 6044, Loss Train(MSE): 0.12521084847970332, R2 Train: 0.4991566060811867\n",
      "Epoch 6045, Loss Train(MSE): 0.12521077743606118, R2 Train: 0.49915689025575527\n",
      "Epoch 6046, Loss Train(MSE): 0.12521071397359812, R2 Train: 0.4991571441056075\n",
      "Epoch 6047, Loss Train(MSE): 0.12521064867836285, R2 Train: 0.4991574052865486\n",
      "Epoch 6048, Loss Train(MSE): 0.12521057777877456, R2 Train: 0.49915768888490175\n",
      "Epoch 6049, Loss Train(MSE): 0.12521051436953615, R2 Train: 0.4991579425218554\n",
      "Epoch 6050, Loss Train(MSE): 0.1252104492663099, R2 Train: 0.49915820293476043\n",
      "Epoch 6051, Loss Train(MSE): 0.12521037851017833, R2 Train: 0.4991584859592867\n",
      "Epoch 6052, Loss Train(MSE): 0.1252103151165465, R2 Train: 0.49915873953381396\n",
      "Epoch 6053, Loss Train(MSE): 0.12521025024204618, R2 Train: 0.49915899903181526\n",
      "Epoch 6054, Loss Train(MSE): 0.1252101796287792, R2 Train: 0.4991592814848832\n",
      "Epoch 6055, Loss Train(MSE): 0.1252101162135986, R2 Train: 0.4991595351456056\n",
      "Epoch 6056, Loss Train(MSE): 0.1252100516040856, R2 Train: 0.4991597935836576\n",
      "Epoch 6057, Loss Train(MSE): 0.12520998113309598, R2 Train: 0.4991600754676161\n",
      "Epoch 6058, Loss Train(MSE): 0.12520991765966819, R2 Train: 0.49916032936132726\n",
      "Epoch 6059, Loss Train(MSE): 0.12520985335095383, R2 Train: 0.4991605865961847\n",
      "Epoch 6060, Loss Train(MSE): 0.12520978302165914, R2 Train: 0.49916086791336345\n",
      "Epoch 6061, Loss Train(MSE): 0.12520971945373774, R2 Train: 0.49916112218504904\n",
      "Epoch 6062, Loss Train(MSE): 0.1252096554811883, R2 Train: 0.49916137807524685\n",
      "Epoch 6063, Loss Train(MSE): 0.12520958529301102, R2 Train: 0.49916165882795593\n",
      "Epoch 6064, Loss Train(MSE): 0.12520952159479604, R2 Train: 0.49916191362081586\n",
      "Epoch 6065, Loss Train(MSE): 0.12520945799333807, R2 Train: 0.4991621680266477\n",
      "Epoch 6066, Loss Train(MSE): 0.12520938794570527, R2 Train: 0.4991624482171789\n",
      "Epoch 6067, Loss Train(MSE): 0.12520932408183802, R2 Train: 0.4991627036726479\n",
      "Epoch 6068, Loss Train(MSE): 0.1252092608859635, R2 Train: 0.49916295645614595\n",
      "Epoch 6069, Loss Train(MSE): 0.12520919097830696, R2 Train: 0.49916323608677216\n",
      "Epoch 6070, Loss Train(MSE): 0.12520912691386513, R2 Train: 0.4991634923445395\n",
      "Epoch 6071, Loss Train(MSE): 0.1252090641576363, R2 Train: 0.4991637433694548\n",
      "Epoch 6072, Loss Train(MSE): 0.12520899438939243, R2 Train: 0.4991640224424303\n",
      "Epoch 6073, Loss Train(MSE): 0.1252089300898847, R2 Train: 0.4991642796404612\n",
      "Epoch 6074, Loss Train(MSE): 0.1252088678069393, R2 Train: 0.4991645287722428\n",
      "Epoch 6075, Loss Train(MSE): 0.12520879817754907, R2 Train: 0.4991648072898037\n",
      "Epoch 6076, Loss Train(MSE): 0.12520873360891027, R2 Train: 0.4991650655643589\n",
      "Epoch 6077, Loss Train(MSE): 0.12520867183246634, R2 Train: 0.49916531267013464\n",
      "Epoch 6078, Loss Train(MSE): 0.12520860234137507, R2 Train: 0.4991655906344997\n",
      "Epoch 6079, Loss Train(MSE): 0.1252085374699613, R2 Train: 0.4991658501201548\n",
      "Epoch 6080, Loss Train(MSE): 0.12520847623282202, R2 Train: 0.4991660950687119\n",
      "Epoch 6081, Loss Train(MSE): 0.12520840687947968, R2 Train: 0.4991663724820813\n",
      "Epoch 6082, Loss Train(MSE): 0.12520834167206318, R2 Train: 0.49916663331174727\n",
      "Epoch 6083, Loss Train(MSE): 0.12520828100662174, R2 Train: 0.49916687597351306\n",
      "Epoch 6084, Loss Train(MSE): 0.1252082117904825, R2 Train: 0.49916715283807\n",
      "Epoch 6085, Loss Train(MSE): 0.12520814621424722, R2 Train: 0.4991674151430111\n",
      "Epoch 6086, Loss Train(MSE): 0.1252080861524914, R2 Train: 0.4991676553900344\n",
      "Epoch 6087, Loss Train(MSE): 0.1252080170730138, R2 Train: 0.4991679317079448\n",
      "Epoch 6088, Loss Train(MSE): 0.12520795109555027, R2 Train: 0.4991681956177989\n",
      "Epoch 6089, Loss Train(MSE): 0.12520789166906734, R2 Train: 0.4991684333237306\n",
      "Epoch 6090, Loss Train(MSE): 0.1252078227257143, R2 Train: 0.49916870909714284\n",
      "Epoch 6091, Loss Train(MSE): 0.12520775631501513, R2 Train: 0.4991689747399395\n",
      "Epoch 6092, Loss Train(MSE): 0.12520769755499622, R2 Train: 0.4991692097800151\n",
      "Epoch 6093, Loss Train(MSE): 0.12520762874723473, R2 Train: 0.49916948501106106\n",
      "Epoch 6094, Loss Train(MSE): 0.12520756187169005, R2 Train: 0.4991697525132398\n",
      "Epoch 6095, Loss Train(MSE): 0.12520750380893497, R2 Train: 0.4991699847642601\n",
      "Epoch 6096, Loss Train(MSE): 0.12520743513623614, R2 Train: 0.49917025945505544\n",
      "Epoch 6097, Loss Train(MSE): 0.1252073677646288, R2 Train: 0.49917052894148484\n",
      "Epoch 6098, Loss Train(MSE): 0.1252073104295503, R2 Train: 0.4991707582817988\n",
      "Epoch 6099, Loss Train(MSE): 0.12520724189138951, R2 Train: 0.49917103243444194\n",
      "Epoch 6100, Loss Train(MSE): 0.1252071739928908, R2 Train: 0.4991713040284368\n",
      "Epoch 6101, Loss Train(MSE): 0.12521030548955814, R2 Train: 0.49915877804176745\n",
      "Epoch 6102, Loss Train(MSE): 0.12521106474209232, R2 Train: 0.49915574103163074\n",
      "Epoch 6103, Loss Train(MSE): 0.12521098001468992, R2 Train: 0.49915607994124034\n",
      "Epoch 6104, Loss Train(MSE): 0.12521089535217034, R2 Train: 0.49915641859131865\n",
      "Epoch 6105, Loss Train(MSE): 0.12521252773336447, R2 Train: 0.4991498890665421\n",
      "Epoch 6106, Loss Train(MSE): 0.12523048004553775, R2 Train: 0.499078079817849\n",
      "Epoch 6107, Loss Train(MSE): 0.12523029502490787, R2 Train: 0.4990788199003685\n",
      "Epoch 6108, Loss Train(MSE): 0.12523011035903245, R2 Train: 0.4990795585638702\n",
      "Epoch 6109, Loss Train(MSE): 0.12522992604670827, R2 Train: 0.49908029581316693\n",
      "Epoch 6110, Loss Train(MSE): 0.125229742086738, R2 Train: 0.49908103165304796\n",
      "Epoch 6111, Loss Train(MSE): 0.12522955847793082, R2 Train: 0.4990817660882767\n",
      "Epoch 6112, Loss Train(MSE): 0.12522937521910205, R2 Train: 0.4990824991235918\n",
      "Epoch 6113, Loss Train(MSE): 0.1252291923090729, R2 Train: 0.4990832307637084\n",
      "Epoch 6114, Loss Train(MSE): 0.12522900974667095, R2 Train: 0.4990839610133162\n",
      "Epoch 6115, Loss Train(MSE): 0.12522882753072972, R2 Train: 0.4990846898770811\n",
      "Epoch 6116, Loss Train(MSE): 0.1252286456600887, R2 Train: 0.4990854173596452\n",
      "Epoch 6117, Loss Train(MSE): 0.1252284641335934, R2 Train: 0.4990861434656264\n",
      "Epoch 6118, Loss Train(MSE): 0.12522828295009508, R2 Train: 0.49908686819961967\n",
      "Epoch 6119, Loss Train(MSE): 0.1252281021084511, R2 Train: 0.49908759156619564\n",
      "Epoch 6120, Loss Train(MSE): 0.1252279216075244, R2 Train: 0.49908831356990235\n",
      "Epoch 6121, Loss Train(MSE): 0.12522774144618395, R2 Train: 0.4990890342152642\n",
      "Epoch 6122, Loss Train(MSE): 0.12522756162330426, R2 Train: 0.499089753506783\n",
      "Epoch 6123, Loss Train(MSE): 0.12522738213776563, R2 Train: 0.4990904714489375\n",
      "Epoch 6124, Loss Train(MSE): 0.12522720298845402, R2 Train: 0.4990911880461839\n",
      "Epoch 6125, Loss Train(MSE): 0.125227024174261, R2 Train: 0.49909190330295605\n",
      "Epoch 6126, Loss Train(MSE): 0.12522684569408365, R2 Train: 0.4990926172236654\n",
      "Epoch 6127, Loss Train(MSE): 0.12522666754682474, R2 Train: 0.49909332981270105\n",
      "Epoch 6128, Loss Train(MSE): 0.1252264897313924, R2 Train: 0.49909404107443045\n",
      "Epoch 6129, Loss Train(MSE): 0.12522631224670025, R2 Train: 0.499094751013199\n",
      "Epoch 6130, Loss Train(MSE): 0.12522613509166738, R2 Train: 0.49909545963333046\n",
      "Epoch 6131, Loss Train(MSE): 0.1252259582652183, R2 Train: 0.49909616693912684\n",
      "Epoch 6132, Loss Train(MSE): 0.1252257817662827, R2 Train: 0.49909687293486915\n",
      "Epoch 6133, Loss Train(MSE): 0.12522560559379575, R2 Train: 0.499097577624817\n",
      "Epoch 6134, Loss Train(MSE): 0.12522542974669781, R2 Train: 0.49909828101320874\n",
      "Epoch 6135, Loss Train(MSE): 0.12522525422393446, R2 Train: 0.49909898310426215\n",
      "Epoch 6136, Loss Train(MSE): 0.12522507902445648, R2 Train: 0.4990996839021741\n",
      "Epoch 6137, Loss Train(MSE): 0.12522490414721993, R2 Train: 0.4991003834111203\n",
      "Epoch 6138, Loss Train(MSE): 0.12522472959118586, R2 Train: 0.49910108163525657\n",
      "Epoch 6139, Loss Train(MSE): 0.12522455535532043, R2 Train: 0.4991017785787183\n",
      "Epoch 6140, Loss Train(MSE): 0.12522438143859485, R2 Train: 0.4991024742456206\n",
      "Epoch 6141, Loss Train(MSE): 0.12522420783998545, R2 Train: 0.4991031686400582\n",
      "Epoch 6142, Loss Train(MSE): 0.12522403455847345, R2 Train: 0.4991038617661062\n",
      "Epoch 6143, Loss Train(MSE): 0.12522386159304502, R2 Train: 0.4991045536278199\n",
      "Epoch 6144, Loss Train(MSE): 0.12522368894269129, R2 Train: 0.49910524422923486\n",
      "Epoch 6145, Loss Train(MSE): 0.1252235166064083, R2 Train: 0.49910593357436683\n",
      "Epoch 6146, Loss Train(MSE): 0.12522334458319678, R2 Train: 0.49910662166721287\n",
      "Epoch 6147, Loss Train(MSE): 0.1252231728720625, R2 Train: 0.49910730851175\n",
      "Epoch 6148, Loss Train(MSE): 0.1252230014720159, R2 Train: 0.4991079941119364\n",
      "Epoch 6149, Loss Train(MSE): 0.12522283038207213, R2 Train: 0.4991086784717115\n",
      "Epoch 6150, Loss Train(MSE): 0.12522265960125117, R2 Train: 0.4991093615949953\n",
      "Epoch 6151, Loss Train(MSE): 0.12522248912857753, R2 Train: 0.4991100434856899\n",
      "Epoch 6152, Loss Train(MSE): 0.1252223189630806, R2 Train: 0.49911072414767765\n",
      "Epoch 6153, Loss Train(MSE): 0.12522214910379423, R2 Train: 0.4991114035848231\n",
      "Epoch 6154, Loss Train(MSE): 0.1252219795497568, R2 Train: 0.4991120818009728\n",
      "Epoch 6155, Loss Train(MSE): 0.1252218103000115, R2 Train: 0.499112758799954\n",
      "Epoch 6156, Loss Train(MSE): 0.1252216413536059, R2 Train: 0.49911343458557644\n",
      "Epoch 6157, Loss Train(MSE): 0.12522147270959197, R2 Train: 0.4991141091616321\n",
      "Epoch 6158, Loss Train(MSE): 0.12522130436702644, R2 Train: 0.49911478253189423\n",
      "Epoch 6159, Loss Train(MSE): 0.1252211363249701, R2 Train: 0.49911545470011964\n",
      "Epoch 6160, Loss Train(MSE): 0.1252209685824885, R2 Train: 0.49911612567004604\n",
      "Epoch 6161, Loss Train(MSE): 0.12522080113865147, R2 Train: 0.4991167954453941\n",
      "Epoch 6162, Loss Train(MSE): 0.12522063399253308, R2 Train: 0.4991174640298677\n",
      "Epoch 6163, Loss Train(MSE): 0.1252204671432118, R2 Train: 0.4991181314271528\n",
      "Epoch 6164, Loss Train(MSE): 0.12522030058977046, R2 Train: 0.49911879764091815\n",
      "Epoch 6165, Loss Train(MSE): 0.1252201343312961, R2 Train: 0.49911946267481555\n",
      "Epoch 6166, Loss Train(MSE): 0.12521996836687999, R2 Train: 0.49912012653248006\n",
      "Epoch 6167, Loss Train(MSE): 0.12521980269561767, R2 Train: 0.49912078921752934\n",
      "Epoch 6168, Loss Train(MSE): 0.1252196373166088, R2 Train: 0.49912145073356484\n",
      "Epoch 6169, Loss Train(MSE): 0.12521947222895727, R2 Train: 0.4991221110841709\n",
      "Epoch 6170, Loss Train(MSE): 0.12521930743177098, R2 Train: 0.49912277027291607\n",
      "Epoch 6171, Loss Train(MSE): 0.12521914292416214, R2 Train: 0.49912342830335144\n",
      "Epoch 6172, Loss Train(MSE): 0.12521897870524684, R2 Train: 0.49912408517901263\n",
      "Epoch 6173, Loss Train(MSE): 0.12521881477414534, R2 Train: 0.49912474090341863\n",
      "Epoch 6174, Loss Train(MSE): 0.12521865112998187, R2 Train: 0.4991253954800725\n",
      "Epoch 6175, Loss Train(MSE): 0.1252184877718847, R2 Train: 0.49912604891246115\n",
      "Epoch 6176, Loss Train(MSE): 0.12521832469898606, R2 Train: 0.4991267012040558\n",
      "Epoch 6177, Loss Train(MSE): 0.1252181619104221, R2 Train: 0.4991273523583116\n",
      "Epoch 6178, Loss Train(MSE): 0.12521799940533293, R2 Train: 0.4991280023786683\n",
      "Epoch 6179, Loss Train(MSE): 0.12521783718286258, R2 Train: 0.49912865126854966\n",
      "Epoch 6180, Loss Train(MSE): 0.12521767524215893, R2 Train: 0.49912929903136427\n",
      "Epoch 6181, Loss Train(MSE): 0.1252175135823737, R2 Train: 0.49912994567050517\n",
      "Epoch 6182, Loss Train(MSE): 0.12521735220266247, R2 Train: 0.4991305911893501\n",
      "Epoch 6183, Loss Train(MSE): 0.12521719110218454, R2 Train: 0.4991312355912618\n",
      "Epoch 6184, Loss Train(MSE): 0.12521703028010303, R2 Train: 0.4991318788795879\n",
      "Epoch 6185, Loss Train(MSE): 0.12521686973558496, R2 Train: 0.49913252105766015\n",
      "Epoch 6186, Loss Train(MSE): 0.12521670946780084, R2 Train: 0.49913316212879666\n",
      "Epoch 6187, Loss Train(MSE): 0.12521654947592506, R2 Train: 0.49913380209629976\n",
      "Epoch 6188, Loss Train(MSE): 0.12521638975913557, R2 Train: 0.4991344409634577\n",
      "Epoch 6189, Loss Train(MSE): 0.12521623031661414, R2 Train: 0.49913507873354346\n",
      "Epoch 6190, Loss Train(MSE): 0.12521607114754602, R2 Train: 0.49913571540981594\n",
      "Epoch 6191, Loss Train(MSE): 0.12521591225112008, R2 Train: 0.4991363509955197\n",
      "Epoch 6192, Loss Train(MSE): 0.12521575362652895, R2 Train: 0.4991369854938842\n",
      "Epoch 6193, Loss Train(MSE): 0.1252155952729687, R2 Train: 0.49913761890812525\n",
      "Epoch 6194, Loss Train(MSE): 0.1252154371896389, R2 Train: 0.4991382512414444\n",
      "Epoch 6195, Loss Train(MSE): 0.1252152793757428, R2 Train: 0.49913888249702876\n",
      "Epoch 6196, Loss Train(MSE): 0.1252151218304871, R2 Train: 0.49913951267805157\n",
      "Epoch 6197, Loss Train(MSE): 0.1252149645530819, R2 Train: 0.4991401417876724\n",
      "Epoch 6198, Loss Train(MSE): 0.12521480754274084, R2 Train: 0.49914076982903666\n",
      "Epoch 6199, Loss Train(MSE): 0.12521465079868097, R2 Train: 0.49914139680527614\n",
      "Epoch 6200, Loss Train(MSE): 0.1252144943201228, R2 Train: 0.4991420227195088\n",
      "Epoch 6201, Loss Train(MSE): 0.12521433810629026, R2 Train: 0.49914264757483895\n",
      "Epoch 6202, Loss Train(MSE): 0.12521418215641053, R2 Train: 0.4991432713743579\n",
      "Epoch 6203, Loss Train(MSE): 0.12521402646971422, R2 Train: 0.49914389412114313\n",
      "Epoch 6204, Loss Train(MSE): 0.1252138710454354, R2 Train: 0.49914451581825836\n",
      "Epoch 6205, Loss Train(MSE): 0.1252137158828113, R2 Train: 0.4991451364687548\n",
      "Epoch 6206, Loss Train(MSE): 0.12521356098108247, R2 Train: 0.49914575607567013\n",
      "Epoch 6207, Loss Train(MSE): 0.12521340633949277, R2 Train: 0.49914637464202893\n",
      "Epoch 6208, Loss Train(MSE): 0.12521325195728936, R2 Train: 0.4991469921708426\n",
      "Epoch 6209, Loss Train(MSE): 0.12521309783372256, R2 Train: 0.49914760866510977\n",
      "Epoch 6210, Loss Train(MSE): 0.12521294396804591, R2 Train: 0.49914822412781634\n",
      "Epoch 6211, Loss Train(MSE): 0.12521279035951624, R2 Train: 0.49914883856193504\n",
      "Epoch 6212, Loss Train(MSE): 0.12521263700739352, R2 Train: 0.49914945197042593\n",
      "Epoch 6213, Loss Train(MSE): 0.1252124839109408, R2 Train: 0.49915006435623677\n",
      "Epoch 6214, Loss Train(MSE): 0.12521233106942445, R2 Train: 0.4991506757223022\n",
      "Epoch 6215, Loss Train(MSE): 0.12521217848211375, R2 Train: 0.499151286071545\n",
      "Epoch 6216, Loss Train(MSE): 0.1252120261482812, R2 Train: 0.4991518954068752\n",
      "Epoch 6217, Loss Train(MSE): 0.12521187406720244, R2 Train: 0.49915250373119024\n",
      "Epoch 6218, Loss Train(MSE): 0.12521172223815608, R2 Train: 0.4991531110473757\n",
      "Epoch 6219, Loss Train(MSE): 0.1252115706604239, R2 Train: 0.4991537173583044\n",
      "Epoch 6220, Loss Train(MSE): 0.12521141933329055, R2 Train: 0.4991543226668378\n",
      "Epoch 6221, Loss Train(MSE): 0.12521126825604378, R2 Train: 0.49915492697582486\n",
      "Epoch 6222, Loss Train(MSE): 0.1252111174279744, R2 Train: 0.49915553028810244\n",
      "Epoch 6223, Loss Train(MSE): 0.12521096684837615, R2 Train: 0.4991561326064954\n",
      "Epoch 6224, Loss Train(MSE): 0.12521081651654573, R2 Train: 0.4991567339338171\n",
      "Epoch 6225, Loss Train(MSE): 0.12521066643178264, R2 Train: 0.49915733427286946\n",
      "Epoch 6226, Loss Train(MSE): 0.1252105165933897, R2 Train: 0.49915793362644123\n",
      "Epoch 6227, Loss Train(MSE): 0.12521036700067223, R2 Train: 0.4991585319973111\n",
      "Epoch 6228, Loss Train(MSE): 0.12521021765293863, R2 Train: 0.49915912938824547\n",
      "Epoch 6229, Loss Train(MSE): 0.12521006854950018, R2 Train: 0.4991597258019993\n",
      "Epoch 6230, Loss Train(MSE): 0.1252099196896711, R2 Train: 0.4991603212413156\n",
      "Epoch 6231, Loss Train(MSE): 0.12520977107276818, R2 Train: 0.4991609157089273\n",
      "Epoch 6232, Loss Train(MSE): 0.12520962269811137, R2 Train: 0.49916150920755453\n",
      "Epoch 6233, Loss Train(MSE): 0.12520947456502324, R2 Train: 0.49916210173990705\n",
      "Epoch 6234, Loss Train(MSE): 0.1252093266728292, R2 Train: 0.49916269330868324\n",
      "Epoch 6235, Loss Train(MSE): 0.12520917902085743, R2 Train: 0.4991632839165703\n",
      "Epoch 6236, Loss Train(MSE): 0.1252090316084389, R2 Train: 0.49916387356624436\n",
      "Epoch 6237, Loss Train(MSE): 0.12520888443490735, R2 Train: 0.4991644622603706\n",
      "Epoch 6238, Loss Train(MSE): 0.12520873749959918, R2 Train: 0.49916505000160327\n",
      "Epoch 6239, Loss Train(MSE): 0.12520859080185356, R2 Train: 0.49916563679258574\n",
      "Epoch 6240, Loss Train(MSE): 0.12520844434101236, R2 Train: 0.49916622263595056\n",
      "Epoch 6241, Loss Train(MSE): 0.12520832895935033, R2 Train: 0.4991666841625987\n",
      "Epoch 6242, Loss Train(MSE): 0.1252082462399066, R2 Train: 0.49916701504037364\n",
      "Epoch 6243, Loss Train(MSE): 0.12520816358253056, R2 Train: 0.49916734566987775\n",
      "Epoch 6244, Loss Train(MSE): 0.12520808098715241, R2 Train: 0.49916767605139034\n",
      "Epoch 6245, Loss Train(MSE): 0.12520799845370217, R2 Train: 0.4991680061851913\n",
      "Epoch 6246, Loss Train(MSE): 0.1252079159821102, R2 Train: 0.4991683360715592\n",
      "Epoch 6247, Loss Train(MSE): 0.12520783357230683, R2 Train: 0.4991686657107727\n",
      "Epoch 6248, Loss Train(MSE): 0.1252077512242226, R2 Train: 0.49916899510310964\n",
      "Epoch 6249, Loss Train(MSE): 0.12520766893778804, R2 Train: 0.49916932424884786\n",
      "Epoch 6250, Loss Train(MSE): 0.12520758671293392, R2 Train: 0.49916965314826434\n",
      "Epoch 6251, Loss Train(MSE): 0.12520750454959104, R2 Train: 0.49916998180163585\n",
      "Epoch 6252, Loss Train(MSE): 0.1252074224476903, R2 Train: 0.49917031020923885\n",
      "Epoch 6253, Loss Train(MSE): 0.12520734040716278, R2 Train: 0.4991706383713489\n",
      "Epoch 6254, Loss Train(MSE): 0.1252072625429991, R2 Train: 0.4991709498280036\n",
      "Epoch 6255, Loss Train(MSE): 0.12520720220233617, R2 Train: 0.49917119119065534\n",
      "Epoch 6256, Loss Train(MSE): 0.1252071357453498, R2 Train: 0.4991714570186008\n",
      "Epoch 6257, Loss Train(MSE): 0.12520706523548267, R2 Train: 0.49917173905806933\n",
      "Epoch 6258, Loss Train(MSE): 0.12520700805070634, R2 Train: 0.49917196779717465\n",
      "Epoch 6259, Loss Train(MSE): 0.12520693870054705, R2 Train: 0.4991722451978118\n",
      "Epoch 6260, Loss Train(MSE): 0.1252068702723102, R2 Train: 0.4991725189107592\n",
      "Epoch 6261, Loss Train(MSE): 0.12520681232876005, R2 Train: 0.4991727506849598\n",
      "Epoch 6262, Loss Train(MSE): 0.12520674207041912, R2 Train: 0.4991730317183235\n",
      "Epoch 6263, Loss Train(MSE): 0.12520667671922267, R2 Train: 0.4991732931231093\n",
      "Epoch 6264, Loss Train(MSE): 0.12520661595938148, R2 Train: 0.49917353616247406\n",
      "Epoch 6265, Loss Train(MSE): 0.1252065458585997, R2 Train: 0.4991738165656012\n",
      "Epoch 6266, Loss Train(MSE): 0.12520648351816027, R2 Train: 0.4991740659273589\n",
      "Epoch 6267, Loss Train(MSE): 0.1252064200070572, R2 Train: 0.4991743199717712\n",
      "Epoch 6268, Loss Train(MSE): 0.12520635006299655, R2 Train: 0.4991745997480138\n",
      "Epoch 6269, Loss Train(MSE): 0.1252062906677998, R2 Train: 0.49917483732880075\n",
      "Epoch 6270, Loss Train(MSE): 0.1252062244697084, R2 Train: 0.49917510212116645\n",
      "Epoch 6271, Loss Train(MSE): 0.1252061546815399, R2 Train: 0.4991753812738404\n",
      "Epoch 6272, Loss Train(MSE): 0.1252060981668312, R2 Train: 0.4991756073326752\n",
      "Epoch 6273, Loss Train(MSE): 0.12520602934527858, R2 Train: 0.4991758826188857\n",
      "Epoch 6274, Loss Train(MSE): 0.12520596155573968, R2 Train: 0.4991761537770413\n",
      "Epoch 6275, Loss Train(MSE): 0.12520590416884067, R2 Train: 0.4991763833246373\n",
      "Epoch 6276, Loss Train(MSE): 0.12520583462627422, R2 Train: 0.49917666149490314\n",
      "Epoch 6277, Loss Train(MSE): 0.1252057696433125, R2 Train: 0.49917692142674996\n",
      "Epoch 6278, Loss Train(MSE): 0.12520570970499165, R2 Train: 0.4991771611800334\n",
      "Epoch 6279, Loss Train(MSE): 0.12520564031626893, R2 Train: 0.49917743873492426\n",
      "Epoch 6280, Loss Train(MSE): 0.12520557807690014, R2 Train: 0.49917768769239945\n",
      "Epoch 6281, Loss Train(MSE): 0.1252055156489461, R2 Train: 0.49917793740421557\n",
      "Epoch 6282, Loss Train(MSE): 0.12520544641326825, R2 Train: 0.499178214346927\n",
      "Epoch 6283, Loss Train(MSE): 0.12520538685523658, R2 Train: 0.4991784525790537\n",
      "Epoch 6284, Loss Train(MSE): 0.1252053219987221, R2 Train: 0.49917871200511155\n",
      "Epoch 6285, Loss Train(MSE): 0.1252052529152987, R2 Train: 0.49917898833880525\n",
      "Epoch 6286, Loss Train(MSE): 0.1252051959770678, R2 Train: 0.4991792160917288\n",
      "Epoch 6287, Loss Train(MSE): 0.12520512875235834, R2 Train: 0.49917948499056664\n",
      "Epoch 6288, Loss Train(MSE): 0.12520506051409364, R2 Train: 0.49917975794362546\n",
      "Epoch 6289, Loss Train(MSE): 0.12520500474607002, R2 Train: 0.4991799810157199\n",
      "Epoch 6290, Loss Train(MSE): 0.1252049359026035, R2 Train: 0.49918025638958596\n",
      "Epoch 6291, Loss Train(MSE): 0.12520487021488347, R2 Train: 0.49918051914046613\n",
      "Epoch 6292, Loss Train(MSE): 0.12520481214613555, R2 Train: 0.4991807514154578\n",
      "Epoch 6293, Loss Train(MSE): 0.12520474345297045, R2 Train: 0.4991810261881182\n",
      "Epoch 6294, Loss Train(MSE): 0.12520468025593087, R2 Train: 0.4991812789762765\n",
      "Epoch 6295, Loss Train(MSE): 0.12520461994518162, R2 Train: 0.49918152021927353\n",
      "Epoch 6296, Loss Train(MSE): 0.12520455140155562, R2 Train: 0.4991817943937775\n",
      "Epoch 6297, Loss Train(MSE): 0.12520449063602263, R2 Train: 0.4991820374559095\n",
      "Epoch 6298, Loss Train(MSE): 0.12520442814131644, R2 Train: 0.49918228743473425\n",
      "Epoch 6299, Loss Train(MSE): 0.12520435974647504, R2 Train: 0.49918256101409986\n",
      "Epoch 6300, Loss Train(MSE): 0.12520430135395666, R2 Train: 0.49918279458417336\n",
      "Epoch 6301, Loss Train(MSE): 0.12520423673266737, R2 Train: 0.49918305306933053\n",
      "Epoch 6302, Loss Train(MSE): 0.12520416848586374, R2 Train: 0.49918332605654503\n",
      "Epoch 6303, Loss Train(MSE): 0.1252041124085417, R2 Train: 0.4991835503658332\n",
      "Epoch 6304, Loss Train(MSE): 0.12520404571738067, R2 Train: 0.4991838171304773\n",
      "Epoch 6305, Loss Train(MSE): 0.12520397831339175, R2 Train: 0.499184086746433\n",
      "Epoch 6306, Loss Train(MSE): 0.12520392310187514, R2 Train: 0.49918430759249943\n",
      "Epoch 6307, Loss Train(MSE): 0.12520385508848583, R2 Train: 0.49918457964605667\n",
      "Epoch 6308, Loss Train(MSE): 0.12520378993580253, R2 Train: 0.4991848402567899\n",
      "Epoch 6309, Loss Train(MSE): 0.12520373271661628, R2 Train: 0.4991850691335349\n",
      "Epoch 6310, Loss Train(MSE): 0.12520366484941967, R2 Train: 0.49918534060232134\n",
      "Epoch 6311, Loss Train(MSE): 0.12520360189178384, R2 Train: 0.49918559243286464\n",
      "Epoch 6312, Loss Train(MSE): 0.1252035427201064, R2 Train: 0.49918582911957443\n",
      "Epoch 6313, Loss Train(MSE): 0.12520347499838153, R2 Train: 0.4991861000064739\n",
      "Epoch 6314, Loss Train(MSE): 0.1252034141801816, R2 Train: 0.4991863432792736\n",
      "Epoch 6315, Loss Train(MSE): 0.12520335311055533, R2 Train: 0.49918658755777867\n",
      "Epoch 6316, Loss Train(MSE): 0.12520328553358848, R2 Train: 0.4991868578656461\n",
      "Epoch 6317, Loss Train(MSE): 0.12520322679985202, R2 Train: 0.4991870928005919\n",
      "Epoch 6318, Loss Train(MSE): 0.12520316388619082, R2 Train: 0.4991873444552367\n",
      "Epoch 6319, Loss Train(MSE): 0.12520309645327532, R2 Train: 0.4991876141868987\n",
      "Epoch 6320, Loss Train(MSE): 0.1252030397496611, R2 Train: 0.4991878410013556\n",
      "Epoch 6321, Loss Train(MSE): 0.12520297504525793, R2 Train: 0.4991880998189683\n",
      "Epoch 6322, Loss Train(MSE): 0.125202907755694, R2 Train: 0.49918836897722396\n",
      "Epoch 6323, Loss Train(MSE): 0.1252028530284848, R2 Train: 0.4991885878860608\n",
      "Epoch 6324, Loss Train(MSE): 0.12520278658601877, R2 Train: 0.4991888536559249\n",
      "Epoch 6325, Loss Train(MSE): 0.12520272050808934, R2 Train: 0.49918911796764265\n",
      "Epoch 6326, Loss Train(MSE): 0.12520266556523488, R2 Train: 0.4991893377390605\n",
      "Epoch 6327, Loss Train(MSE): 0.12520259850181412, R2 Train: 0.49918960599274353\n",
      "Epoch 6328, Loss Train(MSE): 0.1252025343422988, R2 Train: 0.4991898626308048\n",
      "Epoch 6329, Loss Train(MSE): 0.12520247771778772, R2 Train: 0.4991900891288491\n",
      "Epoch 6330, Loss Train(MSE): 0.12520241079599004, R2 Train: 0.49919035681603985\n",
      "Epoch 6331, Loss Train(MSE): 0.12520234850260964, R2 Train: 0.49919060598956144\n",
      "Epoch 6332, Loss Train(MSE): 0.12520229024770746, R2 Train: 0.49919083900917016\n",
      "Epoch 6333, Loss Train(MSE): 0.12520222346685664, R2 Train: 0.49919110613257345\n",
      "Epoch 6334, Loss Train(MSE): 0.12520216298793113, R2 Train: 0.4991913480482755\n",
      "Epoch 6335, Loss Train(MSE): 0.12520210315331393, R2 Train: 0.4991915873867443\n",
      "Epoch 6336, Loss Train(MSE): 0.1252020365127402, R2 Train: 0.4991918539490392\n",
      "Epoch 6337, Loss Train(MSE): 0.1252019777971817, R2 Train: 0.4991920888112732\n",
      "Epoch 6338, Loss Train(MSE): 0.12520191643294293, R2 Train: 0.4991923342682283\n",
      "Epoch 6339, Loss Train(MSE): 0.12520184993198305, R2 Train: 0.4991926002720678\n",
      "Epoch 6340, Loss Train(MSE): 0.12520179292928876, R2 Train: 0.49919282828284495\n",
      "Epoch 6341, Loss Train(MSE): 0.12520173008494595, R2 Train: 0.4991930796602162\n",
      "Epoch 6342, Loss Train(MSE): 0.12520166372294306, R2 Train: 0.49919334510822777\n",
      "Epoch 6343, Loss Train(MSE): 0.12520160838318858, R2 Train: 0.49919356646724566\n",
      "Epoch 6344, Loss Train(MSE): 0.12520154410769024, R2 Train: 0.499193823569239\n",
      "Epoch 6345, Loss Train(MSE): 0.12520147788399374, R2 Train: 0.499194088464025\n",
      "Epoch 6346, Loss Train(MSE): 0.12520142415782617, R2 Train: 0.4991943033686953\n",
      "Epoch 6347, Loss Train(MSE): 0.12520135849955835, R2 Train: 0.4991945660017666\n",
      "Epoch 6348, Loss Train(MSE): 0.1252012934052945, R2 Train: 0.49919482637882195\n",
      "Epoch 6349, Loss Train(MSE): 0.12520123925960727, R2 Train: 0.4991950429615709\n",
      "Epoch 6350, Loss Train(MSE): 0.12520117325422478, R2 Train: 0.4991953069831009\n",
      "Epoch 6351, Loss Train(MSE): 0.12520110972181173, R2 Train: 0.4991955611127531\n",
      "Epoch 6352, Loss Train(MSE): 0.12520105424361416, R2 Train: 0.49919578302554335\n",
      "Epoch 6353, Loss Train(MSE): 0.12520098837493068, R2 Train: 0.49919604650027727\n",
      "Epoch 6354, Loss Train(MSE): 0.12520092635632576, R2 Train: 0.49919629457469694\n",
      "Epoch 6355, Loss Train(MSE): 0.1252008695927155, R2 Train: 0.49919652162913797\n",
      "Epoch 6356, Loss Train(MSE): 0.12520080386010154, R2 Train: 0.4991967845595938\n",
      "Epoch 6357, Loss Train(MSE): 0.12520074330781125, R2 Train: 0.499197026768755\n",
      "Epoch 6358, Loss Train(MSE): 0.12520068530534542, R2 Train: 0.4991972587786183\n",
      "Epoch 6359, Loss Train(MSE): 0.12520061970817709, R2 Train: 0.49919752116729166\n",
      "Epoch 6360, Loss Train(MSE): 0.12520056057525092, R2 Train: 0.4991977576989963\n",
      "Epoch 6361, Loss Train(MSE): 0.12520050137995212, R2 Train: 0.49919799448019153\n",
      "Epoch 6362, Loss Train(MSE): 0.12520043591761157, R2 Train: 0.4991982563295537\n",
      "Epoch 6363, Loss Train(MSE): 0.12520037815763552, R2 Train: 0.49919848736945793\n",
      "Epoch 6364, Loss Train(MSE): 0.12520031781499816, R2 Train: 0.49919872874000737\n",
      "Epoch 6365, Loss Train(MSE): 0.12520025248687314, R2 Train: 0.49919899005250745\n",
      "Epoch 6366, Loss Train(MSE): 0.12520019605396362, R2 Train: 0.4991992157841455\n",
      "Epoch 6367, Loss Train(MSE): 0.12520013460896012, R2 Train: 0.4991994615641595\n",
      "Epoch 6368, Loss Train(MSE): 0.12520006941444403, R2 Train: 0.4991997223422239\n",
      "Epoch 6369, Loss Train(MSE): 0.12520001426324173, R2 Train: 0.49919994294703307\n",
      "Epoch 6370, Loss Train(MSE): 0.12519995176032822, R2 Train: 0.4992001929586871\n",
      "Epoch 6371, Loss Train(MSE): 0.12519988669882, R2 Train: 0.49920045320471995\n",
      "Epoch 6372, Loss Train(MSE): 0.12519983278448377, R2 Train: 0.49920066886206493\n",
      "Epoch 6373, Loss Train(MSE): 0.12519976926760634, R2 Train: 0.49920092292957463\n",
      "Epoch 6374, Loss Train(MSE): 0.1251997043385105, R2 Train: 0.49920118264595803\n",
      "Epoch 6375, Loss Train(MSE): 0.12519965161671143, R2 Train: 0.4992013935331543\n",
      "Epoch 6376, Loss Train(MSE): 0.12519958712931167, R2 Train: 0.4992016514827533\n",
      "Epoch 6377, Loss Train(MSE): 0.12519952303103254, R2 Train: 0.4992019078758698\n",
      "Epoch 6378, Loss Train(MSE): 0.12519947005943596, R2 Train: 0.49920211976225615\n",
      "Epoch 6379, Loss Train(MSE): 0.12519940533950677, R2 Train: 0.4992023786419729\n",
      "Epoch 6380, Loss Train(MSE): 0.12519934238919436, R2 Train: 0.49920263044322255\n",
      "Epoch 6381, Loss Train(MSE): 0.12519928849028905, R2 Train: 0.4992028460388438\n",
      "Epoch 6382, Loss Train(MSE): 0.12519922390130106, R2 Train: 0.49920310439479576\n",
      "Epoch 6383, Loss Train(MSE): 0.125199162055792, R2 Train: 0.49920335177683195\n",
      "Epoch 6384, Loss Train(MSE): 0.1251991072718731, R2 Train: 0.49920357091250755\n",
      "Epoch 6385, Loss Train(MSE): 0.12519904281324912, R2 Train: 0.49920382874700353\n",
      "Epoch 6386, Loss Train(MSE): 0.1251989820298728, R2 Train: 0.49920407188050875\n",
      "Epoch 6387, Loss Train(MSE): 0.12519892640275024, R2 Train: 0.49920429438899905\n",
      "Epoch 6388, Loss Train(MSE): 0.12519886207391828, R2 Train: 0.49920455170432687\n",
      "Epoch 6389, Loss Train(MSE): 0.12519880231049102, R2 Train: 0.4992047907580359\n",
      "Epoch 6390, Loss Train(MSE): 0.1251987458814951, R2 Train: 0.4992050164740196\n",
      "Epoch 6391, Loss Train(MSE): 0.1251986816818882, R2 Train: 0.49920527327244724\n",
      "Epoch 6392, Loss Train(MSE): 0.12519862289670783, R2 Train: 0.4992055084131687\n",
      "Epoch 6393, Loss Train(MSE): 0.1251985657066948, R2 Train: 0.4992057371732208\n",
      "Epoch 6394, Loss Train(MSE): 0.12519850163575108, R2 Train: 0.4992059934569957\n",
      "Epoch 6395, Loss Train(MSE): 0.12519844378759143, R2 Train: 0.4992062248496343\n",
      "Epoch 6396, Loss Train(MSE): 0.12519838587694862, R2 Train: 0.4992064564922055\n",
      "Epoch 6397, Loss Train(MSE): 0.12519832193411112, R2 Train: 0.4992067122635555\n",
      "Epoch 6398, Loss Train(MSE): 0.12519826498221648, R2 Train: 0.4992069400711341\n",
      "Epoch 6399, Loss Train(MSE): 0.125198206390868, R2 Train: 0.49920717443652796\n",
      "Epoch 6400, Loss Train(MSE): 0.12519814257558462, R2 Train: 0.4992074296976615\n",
      "Epoch 6401, Loss Train(MSE): 0.12519808647966463, R2 Train: 0.4992076540813415\n",
      "Epoch 6402, Loss Train(MSE): 0.12519802724707615, R2 Train: 0.4992078910116954\n",
      "Epoch 6403, Loss Train(MSE): 0.1251979635587997, R2 Train: 0.4992081457648012\n",
      "Epoch 6404, Loss Train(MSE): 0.12519790827902388, R2 Train: 0.49920836688390446\n",
      "Epoch 6405, Loss Train(MSE): 0.12519784844420814, R2 Train: 0.49920860622316743\n",
      "Epoch 6406, Loss Train(MSE): 0.1251977848823961, R2 Train: 0.49920886047041557\n",
      "Epoch 6407, Loss Train(MSE): 0.12519773037938872, R2 Train: 0.4992090784824451\n",
      "Epoch 6408, Loss Train(MSE): 0.1251976699809105, R2 Train: 0.499209320076358\n",
      "Epoch 6409, Loss Train(MSE): 0.12519760654502515, R2 Train: 0.4992095738198994\n",
      "Epoch 6410, Loss Train(MSE): 0.12519755277986006, R2 Train: 0.4992097888805598\n",
      "Epoch 6411, Loss Train(MSE): 0.1251974918558413, R2 Train: 0.49921003257663477\n",
      "Epoch 6412, Loss Train(MSE): 0.12519742854534943, R2 Train: 0.4992102858186023\n",
      "Epoch 6413, Loss Train(MSE): 0.12519737547954501, R2 Train: 0.49921049808181994\n",
      "Epoch 6414, Loss Train(MSE): 0.12519731406766982, R2 Train: 0.4992107437293207\n",
      "Epoch 6415, Loss Train(MSE): 0.12519725088204287, R2 Train: 0.49921099647182854\n",
      "Epoch 6416, Loss Train(MSE): 0.12519719847755706, R2 Train: 0.49921120608977176\n",
      "Epoch 6417, Loss Train(MSE): 0.12519713661507645, R2 Train: 0.4992114535396942\n",
      "Epoch 6418, Loss Train(MSE): 0.12519707355379037, R2 Train: 0.49921170578483853\n",
      "Epoch 6419, Loss Train(MSE): 0.12519702177301553, R2 Train: 0.49921191290793787\n",
      "Epoch 6420, Loss Train(MSE): 0.12519695949675255, R2 Train: 0.4992121620129898\n",
      "Epoch 6421, Loss Train(MSE): 0.1251968965592877, R2 Train: 0.4992124137628492\n",
      "Epoch 6422, Loss Train(MSE): 0.12519684536504608, R2 Train: 0.4992126185398157\n",
      "Epoch 6423, Loss Train(MSE): 0.12519678271140036, R2 Train: 0.49921286915439855\n",
      "Epoch 6424, Loss Train(MSE): 0.12519672015457864, R2 Train: 0.49921311938168544\n",
      "Epoch 6425, Loss Train(MSE): 0.12519666899527876, R2 Train: 0.49921332401888496\n",
      "Epoch 6426, Loss Train(MSE): 0.1251966062536436, R2 Train: 0.4992135749854256\n",
      "Epoch 6427, Loss Train(MSE): 0.12519654424890883, R2 Train: 0.4992138230043647\n",
      "Epoch 6428, Loss Train(MSE): 0.12519649274547973, R2 Train: 0.4992140290180811\n",
      "Epoch 6429, Loss Train(MSE): 0.12519643012638207, R2 Train: 0.4992142794944717\n",
      "Epoch 6430, Loss Train(MSE): 0.1251963686375211, R2 Train: 0.4992145254499156\n",
      "Epoch 6431, Loss Train(MSE): 0.12519631682541296, R2 Train: 0.49921473269834815\n",
      "Epoch 6432, Loss Train(MSE): 0.12519625432834808, R2 Train: 0.4992149826866077\n",
      "Epoch 6433, Loss Train(MSE): 0.1251961933195616, R2 Train: 0.49921522672175356\n",
      "Epoch 6434, Loss Train(MSE): 0.125196141233817, R2 Train: 0.499215435064732\n",
      "Epoch 6435, Loss Train(MSE): 0.12519607885828438, R2 Train: 0.4992156845668625\n",
      "Epoch 6436, Loss Train(MSE): 0.12519601829418203, R2 Train: 0.49921592682327187\n",
      "Epoch 6437, Loss Train(MSE): 0.12519596596944035, R2 Train: 0.4992161361222386\n",
      "Epoch 6438, Loss Train(MSE): 0.12519590371494366, R2 Train: 0.49921638514022537\n",
      "Epoch 6439, Loss Train(MSE): 0.1251958435605398, R2 Train: 0.49921662575784076\n",
      "Epoch 6440, Loss Train(MSE): 0.12519579103104192, R2 Train: 0.4992168358758323\n",
      "Epoch 6441, Loss Train(MSE): 0.1251957288970888, R2 Train: 0.4992170844116448\n",
      "Epoch 6442, Loss Train(MSE): 0.12519566911779778, R2 Train: 0.4992173235288089\n",
      "Epoch 6443, Loss Train(MSE): 0.1251956164173902, R2 Train: 0.49921753433043925\n",
      "Epoch 6444, Loss Train(MSE): 0.12519555440349245, R2 Train: 0.4992177823860302\n",
      "Epoch 6445, Loss Train(MSE): 0.12519549496512428, R2 Train: 0.4992180201395029\n",
      "Epoch 6446, Loss Train(MSE): 0.12519544212726375, R2 Train: 0.499218231490945\n",
      "Epoch 6447, Loss Train(MSE): 0.1251953802329371, R2 Train: 0.49921847906825156\n",
      "Epoch 6448, Loss Train(MSE): 0.12519532110169296, R2 Train: 0.49921871559322817\n",
      "Epoch 6449, Loss Train(MSE): 0.12519526815945067, R2 Train: 0.4992189273621973\n",
      "Epoch 6450, Loss Train(MSE): 0.1251952063842149, R2 Train: 0.49921917446314035\n",
      "Epoch 6451, Loss Train(MSE): 0.1251951475266829, R2 Train: 0.4992194098932684\n",
      "Epoch 6452, Loss Train(MSE): 0.1251950945127488, R2 Train: 0.4992196219490048\n",
      "Epoch 6453, Loss Train(MSE): 0.12519503285612746, R2 Train: 0.49921986857549017\n",
      "Epoch 6454, Loss Train(MSE): 0.12519497423927822, R2 Train: 0.4992201030428871\n",
      "Epoch 6455, Loss Train(MSE): 0.12519492118596517, R2 Train: 0.4992203152561393\n",
      "Epoch 6456, Loss Train(MSE): 0.12519485964748583, R2 Train: 0.49922056141005666\n",
      "Epoch 6457, Loss Train(MSE): 0.12519480123866833, R2 Train: 0.4992207950453267\n",
      "Epoch 6458, Loss Train(MSE): 0.12519474817791648, R2 Train: 0.4992210072883341\n",
      "Epoch 6459, Loss Train(MSE): 0.12519468675711026, R2 Train: 0.49922125297155895\n",
      "Epoch 6460, Loss Train(MSE): 0.12519462852404772, R2 Train: 0.4992214859038091\n",
      "Epoch 6461, Loss Train(MSE): 0.12519457548742827, R2 Train: 0.49922169805028693\n",
      "Epoch 6462, Loss Train(MSE): 0.12519451418383024, R2 Train: 0.49922194326467906\n",
      "Epoch 6463, Loss Train(MSE): 0.125194456094616, R2 Train: 0.49922217562153604\n",
      "Epoch 6464, Loss Train(MSE): 0.12519440311333532, R2 Train: 0.4992223875466587\n",
      "Epoch 6465, Loss Train(MSE): 0.12519434192648424, R2 Train: 0.49922263229406305\n",
      "Epoch 6466, Loss Train(MSE): 0.12519428394957743, R2 Train: 0.49922286420169026\n",
      "Epoch 6467, Loss Train(MSE): 0.12519423105448155, R2 Train: 0.4992230757820738\n",
      "Epoch 6468, Loss Train(MSE): 0.12519416998391966, R2 Train: 0.49922332006432135\n",
      "Epoch 6469, Loss Train(MSE): 0.1251941120881416, R2 Train: 0.4992235516474336\n",
      "Epoch 6470, Loss Train(MSE): 0.1251940593097194, R2 Train: 0.49922376276112235\n",
      "Epoch 6471, Loss Train(MSE): 0.12519399835499276, R2 Train: 0.49922400658002897\n",
      "Epoch 6472, Loss Train(MSE): 0.12519394050952254, R2 Train: 0.49922423796190984\n",
      "Epoch 6473, Loss Train(MSE): 0.12519388787791036, R2 Train: 0.49922444848835856\n",
      "Epoch 6474, Loss Train(MSE): 0.12519382703856854, R2 Train: 0.4992246918457258\n",
      "Epoch 6475, Loss Train(MSE): 0.12519376921293937, R2 Train: 0.4992249231482425\n",
      "Epoch 6476, Loss Train(MSE): 0.1251937167579244, R2 Train: 0.4992251329683024\n",
      "Epoch 6477, Loss Train(MSE): 0.1251936560335204, R2 Train: 0.4992253758659184\n",
      "Epoch 6478, Loss Train(MSE): 0.12519359819761572, R2 Train: 0.4992256072095371\n",
      "Epoch 6479, Loss Train(MSE): 0.12519354594864004, R2 Train: 0.4992258162054398\n",
      "Epoch 6480, Loss Train(MSE): 0.1251934853387305, R2 Train: 0.49922605864507796\n",
      "Epoch 6481, Loss Train(MSE): 0.12519342746277998, R2 Train: 0.4992262901488801\n",
      "Epoch 6482, Loss Train(MSE): 0.12519337544894424, R2 Train: 0.49922649820422305\n",
      "Epoch 6483, Loss Train(MSE): 0.12519331495308916, R2 Train: 0.49922674018764335\n",
      "Epoch 6484, Loss Train(MSE): 0.12519325700766507, R2 Train: 0.49922697196933974\n",
      "Epoch 6485, Loss Train(MSE): 0.12519320525773234, R2 Train: 0.49922717896907065\n",
      "Epoch 6486, Loss Train(MSE): 0.12519314487549504, R2 Train: 0.4992274204980198\n",
      "Epoch 6487, Loss Train(MSE): 0.12519308683150857, R2 Train: 0.49922765267396574\n",
      "Epoch 6488, Loss Train(MSE): 0.1251930353739078, R2 Train: 0.4992278585043688\n",
      "Epoch 6489, Loss Train(MSE): 0.12519297510485514, R2 Train: 0.49922809958057945\n",
      "Epoch 6490, Loss Train(MSE): 0.12519291693355236, R2 Train: 0.49922833226579055\n",
      "Epoch 6491, Loss Train(MSE): 0.12519286579638234, R2 Train: 0.49922853681447066\n",
      "Epoch 6492, Loss Train(MSE): 0.12519280564008425, R2 Train: 0.499228777439663\n",
      "Epoch 6493, Loss Train(MSE): 0.12519274731304295, R2 Train: 0.4992290107478282\n",
      "Epoch 6494, Loss Train(MSE): 0.12519269652407544, R2 Train: 0.49922921390369823\n",
      "Epoch 6495, Loss Train(MSE): 0.12519263648010542, R2 Train: 0.4992294540795783\n",
      "Epoch 6496, Loss Train(MSE): 0.125192577969231, R2 Train: 0.499229688123076\n",
      "Epoch 6497, Loss Train(MSE): 0.12519252755591484, R2 Train: 0.49922988977634064\n",
      "Epoch 6498, Loss Train(MSE): 0.1251924676238494, R2 Train: 0.49923012950460244\n",
      "Epoch 6499, Loss Train(MSE): 0.12519240890137176, R2 Train: 0.49923036439451296\n",
      "Epoch 6500, Loss Train(MSE): 0.12519235889083588, R2 Train: 0.4992305644366565\n",
      "Epoch 6501, Loss Train(MSE): 0.12519229907025475, R2 Train: 0.499230803718981\n",
      "Epoch 6502, Loss Train(MSE): 0.1251922401087245, R2 Train: 0.49923103956510195\n",
      "Epoch 6503, Loss Train(MSE): 0.12519219052778166, R2 Train: 0.49923123788887336\n",
      "Epoch 6504, Loss Train(MSE): 0.12519213081826788, R2 Train: 0.49923147672692847\n",
      "Epoch 6505, Loss Train(MSE): 0.1251920715905529, R2 Train: 0.4992317136377884\n",
      "Epoch 6506, Loss Train(MSE): 0.12519202246570293, R2 Train: 0.49923191013718826\n",
      "Epoch 6507, Loss Train(MSE): 0.1251919628668426, R2 Train: 0.49923214853262965\n",
      "Epoch 6508, Loss Train(MSE): 0.12519190334612462, R2 Train: 0.4992323866155015\n",
      "Epoch 6509, Loss Train(MSE): 0.12519185470355804, R2 Train: 0.49923258118576785\n",
      "Epoch 6510, Loss Train(MSE): 0.12519179521494025, R2 Train: 0.499232819140239\n",
      "Epoch 6511, Loss Train(MSE): 0.12519173577114107, R2 Train: 0.49923305691543574\n",
      "Epoch 6512, Loss Train(MSE): 0.12519168684352416, R2 Train: 0.49923325262590335\n",
      "Epoch 6513, Loss Train(MSE): 0.12519162786501625, R2 Train: 0.499233488539935\n",
      "Epoch 6514, Loss Train(MSE): 0.12519156853085073, R2 Train: 0.4992337258765971\n",
      "Epoch 6515, Loss Train(MSE): 0.12519151922494234, R2 Train: 0.4992339231002306\n",
      "Epoch 6516, Loss Train(MSE): 0.12519146081248952, R2 Train: 0.4992341567500419\n",
      "Epoch 6517, Loss Train(MSE): 0.12519140158755468, R2 Train: 0.4992343936497813\n",
      "Epoch 6518, Loss Train(MSE): 0.12519135187769367, R2 Train: 0.4992345924892253\n",
      "Epoch 6519, Loss Train(MSE): 0.12519129405634463, R2 Train: 0.4992348237746215\n",
      "Epoch 6520, Loss Train(MSE): 0.12519123494024037, R2 Train: 0.4992350602390385\n",
      "Epoch 6521, Loss Train(MSE): 0.12519118480106392, R2 Train: 0.4992352607957443\n",
      "Epoch 6522, Loss Train(MSE): 0.1251911275955732, R2 Train: 0.49923548961770725\n",
      "Epoch 6523, Loss Train(MSE): 0.12519106858790235, R2 Train: 0.4992357256483906\n",
      "Epoch 6524, Loss Train(MSE): 0.12519101799434273, R2 Train: 0.49923592802262906\n",
      "Epoch 6525, Loss Train(MSE): 0.12519096142917396, R2 Train: 0.49923615428330415\n",
      "Epoch 6526, Loss Train(MSE): 0.12519090252954235, R2 Train: 0.4992363898818306\n",
      "Epoch 6527, Loss Train(MSE): 0.12519085145682363, R2 Train: 0.4992365941727055\n",
      "Epoch 6528, Loss Train(MSE): 0.12519079555615265, R2 Train: 0.4992368177753894\n",
      "Epoch 6529, Loss Train(MSE): 0.12519073676416878, R2 Train: 0.4992370529433249\n",
      "Epoch 6530, Loss Train(MSE): 0.12519068518780382, R2 Train: 0.4992372592487847\n",
      "Epoch 6531, Loss Train(MSE): 0.12519062997552177, R2 Train: 0.4992374800979129\n",
      "Epoch 6532, Loss Train(MSE): 0.1251905712907971, R2 Train: 0.49923771483681156\n",
      "Epoch 6533, Loss Train(MSE): 0.1251905191865845, R2 Train: 0.499237923253662\n",
      "Epoch 6534, Loss Train(MSE): 0.1251904646863008, R2 Train: 0.4992381412547968\n",
      "Epoch 6535, Loss Train(MSE): 0.12519040610844956, R2 Train: 0.49923837556620176\n",
      "Epoch 6536, Loss Train(MSE): 0.12519035345247037, R2 Train: 0.4992385861901185\n",
      "Epoch 6537, Loss Train(MSE): 0.12519029968751588, R2 Train: 0.49923880124993647\n",
      "Epoch 6538, Loss Train(MSE): 0.125190241216155, R2 Train: 0.49923903513538004\n",
      "Epoch 6539, Loss Train(MSE): 0.12519018798476994, R2 Train: 0.49923924806092024\n",
      "Epoch 6540, Loss Train(MSE): 0.12519013497819978, R2 Train: 0.4992394600872009\n",
      "Epoch 6541, Loss Train(MSE): 0.1251900766129489, R2 Train: 0.4992396935482044\n",
      "Epoch 6542, Loss Train(MSE): 0.12519002278279534, R2 Train: 0.49923990886881864\n",
      "Epoch 6543, Loss Train(MSE): 0.1251899705573918, R2 Train: 0.49924011777043276\n",
      "Epoch 6544, Loss Train(MSE): 0.1251899122978734, R2 Train: 0.4992403508085064\n",
      "Epoch 6545, Loss Train(MSE): 0.1251898578458622, R2 Train: 0.4992405686165512\n",
      "Epoch 6546, Loss Train(MSE): 0.125189806424138, R2 Train: 0.49924077430344804\n",
      "Epoch 6547, Loss Train(MSE): 0.125189748269977, R2 Train: 0.499241006920092\n",
      "Epoch 6548, Loss Train(MSE): 0.12518969317328982, R2 Train: 0.49924122730684073\n",
      "Epoch 6549, Loss Train(MSE): 0.12518964257749052, R2 Train: 0.49924142969003793\n",
      "Epoch 6550, Loss Train(MSE): 0.12518958452831475, R2 Train: 0.499241661886741\n",
      "Epoch 6551, Loss Train(MSE): 0.125189528764401, R2 Train: 0.49924188494239596\n",
      "Epoch 6552, Loss Train(MSE): 0.1251894790165081, R2 Train: 0.49924208393396763\n",
      "Epoch 6553, Loss Train(MSE): 0.12518942107194778, R2 Train: 0.4992423157122089\n",
      "Epoch 6554, Loss Train(MSE): 0.12518936461852204, R2 Train: 0.49924254152591185\n",
      "Epoch 6555, Loss Train(MSE): 0.12518931574025563, R2 Train: 0.4992427370389775\n",
      "Epoch 6556, Loss Train(MSE): 0.12518925789994367, R2 Train: 0.4992429684002253\n",
      "Epoch 6557, Loss Train(MSE): 0.12518920073498252, R2 Train: 0.4992431970600699\n",
      "Epoch 6558, Loss Train(MSE): 0.12518915274780437, R2 Train: 0.4992433890087825\n",
      "Epoch 6559, Loss Train(MSE): 0.12518909501137618, R2 Train: 0.4992436199544953\n",
      "Epoch 6560, Loss Train(MSE): 0.12518903731704173, R2 Train: 0.4992438507318331\n",
      "Epoch 6561, Loss Train(MSE): 0.12518898983371704, R2 Train: 0.49924404066513184\n",
      "Epoch 6562, Loss Train(MSE): 0.1251889324085203, R2 Train: 0.4992442703659188\n",
      "Epoch 6563, Loss Train(MSE): 0.12518887481752866, R2 Train: 0.49924450072988535\n",
      "Epoch 6564, Loss Train(MSE): 0.12518882654966132, R2 Train: 0.4992446938013547\n",
      "Epoch 6565, Loss Train(MSE): 0.12518877008720372, R2 Train: 0.4992449196511851\n",
      "Epoch 6566, Loss Train(MSE): 0.12519501705244662, R2 Train: 0.4992199317902135\n",
      "Epoch 6567, Loss Train(MSE): 0.12520699430915366, R2 Train: 0.49917202276338535\n",
      "Epoch 6568, Loss Train(MSE): 0.1252068558843195, R2 Train: 0.49917257646272195\n",
      "Epoch 6569, Loss Train(MSE): 0.125206712056073, R2 Train: 0.499173151775708\n",
      "Epoch 6570, Loss Train(MSE): 0.12520657997973986, R2 Train: 0.49917368008104057\n",
      "Epoch 6571, Loss Train(MSE): 0.12520643260388295, R2 Train: 0.4991742695844682\n",
      "Epoch 6572, Loss Train(MSE): 0.12520630335376917, R2 Train: 0.4991747865849233\n",
      "Epoch 6573, Loss Train(MSE): 0.1252061583448269, R2 Train: 0.4991753666206924\n",
      "Epoch 6574, Loss Train(MSE): 0.12520602361020836, R2 Train: 0.4991759055591666\n",
      "Epoch 6575, Loss Train(MSE): 0.12520588512730968, R2 Train: 0.4991764594907613\n",
      "Epoch 6576, Loss Train(MSE): 0.125205744856168, R2 Train: 0.499177020575328\n",
      "Epoch 6577, Loss Train(MSE): 0.1252056129438781, R2 Train: 0.49917754822448757\n",
      "Epoch 6578, Loss Train(MSE): 0.12520546764208337, R2 Train: 0.4991781294316665\n",
      "Epoch 6579, Loss Train(MSE): 0.12520534124901295, R2 Train: 0.4991786350039482\n",
      "Epoch 6580, Loss Train(MSE): 0.12520519706231115, R2 Train: 0.4991792117507554\n",
      "Epoch 6581, Loss Train(MSE): 0.12520506494504133, R2 Train: 0.4991797402198347\n",
      "Epoch 6582, Loss Train(MSE): 0.12520492749779533, R2 Train: 0.4991802900088187\n",
      "Epoch 6583, Loss Train(MSE): 0.1252047896074313, R2 Train: 0.49918084157027476\n",
      "Epoch 6584, Loss Train(MSE): 0.12520465894136973, R2 Train: 0.4991813642345211\n",
      "Epoch 6585, Loss Train(MSE): 0.1252045156600187, R2 Train: 0.4991819373599252\n",
      "Epoch 6586, Loss Train(MSE): 0.12520439097422092, R2 Train: 0.4991824361031163\n",
      "Epoch 6587, Loss Train(MSE): 0.12520424866699167, R2 Train: 0.4991830053320333\n",
      "Epoch 6588, Loss Train(MSE): 0.12520411802944384, R2 Train: 0.49918352788222464\n",
      "Epoch 6589, Loss Train(MSE): 0.12520398266394261, R2 Train: 0.49918406934422954\n",
      "Epoch 6590, Loss Train(MSE): 0.1252038460287235, R2 Train: 0.499184615885106\n",
      "Epoch 6591, Loss Train(MSE): 0.12520371764397684, R2 Train: 0.49918512942409266\n",
      "Epoch 6592, Loss Train(MSE): 0.12520357633149612, R2 Train: 0.4991856946740155\n",
      "Epoch 6593, Loss Train(MSE): 0.12520345225277238, R2 Train: 0.4991861909889105\n",
      "Epoch 6594, Loss Train(MSE): 0.12520331283618796, R2 Train: 0.49918674865524815\n",
      "Epoch 6595, Loss Train(MSE): 0.1252031825897476, R2 Train: 0.49918726964100957\n",
      "Epoch 6596, Loss Train(MSE): 0.12520305030653045, R2 Train: 0.4991877987738782\n",
      "Epoch 6597, Loss Train(MSE): 0.12520291384928414, R2 Train: 0.49918834460286343\n",
      "Epoch 6598, Loss Train(MSE): 0.12520278873588644, R2 Train: 0.4991888450564542\n",
      "Epoch 6599, Loss Train(MSE): 0.12520264934264239, R2 Train: 0.49918940262943046\n",
      "Epoch 6600, Loss Train(MSE): 0.12520252481818772, R2 Train: 0.4991899007272491\n",
      "Epoch 6601, Loss Train(MSE): 0.12520238925935623, R2 Train: 0.4991904429625751\n",
      "Epoch 6602, Loss Train(MSE): 0.12520225836227197, R2 Train: 0.4991909665509121\n",
      "Epoch 6603, Loss Train(MSE): 0.12520213011829545, R2 Train: 0.4991914795268182\n",
      "Epoch 6604, Loss Train(MSE): 0.12520199280818953, R2 Train: 0.4991920287672419\n",
      "Epoch 6605, Loss Train(MSE): 0.12520187191306728, R2 Train: 0.4991925123477309\n",
      "Epoch 6606, Loss Train(MSE): 0.12520173439126772, R2 Train: 0.4991930624349291\n",
      "Epoch 6607, Loss Train(MSE): 0.12520160841360242, R2 Train: 0.4991935663455903\n",
      "Epoch 6608, Loss Train(MSE): 0.12520147763746556, R2 Train: 0.49919408945013777\n",
      "Epoch 6609, Loss Train(MSE): 0.12520134509280753, R2 Train: 0.4991946196287699\n",
      "Epoch 6610, Loss Train(MSE): 0.12520122180331963, R2 Train: 0.49919511278672146\n",
      "Epoch 6611, Loss Train(MSE): 0.12520108560344448, R2 Train: 0.4991956575862221\n",
      "Epoch 6612, Loss Train(MSE): 0.1252009639501126, R2 Train: 0.4991961441995496\n",
      "Epoch 6613, Loss Train(MSE): 0.12520083119562692, R2 Train: 0.49919667521749234\n",
      "Epoch 6614, Loss Train(MSE): 0.12520070282693976, R2 Train: 0.49919718869224095\n",
      "Epoch 6615, Loss Train(MSE): 0.12520057769171122, R2 Train: 0.4991976892331551\n",
      "Epoch 6616, Loss Train(MSE): 0.12520044279039444, R2 Train: 0.49919822883842224\n",
      "Epoch 6617, Loss Train(MSE): 0.12520032488376406, R2 Train: 0.49919870046494375\n",
      "Epoch 6618, Loss Train(MSE): 0.1252001906883601, R2 Train: 0.49919923724655957\n",
      "Epoch 6619, Loss Train(MSE): 0.12520006592343907, R2 Train: 0.4991997363062437\n",
      "Epoch 6620, Loss Train(MSE): 0.12519993947488006, R2 Train: 0.49920024210047975\n",
      "Epoch 6621, Loss Train(MSE): 0.12519980781746629, R2 Train: 0.49920076873013486\n",
      "Epoch 6622, Loss Train(MSE): 0.12519968914410767, R2 Train: 0.4992012434235693\n",
      "Epoch 6623, Loss Train(MSE): 0.12519955600460975, R2 Train: 0.499201775981561\n",
      "Epoch 6624, Loss Train(MSE): 0.12519943426264585, R2 Train: 0.4992022629494166\n",
      "Epoch 6625, Loss Train(MSE): 0.12519930704284804, R2 Train: 0.49920277182860784\n",
      "Epoch 6626, Loss Train(MSE): 0.12519917827247554, R2 Train: 0.49920328691009785\n",
      "Epoch 6627, Loss Train(MSE): 0.1251990589489869, R2 Train: 0.49920376420405244\n",
      "Epoch 6628, Loss Train(MSE): 0.1251989270547369, R2 Train: 0.4992042917810524\n",
      "Epoch 6629, Loss Train(MSE): 0.12519880779709627, R2 Train: 0.4992047688116149\n",
      "Epoch 6630, Loss Train(MSE): 0.12519868030690068, R2 Train: 0.4992052787723973\n",
      "Epoch 6631, Loss Train(MSE): 0.12519855388995918, R2 Train: 0.4992057844401633\n",
      "Epoch 6632, Loss Train(MSE): 0.12519843441253137, R2 Train: 0.49920626234987453\n",
      "Epoch 6633, Loss Train(MSE): 0.12519830374213212, R2 Train: 0.49920678503147153\n",
      "Epoch 6634, Loss Train(MSE): 0.12519818644591904, R2 Train: 0.49920725421632384\n",
      "Epoch 6635, Loss Train(MSE): 0.1251980591713785, R2 Train: 0.499207763314486\n",
      "Epoch 6636, Loss Train(MSE): 0.12519793458983539, R2 Train: 0.49920826164065846\n",
      "Epoch 6637, Loss Train(MSE): 0.12519781544001848, R2 Train: 0.49920873823992606\n",
      "Epoch 6638, Loss Train(MSE): 0.12519768597261113, R2 Train: 0.49920925610955547\n",
      "Epoch 6639, Loss Train(MSE): 0.1251975701301968, R2 Train: 0.4992097194792128\n",
      "Epoch 6640, Loss Train(MSE): 0.12519744354301424, R2 Train: 0.49921022582794305\n",
      "Epoch 6641, Loss Train(MSE): 0.1251973202939491, R2 Train: 0.49921071882420365\n",
      "Epoch 6642, Loss Train(MSE): 0.12519720193908582, R2 Train: 0.49921119224365673\n",
      "Epoch 6643, Loss Train(MSE): 0.12519707365433141, R2 Train: 0.49921170538267434\n",
      "Epoch 6644, Loss Train(MSE): 0.12519695877289902, R2 Train: 0.4992121649084039\n",
      "Epoch 6645, Loss Train(MSE): 0.12519683333085144, R2 Train: 0.49921266667659425\n",
      "Epoch 6646, Loss Train(MSE): 0.12519671092600643, R2 Train: 0.4992131562959743\n",
      "Epoch 6647, Loss Train(MSE): 0.12519659381965098, R2 Train: 0.4992136247213961\n",
      "Epoch 6648, Loss Train(MSE): 0.12519646669771314, R2 Train: 0.4992141332091474\n",
      "Epoch 6649, Loss Train(MSE): 0.12519635229881815, R2 Train: 0.4992145908047274\n",
      "Epoch 6650, Loss Train(MSE): 0.1251962284461661, R2 Train: 0.4992150862153356\n",
      "Epoch 6651, Loss Train(MSE): 0.12519610641151172, R2 Train: 0.49921557435395314\n",
      "Epoch 6652, Loss Train(MSE): 0.1251959909938346, R2 Train: 0.4992160360246616\n",
      "Epoch 6653, Loss Train(MSE): 0.1251958650153627, R2 Train: 0.4992165399385492\n",
      "Epoch 6654, Loss Train(MSE): 0.1251957506345087, R2 Train: 0.49921699746196524\n",
      "Epoch 6655, Loss Train(MSE): 0.125195628802392, R2 Train: 0.49921748479043204\n",
      "Epoch 6656, Loss Train(MSE): 0.1251955066777077, R2 Train: 0.49921797328916917\n",
      "Epoch 6657, Loss Train(MSE): 0.12519539337588692, R2 Train: 0.4992184264964523\n",
      "Epoch 6658, Loss Train(MSE): 0.1251952685220003, R2 Train: 0.4992189259119988\n",
      "Epoch 6659, Loss Train(MSE): 0.12519515370822845, R2 Train: 0.4992193851670862\n",
      "Epoch 6660, Loss Train(MSE): 0.12519503431504914, R2 Train: 0.49921986273980345\n",
      "Epoch 6661, Loss Train(MSE): 0.12519491165351795, R2 Train: 0.4992203533859282\n",
      "Epoch 6662, Loss Train(MSE): 0.12519480088211762, R2 Train: 0.4992207964715295\n",
      "Epoch 6663, Loss Train(MSE): 0.12519467713438984, R2 Train: 0.49922129146244065\n",
      "Epoch 6664, Loss Train(MSE): 0.12519456144988295, R2 Train: 0.4992217542004682\n",
      "Epoch 6665, Loss Train(MSE): 0.12519444490167528, R2 Train: 0.4992222203932989\n",
      "Epoch 6666, Loss Train(MSE): 0.12519432182455767, R2 Train: 0.4992227127017693\n",
      "Epoch 6667, Loss Train(MSE): 0.12519421289015414, R2 Train: 0.4992231484393834\n",
      "Epoch 6668, Loss Train(MSE): 0.12519409078042548, R2 Train: 0.49922363687829807\n",
      "Epoch 6669, Loss Train(MSE): 0.12519397382168446, R2 Train: 0.4992241047132622\n",
      "Epoch 6670, Loss Train(MSE): 0.1251938604908613, R2 Train: 0.49922455803655486\n",
      "Epoch 6671, Loss Train(MSE): 0.12519373849105375, R2 Train: 0.499225046035785\n",
      "Epoch 6672, Loss Train(MSE): 0.12519362796279776, R2 Train: 0.49922548814880896\n",
      "Epoch 6673, Loss Train(MSE): 0.12519350937146187, R2 Train: 0.4992259625141525\n",
      "Epoch 6674, Loss Train(MSE): 0.12519339072534563, R2 Train: 0.49922643709861747\n",
      "Epoch 6675, Loss Train(MSE): 0.12519328099474, R2 Train: 0.49922687602104\n",
      "Epoch 6676, Loss Train(MSE): 0.12519316005493705, R2 Train: 0.4992273597802518\n",
      "Epoch 6677, Loss Train(MSE): 0.12519304752803734, R2 Train: 0.49922780988785065\n",
      "Epoch 6678, Loss Train(MSE): 0.12519293283000885, R2 Train: 0.4992282686799646\n",
      "Epoch 6679, Loss Train(MSE): 0.12519281253297923, R2 Train: 0.4992287498680831\n",
      "Epoch 6680, Loss Train(MSE): 0.12519270591280365, R2 Train: 0.4992291763487854\n",
      "Epoch 6681, Loss Train(MSE): 0.12519258644890147, R2 Train: 0.49922965420439414\n",
      "Epoch 6682, Loss Train(MSE): 0.1251924715509757, R2 Train: 0.49923011379609716\n",
      "Epoch 6683, Loss Train(MSE): 0.1251923610893979, R2 Train: 0.49923055564240837\n",
      "Epoch 6684, Loss Train(MSE): 0.12519224182525895, R2 Train: 0.4992310326989642\n",
      "Epoch 6685, Loss Train(MSE): 0.12519213253604197, R2 Train: 0.4992314698558321\n",
      "Epoch 6686, Loss Train(MSE): 0.12519201758909534, R2 Train: 0.4992319296436186\n",
      "Epoch 6687, Loss Train(MSE): 0.12519189993818783, R2 Train: 0.4992324002472487\n",
      "Epoch 6688, Loss Train(MSE): 0.12519179406649564, R2 Train: 0.49923282373401745\n",
      "Epoch 6689, Loss Train(MSE): 0.1251916758189725, R2 Train: 0.49923329672411\n",
      "Epoch 6690, Loss Train(MSE): 0.12519156348651211, R2 Train: 0.49923374605395154\n",
      "Epoch 6691, Loss Train(MSE): 0.12519145340259935, R2 Train: 0.4992341863896026\n",
      "Epoch 6692, Loss Train(MSE): 0.12519133577166908, R2 Train: 0.4992346569133237\n",
      "Epoch 6693, Loss Train(MSE): 0.1251912285585871, R2 Train: 0.49923508576565157\n",
      "Epoch 6694, Loss Train(MSE): 0.1251911144512156, R2 Train: 0.49923554219513755\n",
      "Epoch 6695, Loss Train(MSE): 0.1251909987319426, R2 Train: 0.49923600507222965\n",
      "Epoch 6696, Loss Train(MSE): 0.12519089382709356, R2 Train: 0.49923642469162577\n",
      "Epoch 6697, Loss Train(MSE): 0.12519077718726224, R2 Train: 0.49923689125095105\n",
      "Epoch 6698, Loss Train(MSE): 0.12519066630942574, R2 Train: 0.49923733476229704\n",
      "Epoch 6699, Loss Train(MSE): 0.12519055764256204, R2 Train: 0.49923776942975184\n",
      "Epoch 6700, Loss Train(MSE): 0.1251904416039201, R2 Train: 0.4992382335843196\n",
      "Epoch 6701, Loss Train(MSE): 0.12519033537633178, R2 Train: 0.49923865849467286\n",
      "Epoch 6702, Loss Train(MSE): 0.1251902231287028, R2 Train: 0.49923910748518885\n",
      "Epoch 6703, Loss Train(MSE): 0.12519010825942614, R2 Train: 0.49923956696229543\n",
      "Epoch 6704, Loss Train(MSE): 0.12519000533319904, R2 Train: 0.4992399786672038\n",
      "Epoch 6705, Loss Train(MSE): 0.1251898902610632, R2 Train: 0.4992404389557472\n",
      "Epoch 6706, Loss Train(MSE): 0.12518977977587173, R2 Train: 0.4992408808965131\n",
      "Epoch 6707, Loss Train(MSE): 0.1251896735191302, R2 Train: 0.49924130592347915\n",
      "Epoch 6708, Loss Train(MSE): 0.12518955903333429, R2 Train: 0.49924176386666286\n",
      "Epoch 6709, Loss Train(MSE): 0.12518945274858256, R2 Train: 0.49924218900566975\n",
      "Epoch 6710, Loss Train(MSE): 0.1251893433353834, R2 Train: 0.4992426266584664\n",
      "Epoch 6711, Loss Train(MSE): 0.12518922943040509, R2 Train: 0.49924308227837966\n",
      "Epoch 6712, Loss Train(MSE): 0.12518912716532302, R2 Train: 0.49924349133870793\n",
      "Epoch 6713, Loss Train(MSE): 0.1251890147669766, R2 Train: 0.49924394093209357\n",
      "Epoch 6714, Loss Train(MSE): 0.12518890367760802, R2 Train: 0.49924438528956794\n",
      "Epoch 6715, Loss Train(MSE): 0.12518880076128752, R2 Train: 0.4992447969548499\n",
      "Epoch 6716, Loss Train(MSE): 0.12518868779028544, R2 Train: 0.49924524883885824\n",
      "Epoch 6717, Loss Train(MSE): 0.12518862943197837, R2 Train: 0.49924548227208654\n",
      "Epoch 6718, Loss Train(MSE): 0.12518857669905215, R2 Train: 0.4992456932037914\n",
      "Epoch 6719, Loss Train(MSE): 0.12518851709639925, R2 Train: 0.499245931614403\n",
      "Epoch 6720, Loss Train(MSE): 0.12518846604701783, R2 Train: 0.49924613581192867\n",
      "Epoch 6721, Loss Train(MSE): 0.1251884103348537, R2 Train: 0.4992463586605852\n",
      "Epoch 6722, Loss Train(MSE): 0.12518835085959112, R2 Train: 0.4992465965616355\n",
      "Epoch 6723, Loss Train(MSE): 0.1251883029415743, R2 Train: 0.49924678823370283\n",
      "Epoch 6724, Loss Train(MSE): 0.12518824430662676, R2 Train: 0.499247022773493\n",
      "Epoch 6725, Loss Train(MSE): 0.12518818718820665, R2 Train: 0.4992472512471734\n",
      "Epoch 6726, Loss Train(MSE): 0.12518813788288286, R2 Train: 0.4992474484684686\n",
      "Epoch 6727, Loss Train(MSE): 0.12518807860822734, R2 Train: 0.49924768556709065\n",
      "Epoch 6728, Loss Train(MSE): 0.12518802455283623, R2 Train: 0.4992479017886551\n",
      "Epoch 6729, Loss Train(MSE): 0.12518797239155555, R2 Train: 0.4992481104337778\n",
      "Epoch 6730, Loss Train(MSE): 0.12518791324263773, R2 Train: 0.4992483470294491\n",
      "Epoch 6731, Loss Train(MSE): 0.125187862194294, R2 Train: 0.49924855122282397\n",
      "Epoch 6732, Loss Train(MSE): 0.12518780723207462, R2 Train: 0.4992487710717015\n",
      "Epoch 6733, Loss Train(MSE): 0.1251877482082509, R2 Train: 0.4992490071669964\n",
      "Epoch 6734, Loss Train(MSE): 0.1251877001115697, R2 Train: 0.49924919955372116\n",
      "Epoch 6735, Loss Train(MSE): 0.1251876424028429, R2 Train: 0.49924943038862835\n",
      "Epoch 6736, Loss Train(MSE): 0.12518758508097655, R2 Train: 0.4992496596760938\n",
      "Epoch 6737, Loss Train(MSE): 0.1251875367246492, R2 Train: 0.49924985310140324\n",
      "Epoch 6738, Loss Train(MSE): 0.12518747789786336, R2 Train: 0.49925008840854657\n",
      "Epoch 6739, Loss Train(MSE): 0.12518742346232517, R2 Train: 0.4992503061506993\n",
      "Epoch 6740, Loss Train(MSE): 0.12518737242336891, R2 Train: 0.49925051030652434\n",
      "Epoch 6741, Loss Train(MSE): 0.12518731372008302, R2 Train: 0.49925074511966794\n",
      "Epoch 6742, Loss Train(MSE): 0.12518726211689046, R2 Train: 0.49925095153243815\n",
      "Epoch 6743, Loss Train(MSE): 0.12518720844835804, R2 Train: 0.49925116620656784\n",
      "Epoch 6744, Loss Train(MSE): 0.1251871498679507, R2 Train: 0.49925140052819716\n",
      "Epoch 6745, Loss Train(MSE): 0.12518710104369524, R2 Train: 0.49925159582521905\n",
      "Epoch 6746, Loss Train(MSE): 0.12518868551448323, R2 Train: 0.4992452579420671\n",
      "Epoch 6747, Loss Train(MSE): 0.1251906049484573, R2 Train: 0.4992375802061708\n",
      "Epoch 6748, Loss Train(MSE): 0.12519053481047634, R2 Train: 0.4992378607580946\n",
      "Epoch 6749, Loss Train(MSE): 0.12519046472108472, R2 Train: 0.4992381411156611\n",
      "Epoch 6750, Loss Train(MSE): 0.12519039468023158, R2 Train: 0.4992384212790737\n",
      "Epoch 6751, Loss Train(MSE): 0.12519032468786614, R2 Train: 0.4992387012485354\n",
      "Epoch 6752, Loss Train(MSE): 0.1251902547439378, R2 Train: 0.4992389810242488\n",
      "Epoch 6753, Loss Train(MSE): 0.1251901848483959, R2 Train: 0.49923926060641644\n",
      "Epoch 6754, Loss Train(MSE): 0.12519011500119, R2 Train: 0.49923953999523996\n",
      "Epoch 6755, Loss Train(MSE): 0.12519004520226976, R2 Train: 0.499239819190921\n",
      "Epoch 6756, Loss Train(MSE): 0.1251899754515848, R2 Train: 0.4992400981936608\n",
      "Epoch 6757, Loss Train(MSE): 0.1251899057490849, R2 Train: 0.4992403770036604\n",
      "Epoch 6758, Loss Train(MSE): 0.12518983609471993, R2 Train: 0.4992406556211203\n",
      "Epoch 6759, Loss Train(MSE): 0.12518976648843985, R2 Train: 0.4992409340462406\n",
      "Epoch 6760, Loss Train(MSE): 0.12518969693019472, R2 Train: 0.49924121227922114\n",
      "Epoch 6761, Loss Train(MSE): 0.1251896274199346, R2 Train: 0.49924149032026155\n",
      "Epoch 6762, Loss Train(MSE): 0.1251895579576098, R2 Train: 0.4992417681695608\n",
      "Epoch 6763, Loss Train(MSE): 0.12518948854317052, R2 Train: 0.4992420458273179\n",
      "Epoch 6764, Loss Train(MSE): 0.12518941917656712, R2 Train: 0.4992423232937315\n",
      "Epoch 6765, Loss Train(MSE): 0.12518934985775007, R2 Train: 0.4992426005689997\n",
      "Epoch 6766, Loss Train(MSE): 0.12518928058667003, R2 Train: 0.49924287765331987\n",
      "Epoch 6767, Loss Train(MSE): 0.12518921136327749, R2 Train: 0.49924315454689006\n",
      "Epoch 6768, Loss Train(MSE): 0.1251891421875232, R2 Train: 0.4992434312499072\n",
      "Epoch 6769, Loss Train(MSE): 0.12518907305935792, R2 Train: 0.4992437077625683\n",
      "Epoch 6770, Loss Train(MSE): 0.1251890039787326, R2 Train: 0.49924398408506965\n",
      "Epoch 6771, Loss Train(MSE): 0.12518893494559816, R2 Train: 0.49924426021760737\n",
      "Epoch 6772, Loss Train(MSE): 0.12518886595990555, R2 Train: 0.4992445361603778\n",
      "Epoch 6773, Loss Train(MSE): 0.12518879702160599, R2 Train: 0.49924481191357606\n",
      "Epoch 6774, Loss Train(MSE): 0.12518872813065066, R2 Train: 0.4992450874773974\n",
      "Epoch 6775, Loss Train(MSE): 0.12518865928699083, R2 Train: 0.4992453628520367\n",
      "Epoch 6776, Loss Train(MSE): 0.1251885904905778, R2 Train: 0.49924563803768884\n",
      "Epoch 6777, Loss Train(MSE): 0.12518852174136313, R2 Train: 0.49924591303454746\n",
      "Epoch 6778, Loss Train(MSE): 0.1251884530392982, R2 Train: 0.49924618784280717\n",
      "Epoch 6779, Loss Train(MSE): 0.1251883843843347, R2 Train: 0.49924646246266124\n",
      "Epoch 6780, Loss Train(MSE): 0.12518831577642428, R2 Train: 0.4992467368943029\n",
      "Epoch 6781, Loss Train(MSE): 0.12518824721551863, R2 Train: 0.49924701113792547\n",
      "Epoch 6782, Loss Train(MSE): 0.12518817870156973, R2 Train: 0.4992472851937211\n",
      "Epoch 6783, Loss Train(MSE): 0.1251881102345293, R2 Train: 0.4992475590618828\n",
      "Epoch 6784, Loss Train(MSE): 0.1251880418143495, R2 Train: 0.499247832742602\n",
      "Epoch 6785, Loss Train(MSE): 0.1251879734409823, R2 Train: 0.49924810623607085\n",
      "Epoch 6786, Loss Train(MSE): 0.12518790511437983, R2 Train: 0.4992483795424807\n",
      "Epoch 6787, Loss Train(MSE): 0.12518783683449433, R2 Train: 0.4992486526620227\n",
      "Epoch 6788, Loss Train(MSE): 0.12518776860127812, R2 Train: 0.49924892559488754\n",
      "Epoch 6789, Loss Train(MSE): 0.1251877004146835, R2 Train: 0.499249198341266\n",
      "Epoch 6790, Loss Train(MSE): 0.125187632274663, R2 Train: 0.49924947090134797\n",
      "Epoch 6791, Loss Train(MSE): 0.12518756418116905, R2 Train: 0.4992497432753238\n",
      "Epoch 6792, Loss Train(MSE): 0.12518749613415436, R2 Train: 0.49925001546338255\n",
      "Epoch 6793, Loss Train(MSE): 0.12518742813357153, R2 Train: 0.49925028746571387\n",
      "Epoch 6794, Loss Train(MSE): 0.12518736017937324, R2 Train: 0.49925055928250706\n",
      "Epoch 6795, Loss Train(MSE): 0.12518729227151246, R2 Train: 0.49925083091395017\n",
      "Epoch 6796, Loss Train(MSE): 0.12518722440994196, R2 Train: 0.4992511023602322\n",
      "Epoch 6797, Loss Train(MSE): 0.12518715659461474, R2 Train: 0.49925137362154104\n",
      "Epoch 6798, Loss Train(MSE): 0.12518708882548388, R2 Train: 0.4992516446980645\n",
      "Epoch 6799, Loss Train(MSE): 0.12518702110250246, R2 Train: 0.49925191558999016\n",
      "Epoch 6800, Loss Train(MSE): 0.12518695342562364, R2 Train: 0.49925218629750545\n",
      "Epoch 6801, Loss Train(MSE): 0.1251868857948007, R2 Train: 0.4992524568207972\n",
      "Epoch 6802, Loss Train(MSE): 0.125186818209987, R2 Train: 0.499252727160052\n",
      "Epoch 6803, Loss Train(MSE): 0.12518675067113594, R2 Train: 0.49925299731545625\n",
      "Epoch 6804, Loss Train(MSE): 0.12518668317820097, R2 Train: 0.4992532672871961\n",
      "Epoch 6805, Loss Train(MSE): 0.12518661573113568, R2 Train: 0.4992535370754573\n",
      "Epoch 6806, Loss Train(MSE): 0.12518654832989357, R2 Train: 0.49925380668042574\n",
      "Epoch 6807, Loss Train(MSE): 0.1251864809744285, R2 Train: 0.499254076102286\n",
      "Epoch 6808, Loss Train(MSE): 0.12518641366469413, R2 Train: 0.4992543453412235\n",
      "Epoch 6809, Loss Train(MSE): 0.12518634640064427, R2 Train: 0.4992546143974229\n",
      "Epoch 6810, Loss Train(MSE): 0.12518627918223288, R2 Train: 0.4992548832710685\n",
      "Epoch 6811, Loss Train(MSE): 0.12518621200941393, R2 Train: 0.49925515196234427\n",
      "Epoch 6812, Loss Train(MSE): 0.12518614488214147, R2 Train: 0.4992554204714341\n",
      "Epoch 6813, Loss Train(MSE): 0.12518607780036956, R2 Train: 0.4992556887985218\n",
      "Epoch 6814, Loss Train(MSE): 0.12518601076405242, R2 Train: 0.4992559569437903\n",
      "Epoch 6815, Loss Train(MSE): 0.12518594377314426, R2 Train: 0.49925622490742294\n",
      "Epoch 6816, Loss Train(MSE): 0.12518587682759943, R2 Train: 0.49925649268960226\n",
      "Epoch 6817, Loss Train(MSE): 0.12518580992737235, R2 Train: 0.4992567602905106\n",
      "Epoch 6818, Loss Train(MSE): 0.12518574307241748, R2 Train: 0.4992570277103301\n",
      "Epoch 6819, Loss Train(MSE): 0.12518567626268925, R2 Train: 0.499257294949243\n",
      "Epoch 6820, Loss Train(MSE): 0.12518560949814234, R2 Train: 0.49925756200743066\n",
      "Epoch 6821, Loss Train(MSE): 0.12518554277873142, R2 Train: 0.4992578288850743\n",
      "Epoch 6822, Loss Train(MSE): 0.12518547610441114, R2 Train: 0.49925809558235545\n",
      "Epoch 6823, Loss Train(MSE): 0.12518540947513637, R2 Train: 0.4992583620994545\n",
      "Epoch 6824, Loss Train(MSE): 0.12518534289086194, R2 Train: 0.4992586284365522\n",
      "Epoch 6825, Loss Train(MSE): 0.12518527635154283, R2 Train: 0.49925889459382866\n",
      "Epoch 6826, Loss Train(MSE): 0.12518520985713394, R2 Train: 0.49925916057146424\n",
      "Epoch 6827, Loss Train(MSE): 0.1251851434075904, R2 Train: 0.4992594263696384\n",
      "Epoch 6828, Loss Train(MSE): 0.12518507700286735, R2 Train: 0.4992596919885306\n",
      "Epoch 6829, Loss Train(MSE): 0.12518501064291995, R2 Train: 0.4992599574283202\n",
      "Epoch 6830, Loss Train(MSE): 0.12518494432770352, R2 Train: 0.4992602226891859\n",
      "Epoch 6831, Loss Train(MSE): 0.12518487805717335, R2 Train: 0.4992604877713066\n",
      "Epoch 6832, Loss Train(MSE): 0.1251848118312848, R2 Train: 0.4992607526748608\n",
      "Epoch 6833, Loss Train(MSE): 0.1251847456499934, R2 Train: 0.4992610174000264\n",
      "Epoch 6834, Loss Train(MSE): 0.12518467951325463, R2 Train: 0.4992612819469815\n",
      "Epoch 6835, Loss Train(MSE): 0.1251846134210241, R2 Train: 0.4992615463159036\n",
      "Epoch 6836, Loss Train(MSE): 0.12518454737325743, R2 Train: 0.49926181050697027\n",
      "Epoch 6837, Loss Train(MSE): 0.12518448136991037, R2 Train: 0.49926207452035853\n",
      "Epoch 6838, Loss Train(MSE): 0.12518441541093872, R2 Train: 0.4992623383562451\n",
      "Epoch 6839, Loss Train(MSE): 0.12518434949629828, R2 Train: 0.4992626020148069\n",
      "Epoch 6840, Loss Train(MSE): 0.12518428362594505, R2 Train: 0.4992628654962198\n",
      "Epoch 6841, Loss Train(MSE): 0.1251842177998349, R2 Train: 0.49926312880066037\n",
      "Epoch 6842, Loss Train(MSE): 0.12518415201792393, R2 Train: 0.49926339192830427\n",
      "Epoch 6843, Loss Train(MSE): 0.12518408628016825, R2 Train: 0.499263654879327\n",
      "Epoch 6844, Loss Train(MSE): 0.12518402058652398, R2 Train: 0.4992639176539041\n",
      "Epoch 6845, Loss Train(MSE): 0.12518395493694737, R2 Train: 0.49926418025221053\n",
      "Epoch 6846, Loss Train(MSE): 0.12518388933139468, R2 Train: 0.49926444267442127\n",
      "Epoch 6847, Loss Train(MSE): 0.12518382376982237, R2 Train: 0.4992647049207105\n",
      "Epoch 6848, Loss Train(MSE): 0.12518375825218678, R2 Train: 0.4992649669912529\n",
      "Epoch 6849, Loss Train(MSE): 0.12518369277844438, R2 Train: 0.4992652288862225\n",
      "Epoch 6850, Loss Train(MSE): 0.1251836273485517, R2 Train: 0.49926549060579317\n",
      "Epoch 6851, Loss Train(MSE): 0.12518356196246544, R2 Train: 0.49926575215013824\n",
      "Epoch 6852, Loss Train(MSE): 0.12518349662014217, R2 Train: 0.49926601351943134\n",
      "Epoch 6853, Loss Train(MSE): 0.12518343132153864, R2 Train: 0.49926627471384544\n",
      "Epoch 6854, Loss Train(MSE): 0.12518336606661165, R2 Train: 0.4992665357335534\n",
      "Epoch 6855, Loss Train(MSE): 0.12518330085531804, R2 Train: 0.49926679657872786\n",
      "Epoch 6856, Loss Train(MSE): 0.12518323568761477, R2 Train: 0.4992670572495409\n",
      "Epoch 6857, Loss Train(MSE): 0.12518317056345873, R2 Train: 0.49926731774616506\n",
      "Epoch 6858, Loss Train(MSE): 0.12518310548280703, R2 Train: 0.49926757806877187\n",
      "Epoch 6859, Loss Train(MSE): 0.1251830404456167, R2 Train: 0.4992678382175332\n",
      "Epoch 6860, Loss Train(MSE): 0.12518297545184492, R2 Train: 0.4992680981926203\n",
      "Epoch 6861, Loss Train(MSE): 0.12518291050144895, R2 Train: 0.4992683579942042\n",
      "Epoch 6862, Loss Train(MSE): 0.12518284559438597, R2 Train: 0.4992686176224561\n",
      "Epoch 6863, Loss Train(MSE): 0.12518278073061337, R2 Train: 0.4992688770775465\n",
      "Epoch 6864, Loss Train(MSE): 0.12518271591008862, R2 Train: 0.49926913635964554\n",
      "Epoch 6865, Loss Train(MSE): 0.12518265113276902, R2 Train: 0.4992693954689239\n",
      "Epoch 6866, Loss Train(MSE): 0.1251825863986122, R2 Train: 0.49926965440555116\n",
      "Epoch 6867, Loss Train(MSE): 0.12518252170757568, R2 Train: 0.49926991316969727\n",
      "Epoch 6868, Loss Train(MSE): 0.12518245705961714, R2 Train: 0.49927017176153143\n",
      "Epoch 6869, Loss Train(MSE): 0.1251823924546942, R2 Train: 0.4992704301812232\n",
      "Epoch 6870, Loss Train(MSE): 0.12518232789276468, R2 Train: 0.4992706884289413\n",
      "Epoch 6871, Loss Train(MSE): 0.12518226337378638, R2 Train: 0.4992709465048545\n",
      "Epoch 6872, Loss Train(MSE): 0.12518219889771712, R2 Train: 0.49927120440913153\n",
      "Epoch 6873, Loss Train(MSE): 0.12518213446451484, R2 Train: 0.4992714621419406\n",
      "Epoch 6874, Loss Train(MSE): 0.12518207007413754, R2 Train: 0.49927171970344986\n",
      "Epoch 6875, Loss Train(MSE): 0.1251820057265433, R2 Train: 0.49927197709382676\n",
      "Epoch 6876, Loss Train(MSE): 0.12518194142169012, R2 Train: 0.4992722343132395\n",
      "Epoch 6877, Loss Train(MSE): 0.1251818771595363, R2 Train: 0.4992724913618548\n",
      "Epoch 6878, Loss Train(MSE): 0.12518181294003994, R2 Train: 0.49927274823984025\n",
      "Epoch 6879, Loss Train(MSE): 0.12518174876315938, R2 Train: 0.4992730049473625\n",
      "Epoch 6880, Loss Train(MSE): 0.12518168462885287, R2 Train: 0.4992732614845885\n",
      "Epoch 6881, Loss Train(MSE): 0.12518162053707893, R2 Train: 0.4992735178516843\n",
      "Epoch 6882, Loss Train(MSE): 0.1251815564877959, R2 Train: 0.49927377404881645\n",
      "Epoch 6883, Loss Train(MSE): 0.12518149248096228, R2 Train: 0.4992740300761509\n",
      "Epoch 6884, Loss Train(MSE): 0.12518142851653666, R2 Train: 0.49927428593385337\n",
      "Epoch 6885, Loss Train(MSE): 0.1251813645944777, R2 Train: 0.4992745416220892\n",
      "Epoch 6886, Loss Train(MSE): 0.12518130071474404, R2 Train: 0.49927479714102385\n",
      "Epoch 6887, Loss Train(MSE): 0.12518123687729435, R2 Train: 0.4992750524908226\n",
      "Epoch 6888, Loss Train(MSE): 0.1251811730820875, R2 Train: 0.49927530767164996\n",
      "Epoch 6889, Loss Train(MSE): 0.12518110932908227, R2 Train: 0.4992755626836709\n",
      "Epoch 6890, Loss Train(MSE): 0.12518104561823767, R2 Train: 0.4992758175270493\n",
      "Epoch 6891, Loss Train(MSE): 0.1251809819495125, R2 Train: 0.49927607220195\n",
      "Epoch 6892, Loss Train(MSE): 0.12518091832286582, R2 Train: 0.4992763267085367\n",
      "Epoch 6893, Loss Train(MSE): 0.12518085473825674, R2 Train: 0.49927658104697303\n",
      "Epoch 6894, Loss Train(MSE): 0.1251807911956444, R2 Train: 0.4992768352174224\n",
      "Epoch 6895, Loss Train(MSE): 0.12518072769498792, R2 Train: 0.4992770892200483\n",
      "Epoch 6896, Loss Train(MSE): 0.12518066423624657, R2 Train: 0.4992773430550137\n",
      "Epoch 6897, Loss Train(MSE): 0.12518060081937957, R2 Train: 0.49927759672248173\n",
      "Epoch 6898, Loss Train(MSE): 0.1251805374443463, R2 Train: 0.49927785022261484\n",
      "Epoch 6899, Loss Train(MSE): 0.12518047411110617, R2 Train: 0.49927810355557534\n",
      "Epoch 6900, Loss Train(MSE): 0.12518041081961861, R2 Train: 0.49927835672152554\n",
      "Epoch 6901, Loss Train(MSE): 0.12518034756984317, R2 Train: 0.4992786097206273\n",
      "Epoch 6902, Loss Train(MSE): 0.1251802843617394, R2 Train: 0.49927886255304244\n",
      "Epoch 6903, Loss Train(MSE): 0.12518022119526684, R2 Train: 0.49927911521893265\n",
      "Epoch 6904, Loss Train(MSE): 0.1251801580703852, R2 Train: 0.49927936771845915\n",
      "Epoch 6905, Loss Train(MSE): 0.12518009498705426, R2 Train: 0.49927962005178295\n",
      "Epoch 6906, Loss Train(MSE): 0.12518003194523378, R2 Train: 0.4992798722190649\n",
      "Epoch 6907, Loss Train(MSE): 0.1251799689448835, R2 Train: 0.49928012422046597\n",
      "Epoch 6908, Loss Train(MSE): 0.1251799059859634, R2 Train: 0.4992803760561464\n",
      "Epoch 6909, Loss Train(MSE): 0.12517984306843338, R2 Train: 0.49928062772626647\n",
      "Epoch 6910, Loss Train(MSE): 0.12517978019225345, R2 Train: 0.4992808792309862\n",
      "Epoch 6911, Loss Train(MSE): 0.12517971735738362, R2 Train: 0.4992811305704655\n",
      "Epoch 6912, Loss Train(MSE): 0.12517965456378405, R2 Train: 0.4992813817448638\n",
      "Epoch 6913, Loss Train(MSE): 0.1251795918114148, R2 Train: 0.4992816327543408\n",
      "Epoch 6914, Loss Train(MSE): 0.1251795291002362, R2 Train: 0.49928188359905523\n",
      "Epoch 6915, Loss Train(MSE): 0.12517946643020839, R2 Train: 0.49928213427916646\n",
      "Epoch 6916, Loss Train(MSE): 0.1251794038012917, R2 Train: 0.4992823847948332\n",
      "Epoch 6917, Loss Train(MSE): 0.1251793412134466, R2 Train: 0.4992826351462136\n",
      "Epoch 6918, Loss Train(MSE): 0.12517927866663336, R2 Train: 0.49928288533346654\n",
      "Epoch 6919, Loss Train(MSE): 0.1251792161608125, R2 Train: 0.49928313535674995\n",
      "Epoch 6920, Loss Train(MSE): 0.12517916600232887, R2 Train: 0.49928333599068453\n",
      "Epoch 6921, Loss Train(MSE): 0.1251791133164009, R2 Train: 0.49928354673439646\n",
      "Epoch 6922, Loss Train(MSE): 0.12517906808937249, R2 Train: 0.49928372764251006\n",
      "Epoch 6923, Loss Train(MSE): 0.1251790178960607, R2 Train: 0.4992839284157572\n",
      "Epoch 6924, Loss Train(MSE): 0.12517896530429232, R2 Train: 0.49928413878283073\n",
      "Epoch 6925, Loss Train(MSE): 0.1251789202462896, R2 Train: 0.4992843190148416\n",
      "Epoch 6926, Loss Train(MSE): 0.12517887004377198, R2 Train: 0.49928451982491207\n",
      "Epoch 6927, Loss Train(MSE): 0.12517881754581356, R2 Train: 0.49928472981674576\n",
      "Epoch 6928, Loss Train(MSE): 0.1251787726309644, R2 Train: 0.49928490947614235\n",
      "Epoch 6929, Loss Train(MSE): 0.12517872244458694, R2 Train: 0.49928511022165223\n",
      "Epoch 6930, Loss Train(MSE): 0.12517867004009142, R2 Train: 0.4992853198396343\n",
      "Epoch 6931, Loss Train(MSE): 0.12517862524280085, R2 Train: 0.4992854990287966\n",
      "Epoch 6932, Loss Train(MSE): 0.12517857509763625, R2 Train: 0.499285699609455\n",
      "Epoch 6933, Loss Train(MSE): 0.12517852278625913, R2 Train: 0.4992859088549635\n",
      "Epoch 6934, Loss Train(MSE): 0.12517847808120647, R2 Train: 0.49928608767517413\n",
      "Epoch 6935, Loss Train(MSE): 0.12517842800205675, R2 Train: 0.499286287991773\n",
      "Epoch 6936, Loss Train(MSE): 0.12517837578345617, R2 Train: 0.4992864968661753\n",
      "Epoch 6937, Loss Train(MSE): 0.12517833114559226, R2 Train: 0.49928667541763094\n",
      "Epoch 6938, Loss Train(MSE): 0.1251782811569916, R2 Train: 0.49928687537203364\n",
      "Epoch 6939, Loss Train(MSE): 0.12517822903082823, R2 Train: 0.4992870838766871\n",
      "Epoch 6940, Loss Train(MSE): 0.12517818443537254, R2 Train: 0.49928726225850983\n",
      "Epoch 6941, Loss Train(MSE): 0.12517813456159005, R2 Train: 0.4992874617536398\n",
      "Epoch 6942, Loss Train(MSE): 0.1251780825275271, R2 Train: 0.4992876698898916\n",
      "Epoch 6943, Loss Train(MSE): 0.125178037949965, R2 Train: 0.49928784820014005\n",
      "Epoch 6944, Loss Train(MSE): 0.1251779882150075, R2 Train: 0.49928804713997\n",
      "Epoch 6945, Loss Train(MSE): 0.12517793627271054, R2 Train: 0.49928825490915785\n",
      "Epoch 6946, Loss Train(MSE): 0.1251778916887907, R2 Train: 0.4992884332448372\n",
      "Epoch 6947, Loss Train(MSE): 0.12517784211640529, R2 Train: 0.49928863153437886\n",
      "Epoch 6948, Loss Train(MSE): 0.12517779026554246, R2 Train: 0.49928883893783016\n",
      "Epoch 6949, Loss Train(MSE): 0.12517774565127393, R2 Train: 0.4992890173949043\n",
      "Epoch 6950, Loss Train(MSE): 0.12517769626495073, R2 Train: 0.49928921494019707\n",
      "Epoch 6951, Loss Train(MSE): 0.12517764450519253, R2 Train: 0.4992894219792299\n",
      "Epoch 6952, Loss Train(MSE): 0.12517759983684223, R2 Train: 0.49928960065263106\n",
      "Epoch 6953, Loss Train(MSE): 0.12517755065981692, R2 Train: 0.4992897973607323\n",
      "Epoch 6954, Loss Train(MSE): 0.12517749899083627, R2 Train: 0.4992900040366549\n",
      "Epoch 6955, Loss Train(MSE): 0.12517745424492632, R2 Train: 0.4992901830202947\n",
      "Epoch 6956, Loss Train(MSE): 0.12517740530018287, R2 Train: 0.4992903787992685\n",
      "Epoch 6957, Loss Train(MSE): 0.12517735372165503, R2 Train: 0.49929058511337987\n",
      "Epoch 6958, Loss Train(MSE): 0.12517730887496006, R2 Train: 0.49929076450015974\n",
      "Epoch 6959, Loss Train(MSE): 0.12517726018523317, R2 Train: 0.4992909592590673\n",
      "Epoch 6960, Loss Train(MSE): 0.12517720869683585, R2 Train: 0.4992911652126566\n",
      "Epoch 6961, Loss Train(MSE): 0.12517716372638055, R2 Train: 0.4992913450944778\n",
      "Epoch 6962, Loss Train(MSE): 0.1251771153141583, R2 Train: 0.49929153874336685\n",
      "Epoch 6963, Loss Train(MSE): 0.12517706391557137, R2 Train: 0.4992917443377145\n",
      "Epoch 6964, Loss Train(MSE): 0.12517701879862783, R2 Train: 0.4992919248054887\n",
      "Epoch 6965, Loss Train(MSE): 0.12517697068615402, R2 Train: 0.4992921172553839\n",
      "Epoch 6966, Loss Train(MSE): 0.1251769193770598, R2 Train: 0.4992923224917608\n",
      "Epoch 6967, Loss Train(MSE): 0.12517687409114517, R2 Train: 0.4992925036354193\n",
      "Epoch 6968, Loss Train(MSE): 0.125176826300422, R2 Train: 0.49929269479831195\n",
      "Epoch 6969, Loss Train(MSE): 0.12517677508050484, R2 Train: 0.49929289967798063\n",
      "Epoch 6970, Loss Train(MSE): 0.12517672960337856, R2 Train: 0.4992930815864858\n",
      "Epoch 6971, Loss Train(MSE): 0.12517668215616903, R2 Train: 0.49929327137532387\n",
      "Epoch 6972, Loss Train(MSE): 0.1251766310251157, R2 Train: 0.4992934758995372\n",
      "Epoch 6973, Loss Train(MSE): 0.12517658533477732, R2 Train: 0.4992936586608907\n",
      "Epoch 6974, Loss Train(MSE): 0.1251765382526075, R2 Train: 0.49929384698957\n",
      "Epoch 6975, Loss Train(MSE): 0.12517648721010696, R2 Train: 0.49929405115957215\n",
      "Epoch 6976, Loss Train(MSE): 0.12517644128479347, R2 Train: 0.49929423486082614\n",
      "Epoch 6977, Loss Train(MSE): 0.1251763945889551, R2 Train: 0.4992944216441796\n",
      "Epoch 6978, Loss Train(MSE): 0.12517634363469848, R2 Train: 0.49929462546120607\n",
      "Epoch 6979, Loss Train(MSE): 0.12517629745288208, R2 Train: 0.49929481018847166\n",
      "Epoch 6980, Loss Train(MSE): 0.1251762511644348, R2 Train: 0.49929499534226085\n",
      "Epoch 6981, Loss Train(MSE): 0.12517620029811538, R2 Train: 0.4992951988075385\n",
      "Epoch 6982, Loss Train(MSE): 0.12517615383850103, R2 Train: 0.4992953846459959\n",
      "Epoch 6983, Loss Train(MSE): 0.1251761079782748, R2 Train: 0.4992955680869008\n",
      "Epoch 6984, Loss Train(MSE): 0.12517605719958805, R2 Train: 0.4992957712016478\n",
      "Epoch 6985, Loss Train(MSE): 0.12517601044111104, R2 Train: 0.49929595823555584\n",
      "Epoch 6986, Loss Train(MSE): 0.1251759650297085, R2 Train: 0.49929613988116595\n",
      "Epoch 6987, Loss Train(MSE): 0.1251759143383519, R2 Train: 0.4992963426465924\n",
      "Epoch 6988, Loss Train(MSE): 0.1251758672601757, R2 Train: 0.49929653095929716\n",
      "Epoch 6989, Loss Train(MSE): 0.1251758223179743, R2 Train: 0.4992967107281028\n",
      "Epoch 6990, Loss Train(MSE): 0.12517577171364755, R2 Train: 0.4992969131454098\n",
      "Epoch 6991, Loss Train(MSE): 0.12517572429516138, R2 Train: 0.49929710281935447\n",
      "Epoch 6992, Loss Train(MSE): 0.12517567984231584, R2 Train: 0.49929728063073664\n",
      "Epoch 6993, Loss Train(MSE): 0.12517562932472054, R2 Train: 0.49929748270111785\n",
      "Epoch 6994, Loss Train(MSE): 0.12517558154553718, R2 Train: 0.4992976738178513\n",
      "Epoch 6995, Loss Train(MSE): 0.12517553760198155, R2 Train: 0.4992978495920738\n",
      "Epoch 6996, Loss Train(MSE): 0.1251754871708215, R2 Train: 0.499298051316714\n",
      "Epoch 6997, Loss Train(MSE): 0.12517543901077488, R2 Train: 0.4992982439569005\n",
      "Epoch 6998, Loss Train(MSE): 0.1251753955962249, R2 Train: 0.4992984176151004\n",
      "Epoch 6999, Loss Train(MSE): 0.12517534525120597, R2 Train: 0.49929861899517614\n",
      "Epoch 7000, Loss Train(MSE): 0.12517529669034905, R2 Train: 0.4992988132386038\n",
      "Epoch 7001, Loss Train(MSE): 0.12517525382430433, R2 Train: 0.49929898470278267\n",
      "Epoch 7002, Loss Train(MSE): 0.1251752035651341, R2 Train: 0.49929918573946364\n",
      "Epoch 7003, Loss Train(MSE): 0.12517515458373688, R2 Train: 0.4992993816650525\n",
      "Epoch 7004, Loss Train(MSE): 0.1251751122854829, R2 Train: 0.49929955085806843\n",
      "Epoch 7005, Loss Train(MSE): 0.12517506211187127, R2 Train: 0.49929975155251494\n",
      "Epoch 7006, Loss Train(MSE): 0.12517501269041814, R2 Train: 0.49929994923832743\n",
      "Epoch 7007, Loss Train(MSE): 0.12517497097902863, R2 Train: 0.49930011608388547\n",
      "Epoch 7008, Loss Train(MSE): 0.12517492089068724, R2 Train: 0.49930031643725103\n",
      "Epoch 7009, Loss Train(MSE): 0.12517487100987526, R2 Train: 0.49930051596049896\n",
      "Epoch 7010, Loss Train(MSE): 0.1251748299042142, R2 Train: 0.49930068038314324\n",
      "Epoch 7011, Loss Train(MSE): 0.12517477990085674, R2 Train: 0.49930088039657305\n",
      "Epoch 7012, Loss Train(MSE): 0.12517472993200573, R2 Train: 0.49930108027197706\n",
      "Epoch 7013, Loss Train(MSE): 0.1251746886695296, R2 Train: 0.4993012453218816\n",
      "Epoch 7014, Loss Train(MSE): 0.12517463914432078, R2 Train: 0.4993014434227169\n",
      "Epoch 7015, Loss Train(MSE): 0.12517458926002883, R2 Train: 0.49930164295988466\n",
      "Epoch 7016, Loss Train(MSE): 0.12517454747546794, R2 Train: 0.49930181009812824\n",
      "Epoch 7017, Loss Train(MSE): 0.12517449861765195, R2 Train: 0.4993020055293922\n",
      "Epoch 7018, Loss Train(MSE): 0.1251744488176375, R2 Train: 0.49930220472944997\n",
      "Epoch 7019, Loss Train(MSE): 0.1251744064924783, R2 Train: 0.4993023740300868\n",
      "Epoch 7020, Loss Train(MSE): 0.12517435832013932, R2 Train: 0.49930256671944273\n",
      "Epoch 7021, Loss Train(MSE): 0.1251743086041229, R2 Train: 0.4993027655835084\n",
      "Epoch 7022, Loss Train(MSE): 0.12517426572005433, R2 Train: 0.4993029371197827\n",
      "Epoch 7023, Loss Train(MSE): 0.12517421825107664, R2 Train: 0.49930312699569346\n",
      "Epoch 7024, Loss Train(MSE): 0.1251741686187805, R2 Train: 0.499303325524878\n",
      "Epoch 7025, Loss Train(MSE): 0.12517412515769216, R2 Train: 0.49930349936923135\n",
      "Epoch 7026, Loss Train(MSE): 0.12517407840976194, R2 Train: 0.49930368636095224\n",
      "Epoch 7027, Loss Train(MSE): 0.12517402886091014, R2 Train: 0.49930388455635943\n",
      "Epoch 7028, Loss Train(MSE): 0.12517398480489048, R2 Train: 0.4993040607804381\n",
      "Epoch 7029, Loss Train(MSE): 0.12517393879549776, R2 Train: 0.49930424481800895\n",
      "Epoch 7030, Loss Train(MSE): 0.12517388932981624, R2 Train: 0.49930444268073504\n",
      "Epoch 7031, Loss Train(MSE): 0.1251738446611502, R2 Train: 0.4993046213553992\n",
      "Epoch 7032, Loss Train(MSE): 0.1251737994075909, R2 Train: 0.49930480236963637\n",
      "Epoch 7033, Loss Train(MSE): 0.12517375002480738, R2 Train: 0.4993049999007705\n",
      "Epoch 7034, Loss Train(MSE): 0.12517370472597464, R2 Train: 0.49930518109610145\n",
      "Epoch 7035, Loss Train(MSE): 0.12517366024535262, R2 Train: 0.4993053590185895\n",
      "Epoch 7036, Loss Train(MSE): 0.12517361094519652, R2 Train: 0.4993055562192139\n",
      "Epoch 7037, Loss Train(MSE): 0.12517356499886972, R2 Train: 0.4993057400045211\n",
      "Epoch 7038, Loss Train(MSE): 0.1251735213080982, R2 Train: 0.4993059147676072\n",
      "Epoch 7039, Loss Train(MSE): 0.12517347209030072, R2 Train: 0.4993061116387971\n",
      "Epoch 7040, Loss Train(MSE): 0.12517342547934332, R2 Train: 0.4993062980826267\n",
      "Epoch 7041, Loss Train(MSE): 0.12517338259514735, R2 Train: 0.4993064696194106\n",
      "Epoch 7042, Loss Train(MSE): 0.12517333345944145, R2 Train: 0.4993066661622342\n",
      "Epoch 7043, Loss Train(MSE): 0.12517328616690582, R2 Train: 0.49930685533237673\n",
      "Epoch 7044, Loss Train(MSE): 0.12517324410582384, R2 Train: 0.49930702357670464\n",
      "Epoch 7045, Loss Train(MSE): 0.12517319505194419, R2 Train: 0.49930721979222326\n",
      "Epoch 7046, Loss Train(MSE): 0.12517314706106997, R2 Train: 0.49930741175572013\n",
      "Epoch 7047, Loss Train(MSE): 0.12517310583945565, R2 Train: 0.4993075766421774\n",
      "Epoch 7048, Loss Train(MSE): 0.12517305686713853, R2 Train: 0.49930777253144587\n",
      "Epoch 7049, Loss Train(MSE): 0.12517300816135063, R2 Train: 0.4993079673545975\n",
      "Epoch 7050, Loss Train(MSE): 0.1251729677953747, R2 Train: 0.4993081288185012\n",
      "Epoch 7051, Loss Train(MSE): 0.12517291890435822, R2 Train: 0.4993083243825671\n",
      "Epoch 7052, Loss Train(MSE): 0.12517287004626912, R2 Train: 0.4993085198149235\n",
      "Epoch 7053, Loss Train(MSE): 0.1251728293934079, R2 Train: 0.4993086824263684\n",
      "Epoch 7054, Loss Train(MSE): 0.1251727811654332, R2 Train: 0.4993088753382672\n",
      "Epoch 7055, Loss Train(MSE): 0.1251727323882557, R2 Train: 0.49930907044697725\n",
      "Epoch 7056, Loss Train(MSE): 0.12517269096477046, R2 Train: 0.49930923614091816\n",
      "Epoch 7057, Loss Train(MSE): 0.12517264364716751, R2 Train: 0.49930942541132994\n",
      "Epoch 7058, Loss Train(MSE): 0.1251725949506438, R2 Train: 0.49930962019742475\n",
      "Epoch 7059, Loss Train(MSE): 0.12517255274065456, R2 Train: 0.49930978903738177\n",
      "Epoch 7060, Loss Train(MSE): 0.12517250634890748, R2 Train: 0.4993099746043701\n",
      "Epoch 7061, Loss Train(MSE): 0.12517245773278146, R2 Train: 0.49931016906887415\n",
      "Epoch 7062, Loss Train(MSE): 0.12517241472058485, R2 Train: 0.4993103411176606\n",
      "Epoch 7063, Loss Train(MSE): 0.1251723692700032, R2 Train: 0.49931052291998723\n",
      "Epoch 7064, Loss Train(MSE): 0.1251723207340205, R2 Train: 0.49931071706391805\n",
      "Epoch 7065, Loss Train(MSE): 0.1251722769040882, R2 Train: 0.49931089238364723\n",
      "Epoch 7066, Loss Train(MSE): 0.12517223240980876, R2 Train: 0.49931107036076494\n",
      "Epoch 7067, Loss Train(MSE): 0.1251721839537164, R2 Train: 0.4993112641851344\n",
      "Epoch 7068, Loss Train(MSE): 0.12517213929069348, R2 Train: 0.49931144283722606\n",
      "Epoch 7069, Loss Train(MSE): 0.12517209576768204, R2 Train: 0.49931161692927184\n",
      "Epoch 7070, Loss Train(MSE): 0.12517204739122864, R2 Train: 0.49931181043508543\n",
      "Epoch 7071, Loss Train(MSE): 0.12517200187993177, R2 Train: 0.4993119924802729\n",
      "Epoch 7072, Loss Train(MSE): 0.1251719593429846, R2 Train: 0.49931216262806155\n",
      "Epoch 7073, Loss Train(MSE): 0.12517785653787905, R2 Train: 0.4992885738484838\n",
      "Epoch 7074, Loss Train(MSE): 0.12518891580365002, R2 Train: 0.4992443367853999\n",
      "Epoch 7075, Loss Train(MSE): 0.12518880523530954, R2 Train: 0.49924477905876186\n",
      "Epoch 7076, Loss Train(MSE): 0.12518868046966075, R2 Train: 0.499245278121357\n",
      "Epoch 7077, Loss Train(MSE): 0.12518856975953338, R2 Train: 0.4992457209618665\n",
      "Epoch 7078, Loss Train(MSE): 0.1251884482336729, R2 Train: 0.4992462070653084\n",
      "Epoch 7079, Loss Train(MSE): 0.12518833281699243, R2 Train: 0.4992466687320303\n",
      "Epoch 7080, Loss Train(MSE): 0.12518821681647127, R2 Train: 0.4992471327341149\n",
      "Epoch 7081, Loss Train(MSE): 0.12518809665264416, R2 Train: 0.49924761338942336\n",
      "Epoch 7082, Loss Train(MSE): 0.12518798621258329, R2 Train: 0.49924805514966686\n",
      "Epoch 7083, Loss Train(MSE): 0.12518786307768162, R2 Train: 0.4992485476892735\n",
      "Epoch 7084, Loss Train(MSE): 0.1251877546154262, R2 Train: 0.4992489815382952\n",
      "Epoch 7085, Loss Train(MSE): 0.12518763373584954, R2 Train: 0.49924946505660184\n",
      "Epoch 7086, Loss Train(MSE): 0.1251875203793924, R2 Train: 0.4992499184824304\n",
      "Epoch 7087, Loss Train(MSE): 0.1251874051934935, R2 Train: 0.499250379226026\n",
      "Epoch 7088, Loss Train(MSE): 0.125187286904518, R2 Train: 0.499250852381928\n",
      "Epoch 7089, Loss Train(MSE): 0.12518717744534116, R2 Train: 0.49925129021863535\n",
      "Epoch 7090, Loss Train(MSE): 0.125187055901814, R2 Train: 0.499251776392744\n",
      "Epoch 7091, Loss Train(MSE): 0.12518694878505238, R2 Train: 0.4992522048597905\n",
      "Epoch 7092, Loss Train(MSE): 0.12518682938610656, R2 Train: 0.49925268245557375\n",
      "Epoch 7093, Loss Train(MSE): 0.12518671719635976, R2 Train: 0.49925313121456094\n",
      "Epoch 7094, Loss Train(MSE): 0.12518660365126608, R2 Train: 0.4992535853949357\n",
      "Epoch 7095, Loss Train(MSE): 0.12518648635239238, R2 Train: 0.4992540545904305\n",
      "Epoch 7096, Loss Train(MSE): 0.12518637869221017, R2 Train: 0.49925448523115934\n",
      "Epoch 7097, Loss Train(MSE): 0.12518625870210864, R2 Train: 0.4992549651915654\n",
      "Epoch 7098, Loss Train(MSE): 0.1251861520647017, R2 Train: 0.49925539174119316\n",
      "Epoch 7099, Loss Train(MSE): 0.1251860349469481, R2 Train: 0.4992558602122076\n",
      "Epoch 7100, Loss Train(MSE): 0.12518592306625587, R2 Train: 0.49925630773497653\n",
      "Epoch 7101, Loss Train(MSE): 0.12518581195471076, R2 Train: 0.49925675218115695\n",
      "Epoch 7102, Loss Train(MSE): 0.12518569479666933, R2 Train: 0.4992572208133227\n",
      "Epoch 7103, Loss Train(MSE): 0.12518558972049468, R2 Train: 0.4992576411180213\n",
      "Epoch 7104, Loss Train(MSE): 0.1251854712472224, R2 Train: 0.4992581150111104\n",
      "Epoch 7105, Loss Train(MSE): 0.1251853642577819, R2 Train: 0.4992585429688724\n",
      "Epoch 7106, Loss Train(MSE): 0.12518525018936033, R2 Train: 0.4992589992425587\n",
      "Epoch 7107, Loss Train(MSE): 0.12518513779445414, R2 Train: 0.4992594488221834\n",
      "Epoch 7108, Loss Train(MSE): 0.1251850298771096, R2 Train: 0.49925988049156156\n",
      "Epoch 7109, Loss Train(MSE): 0.12518491247653327, R2 Train: 0.49926035009386693\n",
      "Epoch 7110, Loss Train(MSE): 0.12518480988766278, R2 Train: 0.49926076044934886\n",
      "Epoch 7111, Loss Train(MSE): 0.12518469332137724, R2 Train: 0.499261226714491\n",
      "Epoch 7112, Loss Train(MSE): 0.12518458520324788, R2 Train: 0.4992616591870085\n",
      "Epoch 7113, Loss Train(MSE): 0.12518447489973422, R2 Train: 0.4992621004010631\n",
      "Epoch 7114, Loss Train(MSE): 0.12518436122169305, R2 Train: 0.4992625551132278\n",
      "Epoch 7115, Loss Train(MSE): 0.12518425720699183, R2 Train: 0.49926297117203267\n",
      "Epoch 7116, Loss Train(MSE): 0.12518414126320376, R2 Train: 0.49926343494718495\n",
      "Epoch 7117, Loss Train(MSE): 0.12518403692775631, R2 Train: 0.49926385228897474\n",
      "Epoch 7118, Loss Train(MSE): 0.12518392470163486, R2 Train: 0.49926430119346055\n",
      "Epoch 7119, Loss Train(MSE): 0.12518381468809267, R2 Train: 0.4992647412476293\n",
      "Epoch 7120, Loss Train(MSE): 0.12518370885728242, R2 Train: 0.49926516457087033\n",
      "Epoch 7121, Loss Train(MSE): 0.12518359394415013, R2 Train: 0.4992656242233995\n",
      "Epoch 7122, Loss Train(MSE): 0.12518349293169095, R2 Train: 0.4992660282732362\n",
      "Epoch 7123, Loss Train(MSE): 0.12518337921283, R2 Train: 0.49926648314868005\n",
      "Epoch 7124, Loss Train(MSE): 0.12518327240811244, R2 Train: 0.4992669103675502\n",
      "Epoch 7125, Loss Train(MSE): 0.12518316518732803, R2 Train: 0.4992673392506879\n",
      "Epoch 7126, Loss Train(MSE): 0.1251830525628555, R2 Train: 0.499267789748578\n",
      "Epoch 7127, Loss Train(MSE): 0.12518295186329773, R2 Train: 0.4992681925468091\n",
      "Epoch 7128, Loss Train(MSE): 0.12518283835041483, R2 Train: 0.4992686465983407\n",
      "Epoch 7129, Loss Train(MSE): 0.1251827342910841, R2 Train: 0.49926906283566364\n",
      "Epoch 7130, Loss Train(MSE): 0.12518262611498385, R2 Train: 0.4992694955400646\n",
      "Epoch 7131, Loss Train(MSE): 0.1251825161269475, R2 Train: 0.49926993549221\n",
      "Epoch 7132, Loss Train(MSE): 0.12518241457000692, R2 Train: 0.4992703417199723\n",
      "Epoch 7133, Loss Train(MSE): 0.12518230204816233, R2 Train: 0.4992707918073507\n",
      "Epoch 7134, Loss Train(MSE): 0.12518220030345484, R2 Train: 0.49927119878618065\n",
      "Epoch 7135, Loss Train(MSE): 0.12518209157467178, R2 Train: 0.4992716337013129\n",
      "Epoch 7136, Loss Train(MSE): 0.12518198379604775, R2 Train: 0.499272064815809\n",
      "Epoch 7137, Loss Train(MSE): 0.125181881780881, R2 Train: 0.499272472876476\n",
      "Epoch 7138, Loss Train(MSE): 0.1251817702341353, R2 Train: 0.49927291906345883\n",
      "Epoch 7139, Loss Train(MSE): 0.12518167038500175, R2 Train: 0.499273318459993\n",
      "Epoch 7140, Loss Train(MSE): 0.1251815614951244, R2 Train: 0.49927375401950236\n",
      "Epoch 7141, Loss Train(MSE): 0.12518145551049137, R2 Train: 0.4992741779580345\n",
      "Epoch 7142, Loss Train(MSE): 0.12518135342531345, R2 Train: 0.4992745862987462\n",
      "Epoch 7143, Loss Train(MSE): 0.12518124283810625, R2 Train: 0.499275028647575\n",
      "Epoch 7144, Loss Train(MSE): 0.12518114447688378, R2 Train: 0.4992754220924649\n",
      "Epoch 7145, Loss Train(MSE): 0.12518103580676165, R2 Train: 0.4992758567729534\n",
      "Epoch 7146, Loss Train(MSE): 0.1251809312119767, R2 Train: 0.49927627515209316\n",
      "Epoch 7147, Loss Train(MSE): 0.12518082943436312, R2 Train: 0.4992766822625475\n",
      "Epoch 7148, Loss Train(MSE): 0.12518071979150078, R2 Train: 0.4992771208339969\n",
      "Epoch 7149, Loss Train(MSE): 0.1251806225215955, R2 Train: 0.49927750991361797\n",
      "Epoch 7150, Loss Train(MSE): 0.12518051444163505, R2 Train: 0.4992779422334598\n",
      "Epoch 7151, Loss Train(MSE): 0.12518041084352005, R2 Train: 0.4992783566259198\n",
      "Epoch 7152, Loss Train(MSE): 0.1251803097406995, R2 Train: 0.49927876103720203\n",
      "Epoch 7153, Loss Train(MSE): 0.1251802010273432, R2 Train: 0.4992791958906272\n",
      "Epoch 7154, Loss Train(MSE): 0.12518010446292302, R2 Train: 0.4992795821483079\n",
      "Epoch 7155, Loss Train(MSE): 0.12517999733337462, R2 Train: 0.4992800106665015\n",
      "Epoch 7156, Loss Train(MSE): 0.12517989434941226, R2 Train: 0.49928042260235095\n",
      "Epoch 7157, Loss Train(MSE): 0.1251797942785504, R2 Train: 0.49928082288579845\n",
      "Epoch 7158, Loss Train(MSE): 0.1251796864802047, R2 Train: 0.4992812540791812\n",
      "Epoch 7159, Loss Train(MSE): 0.1251795902459021, R2 Train: 0.49928163901639155\n",
      "Epoch 7160, Loss Train(MSE): 0.12517948441713758, R2 Train: 0.49928206233144967\n",
      "Epoch 7161, Loss Train(MSE): 0.12517938167517742, R2 Train: 0.4992824732992903\n",
      "Epoch 7162, Loss Train(MSE): 0.12517928298365177, R2 Train: 0.4992828680653929\n",
      "Epoch 7163, Loss Train(MSE): 0.12517917608615386, R2 Train: 0.49928329565538454\n",
      "Epoch 7164, Loss Train(MSE): 0.1251790798167778, R2 Train: 0.49928368073288876\n",
      "Epoch 7165, Loss Train(MSE): 0.1251789756295597, R2 Train: 0.49928409748176117\n",
      "Epoch 7166, Loss Train(MSE): 0.12517887276753345, R2 Train: 0.4992845089298662\n",
      "Epoch 7167, Loss Train(MSE): 0.1251787757931995, R2 Train: 0.499284896827202\n",
      "Epoch 7168, Loss Train(MSE): 0.1251786697827087, R2 Train: 0.4992853208691652\n",
      "Epoch 7169, Loss Train(MSE): 0.12517857312296585, R2 Train: 0.4992857075081366\n",
      "Epoch 7170, Loss Train(MSE): 0.1251784709087084, R2 Train: 0.4992861163651664\n",
      "Epoch 7171, Loss Train(MSE): 0.12517836757435422, R2 Train: 0.49928652970258314\n",
      "Epoch 7172, Loss Train(MSE): 0.12517827264580358, R2 Train: 0.49928690941678566\n",
      "Epoch 7173, Loss Train(MSE): 0.12517816750879124, R2 Train: 0.49928732996483505\n",
      "Epoch 7174, Loss Train(MSE): 0.12517807011301615, R2 Train: 0.4992877195479354\n",
      "Epoch 7175, Loss Train(MSE): 0.12517797019403787, R2 Train: 0.4992881192238485\n",
      "Epoch 7176, Loss Train(MSE): 0.12517786604463324, R2 Train: 0.49928853582146704\n",
      "Epoch 7177, Loss Train(MSE): 0.12517777348144382, R2 Train: 0.4992889060742247\n",
      "Epoch 7178, Loss Train(MSE): 0.1251776692046838, R2 Train: 0.49928932318126484\n",
      "Epoch 7179, Loss Train(MSE): 0.12517757073657723, R2 Train: 0.4992897170536911\n",
      "Epoch 7180, Loss Train(MSE): 0.12517747342634605, R2 Train: 0.4992901062946158\n",
      "Epoch 7181, Loss Train(MSE): 0.12517736967164714, R2 Train: 0.49929052131341145\n",
      "Epoch 7182, Loss Train(MSE): 0.12517727670946124, R2 Train: 0.499290893162155\n",
      "Epoch 7183, Loss Train(MSE): 0.12517717481919366, R2 Train: 0.49929130072322536\n",
      "Epoch 7184, Loss Train(MSE): 0.12517707496835834, R2 Train: 0.49929170012656665\n",
      "Epoch 7185, Loss Train(MSE): 0.1251769805549024, R2 Train: 0.4992920777803904\n",
      "Epoch 7186, Loss Train(MSE): 0.12517687763970167, R2 Train: 0.4992924894411933\n",
      "Epoch 7187, Loss Train(MSE): 0.12517678304772026, R2 Train: 0.49929286780911897\n",
      "Epoch 7188, Loss Train(MSE): 0.1251766842878852, R2 Train: 0.4992932628484592\n",
      "Epoch 7189, Loss Train(MSE): 0.12517658273566284, R2 Train: 0.4992936690573486\n",
      "Epoch 7190, Loss Train(MSE): 0.1251764915157949, R2 Train: 0.49929403393682037\n",
      "Epoch 7191, Loss Train(MSE): 0.1251763894276146, R2 Train: 0.49929444228954156\n",
      "Epoch 7192, Loss Train(MSE): 0.1251762928931716, R2 Train: 0.49929482842731365\n",
      "Epoch 7193, Loss Train(MSE): 0.1251761975548787, R2 Train: 0.49929520978048525\n",
      "Epoch 7194, Loss Train(MSE): 0.12517609596868892, R2 Train: 0.4992956161252443\n",
      "Epoch 7195, Loss Train(MSE): 0.12517600428700762, R2 Train: 0.4992959828519695\n",
      "Epoch 7196, Loss Train(MSE): 0.1251759049873985, R2 Train: 0.499296380050406\n",
      "Epoch 7197, Loss Train(MSE): 0.12517580622240562, R2 Train: 0.4992967751103775\n",
      "Epoch 7198, Loss Train(MSE): 0.1251757145726115, R2 Train: 0.49929714170955397\n",
      "Epoch 7199, Loss Train(MSE): 0.1251756137938495, R2 Train: 0.499297544824602\n",
      "Epoch 7200, Loss Train(MSE): 0.12517551965008156, R2 Train: 0.49929792139967377\n",
      "Epoch 7201, Loss Train(MSE): 0.12517542425783623, R2 Train: 0.49929830296865507\n",
      "Epoch 7202, Loss Train(MSE): 0.1251753239692264, R2 Train: 0.4992987041230944\n",
      "Epoch 7203, Loss Train(MSE): 0.12517523428776842, R2 Train: 0.49929906284892633\n",
      "Epoch 7204, Loss Train(MSE): 0.12517513530434635, R2 Train: 0.4992994587826146\n",
      "Epoch 7205, Loss Train(MSE): 0.125175038423524, R2 Train: 0.49929984630590396\n",
      "Epoch 7206, Loss Train(MSE): 0.12517494719319547, R2 Train: 0.4993002112272181\n",
      "Epoch 7207, Loss Train(MSE): 0.12517484769309425, R2 Train: 0.499300609227623\n",
      "Epoch 7208, Loss Train(MSE): 0.1251747550521895, R2 Train: 0.499300979791242\n",
      "Epoch 7209, Loss Train(MSE): 0.12517466044083253, R2 Train: 0.4993013582366699\n",
      "Epoch 7210, Loss Train(MSE): 0.1251745614194493, R2 Train: 0.4993017543222028\n",
      "Epoch 7211, Loss Train(MSE): 0.1251744728654041, R2 Train: 0.49930210853838364\n",
      "Epoch 7212, Loss Train(MSE): 0.12517437501869214, R2 Train: 0.49930249992523146\n",
      "Epoch 7213, Loss Train(MSE): 0.1251742791559303, R2 Train: 0.4993028833762788\n",
      "Epoch 7214, Loss Train(MSE): 0.12517418915931747, R2 Train: 0.49930324336273013\n",
      "Epoch 7215, Loss Train(MSE): 0.125174090908166, R2 Train: 0.499303636367336\n",
      "Epoch 7216, Loss Train(MSE): 0.12517399891864112, R2 Train: 0.49930400432543554\n",
      "Epoch 7217, Loss Train(MSE): 0.1251739058884597, R2 Train: 0.4993043764461612\n",
      "Epoch 7218, Loss Train(MSE): 0.12517380810497783, R2 Train: 0.4993047675800887\n",
      "Epoch 7219, Loss Train(MSE): 0.12517371984125447, R2 Train: 0.4993051206349821\n",
      "Epoch 7220, Loss Train(MSE): 0.12517362391779954, R2 Train: 0.49930550432880183\n",
      "Epoch 7221, Loss Train(MSE): 0.12517352824240638, R2 Train: 0.4993058870303745\n",
      "Epoch 7222, Loss Train(MSE): 0.12517344026006516, R2 Train: 0.49930623895973936\n",
      "Epoch 7223, Loss Train(MSE): 0.12517334322915125, R2 Train: 0.499306627083395\n",
      "Epoch 7224, Loss Train(MSE): 0.12517325107434987, R2 Train: 0.4993069957026005\n",
      "Epoch 7225, Loss Train(MSE): 0.12517316039249807, R2 Train: 0.49930735843000773\n",
      "Epoch 7226, Loss Train(MSE): 0.1251730638185735, R2 Train: 0.49930774472570605\n",
      "Epoch 7227, Loss Train(MSE): 0.12517297504232783, R2 Train: 0.4993080998306887\n",
      "Epoch 7228, Loss Train(MSE): 0.12517288179609456, R2 Train: 0.49930847281562174\n",
      "Epoch 7229, Loss Train(MSE): 0.12517278567517995, R2 Train: 0.4993088572992802\n",
      "Epoch 7230, Loss Train(MSE): 0.12517270013751727, R2 Train: 0.4993091994499309\n",
      "Epoch 7231, Loss Train(MSE): 0.1251726044600542, R2 Train: 0.49930958215978316\n",
      "Epoch 7232, Loss Train(MSE): 0.12517251137111737, R2 Train: 0.4993099545155305\n",
      "Epoch 7233, Loss Train(MSE): 0.12517242375853554, R2 Train: 0.4993103049658578\n",
      "Epoch 7234, Loss Train(MSE): 0.12517232836674932, R2 Train: 0.4993106865330027\n",
      "Epoch 7235, Loss Train(MSE): 0.12517223832234936, R2 Train: 0.49931104671060256\n",
      "Epoch 7236, Loss Train(MSE): 0.12517214846164626, R2 Train: 0.49931140615341496\n",
      "Epoch 7237, Loss Train(MSE): 0.12517205351267177, R2 Train: 0.4993117859493129\n",
      "Epoch 7238, Loss Train(MSE): 0.1251719663779424, R2 Train: 0.4993121344882304\n",
      "Epoch 7239, Loss Train(MSE): 0.12517187439738253, R2 Train: 0.49931250241046987\n",
      "Epoch 7240, Loss Train(MSE): 0.12517177988741834, R2 Train: 0.4993128804503266\n",
      "Epoch 7241, Loss Train(MSE): 0.1251717040757009, R2 Train: 0.49931318369719635\n",
      "Epoch 7242, Loss Train(MSE): 0.1251716543768878, R2 Train: 0.49931338249244883\n",
      "Epoch 7243, Loss Train(MSE): 0.12517160651534562, R2 Train: 0.4993135739386175\n",
      "Epoch 7244, Loss Train(MSE): 0.12517156480024672, R2 Train: 0.49931374079901314\n",
      "Epoch 7245, Loss Train(MSE): 0.12517151494684606, R2 Train: 0.49931394021261577\n",
      "Epoch 7246, Loss Train(MSE): 0.12517146943852966, R2 Train: 0.49931412224588134\n",
      "Epoch 7247, Loss Train(MSE): 0.12517142552993418, R2 Train: 0.49931429788026327\n",
      "Epoch 7248, Loss Train(MSE): 0.12517137577330964, R2 Train: 0.49931449690676144\n",
      "Epoch 7249, Loss Train(MSE): 0.12517133257640334, R2 Train: 0.49931466969438665\n",
      "Epoch 7250, Loss Train(MSE): 0.12517128651545645, R2 Train: 0.4993148539381742\n",
      "Epoch 7251, Loss Train(MSE): 0.12517123685515988, R2 Train: 0.4993150525793605\n",
      "Epoch 7252, Loss Train(MSE): 0.1251711959282604, R2 Train: 0.4993152162869584\n",
      "Epoch 7253, Loss Train(MSE): 0.125171147755701, R2 Train: 0.49931540897719595\n",
      "Epoch 7254, Loss Train(MSE): 0.12517109892899186, R2 Train: 0.49931560428403254\n",
      "Epoch 7255, Loss Train(MSE): 0.12517105875461734, R2 Train: 0.49931576498153063\n",
      "Epoch 7256, Loss Train(MSE): 0.12517100924616695, R2 Train: 0.4993159630153322\n",
      "Epoch 7257, Loss Train(MSE): 0.12517096264125782, R2 Train: 0.4993161494349687\n",
      "Epoch 7258, Loss Train(MSE): 0.1251709204024623, R2 Train: 0.4993163183901508\n",
      "Epoch 7259, Loss Train(MSE): 0.12517087098922852, R2 Train: 0.4993165160430859\n",
      "Epoch 7260, Loss Train(MSE): 0.12517082656568393, R2 Train: 0.4993166937372643\n",
      "Epoch 7261, Loss Train(MSE): 0.12517078230225345, R2 Train: 0.4993168707909862\n",
      "Epoch 7262, Loss Train(MSE): 0.12517073298380224, R2 Train: 0.49931706806479104\n",
      "Epoch 7263, Loss Train(MSE): 0.12517069070158457, R2 Train: 0.4993172371936617\n",
      "Epoch 7264, Loss Train(MSE): 0.12517064445291326, R2 Train: 0.49931742218834696\n",
      "Epoch 7265, Loss Train(MSE): 0.12517059522881452, R2 Train: 0.4993176190847419\n",
      "Epoch 7266, Loss Train(MSE): 0.12517055504827992, R2 Train: 0.4993177798068803\n",
      "Epoch 7267, Loss Train(MSE): 0.12517050685337386, R2 Train: 0.49931797258650457\n",
      "Epoch 7268, Loss Train(MSE): 0.1251704587527237, R2 Train: 0.4993181649891052\n",
      "Epoch 7269, Loss Train(MSE): 0.12517041857458266, R2 Train: 0.49931832570166934\n",
      "Epoch 7270, Loss Train(MSE): 0.12517036949925756, R2 Train: 0.4993185220029698\n",
      "Epoch 7271, Loss Train(MSE): 0.12517032345464177, R2 Train: 0.4993187061814329\n",
      "Epoch 7272, Loss Train(MSE): 0.12517028137493982, R2 Train: 0.4993188745002407\n",
      "Epoch 7273, Loss Train(MSE): 0.12517023239290068, R2 Train: 0.4993190704283973\n",
      "Epoch 7274, Loss Train(MSE): 0.1251701883655972, R2 Train: 0.49931924653761117\n",
      "Epoch 7275, Loss Train(MSE): 0.12517014442243268, R2 Train: 0.49931942231026927\n",
      "Epoch 7276, Loss Train(MSE): 0.12517009553326258, R2 Train: 0.49931961786694967\n",
      "Epoch 7277, Loss Train(MSE): 0.12517005348492935, R2 Train: 0.4993197860602826\n",
      "Epoch 7278, Loss Train(MSE): 0.12517000771602607, R2 Train: 0.4993199691358957\n",
      "Epoch 7279, Loss Train(MSE): 0.1251699589193119, R2 Train: 0.4993201643227524\n",
      "Epoch 7280, Loss Train(MSE): 0.12516991881198294, R2 Train: 0.4993203247520682\n",
      "Epoch 7281, Loss Train(MSE): 0.12516987125469417, R2 Train: 0.4993205149812233\n",
      "Epoch 7282, Loss Train(MSE): 0.12516982321016118, R2 Train: 0.4993207071593553\n",
      "Epoch 7283, Loss Train(MSE): 0.12516978368506781, R2 Train: 0.49932086525972874\n",
      "Epoch 7284, Loss Train(MSE): 0.12516973503417744, R2 Train: 0.49932105986329023\n",
      "Epoch 7285, Loss Train(MSE): 0.12516968888743474, R2 Train: 0.49932124445026105\n",
      "Epoch 7286, Loss Train(MSE): 0.12516964761623303, R2 Train: 0.4993214095350679\n",
      "Epoch 7287, Loss Train(MSE): 0.12516959905677427, R2 Train: 0.4993216037729029\n",
      "Epoch 7288, Loss Train(MSE): 0.1251695547707341, R2 Train: 0.4993217809170636\n",
      "Epoch 7289, Loss Train(MSE): 0.12516951178991217, R2 Train: 0.4993219528403513\n",
      "Epoch 7290, Loss Train(MSE): 0.1251694633214844, R2 Train: 0.4993221467140624\n",
      "Epoch 7291, Loss Train(MSE): 0.12516942085942223, R2 Train: 0.4993223165623111\n",
      "Epoch 7292, Loss Train(MSE): 0.12516937620511032, R2 Train: 0.49932249517955873\n",
      "Epoch 7293, Loss Train(MSE): 0.12516932782731632, R2 Train: 0.4993226886907347\n",
      "Epoch 7294, Loss Train(MSE): 0.12516928715286693, R2 Train: 0.4993228513885323\n",
      "Epoch 7295, Loss Train(MSE): 0.12516924086084089, R2 Train: 0.49932303655663646\n",
      "Epoch 7296, Loss Train(MSE): 0.125169192573287, R2 Train: 0.49932322970685195\n",
      "Epoch 7297, Loss Train(MSE): 0.1251691536504409, R2 Train: 0.4993233853982364\n",
      "Epoch 7298, Loss Train(MSE): 0.1251691057561261, R2 Train: 0.49932357697549556\n",
      "Epoch 7299, Loss Train(MSE): 0.1251690588771169, R2 Train: 0.4993237644915324\n",
      "Epoch 7300, Loss Train(MSE): 0.12516901903202074, R2 Train: 0.49932392387191704\n",
      "Epoch 7301, Loss Train(MSE): 0.12516897088684498, R2 Train: 0.4993241164526201\n",
      "Epoch 7302, Loss Train(MSE): 0.12516892571905866, R2 Train: 0.49932429712376536\n",
      "Epoch 7303, Loss Train(MSE): 0.12516888431116074, R2 Train: 0.499324462755357\n",
      "Epoch 7304, Loss Train(MSE): 0.12516883625524913, R2 Train: 0.49932465497900347\n",
      "Epoch 7305, Loss Train(MSE): 0.12516879276350443, R2 Train: 0.4993248289459823\n",
      "Epoch 7306, Loss Train(MSE): 0.12516874982741405, R2 Train: 0.4993250006903438\n",
      "Epoch 7307, Loss Train(MSE): 0.12516870186038467, R2 Train: 0.4993251925584613\n",
      "Epoch 7308, Loss Train(MSE): 0.1251686600098437, R2 Train: 0.4993253599606252\n",
      "Epoch 7309, Loss Train(MSE): 0.12516861557983167, R2 Train: 0.49932553768067334\n",
      "Epoch 7310, Loss Train(MSE): 0.12516856770130558, R2 Train: 0.4993257291947777\n",
      "Epoch 7311, Loss Train(MSE): 0.12516852745747056, R2 Train: 0.49932589017011775\n",
      "Epoch 7312, Loss Train(MSE): 0.12516848156747232, R2 Train: 0.49932607373011073\n",
      "Epoch 7313, Loss Train(MSE): 0.125168433777074, R2 Train: 0.49932626489170395\n",
      "Epoch 7314, Loss Train(MSE): 0.12516839510578381, R2 Train: 0.49932641957686474\n",
      "Epoch 7315, Loss Train(MSE): 0.12516834778940283, R2 Train: 0.49932660884238866\n",
      "Epoch 7316, Loss Train(MSE): 0.12516830114729513, R2 Train: 0.4993267954108195\n",
      "Epoch 7317, Loss Train(MSE): 0.12516826189293906, R2 Train: 0.49932695242824376\n",
      "Epoch 7318, Loss Train(MSE): 0.1251682142416339, R2 Train: 0.49932714303346437\n",
      "Epoch 7319, Loss Train(MSE): 0.12516816913436082, R2 Train: 0.4993273234625567\n",
      "Epoch 7320, Loss Train(MSE): 0.12516812849047917, R2 Train: 0.4993274860380833\n",
      "Epoch 7321, Loss Train(MSE): 0.12516808092637105, R2 Train: 0.4993276762945158\n",
      "Epoch 7322, Loss Train(MSE): 0.12516803732055262, R2 Train: 0.4993278507177895\n",
      "Epoch 7323, Loss Train(MSE): 0.12516799531997916, R2 Train: 0.49932801872008337\n",
      "Epoch 7324, Loss Train(MSE): 0.12516794784270333, R2 Train: 0.4993282086291867\n",
      "Epoch 7325, Loss Train(MSE): 0.12516790570528488, R2 Train: 0.49932837717886047\n",
      "Epoch 7326, Loss Train(MSE): 0.1251678623805325, R2 Train: 0.49932855047786995\n",
      "Epoch 7327, Loss Train(MSE): 0.12516781498972732, R2 Train: 0.49932874004109074\n",
      "Epoch 7328, Loss Train(MSE): 0.12516777428797604, R2 Train: 0.49932890284809583\n",
      "Epoch 7329, Loss Train(MSE): 0.12516772967124035, R2 Train: 0.4993290813150386\n",
      "Epoch 7330, Loss Train(MSE): 0.1251676823665472, R2 Train: 0.49932927053381115\n",
      "Epoch 7331, Loss Train(MSE): 0.12516764306804895, R2 Train: 0.4993294277278042\n",
      "Epoch 7332, Loss Train(MSE): 0.12516759719121112, R2 Train: 0.4993296112351555\n",
      "Epoch 7333, Loss Train(MSE): 0.1251675499722744, R2 Train: 0.49932980011090244\n",
      "Epoch 7334, Loss Train(MSE): 0.12516751204493054, R2 Train: 0.49932995182027784\n",
      "Epoch 7335, Loss Train(MSE): 0.1251674649395606, R2 Train: 0.4993301402417576\n",
      "Epoch 7336, Loss Train(MSE): 0.12516741902749318, R2 Train: 0.4993303238900273\n",
      "Epoch 7337, Loss Train(MSE): 0.12516737999597866, R2 Train: 0.4993304800160854\n",
      "Epoch 7338, Loss Train(MSE): 0.1251673329124474, R2 Train: 0.49933066835021045\n",
      "Epoch 7339, Loss Train(MSE): 0.12516728833679636, R2 Train: 0.4993308466528146\n",
      "Epoch 7340, Loss Train(MSE): 0.12516724811067886, R2 Train: 0.49933100755728455\n",
      "Epoch 7341, Loss Train(MSE): 0.12516720111202273, R2 Train: 0.4993311955519091\n",
      "Epoch 7342, Loss Train(MSE): 0.12516715784141902, R2 Train: 0.4993313686343239\n",
      "Epoch 7343, Loss Train(MSE): 0.12516711645154951, R2 Train: 0.49933153419380194\n",
      "Epoch 7344, Loss Train(MSE): 0.1251670695374228, R2 Train: 0.49933172185030883\n",
      "Epoch 7345, Loss Train(MSE): 0.1251670275408024, R2 Train: 0.49933188983679044\n",
      "Epoch 7346, Loss Train(MSE): 0.12516698501773105, R2 Train: 0.4993320599290758\n",
      "Epoch 7347, Loss Train(MSE): 0.125166938187791, R2 Train: 0.49933224724883596\n",
      "Epoch 7348, Loss Train(MSE): 0.12516689743439163, R2 Train: 0.49933241026243347\n",
      "Epoch 7349, Loss Train(MSE): 0.1251668538083708, R2 Train: 0.49933258476651676\n",
      "Epoch 7350, Loss Train(MSE): 0.12516680706227745, R2 Train: 0.4993327717508902\n",
      "Epoch 7351, Loss Train(MSE): 0.12516676752163586, R2 Train: 0.49933292991345657\n",
      "Epoch 7352, Loss Train(MSE): 0.12516672282262298, R2 Train: 0.4993331087095081\n",
      "Epoch 7353, Loss Train(MSE): 0.12516667616003907, R2 Train: 0.4993332953598437\n",
      "Epoch 7354, Loss Train(MSE): 0.1251666378019879, R2 Train: 0.49933344879204844\n",
      "Epoch 7355, Loss Train(MSE): 0.12516659205964856, R2 Train: 0.49933363176140577\n",
      "Epoch 7356, Loss Train(MSE): 0.12516654570821548, R2 Train: 0.4993338171671381\n",
      "Epoch 7357, Loss Train(MSE): 0.12516650804641882, R2 Train: 0.4993339678143247\n",
      "Epoch 7358, Loss Train(MSE): 0.12516646151574573, R2 Train: 0.4993341539370171\n",
      "Epoch 7359, Loss Train(MSE): 0.12516641631493888, R2 Train: 0.49933433474024447\n",
      "Epoch 7360, Loss Train(MSE): 0.12516637764101166, R2 Train: 0.4993344894359534\n",
      "Epoch 7361, Loss Train(MSE): 0.12516633119301238, R2 Train: 0.4993346752279505\n",
      "Epoch 7362, Loss Train(MSE): 0.12516628711334601, R2 Train: 0.49933485154661594\n",
      "Epoch 7363, Loss Train(MSE): 0.12516624745628185, R2 Train: 0.4993350101748726\n",
      "Epoch 7364, Loss Train(MSE): 0.12516620109062837, R2 Train: 0.4993351956374865\n",
      "Epoch 7365, Loss Train(MSE): 0.12516615810290288, R2 Train: 0.4993353675883885\n",
      "Epoch 7366, Loss Train(MSE): 0.1251661174914131, R2 Train: 0.4993355300343476\n",
      "Epoch 7367, Loss Train(MSE): 0.12516607120778006, R2 Train: 0.49933571516887976\n",
      "Epoch 7368, Loss Train(MSE): 0.12516602928307927, R2 Train: 0.49933588286768293\n",
      "Epoch 7369, Loss Train(MSE): 0.12516598774559548, R2 Train: 0.4993360490176181\n",
      "Epoch 7370, Loss Train(MSE): 0.12516594154366015, R2 Train: 0.4993362338253594\n",
      "Epoch 7371, Loss Train(MSE): 0.12516590065334834, R2 Train: 0.4993363973866066\n",
      "Epoch 7372, Loss Train(MSE): 0.12516585821802548, R2 Train: 0.4993365671278981\n",
      "Epoch 7373, Loss Train(MSE): 0.12516581209746747, R2 Train: 0.49933675161013014\n",
      "Epoch 7374, Loss Train(MSE): 0.12516577221318692, R2 Train: 0.4993369111472523\n",
      "Epoch 7375, Loss Train(MSE): 0.1251657289079056, R2 Train: 0.49933708436837765\n",
      "Epoch 7376, Loss Train(MSE): 0.12516568286840735, R2 Train: 0.4993372685263706\n",
      "Epoch 7377, Loss Train(MSE): 0.12516564396207525, R2 Train: 0.499337424151699\n",
      "Epoch 7378, Loss Train(MSE): 0.12516559981444478, R2 Train: 0.4993376007422209\n",
      "Epoch 7379, Loss Train(MSE): 0.12516555385569103, R2 Train: 0.4993377845772359\n",
      "Epoch 7380, Loss Train(MSE): 0.12516551589949704, R2 Train: 0.49933793640201185\n",
      "Epoch 7381, Loss Train(MSE): 0.12516547093685793, R2 Train: 0.4993381162525683\n",
      "Epoch 7382, Loss Train(MSE): 0.12516542505853584, R2 Train: 0.49933829976585664\n",
      "Epoch 7383, Loss Train(MSE): 0.1251653880249394, R2 Train: 0.4993384479002424\n",
      "Epoch 7384, Loss Train(MSE): 0.12516534227436593, R2 Train: 0.4993386309025363\n",
      "Epoch 7385, Loss Train(MSE): 0.12516529723902514, R2 Train: 0.49933881104389943\n",
      "Epoch 7386, Loss Train(MSE): 0.12516525957465588, R2 Train: 0.4993389617013765\n",
      "Epoch 7387, Loss Train(MSE): 0.12516521382345885, R2 Train: 0.4993391447061646\n",
      "Epoch 7388, Loss Train(MSE): 0.12516516968251123, R2 Train: 0.49933932126995506\n",
      "Epoch 7389, Loss Train(MSE): 0.12516513125769754, R2 Train: 0.49933947496920983\n",
      "Epoch 7390, Loss Train(MSE): 0.12516508558615735, R2 Train: 0.4993396576553706\n",
      "Epoch 7391, Loss Train(MSE): 0.12516504231267972, R2 Train: 0.4993398307492811\n",
      "Epoch 7392, Loss Train(MSE): 0.12516500315388698, R2 Train: 0.49933998738445207\n",
      "Epoch 7393, Loss Train(MSE): 0.1251649575616992, R2 Train: 0.49934016975320317\n",
      "Epoch 7394, Loss Train(MSE): 0.12516491512902925, R2 Train: 0.499340339483883\n",
      "Epoch 7395, Loss Train(MSE): 0.12516487526246559, R2 Train: 0.49934049895013766\n",
      "Epoch 7396, Loss Train(MSE): 0.12516482974932808, R2 Train: 0.4993406810026877\n",
      "Epoch 7397, Loss Train(MSE): 0.12516478813106194, R2 Train: 0.49934084747575225\n",
      "Epoch 7398, Loss Train(MSE): 0.12516474758268026, R2 Train: 0.49934100966927897\n",
      "Epoch 7399, Loss Train(MSE): 0.12516470214829317, R2 Train: 0.4993411914068273\n",
      "Epoch 7400, Loss Train(MSE): 0.12516466131828285, R2 Train: 0.4993413547268686\n",
      "Epoch 7401, Loss Train(MSE): 0.1251646201137836, R2 Train: 0.4993415195448656\n",
      "Epoch 7402, Loss Train(MSE): 0.1251645747578494, R2 Train: 0.4993417009686024\n",
      "Epoch 7403, Loss Train(MSE): 0.12516453469020036, R2 Train: 0.49934186123919855\n",
      "Epoch 7404, Loss Train(MSE): 0.12516449285503375, R2 Train: 0.499342028579865\n",
      "Epoch 7405, Loss Train(MSE): 0.12516444757725723, R2 Train: 0.4993422096909711\n",
      "Epoch 7406, Loss Train(MSE): 0.1251644082463259, R2 Train: 0.49934236701469636\n",
      "Epoch 7407, Loss Train(MSE): 0.12516436580569446, R2 Train: 0.4993425367772222\n",
      "Epoch 7408, Loss Train(MSE): 0.12516432060578248, R2 Train: 0.4993427175768701\n",
      "Epoch 7409, Loss Train(MSE): 0.125164281986174, R2 Train: 0.49934287205530403\n",
      "Epoch 7410, Loss Train(MSE): 0.12516423896503487, R2 Train: 0.4993430441398605\n",
      "Epoch 7411, Loss Train(MSE): 0.12516419384269659, R2 Train: 0.49934322462921366\n",
      "Epoch 7412, Loss Train(MSE): 0.12516415590926205, R2 Train: 0.4993433763629518\n",
      "Epoch 7413, Loss Train(MSE): 0.1251641123323294, R2 Train: 0.49934355067068237\n",
      "Epoch 7414, Loss Train(MSE): 0.12516406728727625, R2 Train: 0.499343730850895\n",
      "Epoch 7415, Loss Train(MSE): 0.12516403001511067, R2 Train: 0.4993438799395573\n",
      "Epoch 7416, Loss Train(MSE): 0.1251639859068581, R2 Train: 0.4993440563725676\n",
      "Epoch 7417, Loss Train(MSE): 0.12516394093880354, R2 Train: 0.49934423624478586\n",
      "Epoch 7418, Loss Train(MSE): 0.12516390430324326, R2 Train: 0.499344382787027\n",
      "Epoch 7419, Loss Train(MSE): 0.125163859687906, R2 Train: 0.499344561248376\n",
      "Epoch 7420, Loss Train(MSE): 0.12516381505107488, R2 Train: 0.4993447397957005\n",
      "Epoch 7421, Loss Train(MSE): 0.12516377851844507, R2 Train: 0.49934488592621973\n",
      "Epoch 7422, Loss Train(MSE): 0.12516373367217615, R2 Train: 0.4993450653112954\n",
      "Epoch 7423, Loss Train(MSE): 0.12516368964784225, R2 Train: 0.499345241408631\n",
      "Epoch 7424, Loss Train(MSE): 0.12516365263157875, R2 Train: 0.499345389473685\n",
      "Epoch 7425, Loss Train(MSE): 0.12516360786159883, R2 Train: 0.4993455685536047\n",
      "Epoch 7426, Loss Train(MSE): 0.12516356442564677, R2 Train: 0.4993457422974129\n",
      "Epoch 7427, Loss Train(MSE): 0.12516352694944408, R2 Train: 0.4993458922022237\n",
      "Epoch 7428, Loss Train(MSE): 0.12516348225547383, R2 Train: 0.4993460709781047\n",
      "Epoch 7429, Loss Train(MSE): 0.12516343938402194, R2 Train: 0.49934624246391224\n",
      "Epoch 7430, Loss Train(MSE): 0.12516340147134383, R2 Train: 0.49934639411462467\n",
      "Epoch 7431, Loss Train(MSE): 0.12516335685310612, R2 Train: 0.49934657258757553\n",
      "Epoch 7432, Loss Train(MSE): 0.12516331452250432, R2 Train: 0.49934674190998274\n",
      "Epoch 7433, Loss Train(MSE): 0.12516327619658607, R2 Train: 0.4993468952136557\n",
      "Epoch 7434, Loss Train(MSE): 0.12516323165380572, R2 Train: 0.49934707338477713\n",
      "Epoch 7435, Loss Train(MSE): 0.12516318984063304, R2 Train: 0.4993472406374678\n",
      "Epoch 7436, Loss Train(MSE): 0.1251631511244834, R2 Train: 0.4993473955020664\n",
      "Epoch 7437, Loss Train(MSE): 0.12516310665688735, R2 Train: 0.4993475733724506\n",
      "Epoch 7438, Loss Train(MSE): 0.12516306533795, R2 Train: 0.49934773864820003\n",
      "Epoch 7439, Loss Train(MSE): 0.12516302625435377, R2 Train: 0.4993478949825849\n",
      "Epoch 7440, Loss Train(MSE): 0.12516298186167082, R2 Train: 0.4993480725533167\n",
      "Epoch 7441, Loss Train(MSE): 0.12516294101399997, R2 Train: 0.49934823594400013\n",
      "Epoch 7442, Loss Train(MSE): 0.12516290158551965, R2 Train: 0.4993483936579214\n",
      "Epoch 7443, Loss Train(MSE): 0.1251628572674806, R2 Train: 0.49934857093007756\n",
      "Epoch 7444, Loss Train(MSE): 0.12516281686833025, R2 Train: 0.499348732526679\n",
      "Epoch 7445, Loss Train(MSE): 0.12516277711730844, R2 Train: 0.4993488915307662\n",
      "Epoch 7446, Loss Train(MSE): 0.12516273287364613, R2 Train: 0.4993490685054155\n",
      "Epoch 7447, Loss Train(MSE): 0.12516269290049079, R2 Train: 0.49934922839803686\n",
      "Epoch 7448, Loss Train(MSE): 0.12516265284905223, R2 Train: 0.4993493886037911\n",
      "Epoch 7449, Loss Train(MSE): 0.12516260867950132, R2 Train: 0.4993495652819947\n",
      "Epoch 7450, Loss Train(MSE): 0.12516256911003418, R2 Train: 0.4993497235598633\n",
      "Epoch 7451, Loss Train(MSE): 0.12516252878008785, R2 Train: 0.4993498848796486\n",
      "Epoch 7452, Loss Train(MSE): 0.12516248468438484, R2 Train: 0.4993500612624606\n",
      "Epoch 7453, Loss Train(MSE): 0.12516244549651565, R2 Train: 0.4993502180139374\n",
      "Epoch 7454, Loss Train(MSE): 0.1251624049097566, R2 Train: 0.49935038036097357\n",
      "Epoch 7455, Loss Train(MSE): 0.1251623608876401, R2 Train: 0.4993505564494396\n",
      "Epoch 7456, Loss Train(MSE): 0.12516232205949288, R2 Train: 0.4993507117620285\n",
      "Epoch 7457, Loss Train(MSE): 0.12516228123740458, R2 Train: 0.49935087505038167\n",
      "Epoch 7458, Loss Train(MSE): 0.12516223728861484, R2 Train: 0.49935105084554066\n",
      "Epoch 7459, Loss Train(MSE): 0.1251621987985262, R2 Train: 0.49935120480589523\n",
      "Epoch 7460, Loss Train(MSE): 0.1251621577623823, R2 Train: 0.49935136895047083\n",
      "Epoch 7461, Loss Train(MSE): 0.12516211388666154, R2 Train: 0.49935154445335383\n",
      "Epoch 7462, Loss Train(MSE): 0.12516207571317833, R2 Train: 0.49935169714728667\n",
      "Epoch 7463, Loss Train(MSE): 0.1251620344840447, R2 Train: 0.4993518620638212\n",
      "Epoch 7464, Loss Train(MSE): 0.12516199068113687, R2 Train: 0.4993520372754525\n",
      "Epoch 7465, Loss Train(MSE): 0.12516195280301454, R2 Train: 0.49935218878794185\n",
      "Epoch 7466, Loss Train(MSE): 0.12516191140175123, R2 Train: 0.4993523543929951\n",
      "Epoch 7467, Loss Train(MSE): 0.12516186767140217, R2 Train: 0.49935252931439134\n",
      "Epoch 7468, Loss Train(MSE): 0.12516183006760254, R2 Train: 0.49935267972958985\n",
      "Epoch 7469, Loss Train(MSE): 0.12516178851486567, R2 Train: 0.4993528459405373\n",
      "Epoch 7470, Loss Train(MSE): 0.125161744856823, R2 Train: 0.49935302057270803\n",
      "Epoch 7471, Loss Train(MSE): 0.12516170750651234, R2 Train: 0.49935316997395063\n",
      "Epoch 7472, Loss Train(MSE): 0.12516166582275623, R2 Train: 0.4993533367089751\n",
      "Epoch 7473, Loss Train(MSE): 0.12516162223676924, R2 Train: 0.499353511052923\n",
      "Epoch 7474, Loss Train(MSE): 0.1251615851193165, R2 Train: 0.49935365952273403\n",
      "Epoch 7475, Loss Train(MSE): 0.12516154332479532, R2 Train: 0.49935382670081874\n",
      "Epoch 7476, Loss Train(MSE): 0.12516149981061514, R2 Train: 0.49935400075753944\n",
      "Epoch 7477, Loss Train(MSE): 0.12516146290558985, R2 Train: 0.4993541483776406\n",
      "Epoch 7478, Loss Train(MSE): 0.12516311400747135, R2 Train: 0.4993475439701146\n",
      "Epoch 7479, Loss Train(MSE): 0.1251646225701613, R2 Train: 0.49934150971935476\n",
      "Epoch 7480, Loss Train(MSE): 0.12516456905182424, R2 Train: 0.49934172379270303\n",
      "Epoch 7481, Loss Train(MSE): 0.1251645155666231, R2 Train: 0.49934193773350755\n",
      "Epoch 7482, Loss Train(MSE): 0.12516446211452495, R2 Train: 0.4993421515419002\n",
      "Epoch 7483, Loss Train(MSE): 0.12516440869549691, R2 Train: 0.49934236521801234\n",
      "Epoch 7484, Loss Train(MSE): 0.1251643553095061, R2 Train: 0.49934257876197563\n",
      "Epoch 7485, Loss Train(MSE): 0.12516430195651967, R2 Train: 0.4993427921739213\n",
      "Epoch 7486, Loss Train(MSE): 0.12516424863650508, R2 Train: 0.4993430054539797\n",
      "Epoch 7487, Loss Train(MSE): 0.12516419534942952, R2 Train: 0.4993432186022819\n",
      "Epoch 7488, Loss Train(MSE): 0.12516414209526056, R2 Train: 0.4993434316189578\n",
      "Epoch 7489, Loss Train(MSE): 0.12516408887396566, R2 Train: 0.4993436445041374\n",
      "Epoch 7490, Loss Train(MSE): 0.12516403568551232, R2 Train: 0.4993438572579507\n",
      "Epoch 7491, Loss Train(MSE): 0.12516398252986835, R2 Train: 0.4993440698805266\n",
      "Epoch 7492, Loss Train(MSE): 0.12516392940700133, R2 Train: 0.4993442823719947\n",
      "Epoch 7493, Loss Train(MSE): 0.12516387631687909, R2 Train: 0.49934449473248366\n",
      "Epoch 7494, Loss Train(MSE): 0.12516382325946943, R2 Train: 0.4993447069621223\n",
      "Epoch 7495, Loss Train(MSE): 0.12516377023474035, R2 Train: 0.4993449190610386\n",
      "Epoch 7496, Loss Train(MSE): 0.12516371724265976, R2 Train: 0.49934513102936096\n",
      "Epoch 7497, Loss Train(MSE): 0.12516366428319567, R2 Train: 0.4993453428672173\n",
      "Epoch 7498, Loss Train(MSE): 0.12516361135631632, R2 Train: 0.49934555457473473\n",
      "Epoch 7499, Loss Train(MSE): 0.12516355846198984, R2 Train: 0.4993457661520406\n",
      "Epoch 7500, Loss Train(MSE): 0.12516350560018438, R2 Train: 0.49934597759926247\n",
      "Epoch 7501, Loss Train(MSE): 0.12516345277086835, R2 Train: 0.4993461889165266\n",
      "Epoch 7502, Loss Train(MSE): 0.1251633999740101, R2 Train: 0.4993464001039596\n",
      "Epoch 7503, Loss Train(MSE): 0.125163347209578, R2 Train: 0.499346611161688\n",
      "Epoch 7504, Loss Train(MSE): 0.12516329447754068, R2 Train: 0.4993468220898373\n",
      "Epoch 7505, Loss Train(MSE): 0.12516324177786656, R2 Train: 0.49934703288853377\n",
      "Epoch 7506, Loss Train(MSE): 0.12516318911052432, R2 Train: 0.4993472435579027\n",
      "Epoch 7507, Loss Train(MSE): 0.12516313647548266, R2 Train: 0.49934745409806935\n",
      "Epoch 7508, Loss Train(MSE): 0.1251630838727103, R2 Train: 0.4993476645091588\n",
      "Epoch 7509, Loss Train(MSE): 0.12516303130217604, R2 Train: 0.49934787479129583\n",
      "Epoch 7510, Loss Train(MSE): 0.12516297876384883, R2 Train: 0.4993480849446047\n",
      "Epoch 7511, Loss Train(MSE): 0.12516292625769743, R2 Train: 0.4993482949692103\n",
      "Epoch 7512, Loss Train(MSE): 0.12516287378369104, R2 Train: 0.49934850486523585\n",
      "Epoch 7513, Loss Train(MSE): 0.1251628213417985, R2 Train: 0.49934871463280595\n",
      "Epoch 7514, Loss Train(MSE): 0.12516276893198908, R2 Train: 0.4993489242720437\n",
      "Epoch 7515, Loss Train(MSE): 0.1251627165542319, R2 Train: 0.49934913378307244\n",
      "Epoch 7516, Loss Train(MSE): 0.12516266420849614, R2 Train: 0.4993493431660154\n",
      "Epoch 7517, Loss Train(MSE): 0.1251626118947511, R2 Train: 0.49934955242099555\n",
      "Epoch 7518, Loss Train(MSE): 0.12516255961296618, R2 Train: 0.49934976154813526\n",
      "Epoch 7519, Loss Train(MSE): 0.12516250736311071, R2 Train: 0.49934997054755714\n",
      "Epoch 7520, Loss Train(MSE): 0.1251624551451542, R2 Train: 0.4993501794193832\n",
      "Epoch 7521, Loss Train(MSE): 0.1251624029590661, R2 Train: 0.49935038816373556\n",
      "Epoch 7522, Loss Train(MSE): 0.12516235080481605, R2 Train: 0.4993505967807358\n",
      "Epoch 7523, Loss Train(MSE): 0.12516229868237366, R2 Train: 0.49935080527050535\n",
      "Epoch 7524, Loss Train(MSE): 0.12516224659170858, R2 Train: 0.4993510136331657\n",
      "Epoch 7525, Loss Train(MSE): 0.12516219453279057, R2 Train: 0.49935122186883774\n",
      "Epoch 7526, Loss Train(MSE): 0.1251621425055894, R2 Train: 0.49935142997764237\n",
      "Epoch 7527, Loss Train(MSE): 0.125162090510075, R2 Train: 0.49935163795969995\n",
      "Epoch 7528, Loss Train(MSE): 0.1251620385462172, R2 Train: 0.49935184581513115\n",
      "Epoch 7529, Loss Train(MSE): 0.12516198661398598, R2 Train: 0.4993520535440561\n",
      "Epoch 7530, Loss Train(MSE): 0.12516193471335135, R2 Train: 0.4993522611465946\n",
      "Epoch 7531, Loss Train(MSE): 0.12516188284428342, R2 Train: 0.4993524686228663\n",
      "Epoch 7532, Loss Train(MSE): 0.12516183100675224, R2 Train: 0.49935267597299104\n",
      "Epoch 7533, Loss Train(MSE): 0.12516177920072805, R2 Train: 0.4993528831970878\n",
      "Epoch 7534, Loss Train(MSE): 0.12516172742618104, R2 Train: 0.49935309029527586\n",
      "Epoch 7535, Loss Train(MSE): 0.12516167568308154, R2 Train: 0.49935329726767386\n",
      "Epoch 7536, Loss Train(MSE): 0.12516162397139977, R2 Train: 0.49935350411440094\n",
      "Epoch 7537, Loss Train(MSE): 0.12516157229110622, R2 Train: 0.4993537108355751\n",
      "Epoch 7538, Loss Train(MSE): 0.12516152064217131, R2 Train: 0.49935391743131474\n",
      "Epoch 7539, Loss Train(MSE): 0.12516146902456549, R2 Train: 0.49935412390173806\n",
      "Epoch 7540, Loss Train(MSE): 0.1251614174382594, R2 Train: 0.4993543302469624\n",
      "Epoch 7541, Loss Train(MSE): 0.1251613658832235, R2 Train: 0.49935453646710604\n",
      "Epoch 7542, Loss Train(MSE): 0.12516131435942848, R2 Train: 0.49935474256228607\n",
      "Epoch 7543, Loss Train(MSE): 0.1251612628668451, R2 Train: 0.49935494853261964\n",
      "Epoch 7544, Loss Train(MSE): 0.125161211405444, R2 Train: 0.499355154378224\n",
      "Epoch 7545, Loss Train(MSE): 0.12516115997519608, R2 Train: 0.49935536009921566\n",
      "Epoch 7546, Loss Train(MSE): 0.12516110857607216, R2 Train: 0.4993555656957114\n",
      "Epoch 7547, Loss Train(MSE): 0.12516105720804305, R2 Train: 0.4993557711678278\n",
      "Epoch 7548, Loss Train(MSE): 0.12516100587107978, R2 Train: 0.49935597651568087\n",
      "Epoch 7549, Loss Train(MSE): 0.1251609545651533, R2 Train: 0.4993561817393868\n",
      "Epoch 7550, Loss Train(MSE): 0.12516090329023466, R2 Train: 0.49935638683906136\n",
      "Epoch 7551, Loss Train(MSE): 0.12516085204629498, R2 Train: 0.49935659181482006\n",
      "Epoch 7552, Loss Train(MSE): 0.1251608008333054, R2 Train: 0.49935679666677835\n",
      "Epoch 7553, Loss Train(MSE): 0.12516074965123705, R2 Train: 0.4993570013950518\n",
      "Epoch 7554, Loss Train(MSE): 0.12516069850006126, R2 Train: 0.49935720599975497\n",
      "Epoch 7555, Loss Train(MSE): 0.1251606473797492, R2 Train: 0.49935741048100324\n",
      "Epoch 7556, Loss Train(MSE): 0.12516059629027224, R2 Train: 0.49935761483891106\n",
      "Epoch 7557, Loss Train(MSE): 0.12516054523160183, R2 Train: 0.49935781907359267\n",
      "Epoch 7558, Loss Train(MSE): 0.12516049420370928, R2 Train: 0.49935802318516287\n",
      "Epoch 7559, Loss Train(MSE): 0.12516044320656614, R2 Train: 0.49935822717373546\n",
      "Epoch 7560, Loss Train(MSE): 0.12516039224014394, R2 Train: 0.49935843103942423\n",
      "Epoch 7561, Loss Train(MSE): 0.1251603413044142, R2 Train: 0.4993586347823432\n",
      "Epoch 7562, Loss Train(MSE): 0.12516029039934853, R2 Train: 0.49935883840260586\n",
      "Epoch 7563, Loss Train(MSE): 0.12516023952491864, R2 Train: 0.49935904190032543\n",
      "Epoch 7564, Loss Train(MSE): 0.12516018868109616, R2 Train: 0.49935924527561537\n",
      "Epoch 7565, Loss Train(MSE): 0.12516013786785288, R2 Train: 0.4993594485285885\n",
      "Epoch 7566, Loss Train(MSE): 0.1251600870851606, R2 Train: 0.4993596516593576\n",
      "Epoch 7567, Loss Train(MSE): 0.12516003633299114, R2 Train: 0.49935985466803545\n",
      "Epoch 7568, Loss Train(MSE): 0.1251599856113164, R2 Train: 0.49936005755473445\n",
      "Epoch 7569, Loss Train(MSE): 0.12515993492010832, R2 Train: 0.4993602603195667\n",
      "Epoch 7570, Loss Train(MSE): 0.12515988425933885, R2 Train: 0.4993604629626446\n",
      "Epoch 7571, Loss Train(MSE): 0.12515983362897998, R2 Train: 0.4993606654840801\n",
      "Epoch 7572, Loss Train(MSE): 0.12515978302900385, R2 Train: 0.4993608678839846\n",
      "Epoch 7573, Loss Train(MSE): 0.12515973245938256, R2 Train: 0.4993610701624698\n",
      "Epoch 7574, Loss Train(MSE): 0.12515968192008822, R2 Train: 0.4993612723196471\n",
      "Epoch 7575, Loss Train(MSE): 0.12515963141109296, R2 Train: 0.49936147435562817\n",
      "Epoch 7576, Loss Train(MSE): 0.12515958093236912, R2 Train: 0.4993616762705235\n",
      "Epoch 7577, Loss Train(MSE): 0.12515953048388898, R2 Train: 0.4993618780644441\n",
      "Epoch 7578, Loss Train(MSE): 0.12515948006562477, R2 Train: 0.4993620797375009\n",
      "Epoch 7579, Loss Train(MSE): 0.12515942967754895, R2 Train: 0.4993622812898042\n",
      "Epoch 7580, Loss Train(MSE): 0.12515937931963395, R2 Train: 0.4993624827214642\n",
      "Epoch 7581, Loss Train(MSE): 0.12515932899185211, R2 Train: 0.49936268403259154\n",
      "Epoch 7582, Loss Train(MSE): 0.125159278694176, R2 Train: 0.49936288522329597\n",
      "Epoch 7583, Loss Train(MSE): 0.12515922842657815, R2 Train: 0.4993630862936874\n",
      "Epoch 7584, Loss Train(MSE): 0.12515917818903113, R2 Train: 0.49936328724387546\n",
      "Epoch 7585, Loss Train(MSE): 0.12515912798150755, R2 Train: 0.4993634880739698\n",
      "Epoch 7586, Loss Train(MSE): 0.12515907780398008, R2 Train: 0.49936368878407966\n",
      "Epoch 7587, Loss Train(MSE): 0.12515902765642145, R2 Train: 0.4993638893743142\n",
      "Epoch 7588, Loss Train(MSE): 0.1251589775388044, R2 Train: 0.4993640898447824\n",
      "Epoch 7589, Loss Train(MSE): 0.12515892745110163, R2 Train: 0.4993642901955935\n",
      "Epoch 7590, Loss Train(MSE): 0.1251588773932861, R2 Train: 0.4993644904268556\n",
      "Epoch 7591, Loss Train(MSE): 0.1251588273653306, R2 Train: 0.4993646905386776\n",
      "Epoch 7592, Loss Train(MSE): 0.125158777367208, R2 Train: 0.49936489053116795\n",
      "Epoch 7593, Loss Train(MSE): 0.1251587273988914, R2 Train: 0.49936509040443444\n",
      "Epoch 7594, Loss Train(MSE): 0.1251586774603536, R2 Train: 0.49936529015858555\n",
      "Epoch 7595, Loss Train(MSE): 0.12515862755156776, R2 Train: 0.49936548979372897\n",
      "Epoch 7596, Loss Train(MSE): 0.12515857767250693, R2 Train: 0.49936568930997227\n",
      "Epoch 7597, Loss Train(MSE): 0.12515852782314418, R2 Train: 0.49936588870742327\n",
      "Epoch 7598, Loss Train(MSE): 0.1251584780034527, R2 Train: 0.4993660879861892\n",
      "Epoch 7599, Loss Train(MSE): 0.12515842821340561, R2 Train: 0.49936628714637754\n",
      "Epoch 7600, Loss Train(MSE): 0.12515837845297625, R2 Train: 0.499366486188095\n",
      "Epoch 7601, Loss Train(MSE): 0.1251583287221378, R2 Train: 0.49936668511144877\n",
      "Epoch 7602, Loss Train(MSE): 0.12515827902086357, R2 Train: 0.4993668839165457\n",
      "Epoch 7603, Loss Train(MSE): 0.125158229349127, R2 Train: 0.49936708260349205\n",
      "Epoch 7604, Loss Train(MSE): 0.12515817970690132, R2 Train: 0.4993672811723947\n",
      "Epoch 7605, Loss Train(MSE): 0.12515813009416007, R2 Train: 0.4993674796233597\n",
      "Epoch 7606, Loss Train(MSE): 0.12515808051087673, R2 Train: 0.49936767795649306\n",
      "Epoch 7607, Loss Train(MSE): 0.1251580309570247, R2 Train: 0.49936787617190115\n",
      "Epoch 7608, Loss Train(MSE): 0.1251579814325776, R2 Train: 0.49936807426968965\n",
      "Epoch 7609, Loss Train(MSE): 0.12515793193750896, R2 Train: 0.49936827224996416\n",
      "Epoch 7610, Loss Train(MSE): 0.12515788247179244, R2 Train: 0.49936847011283025\n",
      "Epoch 7611, Loss Train(MSE): 0.12515783303540168, R2 Train: 0.4993686678583933\n",
      "Epoch 7612, Loss Train(MSE): 0.12515778362831032, R2 Train: 0.49936886548675874\n",
      "Epoch 7613, Loss Train(MSE): 0.12515773425049215, R2 Train: 0.4993690629980314\n",
      "Epoch 7614, Loss Train(MSE): 0.1251576849019209, R2 Train: 0.49936926039231644\n",
      "Epoch 7615, Loss Train(MSE): 0.1251576355825704, R2 Train: 0.4993694576697184\n",
      "Epoch 7616, Loss Train(MSE): 0.12515758629241447, R2 Train: 0.4993696548303421\n",
      "Epoch 7617, Loss Train(MSE): 0.125157537031427, R2 Train: 0.49936985187429195\n",
      "Epoch 7618, Loss Train(MSE): 0.12515748779958194, R2 Train: 0.49937004880167224\n",
      "Epoch 7619, Loss Train(MSE): 0.12515743859685313, R2 Train: 0.49937024561258747\n",
      "Epoch 7620, Loss Train(MSE): 0.1251573894232147, R2 Train: 0.4993704423071412\n",
      "Epoch 7621, Loss Train(MSE): 0.12515734027864056, R2 Train: 0.49937063888543776\n",
      "Epoch 7622, Loss Train(MSE): 0.12515729116310487, R2 Train: 0.49937083534758053\n",
      "Epoch 7623, Loss Train(MSE): 0.12515724207658163, R2 Train: 0.4993710316936735\n",
      "Epoch 7624, Loss Train(MSE): 0.12515719301904504, R2 Train: 0.4993712279238198\n",
      "Epoch 7625, Loss Train(MSE): 0.1251571439904693, R2 Train: 0.49937142403812285\n",
      "Epoch 7626, Loss Train(MSE): 0.1251570949908285, R2 Train: 0.499371620036686\n",
      "Epoch 7627, Loss Train(MSE): 0.12515704602009697, R2 Train: 0.4993718159196121\n",
      "Epoch 7628, Loss Train(MSE): 0.12515984044472864, R2 Train: 0.49936063822108545\n",
      "Epoch 7629, Loss Train(MSE): 0.12517269317750343, R2 Train: 0.4993092272899863\n",
      "Epoch 7630, Loss Train(MSE): 0.12517258352712565, R2 Train: 0.4993096658914974\n",
      "Epoch 7631, Loss Train(MSE): 0.12517247404538956, R2 Train: 0.49931010381844176\n",
      "Epoch 7632, Loss Train(MSE): 0.12517236473182508, R2 Train: 0.4993105410726997\n",
      "Epoch 7633, Loss Train(MSE): 0.1251722555859643, R2 Train: 0.49931097765614285\n",
      "Epoch 7634, Loss Train(MSE): 0.1251721466073413, R2 Train: 0.4993114135706348\n",
      "Epoch 7635, Loss Train(MSE): 0.1251720377954925, R2 Train: 0.49931184881803004\n",
      "Epoch 7636, Loss Train(MSE): 0.1251719291499562, R2 Train: 0.4993122834001752\n",
      "Epoch 7637, Loss Train(MSE): 0.12517182067027297, R2 Train: 0.49931271731890814\n",
      "Epoch 7638, Loss Train(MSE): 0.1251717123559854, R2 Train: 0.4993131505760584\n",
      "Epoch 7639, Loss Train(MSE): 0.12517160420663812, R2 Train: 0.49931358317344754\n",
      "Epoch 7640, Loss Train(MSE): 0.12517149622177776, R2 Train: 0.499314015112889\n",
      "Epoch 7641, Loss Train(MSE): 0.12517138840095313, R2 Train: 0.4993144463961875\n",
      "Epoch 7642, Loss Train(MSE): 0.12517128074371495, R2 Train: 0.4993148770251402\n",
      "Epoch 7643, Loss Train(MSE): 0.12517117324961594, R2 Train: 0.49931530700153626\n",
      "Epoch 7644, Loss Train(MSE): 0.1251710659182109, R2 Train: 0.4993157363271564\n",
      "Epoch 7645, Loss Train(MSE): 0.12517095874905648, R2 Train: 0.49931616500377407\n",
      "Epoch 7646, Loss Train(MSE): 0.12517085174171141, R2 Train: 0.49931659303315434\n",
      "Epoch 7647, Loss Train(MSE): 0.12517074489573637, R2 Train: 0.4993170204170545\n",
      "Epoch 7648, Loss Train(MSE): 0.12517063821069385, R2 Train: 0.4993174471572246\n",
      "Epoch 7649, Loss Train(MSE): 0.1251705316861484, R2 Train: 0.49931787325540644\n",
      "Epoch 7650, Loss Train(MSE): 0.12517042532166636, R2 Train: 0.49931829871333455\n",
      "Epoch 7651, Loss Train(MSE): 0.12517031911681614, R2 Train: 0.49931872353273543\n",
      "Epoch 7652, Loss Train(MSE): 0.12517021307116785, R2 Train: 0.4993191477153286\n",
      "Epoch 7653, Loss Train(MSE): 0.12517010718429364, R2 Train: 0.49931957126282545\n",
      "Epoch 7654, Loss Train(MSE): 0.12517000145576734, R2 Train: 0.4993199941769306\n",
      "Epoch 7655, Loss Train(MSE): 0.12516989588516483, R2 Train: 0.49932041645934067\n",
      "Epoch 7656, Loss Train(MSE): 0.12516979047206364, R2 Train: 0.49932083811174544\n",
      "Epoch 7657, Loss Train(MSE): 0.1251696852160432, R2 Train: 0.49932125913582714\n",
      "Epoch 7658, Loss Train(MSE): 0.1251695801166848, R2 Train: 0.4993216795332608\n",
      "Epoch 7659, Loss Train(MSE): 0.12516947517357147, R2 Train: 0.49932209930571414\n",
      "Epoch 7660, Loss Train(MSE): 0.125169370386288, R2 Train: 0.499322518454848\n",
      "Epoch 7661, Loss Train(MSE): 0.12516926575442103, R2 Train: 0.4993229369823159\n",
      "Epoch 7662, Loss Train(MSE): 0.12516916127755892, R2 Train: 0.4993233548897643\n",
      "Epoch 7663, Loss Train(MSE): 0.12516905695529176, R2 Train: 0.49932377217883295\n",
      "Epoch 7664, Loss Train(MSE): 0.1251689527872114, R2 Train: 0.49932418885115437\n",
      "Epoch 7665, Loss Train(MSE): 0.12516884877291148, R2 Train: 0.49932460490835406\n",
      "Epoch 7666, Loss Train(MSE): 0.1251687449119873, R2 Train: 0.49932502035205084\n",
      "Epoch 7667, Loss Train(MSE): 0.12516864120403579, R2 Train: 0.49932543518385686\n",
      "Epoch 7668, Loss Train(MSE): 0.12516853764865568, R2 Train: 0.4993258494053773\n",
      "Epoch 7669, Loss Train(MSE): 0.12516843424544738, R2 Train: 0.4993262630182105\n",
      "Epoch 7670, Loss Train(MSE): 0.12516833099401298, R2 Train: 0.4993266760239481\n",
      "Epoch 7671, Loss Train(MSE): 0.1251682278939561, R2 Train: 0.4993270884241756\n",
      "Epoch 7672, Loss Train(MSE): 0.12516812494488225, R2 Train: 0.499327500220471\n",
      "Epoch 7673, Loss Train(MSE): 0.1251680221463983, R2 Train: 0.4993279114144068\n",
      "Epoch 7674, Loss Train(MSE): 0.12516791949811296, R2 Train: 0.4993283220075482\n",
      "Epoch 7675, Loss Train(MSE): 0.1251678185489415, R2 Train: 0.49932872580423404\n",
      "Epoch 7676, Loss Train(MSE): 0.12516773212197263, R2 Train: 0.4993290715121095\n",
      "Epoch 7677, Loss Train(MSE): 0.12516763663675937, R2 Train: 0.49932945345296254\n",
      "Epoch 7678, Loss Train(MSE): 0.12516754528261867, R2 Train: 0.4993298188695253\n",
      "Epoch 7679, Loss Train(MSE): 0.1251674552800574, R2 Train: 0.4993301788797704\n",
      "Epoch 7680, Loss Train(MSE): 0.12516735897714923, R2 Train: 0.4993305640914031\n",
      "Epoch 7681, Loss Train(MSE): 0.12516727447564055, R2 Train: 0.4993309020974378\n",
      "Epoch 7682, Loss Train(MSE): 0.12516717823896914, R2 Train: 0.49933128704412344\n",
      "Epoch 7683, Loss Train(MSE): 0.12516708919435487, R2 Train: 0.4993316432225805\n",
      "Epoch 7684, Loss Train(MSE): 0.12516699829210673, R2 Train: 0.4993320068315731\n",
      "Epoch 7685, Loss Train(MSE): 0.1251669042129501, R2 Train: 0.4993323831481996\n",
      "Epoch 7686, Loss Train(MSE): 0.12516681888942488, R2 Train: 0.4993327244423005\n",
      "Epoch 7687, Loss Train(MSE): 0.12516672343392987, R2 Train: 0.4993331062642805\n",
      "Epoch 7688, Loss Train(MSE): 0.12516663635942885, R2 Train: 0.4993334545622846\n",
      "Epoch 7689, Loss Train(MSE): 0.12516654487620868, R2 Train: 0.4993338204951653\n",
      "Epoch 7690, Loss Train(MSE): 0.1251664526841298, R2 Train: 0.49933418926348083\n",
      "Epoch 7691, Loss Train(MSE): 0.12516636685474755, R2 Train: 0.4993345325810098\n",
      "Epoch 7692, Loss Train(MSE): 0.12516627216869658, R2 Train: 0.4993349113252137\n",
      "Epoch 7693, Loss Train(MSE): 0.12516618673348165, R2 Train: 0.4993352530660734\n",
      "Epoch 7694, Loss Train(MSE): 0.12516609497988623, R2 Train: 0.49933562008045507\n",
      "Epoch 7695, Loss Train(MSE): 0.12516600434672098, R2 Train: 0.49933598261311607\n",
      "Epoch 7696, Loss Train(MSE): 0.12516591831959348, R2 Train: 0.4993363267216261\n",
      "Epoch 7697, Loss Train(MSE): 0.1251658243915183, R2 Train: 0.4993367024339268\n",
      "Epoch 7698, Loss Train(MSE): 0.12516574027312405, R2 Train: 0.4993370389075038\n",
      "Epoch 7699, Loss Train(MSE): 0.12516564855184112, R2 Train: 0.4993374057926355\n",
      "Epoch 7700, Loss Train(MSE): 0.12516555915771305, R2 Train: 0.4993377633691478\n",
      "Epoch 7701, Loss Train(MSE): 0.125165473233111, R2 Train: 0.499338107067556\n",
      "Epoch 7702, Loss Train(MSE): 0.1251653800517994, R2 Train: 0.49933847979280244\n",
      "Epoch 7703, Loss Train(MSE): 0.12516529693590434, R2 Train: 0.49933881225638266\n",
      "Epoch 7704, Loss Train(MSE): 0.12516520554191574, R2 Train: 0.49933917783233706\n",
      "Epoch 7705, Loss Train(MSE): 0.12516511707502076, R2 Train: 0.49933953169991696\n",
      "Epoch 7706, Loss Train(MSE): 0.12516503154557546, R2 Train: 0.4993398738176982\n",
      "Epoch 7707, Loss Train(MSE): 0.1251649391000628, R2 Train: 0.4993402435997488\n",
      "Epoch 7708, Loss Train(MSE): 0.12516485668027835, R2 Train: 0.4993405732788866\n",
      "Epoch 7709, Loss Train(MSE): 0.1251647659010572, R2 Train: 0.49934093639577115\n",
      "Epoch 7710, Loss Train(MSE): 0.1251646780574545, R2 Train: 0.49934128777018205\n",
      "Epoch 7711, Loss Train(MSE): 0.1251645932083527, R2 Train: 0.49934162716658925\n",
      "Epoch 7712, Loss Train(MSE): 0.1251645014879146, R2 Train: 0.49934199404834156\n",
      "Epoch 7713, Loss Train(MSE): 0.1251644194655801, R2 Train: 0.4993423221376796\n",
      "Epoch 7714, Loss Train(MSE): 0.12516432958128243, R2 Train: 0.4993426816748703\n",
      "Epoch 7715, Loss Train(MSE): 0.12516424206469187, R2 Train: 0.4993430317412325\n",
      "Epoch 7716, Loss Train(MSE): 0.12516415817386525, R2 Train: 0.499343367304539\n",
      "Epoch 7717, Loss Train(MSE): 0.1251640671680101, R2 Train: 0.4993437313279596\n",
      "Epoch 7718, Loss Train(MSE): 0.12516398525199435, R2 Train: 0.4993440589920226\n",
      "Epoch 7719, Loss Train(MSE): 0.1251638965356447, R2 Train: 0.49934441385742123\n",
      "Epoch 7720, Loss Train(MSE): 0.12516380905725039, R2 Train: 0.49934476377099846\n",
      "Epoch 7721, Loss Train(MSE): 0.1251637263955597, R2 Train: 0.4993450944177612\n",
      "Epoch 7722, Loss Train(MSE): 0.12516363609402148, R2 Train: 0.4993454556239141\n",
      "Epoch 7723, Loss Train(MSE): 0.12516355400053, R2 Train: 0.49934578399788\n",
      "Epoch 7724, Loss Train(MSE): 0.12516346671820197, R2 Train: 0.4993461331271921\n",
      "Epoch 7725, Loss Train(MSE): 0.1251633789964613, R2 Train: 0.4993464840141548\n",
      "Epoch 7726, Loss Train(MSE): 0.1251632978278751, R2 Train: 0.49934680868849957\n",
      "Epoch 7727, Loss Train(MSE): 0.12516320822060667, R2 Train: 0.4993471671175733\n",
      "Epoch 7728, Loss Train(MSE): 0.1251631256729945, R2 Train: 0.49934749730802197\n",
      "Epoch 7729, Loss Train(MSE): 0.12516304008398613, R2 Train: 0.4993478396640555\n",
      "Epoch 7730, Loss Train(MSE): 0.12516295184444484, R2 Train: 0.49934819262222063\n",
      "Epoch 7731, Loss Train(MSE): 0.12516287242621296, R2 Train: 0.49934851029514815\n",
      "Epoch 7732, Loss Train(MSE): 0.12516278350337945, R2 Train: 0.4993488659864822\n",
      "Epoch 7733, Loss Train(MSE): 0.1251627002319698, R2 Train: 0.49934919907212083\n",
      "Epoch 7734, Loss Train(MSE): 0.12516261658897387, R2 Train: 0.4993495336441045\n",
      "Epoch 7735, Loss Train(MSE): 0.12516252808184378, R2 Train: 0.4993498876726249\n",
      "Epoch 7736, Loss Train(MSE): 0.12516244963809803, R2 Train: 0.4993502014476079\n",
      "Epoch 7737, Loss Train(MSE): 0.12516236190457083, R2 Train: 0.49935055238171666\n",
      "Epoch 7738, Loss Train(MSE): 0.1251622776598744, R2 Train: 0.4993508893605024\n",
      "Epoch 7739, Loss Train(MSE): 0.1251621961957203, R2 Train: 0.4993512152171188\n",
      "Epoch 7740, Loss Train(MSE): 0.12516210835756775, R2 Train: 0.499351566569729\n",
      "Epoch 7741, Loss Train(MSE): 0.12516202874215196, R2 Train: 0.49935188503139216\n",
      "Epoch 7742, Loss Train(MSE): 0.12516194337585324, R2 Train: 0.49935222649658706\n",
      "Epoch 7743, Loss Train(MSE): 0.12516185790135342, R2 Train: 0.4993525683945863\n",
      "Epoch 7744, Loss Train(MSE): 0.1251617788562682, R2 Train: 0.4993528845749272\n",
      "Epoch 7745, Loss Train(MSE): 0.12516169167778482, R2 Train: 0.4993532332888607\n",
      "Epoch 7746, Loss Train(MSE): 0.12516161063865963, R2 Train: 0.4993535574453615\n",
      "Epoch 7747, Loss Train(MSE): 0.12516152787554297, R2 Train: 0.49935388849782814\n",
      "Epoch 7748, Loss Train(MSE): 0.1251614410977911, R2 Train: 0.49935423560883563\n",
      "Epoch 7749, Loss Train(MSE): 0.12516136436130948, R2 Train: 0.4993545425547621\n",
      "Epoch 7750, Loss Train(MSE): 0.125161278006984, R2 Train: 0.49935488797206395\n",
      "Epoch 7751, Loss Train(MSE): 0.12516119531134123, R2 Train: 0.49935521875463507\n",
      "Epoch 7752, Loss Train(MSE): 0.12516111536842783, R2 Train: 0.49935553852628867\n",
      "Epoch 7753, Loss Train(MSE): 0.12516102923570568, R2 Train: 0.49935588305717726\n",
      "Epoch 7754, Loss Train(MSE): 0.12516095065594174, R2 Train: 0.49935619737623305\n",
      "Epoch 7755, Loss Train(MSE): 0.12516086729910825, R2 Train: 0.499356530803567\n",
      "Epoch 7756, Loss Train(MSE): 0.12516078270721387, R2 Train: 0.4993568691711445\n",
      "Epoch 7757, Loss Train(MSE): 0.125160705808795, R2 Train: 0.49935717676482005\n",
      "Epoch 7758, Loss Train(MSE): 0.12516062031228198, R2 Train: 0.4993575187508721\n",
      "Epoch 7759, Loss Train(MSE): 0.1251605396537008, R2 Train: 0.4993578413851968\n",
      "Epoch 7760, Loss Train(MSE): 0.12516045951464047, R2 Train: 0.4993581619414381\n",
      "Epoch 7761, Loss Train(MSE): 0.12516037440467068, R2 Train: 0.4993585023813173\n",
      "Epoch 7762, Loss Train(MSE): 0.12516029755393912, R2 Train: 0.4993588097842435\n",
      "Epoch 7763, Loss Train(MSE): 0.12516021429409532, R2 Train: 0.4993591428236187\n",
      "Epoch 7764, Loss Train(MSE): 0.125160131339494, R2 Train: 0.499359474642024\n",
      "Epoch 7765, Loss Train(MSE): 0.12516005462043175, R2 Train: 0.499359781518273\n",
      "Epoch 7766, Loss Train(MSE): 0.12515997013279054, R2 Train: 0.49936011946883785\n",
      "Epoch 7767, Loss Train(MSE): 0.12515989081017623, R2 Train: 0.4993604367592951\n",
      "Epoch 7768, Loss Train(MSE): 0.1251598111372001, R2 Train: 0.4993607554511996\n",
      "Epoch 7769, Loss Train(MSE): 0.1251597270277039, R2 Train: 0.4993610918891844\n",
      "Epoch 7770, Loss Train(MSE): 0.12515965121591222, R2 Train: 0.49936139513635114\n",
      "Epoch 7771, Loss Train(MSE): 0.1251595687047571, R2 Train: 0.49936172518097155\n",
      "Epoch 7772, Loss Train(MSE): 0.1251594867022765, R2 Train: 0.499362053190894\n",
      "Epoch 7773, Loss Train(MSE): 0.12515941080964418, R2 Train: 0.49936235676142327\n",
      "Epoch 7774, Loss Train(MSE): 0.12515932730903756, R2 Train: 0.49936269076384976\n",
      "Epoch 7775, Loss Train(MSE): 0.12515924864796413, R2 Train: 0.4993630054081435\n",
      "Epoch 7776, Loss Train(MSE): 0.12515917007785635, R2 Train: 0.4993633196885746\n",
      "Epoch 7777, Loss Train(MSE): 0.12515908694726202, R2 Train: 0.4993636522109519\n",
      "Epoch 7778, Loss Train(MSE): 0.12515901151057018, R2 Train: 0.49936395395771926\n",
      "Epoch 7779, Loss Train(MSE): 0.12515893037475131, R2 Train: 0.49936427850099474\n",
      "Epoch 7780, Loss Train(MSE): 0.1251588486652652, R2 Train: 0.4993646053389392\n",
      "Epoch 7781, Loss Train(MSE): 0.12515877422127808, R2 Train: 0.4993649031148877\n",
      "Epoch 7782, Loss Train(MSE): 0.125158691686557, R2 Train: 0.499365233253772\n",
      "Epoch 7783, Loss Train(MSE): 0.12515861303823944, R2 Train: 0.49936554784704223\n",
      "Epoch 7784, Loss Train(MSE): 0.12515853618331121, R2 Train: 0.49936585526675514\n",
      "Epoch 7785, Loss Train(MSE): 0.12515845401072373, R2 Train: 0.4993661839571051\n",
      "Epoch 7786, Loss Train(MSE): 0.12515837831053483, R2 Train: 0.49936648675786066\n",
      "Epoch 7787, Loss Train(MSE): 0.12515829915260548, R2 Train: 0.49936680338957806\n",
      "Epoch 7788, Loss Train(MSE): 0.12515821733921179, R2 Train: 0.49936713064315286\n",
      "Epoch 7789, Loss Train(MSE): 0.1251581444756376, R2 Train: 0.49936742209744955\n",
      "Epoch 7790, Loss Train(MSE): 0.12515806312118086, R2 Train: 0.49936774751527657\n",
      "Epoch 7791, Loss Train(MSE): 0.1251579838729341, R2 Train: 0.49936806450826365\n",
      "Epoch 7792, Loss Train(MSE): 0.1251579093104899, R2 Train: 0.4993683627580404\n",
      "Epoch 7793, Loss Train(MSE): 0.1251578280756528, R2 Train: 0.49936868769738885\n",
      "Epoch 7794, Loss Train(MSE): 0.1251577515090656, R2 Train: 0.4993689939637376\n",
      "Epoch 7795, Loss Train(MSE): 0.1251576748969585, R2 Train: 0.499369300412166\n",
      "Epoch 7796, Loss Train(MSE): 0.1251575940137698, R2 Train: 0.4993696239449208\n",
      "Epoch 7797, Loss Train(MSE): 0.12515752002111194, R2 Train: 0.49936991991555224\n",
      "Epoch 7798, Loss Train(MSE): 0.12515744146218372, R2 Train: 0.49937023415126514\n",
      "Epoch 7799, Loss Train(MSE): 0.1251573610132751, R2 Train: 0.4993705559468996\n",
      "Epoch 7800, Loss Train(MSE): 0.12515728930976358, R2 Train: 0.4993708427609457\n",
      "Epoch 7801, Loss Train(MSE): 0.12515720899305027, R2 Train: 0.4993711640277989\n",
      "Epoch 7802, Loss Train(MSE): 0.1251571309687763, R2 Train: 0.49937147612489485\n",
      "Epoch 7803, Loss Train(MSE): 0.1251570574598903, R2 Train: 0.49937177016043877\n",
      "Epoch 7804, Loss Train(MSE): 0.12515697748750948, R2 Train: 0.4993720900499621\n",
      "Epoch 7805, Loss Train(MSE): 0.12515690178378566, R2 Train: 0.49937239286485735\n",
      "Epoch 7806, Loss Train(MSE): 0.12515682656886684, R2 Train: 0.49937269372453263\n",
      "Epoch 7807, Loss Train(MSE): 0.12515674693808673, R2 Train: 0.49937301224765307\n",
      "Epoch 7808, Loss Train(MSE): 0.1251566734522265, R2 Train: 0.499373306191094\n",
      "Epoch 7809, Loss Train(MSE): 0.12515659662927262, R2 Train: 0.49937361348290954\n",
      "Epoch 7810, Loss Train(MSE): 0.12515651733739303, R2 Train: 0.4993739306504279\n",
      "Epoch 7811, Loss Train(MSE): 0.12515644596808803, R2 Train: 0.4993742161276479\n",
      "Epoch 7812, Loss Train(MSE): 0.12515636763377147, R2 Train: 0.4993745294649141\n",
      "Epoch 7813, Loss Train(MSE): 0.1251562896806832, R2 Train: 0.49937484127726717\n",
      "Epoch 7814, Loss Train(MSE): 0.12515623550867266, R2 Train: 0.49937505796530934\n",
      "Epoch 7815, Loss Train(MSE): 0.1251561937341022, R2 Train: 0.4993752250635912\n",
      "Epoch 7816, Loss Train(MSE): 0.12515615464184504, R2 Train: 0.49937538143261984\n",
      "Epoch 7817, Loss Train(MSE): 0.1251561187130089, R2 Train: 0.49937552514796435\n",
      "Epoch 7818, Loss Train(MSE): 0.12515607701222417, R2 Train: 0.4993756919511033\n",
      "Epoch 7819, Loss Train(MSE): 0.12515603960985927, R2 Train: 0.4993758415605629\n",
      "Epoch 7820, Loss Train(MSE): 0.1251560021128549, R2 Train: 0.49937599154858037\n",
      "Epoch 7821, Loss Train(MSE): 0.12515596048554978, R2 Train: 0.4993761580578009\n",
      "Epoch 7822, Loss Train(MSE): 0.12515592474282813, R2 Train: 0.49937630102868746\n",
      "Epoch 7823, Loss Train(MSE): 0.1251558857074504, R2 Train: 0.4993764571701984\n",
      "Epoch 7824, Loss Train(MSE): 0.1251558441533212, R2 Train: 0.4993766233867152\n",
      "Epoch 7825, Loss Train(MSE): 0.1251558100402698, R2 Train: 0.4993767598389208\n",
      "Epoch 7826, Loss Train(MSE): 0.12515576949604126, R2 Train: 0.49937692201583495\n",
      "Epoch 7827, Loss Train(MSE): 0.12515572860179955, R2 Train: 0.4993770855928018\n",
      "Epoch 7828, Loss Train(MSE): 0.12515569491397777, R2 Train: 0.49937722034408893\n",
      "Epoch 7829, Loss Train(MSE): 0.12515565347530624, R2 Train: 0.499377386098775\n",
      "Epoch 7830, Loss Train(MSE): 0.12515561417663576, R2 Train: 0.49937754329345696\n",
      "Epoch 7831, Loss Train(MSE): 0.12515557901337288, R2 Train: 0.4993776839465085\n",
      "Epoch 7832, Loss Train(MSE): 0.12515553764712375, R2 Train: 0.499377849411505\n",
      "Epoch 7833, Loss Train(MSE): 0.12515549991469646, R2 Train: 0.49937800034121416\n",
      "Epoch 7834, Loss Train(MSE): 0.1251554633048791, R2 Train: 0.49937814678048364\n",
      "Epoch 7835, Loss Train(MSE): 0.12515542201075744, R2 Train: 0.49937831195697024\n",
      "Epoch 7836, Loss Train(MSE): 0.12515538581551228, R2 Train: 0.4993784567379509\n",
      "Epoch 7837, Loss Train(MSE): 0.12515534778776355, R2 Train: 0.4993786088489458\n",
      "Epoch 7838, Loss Train(MSE): 0.1251553065654766, R2 Train: 0.4993787737380936\n",
      "Epoch 7839, Loss Train(MSE): 0.12515527187861716, R2 Train: 0.49937891248553135\n",
      "Epoch 7840, Loss Train(MSE): 0.1251552324612991, R2 Train: 0.49937907015480365\n",
      "Epoch 7841, Loss Train(MSE): 0.12515519131055655, R2 Train: 0.4993792347577738\n",
      "Epoch 7842, Loss Train(MSE): 0.1251551581035483, R2 Train: 0.4993793675858068\n",
      "Epoch 7843, Loss Train(MSE): 0.1251551173247645, R2 Train: 0.499379530700942\n",
      "Epoch 7844, Loss Train(MSE): 0.12515507732172024, R2 Train: 0.49937969071311905\n",
      "Epoch 7845, Loss Train(MSE): 0.12515504341276204, R2 Train: 0.49937982634895184\n",
      "Epoch 7846, Loss Train(MSE): 0.12515500237493798, R2 Train: 0.49937999050024806\n",
      "Epoch 7847, Loss Train(MSE): 0.125154963819731, R2 Train: 0.49938014472107595\n",
      "Epoch 7848, Loss Train(MSE): 0.1251549285806614, R2 Train: 0.4993802856773544\n",
      "Epoch 7849, Loss Train(MSE): 0.12515488761366136, R2 Train: 0.49938044954535454\n",
      "Epoch 7850, Loss Train(MSE): 0.12515485047836794, R2 Train: 0.49938059808652824\n",
      "Epoch 7851, Loss Train(MSE): 0.12515481393668812, R2 Train: 0.4993807442532475\n",
      "Epoch 7852, Loss Train(MSE): 0.1251547730402298, R2 Train: 0.49938090783908085\n",
      "Epoch 7853, Loss Train(MSE): 0.1251547372971798, R2 Train: 0.4993810508112808\n",
      "Epoch 7854, Loss Train(MSE): 0.12515469948014066, R2 Train: 0.49938120207943737\n",
      "Epoch 7855, Loss Train(MSE): 0.12515465865394396, R2 Train: 0.4993813653842242\n",
      "Epoch 7856, Loss Train(MSE): 0.1251546242757185, R2 Train: 0.499381502897126\n",
      "Epoch 7857, Loss Train(MSE): 0.12515458521032297, R2 Train: 0.49938165915870814\n",
      "Epoch 7858, Loss Train(MSE): 0.12515454445411003, R2 Train: 0.49938182218355986\n",
      "Epoch 7859, Loss Train(MSE): 0.12515451141353925, R2 Train: 0.499381954345843\n",
      "Epoch 7860, Loss Train(MSE): 0.12515447112654454, R2 Train: 0.49938211549382183\n",
      "Epoch 7861, Loss Train(MSE): 0.1251544312782647, R2 Train: 0.4993822748869412\n",
      "Epoch 7862, Loss Train(MSE): 0.1251543978714021, R2 Train: 0.49938240851439164\n",
      "Epoch 7863, Loss Train(MSE): 0.12515435722567866, R2 Train: 0.49938257109728534\n",
      "Epoch 7864, Loss Train(MSE): 0.12515431868501256, R2 Train: 0.49938272525994976\n",
      "Epoch 7865, Loss Train(MSE): 0.12515428408595983, R2 Train: 0.4993828636561607\n",
      "Epoch 7866, Loss Train(MSE): 0.12515424350953078, R2 Train: 0.4993830259618769\n",
      "Epoch 7867, Loss Train(MSE): 0.12515420624988705, R2 Train: 0.4993831750004518\n",
      "Epoch 7868, Loss Train(MSE): 0.12515417048483077, R2 Train: 0.49938331806067693\n",
      "Epoch 7869, Loss Train(MSE): 0.12515412997742578, R2 Train: 0.49938348009029687\n",
      "Epoch 7870, Loss Train(MSE): 0.12515409397245397, R2 Train: 0.4993836241101841\n",
      "Epoch 7871, Loss Train(MSE): 0.12515405706734287, R2 Train: 0.49938377173062853\n",
      "Epoch 7872, Loss Train(MSE): 0.12515401662869358, R2 Train: 0.4993839334852257\n",
      "Epoch 7873, Loss Train(MSE): 0.12515398185228213, R2 Train: 0.4993840725908715\n",
      "Epoch 7874, Loss Train(MSE): 0.12515394383282907, R2 Train: 0.4993842246686837\n",
      "Epoch 7875, Loss Train(MSE): 0.12515390346266939, R2 Train: 0.49938438614932246\n",
      "Epoch 7876, Loss Train(MSE): 0.12515386988894325, R2 Train: 0.499384520444227\n",
      "Epoch 7877, Loss Train(MSE): 0.12515383078062764, R2 Train: 0.49938467687748944\n",
      "Epoch 7878, Loss Train(MSE): 0.1251537904786935, R2 Train: 0.49938483808522605\n",
      "Epoch 7879, Loss Train(MSE): 0.12515375808201204, R2 Train: 0.49938496767195184\n",
      "Epoch 7880, Loss Train(MSE): 0.12515371791008184, R2 Train: 0.49938512835967264\n",
      "Epoch 7881, Loss Train(MSE): 0.1251536786943283, R2 Train: 0.49938528522268677\n",
      "Epoch 7882, Loss Train(MSE): 0.12515364541235324, R2 Train: 0.49938541835058703\n",
      "Epoch 7883, Loss Train(MSE): 0.12515360521817226, R2 Train: 0.49938557912731096\n",
      "Epoch 7884, Loss Train(MSE): 0.12515356715162868, R2 Train: 0.4993857313934853\n",
      "Epoch 7885, Loss Train(MSE): 0.1251535328332725, R2 Train: 0.49938586866691004\n",
      "Epoch 7886, Loss Train(MSE): 0.1251534927066618, R2 Train: 0.49938602917335284\n",
      "Epoch 7887, Loss Train(MSE): 0.1251534557642303, R2 Train: 0.49938617694307885\n",
      "Epoch 7888, Loss Train(MSE): 0.12515342043420555, R2 Train: 0.4993863182631778\n",
      "Epoch 7889, Loss Train(MSE): 0.12515338037490797, R2 Train: 0.4993864785003681\n",
      "Epoch 7890, Loss Train(MSE): 0.12515334453171775, R2 Train: 0.499386621873129\n",
      "Epoch 7891, Loss Train(MSE): 0.12515330821451268, R2 Train: 0.4993867671419493\n",
      "Epoch 7892, Loss Train(MSE): 0.1251532682222731, R2 Train: 0.4993869271109076\n",
      "Epoch 7893, Loss Train(MSE): 0.1251532334536784, R2 Train: 0.4993870661852864\n",
      "Epoch 7894, Loss Train(MSE): 0.12515319617355922, R2 Train: 0.49938721530576313\n",
      "Epoch 7895, Loss Train(MSE): 0.12515315624812423, R2 Train: 0.4993873750075031\n",
      "Epoch 7896, Loss Train(MSE): 0.12515312252970207, R2 Train: 0.49938750988119174\n",
      "Epoch 7897, Loss Train(MSE): 0.12515308431071487, R2 Train: 0.4993876627571405\n",
      "Epoch 7898, Loss Train(MSE): 0.12515304445183323, R2 Train: 0.4993878221926671\n",
      "Epoch 7899, Loss Train(MSE): 0.1251530117593815, R2 Train: 0.49938795296247396\n",
      "Epoch 7900, Loss Train(MSE): 0.1251529726253544, R2 Train: 0.49938810949858237\n",
      "Epoch 7901, Loss Train(MSE): 0.12515293310637524, R2 Train: 0.49938826757449906\n",
      "Epoch 7902, Loss Train(MSE): 0.125152900868291, R2 Train: 0.499388396526836\n",
      "Epoch 7903, Loss Train(MSE): 0.12515286111456006, R2 Train: 0.49938855554175976\n",
      "Epoch 7904, Loss Train(MSE): 0.12515282259578536, R2 Train: 0.49938870961685855\n",
      "Epoch 7905, Loss Train(MSE): 0.12515278946785413, R2 Train: 0.4993888421285835\n",
      "Epoch 7906, Loss Train(MSE): 0.12515274978005306, R2 Train: 0.49938900087978777\n",
      "Epoch 7907, Loss Train(MSE): 0.12515271223779031, R2 Train: 0.49938915104883874\n",
      "Epoch 7908, Loss Train(MSE): 0.1251526782433373, R2 Train: 0.49938928702665075\n",
      "Epoch 7909, Loss Train(MSE): 0.12515263862122117, R2 Train: 0.4993894455151153\n",
      "Epoch 7910, Loss Train(MSE): 0.12515260203199177, R2 Train: 0.4993895918720329\n",
      "Epoch 7911, Loss Train(MSE): 0.12515256719413095, R2 Train: 0.4993897312234762\n",
      "Epoch 7912, Loss Train(MSE): 0.12515252763745677, R2 Train: 0.49938988945017293\n",
      "Epoch 7913, Loss Train(MSE): 0.12515249197799425, R2 Train: 0.499390032088023\n",
      "Epoch 7914, Loss Train(MSE): 0.12515245631963012, R2 Train: 0.4993901747214795\n",
      "Epoch 7915, Loss Train(MSE): 0.12515241682815662, R2 Train: 0.49939033268737354\n",
      "Epoch 7916, Loss Train(MSE): 0.1251523820754045, R2 Train: 0.499390471698382\n",
      "Epoch 7917, Loss Train(MSE): 0.1251523456192341, R2 Train: 0.4993906175230636\n",
      "Epoch 7918, Loss Train(MSE): 0.12515230619272177, R2 Train: 0.4993907752291129\n",
      "Epoch 7919, Loss Train(MSE): 0.12515227232383194, R2 Train: 0.49939091070467223\n",
      "Epoch 7920, Loss Train(MSE): 0.12515223509234666, R2 Train: 0.49939105963061337\n",
      "Epoch 7921, Loss Train(MSE): 0.12515219573055777, R2 Train: 0.4993912170777689\n",
      "Epoch 7922, Loss Train(MSE): 0.12515216272288823, R2 Train: 0.49939134910844707\n",
      "Epoch 7923, Loss Train(MSE): 0.12515212473837584, R2 Train: 0.49939150104649666\n",
      "Epoch 7924, Loss Train(MSE): 0.1251520854410744, R2 Train: 0.4993916582357024\n",
      "Epoch 7925, Loss Train(MSE): 0.12515205327218767, R2 Train: 0.4993917869112493\n",
      "Epoch 7926, Loss Train(MSE): 0.12515201455673391, R2 Train: 0.49939194177306434\n",
      "Epoch 7927, Loss Train(MSE): 0.12515197555470498, R2 Train: 0.4993920977811801\n",
      "Epoch 7928, Loss Train(MSE): 0.12515194373999464, R2 Train: 0.49939222504002145\n",
      "Epoch 7929, Loss Train(MSE): 0.12515190454462916, R2 Train: 0.4993923818214834\n",
      "Epoch 7930, Loss Train(MSE): 0.12515186635814718, R2 Train: 0.4993925345674113\n",
      "Epoch 7931, Loss Train(MSE): 0.12515183383519218, R2 Train: 0.49939266465923127\n",
      "Epoch 7932, Loss Train(MSE): 0.12515179470372895, R2 Train: 0.4993928211850842\n",
      "Epoch 7933, Loss Train(MSE): 0.1251517573108257, R2 Train: 0.4993929707566972\n",
      "Epoch 7934, Loss Train(MSE): 0.1251517241012487, R2 Train: 0.4993931035950052\n",
      "Epoch 7935, Loss Train(MSE): 0.12515168503345767, R2 Train: 0.49939325986616934\n",
      "Epoch 7936, Loss Train(MSE): 0.12515164841236306, R2 Train: 0.49939340635054774\n",
      "Epoch 7937, Loss Train(MSE): 0.1251516145375909, R2 Train: 0.49939354184963636\n",
      "Epoch 7938, Loss Train(MSE): 0.12515157553324363, R2 Train: 0.4993936978670255\n",
      "Epoch 7939, Loss Train(MSE): 0.12515153966238407, R2 Train: 0.4993938413504637\n",
      "Epoch 7940, Loss Train(MSE): 0.12515150514364964, R2 Train: 0.4993939794254014\n",
      "Epoch 7941, Loss Train(MSE): 0.12515146620251938, R2 Train: 0.4993941351899225\n",
      "Epoch 7942, Loss Train(MSE): 0.12515143106051577, R2 Train: 0.49939427575793693\n",
      "Epoch 7943, Loss Train(MSE): 0.12515139591885974, R2 Train: 0.49939441632456105\n",
      "Epoch 7944, Loss Train(MSE): 0.12515135704072125, R2 Train: 0.499394571837115\n",
      "Epoch 7945, Loss Train(MSE): 0.12515132260638742, R2 Train: 0.4993947095744503\n",
      "Epoch 7946, Loss Train(MSE): 0.12515128686265994, R2 Train: 0.4993948525493602\n",
      "Epoch 7947, Loss Train(MSE): 0.12515124804728978, R2 Train: 0.4993950078108409\n",
      "Epoch 7948, Loss Train(MSE): 0.1251512142996306, R2 Train: 0.49939514280147757\n",
      "Epoch 7949, Loss Train(MSE): 0.12515117797449313, R2 Train: 0.4993952881020275\n",
      "Epoch 7950, Loss Train(MSE): 0.1251511392216692, R2 Train: 0.4993954431133232\n",
      "Epoch 7951, Loss Train(MSE): 0.12515110613987906, R2 Train: 0.49939557544048374\n",
      "Epoch 7952, Loss Train(MSE): 0.12515106925380573, R2 Train: 0.4993957229847771\n",
      "Epoch 7953, Loss Train(MSE): 0.12515103056330784, R2 Train: 0.49939587774676863\n",
      "Epoch 7954, Loss Train(MSE): 0.1251509981267686, R2 Train: 0.49939600749292556\n",
      "Epoch 7955, Loss Train(MSE): 0.12515096070004836, R2 Train: 0.49939615719980657\n",
      "Epoch 7956, Loss Train(MSE): 0.1251509220716576, R2 Train: 0.4993963117133696\n",
      "Epoch 7957, Loss Train(MSE): 0.12515089025993725, R2 Train: 0.499396438960251\n",
      "Epoch 7958, Loss Train(MSE): 0.12515085231267528, R2 Train: 0.4993965907492989\n",
      "Epoch 7959, Loss Train(MSE): 0.12515081374617437, R2 Train: 0.4993967450153025\n",
      "Epoch 7960, Loss Train(MSE): 0.12515078253902528, R2 Train: 0.49939686984389886\n",
      "Epoch 7961, Loss Train(MSE): 0.12515074409114452, R2 Train: 0.49939702363542193\n",
      "Epoch 7962, Loss Train(MSE): 0.12515070604834022, R2 Train: 0.4993971758066391\n",
      "Epoch 7963, Loss Train(MSE): 0.12515067450143005, R2 Train: 0.4993973019942798\n",
      "Epoch 7964, Loss Train(MSE): 0.12515063603282264, R2 Train: 0.49939745586870943\n",
      "Epoch 7965, Loss Train(MSE): 0.1251505985744796, R2 Train: 0.4993976057020816\n",
      "Epoch 7966, Loss Train(MSE): 0.1251505665465625, R2 Train: 0.49939773381374997\n",
      "Epoch 7967, Loss Train(MSE): 0.12515052813930624, R2 Train: 0.499397887442775\n",
      "Epoch 7968, Loss Train(MSE): 0.12515049124559685, R2 Train: 0.4993980350176126\n",
      "Epoch 7969, Loss Train(MSE): 0.12515045875618105, R2 Train: 0.4993981649752758\n",
      "Epoch 7970, Loss Train(MSE): 0.1251504204100639, R2 Train: 0.49939831835974435\n",
      "Epoch 7971, Loss Train(MSE): 0.12515038406133938, R2 Train: 0.4993984637546425\n",
      "Epoch 7972, Loss Train(MSE): 0.1251503511297566, R2 Train: 0.49939859548097365\n",
      "Epoch 7973, Loss Train(MSE): 0.12515031284456782, R2 Train: 0.4993987486217287\n",
      "Epoch 7974, Loss Train(MSE): 0.12515027702135662, R2 Train: 0.49939889191457354\n",
      "Epoch 7975, Loss Train(MSE): 0.1251502436667634, R2 Train: 0.49939902533294644\n",
      "Epoch 7976, Loss Train(MSE): 0.12515020544229388, R2 Train: 0.49939917823082447\n",
      "Epoch 7977, Loss Train(MSE): 0.12515017012530003, R2 Train: 0.4993993194987999\n",
      "Epoch 7978, Loss Train(MSE): 0.12515013636667932, R2 Train: 0.4993994545332827\n",
      "Epoch 7979, Loss Train(MSE): 0.12515009820272144, R2 Train: 0.49939960718911425\n",
      "Epoch 7980, Loss Train(MSE): 0.1251500633728231, R2 Train: 0.49939974650870755\n",
      "Epoch 7981, Loss Train(MSE): 0.12515002922898588, R2 Train: 0.4993998830840565\n",
      "Epoch 7982, Loss Train(MSE): 0.12514999112533332, R2 Train: 0.49940003549866674\n",
      "Epoch 7983, Loss Train(MSE): 0.12514995676358118, R2 Train: 0.4994001729456753\n",
      "Epoch 7984, Loss Train(MSE): 0.12514992225316807, R2 Train: 0.49940031098732773\n",
      "Epoch 7985, Loss Train(MSE): 0.12514988420961592, R2 Train: 0.4994004631615363\n",
      "Epoch 7986, Loss Train(MSE): 0.12514985029723155, R2 Train: 0.4994005988110738\n",
      "Epoch 7987, Loss Train(MSE): 0.12514981543871412, R2 Train: 0.4994007382451435\n",
      "Epoch 7988, Loss Train(MSE): 0.12514977745505898, R2 Train: 0.4994008901797641\n",
      "Epoch 7989, Loss Train(MSE): 0.12514974397343354, R2 Train: 0.49940102410626586\n",
      "Epoch 7990, Loss Train(MSE): 0.1251497087851159, R2 Train: 0.49940116485953645\n",
      "Epoch 7991, Loss Train(MSE): 0.12514967086115555, R2 Train: 0.4994013165553778\n",
      "Epoch 7992, Loss Train(MSE): 0.12514963779184804, R2 Train: 0.49940144883260784\n",
      "Epoch 7993, Loss Train(MSE): 0.12514960229186844, R2 Train: 0.4994015908325262\n",
      "Epoch 7994, Loss Train(MSE): 0.12514956442740233, R2 Train: 0.4994017422903907\n",
      "Epoch 7995, Loss Train(MSE): 0.12514953175213828, R2 Train: 0.4994018729914469\n",
      "Epoch 7996, Loss Train(MSE): 0.12514949595847033, R2 Train: 0.4994020161661187\n",
      "Epoch 7997, Loss Train(MSE): 0.125149458153299, R2 Train: 0.499402167386804\n",
      "Epoch 7998, Loss Train(MSE): 0.125149425853969, R2 Train: 0.49940229658412405\n",
      "Epoch 7999, Loss Train(MSE): 0.12514938978442325, R2 Train: 0.499402440862307\n",
      "Epoch 8000, Loss Train(MSE): 0.12514935203834876, R2 Train: 0.49940259184660496\n",
      "Epoch 8001, Loss Train(MSE): 0.12514932009700677, R2 Train: 0.4994027196119729\n",
      "Epoch 8002, Loss Train(MSE): 0.12514928376923234, R2 Train: 0.4994028649230706\n",
      "Epoch 8003, Loss Train(MSE): 0.12514924608205794, R2 Train: 0.49940301567176826\n",
      "Epoch 8004, Loss Train(MSE): 0.1251492144809202, R2 Train: 0.4994031420763192\n",
      "Epoch 8005, Loss Train(MSE): 0.12514917791240585, R2 Train: 0.4994032883503766\n",
      "Epoch 8006, Loss Train(MSE): 0.12514914028393617, R2 Train: 0.4994034388642553\n",
      "Epoch 8007, Loss Train(MSE): 0.12514910900537948, R2 Train: 0.49940356397848207\n",
      "Epoch 8008, Loss Train(MSE): 0.1251490722134552, R2 Train: 0.4994037111461792\n",
      "Epoch 8009, Loss Train(MSE): 0.12514903464349622, R2 Train: 0.4994038614260151\n",
      "Epoch 8010, Loss Train(MSE): 0.12514900367005669, R2 Train: 0.49940398531977326\n",
      "Epoch 8011, Loss Train(MSE): 0.12514896667189515, R2 Train: 0.4994041333124194\n",
      "Epoch 8012, Loss Train(MSE): 0.12514892916025408, R2 Train: 0.4994042833589837\n",
      "Epoch 8013, Loss Train(MSE): 0.12514889847462554, R2 Train: 0.49940440610149783\n",
      "Epoch 8014, Loss Train(MSE): 0.12514886128724342, R2 Train: 0.4994045548510263\n",
      "Epoch 8015, Loss Train(MSE): 0.1251488238337287, R2 Train: 0.49940470466508524\n",
      "Epoch 8016, Loss Train(MSE): 0.12514879341876156, R2 Train: 0.49940482632495375\n",
      "Epoch 8017, Loss Train(MSE): 0.12514875605902095, R2 Train: 0.4994049757639162\n",
      "Epoch 8018, Loss Train(MSE): 0.12514871881921633, R2 Train: 0.4994051247231347\n",
      "Epoch 8019, Loss Train(MSE): 0.12514868834630155, R2 Train: 0.4994052466147938\n",
      "Epoch 8020, Loss Train(MSE): 0.1251486509848229, R2 Train: 0.49940539606070844\n",
      "Epoch 8021, Loss Train(MSE): 0.12514861399996025, R2 Train: 0.499405544000159\n",
      "Epoch 8022, Loss Train(MSE): 0.1251485833699643, R2 Train: 0.49940566652014284\n",
      "Epoch 8023, Loss Train(MSE): 0.12514854606613726, R2 Train: 0.49940581573545095\n",
      "Epoch 8024, Loss Train(MSE): 0.12514850931941898, R2 Train: 0.49940596272232407\n",
      "Epoch 8025, Loss Train(MSE): 0.1251484785488562, R2 Train: 0.49940608580457524\n",
      "Epoch 8026, Loss Train(MSE): 0.12514844130249358, R2 Train: 0.49940623479002566\n",
      "Epoch 8027, Loss Train(MSE): 0.125148404777274, R2 Train: 0.49940638089090394\n",
      "Epoch 8028, Loss Train(MSE): 0.12514837388250868, R2 Train: 0.49940650446996526\n",
      "Epoch 8029, Loss Train(MSE): 0.12514833669342443, R2 Train: 0.4994066532263023\n",
      "Epoch 8030, Loss Train(MSE): 0.12514830037320854, R2 Train: 0.49940679850716585\n",
      "Epoch 8031, Loss Train(MSE): 0.1251482693704561, R2 Train: 0.49940692251817564\n",
      "Epoch 8032, Loss Train(MSE): 0.12514823223846544, R2 Train: 0.49940707104613824\n",
      "Epoch 8033, Loss Train(MSE): 0.12514819610690736, R2 Train: 0.49940721557237056\n",
      "Epoch 8034, Loss Train(MSE): 0.1251481650122356, R2 Train: 0.49940733995105757\n",
      "Epoch 8035, Loss Train(MSE): 0.125148127937155, R2 Train: 0.49940748825138004\n",
      "Epoch 8036, Loss Train(MSE): 0.12514809197805687, R2 Train: 0.4994076320877725\n",
      "Epoch 8037, Loss Train(MSE): 0.12514806080738747, R2 Train: 0.49940775677045013\n",
      "Epoch 8038, Loss Train(MSE): 0.12514802378903442, R2 Train: 0.4994079048438623\n",
      "Epoch 8039, Loss Train(MSE): 0.12514798798634516, R2 Train: 0.49940804805461936\n",
      "Epoch 8040, Loss Train(MSE): 0.12514795675545473, R2 Train: 0.4994081729781811\n",
      "Epoch 8041, Loss Train(MSE): 0.12514791979364806, R2 Train: 0.49940832082540776\n",
      "Epoch 8042, Loss Train(MSE): 0.1251478841314618, R2 Train: 0.4994084634741528\n",
      "Epoch 8043, Loss Train(MSE): 0.12514785285598332, R2 Train: 0.49940858857606674\n",
      "Epoch 8044, Loss Train(MSE): 0.12514781595054292, R2 Train: 0.49940873619782833\n",
      "Epoch 8045, Loss Train(MSE): 0.12514778041309796, R2 Train: 0.49940887834760816\n",
      "Epoch 8046, Loss Train(MSE): 0.12514774910852183, R2 Train: 0.49940900356591267\n",
      "Epoch 8047, Loss Train(MSE): 0.12514771225926888, R2 Train: 0.4994091509629245\n",
      "Epoch 8048, Loss Train(MSE): 0.1251476768309464, R2 Train: 0.49940929267621437\n",
      "Epoch 8049, Loss Train(MSE): 0.125147645512622, R2 Train: 0.499409417949512\n",
      "Epoch 8050, Loss Train(MSE): 0.12514760871937858, R2 Train: 0.4994095651224857\n",
      "Epoch 8051, Loss Train(MSE): 0.12514757338470148, R2 Train: 0.4994097064611941\n",
      "Epoch 8052, Loss Train(MSE): 0.12514754206783796, R2 Train: 0.49940983172864817\n",
      "Epoch 8053, Loss Train(MSE): 0.12514750533042746, R2 Train: 0.4994099786782902\n",
      "Epoch 8054, Loss Train(MSE): 0.12514747007405896, R2 Train: 0.49941011970376414\n",
      "Epoch 8055, Loss Train(MSE): 0.1251474387737268, R2 Train: 0.4994102449050928\n",
      "Epoch 8056, Loss Train(MSE): 0.1251474020919737, R2 Train: 0.4994103916321052\n",
      "Epoch 8057, Loss Train(MSE): 0.12514736689871617, R2 Train: 0.49941053240513533\n",
      "Epoch 8058, Loss Train(MSE): 0.12514733562984823, R2 Train: 0.4994106574806071\n",
      "Epoch 8059, Loss Train(MSE): 0.12514729900357807, R2 Train: 0.4994108039856877\n",
      "Epoch 8060, Loss Train(MSE): 0.1251472638583719, R2 Train: 0.49941094456651236\n",
      "Epoch 8061, Loss Train(MSE): 0.1251472326357647, R2 Train: 0.49941106945694125\n",
      "Epoch 8062, Loss Train(MSE): 0.12514719606480418, R2 Train: 0.4994112157407833\n",
      "Epoch 8063, Loss Train(MSE): 0.12514716095272654, R2 Train: 0.4994113561890938\n",
      "Epoch 8064, Loss Train(MSE): 0.12514712979104126, R2 Train: 0.499411480835835\n",
      "Epoch 8065, Loss Train(MSE): 0.1251470932752181, R2 Train: 0.49941162689912755\n",
      "Epoch 8066, Loss Train(MSE): 0.1251470581814818, R2 Train: 0.49941176727407277\n",
      "Epoch 8067, Loss Train(MSE): 0.1251470270952457, R2 Train: 0.4994118916190172\n",
      "Epoch 8068, Loss Train(MSE): 0.12514699063438878, R2 Train: 0.4994120374624449\n",
      "Epoch 8069, Loss Train(MSE): 0.12514695554434088, R2 Train: 0.49941217782263647\n",
      "Epoch 8070, Loss Train(MSE): 0.1251469245479483, R2 Train: 0.49941230180820684\n",
      "Epoch 8071, Loss Train(MSE): 0.1251468881418874, R2 Train: 0.4994124474324504\n",
      "Epoch 8072, Loss Train(MSE): 0.12514685304100848, R2 Train: 0.4994125878359661\n",
      "Epoch 8073, Loss Train(MSE): 0.1251468221487219, R2 Train: 0.49941271140511245\n",
      "Epoch 8074, Loss Train(MSE): 0.12514678579728805, R2 Train: 0.4994128568108478\n",
      "Epoch 8075, Loss Train(MSE): 0.12514675067119063, R2 Train: 0.4994129973152375\n",
      "Epoch 8076, Loss Train(MSE): 0.12514671989714207, R2 Train: 0.4994131204114317\n",
      "Epoch 8077, Loss Train(MSE): 0.12514668360016723, R2 Train: 0.4994132655993311\n",
      "Epoch 8078, Loss Train(MSE): 0.1251466484345949, R2 Train: 0.49941340626162045\n",
      "Epoch 8079, Loss Train(MSE): 0.1251466177927868, R2 Train: 0.4994135288288528\n",
      "Epoch 8080, Loss Train(MSE): 0.12514658155010394, R2 Train: 0.49941367379958423\n",
      "Epoch 8081, Loss Train(MSE): 0.12514654633093003, R2 Train: 0.4994138146762799\n",
      "Epoch 8082, Loss Train(MSE): 0.12514651583523656, R2 Train: 0.49941393665905376\n",
      "Epoch 8083, Loss Train(MSE): 0.12514647964667971, R2 Train: 0.49941408141328114\n",
      "Epoch 8084, Loss Train(MSE): 0.12514644435990643, R2 Train: 0.49941422256037427\n",
      "Epoch 8085, Loss Train(MSE): 0.1251464140240743, R2 Train: 0.49941434390370276\n",
      "Epoch 8086, Loss Train(MSE): 0.1251463778894786, R2 Train: 0.4994144884420856\n",
      "Epoch 8087, Loss Train(MSE): 0.12514634252123574, R2 Train: 0.49941462991505703\n",
      "Epoch 8088, Loss Train(MSE): 0.1251463123588855, R2 Train: 0.49941475056445805\n",
      "Epoch 8089, Loss Train(MSE): 0.12514627627808697, R2 Train: 0.4994148948876521\n",
      "Epoch 8090, Loss Train(MSE): 0.12514624081463077, R2 Train: 0.4994150367414769\n",
      "Epoch 8091, Loss Train(MSE): 0.1251462108392581, R2 Train: 0.49941515664296765\n",
      "Epoch 8092, Loss Train(MSE): 0.1251461748120938, R2 Train: 0.49941530075162477\n",
      "Epoch 8093, Loss Train(MSE): 0.12514613923980605, R2 Train: 0.4994154430407758\n",
      "Epoch 8094, Loss Train(MSE): 0.1251461094647823, R2 Train: 0.4994155621408708\n",
      "Epoch 8095, Loss Train(MSE): 0.12514607349109025, R2 Train: 0.499415706035639\n",
      "Epoch 8096, Loss Train(MSE): 0.12514603779647707, R2 Train: 0.49941584881409173\n",
      "Epoch 8097, Loss Train(MSE): 0.12514600823505084, R2 Train: 0.49941596705979663\n",
      "Epoch 8098, Loss Train(MSE): 0.1251459723146701, R2 Train: 0.49941611074131964\n",
      "Epoch 8099, Loss Train(MSE): 0.12514593648436098, R2 Train: 0.4994162540625561\n",
      "Epoch 8100, Loss Train(MSE): 0.12514590714965876, R2 Train: 0.49941637140136497\n",
      "Epoch 8101, Loss Train(MSE): 0.12514587128242932, R2 Train: 0.4994165148702827\n",
      "Epoch 8102, Loss Train(MSE): 0.1251458354368487, R2 Train: 0.4994166582526052\n",
      "Epoch 8103, Loss Train(MSE): 0.12514580607440495, R2 Train: 0.4994167757023802\n",
      "Epoch 8104, Loss Train(MSE): 0.12514577039567418, R2 Train: 0.4994169184173033\n",
      "Epoch 8105, Loss Train(MSE): 0.12514573460300749, R2 Train: 0.49941706158797006\n",
      "Epoch 8106, Loss Train(MSE): 0.12514570506259867, R2 Train: 0.4994171797496053\n",
      "Epoch 8107, Loss Train(MSE): 0.12514566965227006, R2 Train: 0.49941732139091977\n",
      "Epoch 8108, Loss Train(MSE): 0.12514563391235967, R2 Train: 0.49941746435056134\n",
      "Epoch 8109, Loss Train(MSE): 0.12514560418107065, R2 Train: 0.4994175832757174\n",
      "Epoch 8110, Loss Train(MSE): 0.12514556905182023, R2 Train: 0.4994177237927191\n",
      "Epoch 8111, Loss Train(MSE): 0.12514553336450954, R2 Train: 0.49941786654196185\n",
      "Epoch 8112, Loss Train(MSE): 0.12514550342954367, R2 Train: 0.4994179862818253\n",
      "Epoch 8113, Loss Train(MSE): 0.12514546859393016, R2 Train: 0.49941812562427934\n",
      "Epoch 8114, Loss Train(MSE): 0.12514543295906347, R2 Train: 0.4994182681637461\n",
      "Epoch 8115, Loss Train(MSE): 0.12514540280774156, R2 Train: 0.49941838876903377\n",
      "Epoch 8116, Loss Train(MSE): 0.1251453682782078, R2 Train: 0.4994185268871688\n",
      "Epoch 8117, Loss Train(MSE): 0.1251453326956303, R2 Train: 0.4994186692174788\n",
      "Epoch 8118, Loss Train(MSE): 0.12514530231538953, R2 Train: 0.49941879073844186\n",
      "Epoch 8119, Loss Train(MSE): 0.12514526810426313, R2 Train: 0.4994189275829475\n",
      "Epoch 8120, Loss Train(MSE): 0.12514523257382096, R2 Train: 0.49941906970471617\n",
      "Epoch 8121, Loss Train(MSE): 0.12514520195221407, R2 Train: 0.4994191921911437\n",
      "Epoch 8122, Loss Train(MSE): 0.12514516807170833, R2 Train: 0.49941932771316666\n",
      "Epoch 8123, Loss Train(MSE): 0.12514513259324855, R2 Train: 0.4994194696270058\n",
      "Epoch 8124, Loss Train(MSE): 0.12514510171794266, R2 Train: 0.49941959312822937\n",
      "Epoch 8125, Loss Train(MSE): 0.12514506818015794, R2 Train: 0.49941972727936823\n",
      "Epoch 8126, Loss Train(MSE): 0.12514503275352848, R2 Train: 0.4994198689858861\n",
      "Epoch 8127, Loss Train(MSE): 0.12514500161230424, R2 Train: 0.49941999355078304\n",
      "Epoch 8128, Loss Train(MSE): 0.12514496842922848, R2 Train: 0.4994201262830861\n",
      "Epoch 8129, Loss Train(MSE): 0.12514493305427807, R2 Train: 0.49942026778288773\n",
      "Epoch 8130, Loss Train(MSE): 0.12514490163502873, R2 Train: 0.4994203934598851\n",
      "Epoch 8131, Loss Train(MSE): 0.1251448688185387, R2 Train: 0.4994205247258452\n",
      "Epoch 8132, Loss Train(MSE): 0.1251448334951171, R2 Train: 0.4994206660195316\n",
      "Epoch 8133, Loss Train(MSE): 0.12514480178584733, R2 Train: 0.4994207928566107\n",
      "Epoch 8134, Loss Train(MSE): 0.1251447693477094, R2 Train: 0.4994209226091624\n",
      "Epoch 8135, Loss Train(MSE): 0.12514473407566715, R2 Train: 0.4994210636973314\n",
      "Epoch 8136, Loss Train(MSE): 0.12514470206449238, R2 Train: 0.4994211917420305\n",
      "Epoch 8137, Loss Train(MSE): 0.12514467001636362, R2 Train: 0.4994213199345455\n",
      "Epoch 8138, Loss Train(MSE): 0.12514463479555213, R2 Train: 0.4994214608177915\n",
      "Epoch 8139, Loss Train(MSE): 0.1251446024706975, R2 Train: 0.49942159011721\n",
      "Epoch 8140, Loss Train(MSE): 0.12514457082412636, R2 Train: 0.49942171670349456\n",
      "Epoch 8141, Loss Train(MSE): 0.12514453565439787, R2 Train: 0.4994218573824085\n",
      "Epoch 8142, Loss Train(MSE): 0.12514450300419727, R2 Train: 0.4994219879832109\n",
      "Epoch 8143, Loss Train(MSE): 0.12514447177062468, R2 Train: 0.49942211291750127\n",
      "Epoch 8144, Loss Train(MSE): 0.12514443665183236, R2 Train: 0.49942225339267055\n",
      "Epoch 8145, Loss Train(MSE): 0.1251444036647275, R2 Train: 0.49942238534108996\n",
      "Epoch 8146, Loss Train(MSE): 0.1251443728554878, R2 Train: 0.4994225085780488\n",
      "Epoch 8147, Loss Train(MSE): 0.12514433778748552, R2 Train: 0.49942264885005794\n",
      "Epoch 8148, Loss Train(MSE): 0.12514430445202518, R2 Train: 0.4994227821918993\n",
      "Epoch 8149, Loss Train(MSE): 0.1251442740783468, R2 Train: 0.4994229036866128\n",
      "Epoch 8150, Loss Train(MSE): 0.12514423906098943, R2 Train: 0.4994230437560423\n",
      "Epoch 8151, Loss Train(MSE): 0.12514420536582832, R2 Train: 0.4994231785366867\n",
      "Epoch 8152, Loss Train(MSE): 0.12514417543883494, R2 Train: 0.49942329824466025\n",
      "Epoch 8153, Loss Train(MSE): 0.12514414047197803, R2 Train: 0.4994234381120879\n",
      "Epoch 8154, Loss Train(MSE): 0.12514410640587606, R2 Train: 0.49942357437649576\n",
      "Epoch 8155, Loss Train(MSE): 0.12514407693658738, R2 Train: 0.49942369225365046\n",
      "Epoch 8156, Loss Train(MSE): 0.12514404202008736, R2 Train: 0.49942383191965056\n",
      "Epoch 8157, Loss Train(MSE): 0.12514400757190872, R2 Train: 0.49942396971236513\n",
      "Epoch 8158, Loss Train(MSE): 0.1251439785712413, R2 Train: 0.4994240857150348\n",
      "Epoch 8159, Loss Train(MSE): 0.12514394370495535, R2 Train: 0.4994242251801786\n",
      "Epoch 8160, Loss Train(MSE): 0.12514390886366758, R2 Train: 0.4994243645453297\n",
      "Epoch 8161, Loss Train(MSE): 0.12514388034243568, R2 Train: 0.4994244786302573\n",
      "Epoch 8162, Loss Train(MSE): 0.12514384552622193, R2 Train: 0.4994246178951123\n",
      "Epoch 8163, Loss Train(MSE): 0.1251438107303365, R2 Train: 0.49942475707865397\n",
      "Epoch 8164, Loss Train(MSE): 0.12514378180013125, R2 Train: 0.499424872799475\n",
      "Epoch 8165, Loss Train(MSE): 0.12514374748509588, R2 Train: 0.4994250100596165\n",
      "Epoch 8166, Loss Train(MSE): 0.1251437127390717, R2 Train: 0.49942514904371316\n",
      "Epoch 8167, Loss Train(MSE): 0.12514368337948872, R2 Train: 0.4994252664820451\n",
      "Epoch 8168, Loss Train(MSE): 0.12514364957962948, R2 Train: 0.4994254016814821\n",
      "Epoch 8169, Loss Train(MSE): 0.12514361488332668, R2 Train: 0.4994255404666933\n",
      "Epoch 8170, Loss Train(MSE): 0.12514358508371992, R2 Train: 0.4994256596651203\n",
      "Epoch 8171, Loss Train(MSE): 0.12514355180946865, R2 Train: 0.4994257927621254\n",
      "Epoch 8172, Loss Train(MSE): 0.12514351716274802, R2 Train: 0.4994259313490079\n",
      "Epoch 8173, Loss Train(MSE): 0.12514348691257085, R2 Train: 0.4994260523497166\n",
      "Epoch 8174, Loss Train(MSE): 0.12514345417426123, R2 Train: 0.4994261833029551\n",
      "Epoch 8175, Loss Train(MSE): 0.12514341957698438, R2 Train: 0.49942632169206247\n",
      "Epoch 8176, Loss Train(MSE): 0.12514338886578857, R2 Train: 0.4994264445368457\n",
      "Epoch 8177, Loss Train(MSE): 0.1251433566736569, R2 Train: 0.49942657330537243\n",
      "Epoch 8178, Loss Train(MSE): 0.12514332212568618, R2 Train: 0.49942671149725526\n",
      "Epoch 8179, Loss Train(MSE): 0.1251432909431213, R2 Train: 0.4994268362275148\n",
      "Epoch 8180, Loss Train(MSE): 0.12514325930730708, R2 Train: 0.4994269627707717\n",
      "Epoch 8181, Loss Train(MSE): 0.12514322480850568, R2 Train: 0.4994271007659773\n",
      "Epoch 8182, Loss Train(MSE): 0.1251431931443181, R2 Train: 0.4994272274227276\n",
      "Epoch 8183, Loss Train(MSE): 0.1251431620748652, R2 Train: 0.4994273517005392\n",
      "Epoch 8184, Loss Train(MSE): 0.12514312762509702, R2 Train: 0.4994274894996119\n",
      "Epoch 8185, Loss Train(MSE): 0.12514309546912922, R2 Train: 0.49942761812348313\n",
      "Epoch 8186, Loss Train(MSE): 0.1251430649759864, R2 Train: 0.4994277400960544\n",
      "Epoch 8187, Loss Train(MSE): 0.125143030575116, R2 Train: 0.499427877699536\n",
      "Epoch 8188, Loss Train(MSE): 0.12514299791730574, R2 Train: 0.49942800833077705\n",
      "Epoch 8189, Loss Train(MSE): 0.12514296801032762, R2 Train: 0.4994281279586895\n",
      "Epoch 8190, Loss Train(MSE): 0.12514293365822032, R2 Train: 0.4994282653671187\n",
      "Epoch 8191, Loss Train(MSE): 0.12514290048859986, R2 Train: 0.4994283980456006\n",
      "Epoch 8192, Loss Train(MSE): 0.1251428711775475, R2 Train: 0.49942851528981\n",
      "Epoch 8193, Loss Train(MSE): 0.12514283687406952, R2 Train: 0.4994286525037219\n",
      "Epoch 8194, Loss Train(MSE): 0.12514280318276474, R2 Train: 0.499428787268941\n",
      "Epoch 8195, Loss Train(MSE): 0.12514277447730662, R2 Train: 0.4994289020907735\n",
      "Epoch 8196, Loss Train(MSE): 0.1251427402223247, R2 Train: 0.49942903911070125\n",
      "Epoch 8197, Loss Train(MSE): 0.12514270599955446, R2 Train: 0.49942917600178216\n",
      "Epoch 8198, Loss Train(MSE): 0.1251426779092672, R2 Train: 0.4994292883629312\n",
      "Epoch 8199, Loss Train(MSE): 0.12514264370264883, R2 Train: 0.4994294251894047\n",
      "Epoch 8200, Loss Train(MSE): 0.12514260951562625, R2 Train: 0.499429561937495\n",
      "Epoch 8201, Loss Train(MSE): 0.12514258089589242, R2 Train: 0.4994296764164303\n",
      "Epoch 8202, Loss Train(MSE): 0.12514254731619506, R2 Train: 0.49942981073521975\n",
      "Epoch 8203, Loss Train(MSE): 0.12514251317733915, R2 Train: 0.4994299472906434\n",
      "Epoch 8204, Loss Train(MSE): 0.1251424839931018, R2 Train: 0.4994300640275928\n",
      "Epoch 8205, Loss Train(MSE): 0.1251424510611184, R2 Train: 0.4994301957555264\n",
      "Epoch 8206, Loss Train(MSE): 0.12514241697029854, R2 Train: 0.49943033211880583\n",
      "Epoch 8207, Loss Train(MSE): 0.12514238721212637, R2 Train: 0.49943045115149454\n",
      "Epoch 8208, Loss Train(MSE): 0.12514235493708722, R2 Train: 0.4994305802516511\n",
      "Epoch 8209, Loss Train(MSE): 0.12514232089417346, R2 Train: 0.49943071642330616\n",
      "Epoch 8210, Loss Train(MSE): 0.12514229055272452, R2 Train: 0.4994308377891019\n",
      "Epoch 8211, Loss Train(MSE): 0.12514225894377157, R2 Train: 0.49943096422491373\n",
      "Epoch 8212, Loss Train(MSE): 0.12514222494863467, R2 Train: 0.49943110020546133\n",
      "Epoch 8213, Loss Train(MSE): 0.1251421940146555, R2 Train: 0.49943122394137796\n",
      "Epoch 8214, Loss Train(MSE): 0.12514216308084308, R2 Train: 0.49943134767662767\n",
      "Epoch 8215, Loss Train(MSE): 0.12514212913335446, R2 Train: 0.49943148346658217\n",
      "Epoch 8216, Loss Train(MSE): 0.12514209759767952, R2 Train: 0.4994316096092819\n",
      "Epoch 8217, Loss Train(MSE): 0.12514206734797514, R2 Train: 0.4994317306080994\n",
      "Epoch 8218, Loss Train(MSE): 0.1251420334480069, R2 Train: 0.4994318662079724\n",
      "Epoch 8219, Loss Train(MSE): 0.1251420013015577, R2 Train: 0.4994319947937692\n",
      "Epoch 8220, Loss Train(MSE): 0.12514197174484273, R2 Train: 0.49943211302062906\n",
      "Epoch 8221, Loss Train(MSE): 0.12514193789226766, R2 Train: 0.49943224843092937\n",
      "Epoch 8222, Loss Train(MSE): 0.12514190512605203, R2 Train: 0.4994323794957919\n",
      "Epoch 8223, Loss Train(MSE): 0.12514187627112244, R2 Train: 0.49943249491551023\n",
      "Epoch 8224, Loss Train(MSE): 0.12514184246581395, R2 Train: 0.4994326301367442\n",
      "Epoch 8225, Loss Train(MSE): 0.12514180907092545, R2 Train: 0.4994327637162982\n",
      "Epoch 8226, Loss Train(MSE): 0.12514178092649247, R2 Train: 0.4994328762940301\n",
      "Epoch 8227, Loss Train(MSE): 0.12514174716832466, R2 Train: 0.49943301132670137\n",
      "Epoch 8228, Loss Train(MSE): 0.12514171342922828, R2 Train: 0.4994331462830869\n",
      "Epoch 8229, Loss Train(MSE): 0.12514168541700643, R2 Train: 0.4994332583319743\n",
      "Epoch 8230, Loss Train(MSE): 0.12514165200091232, R2 Train: 0.49943339199635073\n",
      "Epoch 8231, Loss Train(MSE): 0.12514161830876944, R2 Train: 0.49943352676492225\n",
      "Epoch 8232, Loss Train(MSE): 0.12514158963710403, R2 Train: 0.49943364145158387\n",
      "Epoch 8233, Loss Train(MSE): 0.12514155696180518, R2 Train: 0.4994337721527793\n",
      "Epoch 8234, Loss Train(MSE): 0.1251415233164916, R2 Train: 0.49943390673403365\n",
      "Epoch 8235, Loss Train(MSE): 0.12514149397680074, R2 Train: 0.499434024092797\n",
      "Epoch 8236, Loss Train(MSE): 0.1251476596475622, R2 Train: 0.49940936140975123\n",
      "Epoch 8237, Loss Train(MSE): 0.12515581405530377, R2 Train: 0.49937674377878494\n",
      "Epoch 8238, Loss Train(MSE): 0.1251557352213539, R2 Train: 0.4993770591145844\n",
      "Epoch 8239, Loss Train(MSE): 0.12515565048959446, R2 Train: 0.4993773980416222\n",
      "Epoch 8240, Loss Train(MSE): 0.12515556846326104, R2 Train: 0.49937772614695586\n",
      "Epoch 8241, Loss Train(MSE): 0.1251554874185508, R2 Train: 0.49937805032579685\n",
      "Epoch 8242, Loss Train(MSE): 0.12515540217474613, R2 Train: 0.49937839130101547\n",
      "Epoch 8243, Loss Train(MSE): 0.12515532483930142, R2 Train: 0.4993787006427943\n",
      "Epoch 8244, Loss Train(MSE): 0.12515523793010497, R2 Train: 0.49937904827958013\n",
      "Epoch 8245, Loss Train(MSE): 0.12515516118114514, R2 Train: 0.49937935527541943\n",
      "Epoch 8246, Loss Train(MSE): 0.125155076114934, R2 Train: 0.49937969554026396\n",
      "Epoch 8247, Loss Train(MSE): 0.12515499605802638, R2 Train: 0.4993800157678945\n",
      "Epoch 8248, Loss Train(MSE): 0.125154914784287, R2 Train: 0.499380340862852\n",
      "Epoch 8249, Loss Train(MSE): 0.12515483139553552, R2 Train: 0.49938067441785794\n",
      "Epoch 8250, Loss Train(MSE): 0.12515475393538802, R2 Train: 0.4993809842584479\n",
      "Epoch 8251, Loss Train(MSE): 0.12515466799180236, R2 Train: 0.49938132803279056\n",
      "Epoch 8252, Loss Train(MSE): 0.12515459277371832, R2 Train: 0.4993816289051267\n",
      "Epoch 8253, Loss Train(MSE): 0.1251545078914025, R2 Train: 0.49938196843439\n",
      "Epoch 8254, Loss Train(MSE): 0.1251544292544501, R2 Train: 0.49938228298219955\n",
      "Epoch 8255, Loss Train(MSE): 0.1251543482657189, R2 Train: 0.49938260693712444\n",
      "Epoch 8256, Loss Train(MSE): 0.1251542661871475, R2 Train: 0.49938293525141\n",
      "Epoch 8257, Loss Train(MSE): 0.12515418911206644, R2 Train: 0.49938324355173425\n",
      "Epoch 8258, Loss Train(MSE): 0.12515410411410308, R2 Train: 0.4993835835435877\n",
      "Epoch 8259, Loss Train(MSE): 0.12515402989177524, R2 Train: 0.49938388043289905\n",
      "Epoch 8260, Loss Train(MSE): 0.12515394569387556, R2 Train: 0.4993842172244978\n",
      "Epoch 8261, Loss Train(MSE): 0.12515386794622935, R2 Train: 0.4993845282150826\n",
      "Epoch 8262, Loss Train(MSE): 0.12515378773887434, R2 Train: 0.49938484904450264\n",
      "Epoch 8263, Loss Train(MSE): 0.12515370644426022, R2 Train: 0.49938517422295914\n",
      "Epoch 8264, Loss Train(MSE): 0.12515363024650059, R2 Train: 0.49938547901399766\n",
      "Epoch 8265, Loss Train(MSE): 0.12515354617481209, R2 Train: 0.49938581530075166\n",
      "Epoch 8266, Loss Train(MSE): 0.12515347243143987, R2 Train: 0.4993861102742405\n",
      "Epoch 8267, Loss Train(MSE): 0.12515338940127, R2 Train: 0.49938644239492\n",
      "Epoch 8268, Loss Train(MSE): 0.125153312030434, R2 Train: 0.499386751878264\n",
      "Epoch 8269, Loss Train(MSE): 0.12515323308376636, R2 Train: 0.49938706766493457\n",
      "Epoch 8270, Loss Train(MSE): 0.12515315206487598, R2 Train: 0.49938739174049607\n",
      "Epoch 8271, Loss Train(MSE): 0.1251530772197845, R2 Train: 0.499387691120862\n",
      "Epoch 8272, Loss Train(MSE): 0.12515299405563446, R2 Train: 0.49938802377746216\n",
      "Epoch 8273, Loss Train(MSE): 0.1251529202920889, R2 Train: 0.49938831883164436\n",
      "Epoch 8274, Loss Train(MSE): 0.12515283889634904, R2 Train: 0.49938864441460384\n",
      "Epoch 8275, Loss Train(MSE): 0.12515276140734025, R2 Train: 0.499388954370639\n",
      "Epoch 8276, Loss Train(MSE): 0.12515268418420214, R2 Train: 0.49938926326319144\n",
      "Epoch 8277, Loss Train(MSE): 0.12515260295015787, R2 Train: 0.4993895881993685\n",
      "Epoch 8278, Loss Train(MSE): 0.1251525299167552, R2 Train: 0.49938988033297915\n",
      "Epoch 8279, Loss Train(MSE): 0.12515244764199002, R2 Train: 0.49939020943203993\n",
      "Epoch 8280, Loss Train(MSE): 0.12515237337619312, R2 Train: 0.49939050649522754\n",
      "Epoch 8281, Loss Train(MSE): 0.125152294065541, R2 Train: 0.49939082373783605\n",
      "Epoch 8282, Loss Train(MSE): 0.12515221598027532, R2 Train: 0.49939113607889873\n",
      "Epoch 8283, Loss Train(MSE): 0.12515214092760485, R2 Train: 0.4993914362895806\n",
      "Epoch 8284, Loss Train(MSE): 0.12515205928375803, R2 Train: 0.4993917628649679\n",
      "Epoch 8285, Loss Train(MSE): 0.12515198795447052, R2 Train: 0.4993920481821179\n",
      "Epoch 8286, Loss Train(MSE): 0.12515190682720315, R2 Train: 0.4993923726911874\n",
      "Epoch 8287, Loss Train(MSE): 0.12515183160615487, R2 Train: 0.49939267357538053\n",
      "Epoch 8288, Loss Train(MSE): 0.12515175480310986, R2 Train: 0.49939298078756056\n",
      "Epoch 8289, Loss Train(MSE): 0.12515167567241553, R2 Train: 0.4993932973103379\n",
      "Epoch 8290, Loss Train(MSE): 0.12515160320916582, R2 Train: 0.4993935871633367\n",
      "Epoch 8291, Loss Train(MSE): 0.12515152242473257, R2 Train: 0.4993939103010697\n",
      "Epoch 8292, Loss Train(MSE): 0.12515144977751502, R2 Train: 0.4993942008899399\n",
      "Epoch 8293, Loss Train(MSE): 0.12515137149917638, R2 Train: 0.4993945140032945\n",
      "Epoch 8294, Loss Train(MSE): 0.12515129487281376, R2 Train: 0.49939482050874495\n",
      "Epoch 8295, Loss Train(MSE): 0.12515122099789913, R2 Train: 0.4993951160084035\n",
      "Epoch 8296, Loss Train(MSE): 0.12515114082331577, R2 Train: 0.4993954367067369\n",
      "Epoch 8297, Loss Train(MSE): 0.12515107047865875, R2 Train: 0.499395718085365\n",
      "Epoch 8298, Loss Train(MSE): 0.125150990981298, R2 Train: 0.499396036074808\n",
      "Epoch 8299, Loss Train(MSE): 0.125150916590009, R2 Train: 0.49939633363996405\n",
      "Epoch 8300, Loss Train(MSE): 0.12515084155781425, R2 Train: 0.499396633768743\n",
      "Epoch 8301, Loss Train(MSE): 0.12515076310358672, R2 Train: 0.4993969475856531\n",
      "Epoch 8302, Loss Train(MSE): 0.1251506925506688, R2 Train: 0.4993972297973248\n",
      "Epoch 8303, Loss Train(MSE): 0.12515061320705287, R2 Train: 0.4993975471717885\n",
      "Epoch 8304, Loss Train(MSE): 0.12515054077580798, R2 Train: 0.4993978368967681\n",
      "Epoch 8305, Loss Train(MSE): 0.1251504648468256, R2 Train: 0.4993981406126976\n",
      "Epoch 8306, Loss Train(MSE): 0.1251503882877785, R2 Train: 0.499398446848886\n",
      "Epoch 8307, Loss Train(MSE): 0.12515031689735917, R2 Train: 0.4993987324105633\n",
      "Epoch 8308, Loss Train(MSE): 0.1251502381435831, R2 Train: 0.49939904742566765\n",
      "Epoch 8309, Loss Train(MSE): 0.12515016741566526, R2 Train: 0.49939933033733896\n",
      "Epoch 8310, Loss Train(MSE): 0.12515077033335095, R2 Train: 0.4993969186665962\n",
      "Epoch 8311, Loss Train(MSE): 0.1251529134543289, R2 Train: 0.49938834618268435\n",
      "Epoch 8312, Loss Train(MSE): 0.12515282922052376, R2 Train: 0.49938868311790496\n",
      "Epoch 8313, Loss Train(MSE): 0.12515274509537094, R2 Train: 0.49938901961851623\n",
      "Epoch 8314, Loss Train(MSE): 0.1251526610786273, R2 Train: 0.4993893556854908\n",
      "Epoch 8315, Loss Train(MSE): 0.12515257717005066, R2 Train: 0.4993896913197974\n",
      "Epoch 8316, Loss Train(MSE): 0.12515249336939968, R2 Train: 0.4993900265224013\n",
      "Epoch 8317, Loss Train(MSE): 0.12515240967643373, R2 Train: 0.49939036129426506\n",
      "Epoch 8318, Loss Train(MSE): 0.12515232609091317, R2 Train: 0.4993906956363473\n",
      "Epoch 8319, Loss Train(MSE): 0.12515224261259905, R2 Train: 0.4993910295496038\n",
      "Epoch 8320, Loss Train(MSE): 0.12515215924125317, R2 Train: 0.4993913630349873\n",
      "Epoch 8321, Loss Train(MSE): 0.12515207597663824, R2 Train: 0.499391696093447\n",
      "Epoch 8322, Loss Train(MSE): 0.12515199281851785, R2 Train: 0.4993920287259286\n",
      "Epoch 8323, Loss Train(MSE): 0.12515190976665613, R2 Train: 0.49939236093337547\n",
      "Epoch 8324, Loss Train(MSE): 0.1251518268208182, R2 Train: 0.49939269271672715\n",
      "Epoch 8325, Loss Train(MSE): 0.12515174398076997, R2 Train: 0.4993930240769201\n",
      "Epoch 8326, Loss Train(MSE): 0.125151661246278, R2 Train: 0.499393355014888\n",
      "Epoch 8327, Loss Train(MSE): 0.12515157861710968, R2 Train: 0.49939368553156127\n",
      "Epoch 8328, Loss Train(MSE): 0.1251514960930333, R2 Train: 0.4993940156278668\n",
      "Epoch 8329, Loss Train(MSE): 0.12515141367381777, R2 Train: 0.4993943453047289\n",
      "Epoch 8330, Loss Train(MSE): 0.12515133135923284, R2 Train: 0.49939467456306863\n",
      "Epoch 8331, Loss Train(MSE): 0.125151249149049, R2 Train: 0.49939500340380405\n",
      "Epoch 8332, Loss Train(MSE): 0.12515116704303747, R2 Train: 0.4993953318278501\n",
      "Epoch 8333, Loss Train(MSE): 0.12515108504097033, R2 Train: 0.49939565983611867\n",
      "Epoch 8334, Loss Train(MSE): 0.12515100314262034, R2 Train: 0.49939598742951863\n",
      "Epoch 8335, Loss Train(MSE): 0.12515092134776093, R2 Train: 0.4993963146089563\n",
      "Epoch 8336, Loss Train(MSE): 0.12515083965616644, R2 Train: 0.49939664137533424\n",
      "Epoch 8337, Loss Train(MSE): 0.12515075806761183, R2 Train: 0.49939696772955267\n",
      "Epoch 8338, Loss Train(MSE): 0.12515067658187293, R2 Train: 0.4993972936725083\n",
      "Epoch 8339, Loss Train(MSE): 0.12515059519872607, R2 Train: 0.49939761920509573\n",
      "Epoch 8340, Loss Train(MSE): 0.12515051391794862, R2 Train: 0.4993979443282055\n",
      "Epoch 8341, Loss Train(MSE): 0.1251504327393184, R2 Train: 0.49939826904272644\n",
      "Epoch 8342, Loss Train(MSE): 0.1251503516626141, R2 Train: 0.4993985933495436\n",
      "Epoch 8343, Loss Train(MSE): 0.1251502706876151, R2 Train: 0.49939891724953955\n",
      "Epoch 8344, Loss Train(MSE): 0.1251501898141015, R2 Train: 0.499399240743594\n",
      "Epoch 8345, Loss Train(MSE): 0.12515010904185417, R2 Train: 0.4993995638325833\n",
      "Epoch 8346, Loss Train(MSE): 0.12515002837065456, R2 Train: 0.49939988651738176\n",
      "Epoch 8347, Loss Train(MSE): 0.1251499478002849, R2 Train: 0.4994002087988604\n",
      "Epoch 8348, Loss Train(MSE): 0.1251498673305282, R2 Train: 0.49940053067788726\n",
      "Epoch 8349, Loss Train(MSE): 0.12514978696116805, R2 Train: 0.4994008521553278\n",
      "Epoch 8350, Loss Train(MSE): 0.12514970669198874, R2 Train: 0.499401173232045\n",
      "Epoch 8351, Loss Train(MSE): 0.12514962652277534, R2 Train: 0.49940149390889865\n",
      "Epoch 8352, Loss Train(MSE): 0.12514954645331355, R2 Train: 0.4994018141867458\n",
      "Epoch 8353, Loss Train(MSE): 0.1251494664833898, R2 Train: 0.49940213406644085\n",
      "Epoch 8354, Loss Train(MSE): 0.12514938661279118, R2 Train: 0.4994024535488353\n",
      "Epoch 8355, Loss Train(MSE): 0.12514930684130546, R2 Train: 0.49940277263477817\n",
      "Epoch 8356, Loss Train(MSE): 0.12514922716872104, R2 Train: 0.49940309132511584\n",
      "Epoch 8357, Loss Train(MSE): 0.12514914759482712, R2 Train: 0.4994034096206915\n",
      "Epoch 8358, Loss Train(MSE): 0.1251490681194134, R2 Train: 0.4994037275223464\n",
      "Epoch 8359, Loss Train(MSE): 0.12514898874227046, R2 Train: 0.4994040450309182\n",
      "Epoch 8360, Loss Train(MSE): 0.12514890946318935, R2 Train: 0.4994043621472426\n",
      "Epoch 8361, Loss Train(MSE): 0.12514883028196183, R2 Train: 0.4994046788721527\n",
      "Epoch 8362, Loss Train(MSE): 0.12514875119838048, R2 Train: 0.4994049952064781\n",
      "Epoch 8363, Loss Train(MSE): 0.1251486722122383, R2 Train: 0.4994053111510468\n",
      "Epoch 8364, Loss Train(MSE): 0.12514859332332903, R2 Train: 0.49940562670668387\n",
      "Epoch 8365, Loss Train(MSE): 0.12514851453144715, R2 Train: 0.4994059418742114\n",
      "Epoch 8366, Loss Train(MSE): 0.1251484358363878, R2 Train: 0.4994062566544488\n",
      "Epoch 8367, Loss Train(MSE): 0.12514835723794648, R2 Train: 0.49940657104821407\n",
      "Epoch 8368, Loss Train(MSE): 0.12514827873591966, R2 Train: 0.49940688505632136\n",
      "Epoch 8369, Loss Train(MSE): 0.1251482003301043, R2 Train: 0.49940719867958283\n",
      "Epoch 8370, Loss Train(MSE): 0.125148122020298, R2 Train: 0.49940751191880794\n",
      "Epoch 8371, Loss Train(MSE): 0.12514804380629904, R2 Train: 0.49940782477480383\n",
      "Epoch 8372, Loss Train(MSE): 0.12514796568790634, R2 Train: 0.49940813724837463\n",
      "Epoch 8373, Loss Train(MSE): 0.12514788766491933, R2 Train: 0.4994084493403227\n",
      "Epoch 8374, Loss Train(MSE): 0.12514780973713813, R2 Train: 0.4994087610514475\n",
      "Epoch 8375, Loss Train(MSE): 0.1251477319043636, R2 Train: 0.4994090723825456\n",
      "Epoch 8376, Loss Train(MSE): 0.12514765416639703, R2 Train: 0.49940938333441187\n",
      "Epoch 8377, Loss Train(MSE): 0.12514757652304045, R2 Train: 0.4994096939078382\n",
      "Epoch 8378, Loss Train(MSE): 0.12514749897409644, R2 Train: 0.49941000410361425\n",
      "Epoch 8379, Loss Train(MSE): 0.1251474215193682, R2 Train: 0.4994103139225272\n",
      "Epoch 8380, Loss Train(MSE): 0.12514734415865963, R2 Train: 0.49941062336536146\n",
      "Epoch 8381, Loss Train(MSE): 0.1251472668917751, R2 Train: 0.4994109324328996\n",
      "Epoch 8382, Loss Train(MSE): 0.1251471897185196, R2 Train: 0.4994112411259216\n",
      "Epoch 8383, Loss Train(MSE): 0.12514711263869882, R2 Train: 0.4994115494452047\n",
      "Epoch 8384, Loss Train(MSE): 0.125147035652119, R2 Train: 0.49941185739152405\n",
      "Epoch 8385, Loss Train(MSE): 0.1251469587585869, R2 Train: 0.49941216496565244\n",
      "Epoch 8386, Loss Train(MSE): 0.12514688195791, R2 Train: 0.49941247216836004\n",
      "Epoch 8387, Loss Train(MSE): 0.12514680524989624, R2 Train: 0.49941277900041503\n",
      "Epoch 8388, Loss Train(MSE): 0.12514672863435425, R2 Train: 0.499413085462583\n",
      "Epoch 8389, Loss Train(MSE): 0.12514665211109322, R2 Train: 0.49941339155562714\n",
      "Epoch 8390, Loss Train(MSE): 0.12514657567992285, R2 Train: 0.4994136972803086\n",
      "Epoch 8391, Loss Train(MSE): 0.1251464993406535, R2 Train: 0.499414002637386\n",
      "Epoch 8392, Loss Train(MSE): 0.12514642309309612, R2 Train: 0.4994143076276155\n",
      "Epoch 8393, Loss Train(MSE): 0.12514634693706217, R2 Train: 0.4994146122517513\n",
      "Epoch 8394, Loss Train(MSE): 0.1251462708723637, R2 Train: 0.49941491651054515\n",
      "Epoch 8395, Loss Train(MSE): 0.12514619489881335, R2 Train: 0.4994152204047466\n",
      "Epoch 8396, Loss Train(MSE): 0.1251461190162243, R2 Train: 0.4994155239351028\n",
      "Epoch 8397, Loss Train(MSE): 0.12514604322441036, R2 Train: 0.49941582710235854\n",
      "Epoch 8398, Loss Train(MSE): 0.1251459675231858, R2 Train: 0.4994161299072568\n",
      "Epoch 8399, Loss Train(MSE): 0.12514589191236553, R2 Train: 0.4994164323505379\n",
      "Epoch 8400, Loss Train(MSE): 0.12514581639176495, R2 Train: 0.4994167344329402\n",
      "Epoch 8401, Loss Train(MSE): 0.12514574096120007, R2 Train: 0.4994170361551997\n",
      "Epoch 8402, Loss Train(MSE): 0.12514566562048743, R2 Train: 0.4994173375180503\n",
      "Epoch 8403, Loss Train(MSE): 0.12514559036944423, R2 Train: 0.4994176385222231\n",
      "Epoch 8404, Loss Train(MSE): 0.12514551520788794, R2 Train: 0.49941793916844823\n",
      "Epoch 8405, Loss Train(MSE): 0.1251454401356369, R2 Train: 0.49941823945745245\n",
      "Epoch 8406, Loss Train(MSE): 0.12514536515250976, R2 Train: 0.49941853938996095\n",
      "Epoch 8407, Loss Train(MSE): 0.12514529025832577, R2 Train: 0.4994188389666969\n",
      "Epoch 8408, Loss Train(MSE): 0.12514521545290483, R2 Train: 0.4994191381883807\n",
      "Epoch 8409, Loss Train(MSE): 0.12514514073606728, R2 Train: 0.4994194370557309\n",
      "Epoch 8410, Loss Train(MSE): 0.12514506610763393, R2 Train: 0.4994197355694643\n",
      "Epoch 8411, Loss Train(MSE): 0.12514499156742628, R2 Train: 0.49942003373029487\n",
      "Epoch 8412, Loss Train(MSE): 0.12514491711526618, R2 Train: 0.4994203315389353\n",
      "Epoch 8413, Loss Train(MSE): 0.12514484275097623, R2 Train: 0.4994206289960951\n",
      "Epoch 8414, Loss Train(MSE): 0.12514476847437936, R2 Train: 0.49942092610248257\n",
      "Epoch 8415, Loss Train(MSE): 0.12514469428529904, R2 Train: 0.49942122285880386\n",
      "Epoch 8416, Loss Train(MSE): 0.1251446201835594, R2 Train: 0.4994215192657624\n",
      "Epoch 8417, Loss Train(MSE): 0.12514454616898493, R2 Train: 0.49942181532406027\n",
      "Epoch 8418, Loss Train(MSE): 0.1251444722414008, R2 Train: 0.4994221110343968\n",
      "Epoch 8419, Loss Train(MSE): 0.12514439840063257, R2 Train: 0.4994224063974697\n",
      "Epoch 8420, Loss Train(MSE): 0.12514432464650624, R2 Train: 0.49942270141397505\n",
      "Epoch 8421, Loss Train(MSE): 0.1251442509788486, R2 Train: 0.4994229960846056\n",
      "Epoch 8422, Loss Train(MSE): 0.12514417739748662, R2 Train: 0.4994232904100535\n",
      "Epoch 8423, Loss Train(MSE): 0.12514410390224806, R2 Train: 0.49942358439100776\n",
      "Epoch 8424, Loss Train(MSE): 0.125144030492961, R2 Train: 0.499423878028156\n",
      "Epoch 8425, Loss Train(MSE): 0.12514395716945403, R2 Train: 0.49942417132218386\n",
      "Epoch 8426, Loss Train(MSE): 0.12514388393155634, R2 Train: 0.4994244642737746\n",
      "Epoch 8427, Loss Train(MSE): 0.1251438107790976, R2 Train: 0.49942475688360966\n",
      "Epoch 8428, Loss Train(MSE): 0.12514373771190793, R2 Train: 0.4994250491523683\n",
      "Epoch 8429, Loss Train(MSE): 0.12514366472981786, R2 Train: 0.49942534108072856\n",
      "Epoch 8430, Loss Train(MSE): 0.12514359183265858, R2 Train: 0.4994256326693657\n",
      "Epoch 8431, Loss Train(MSE): 0.12514351902026177, R2 Train: 0.4994259239189529\n",
      "Epoch 8432, Loss Train(MSE): 0.12514344629245938, R2 Train: 0.4994262148301625\n",
      "Epoch 8433, Loss Train(MSE): 0.12514337364908407, R2 Train: 0.49942650540366373\n",
      "Epoch 8434, Loss Train(MSE): 0.1251433010899689, R2 Train: 0.4994267956401244\n",
      "Epoch 8435, Loss Train(MSE): 0.12514322861494745, R2 Train: 0.4994270855402102\n",
      "Epoch 8436, Loss Train(MSE): 0.12514315622385375, R2 Train: 0.499427375104585\n",
      "Epoch 8437, Loss Train(MSE): 0.12514308391652224, R2 Train: 0.49942766433391106\n",
      "Epoch 8438, Loss Train(MSE): 0.12514301852081688, R2 Train: 0.49942792591673246\n",
      "Epoch 8439, Loss Train(MSE): 0.12514297730266374, R2 Train: 0.499428090789345\n",
      "Epoch 8440, Loss Train(MSE): 0.12514293610695093, R2 Train: 0.4994282555721963\n",
      "Epoch 8441, Loss Train(MSE): 0.12514289493366035, R2 Train: 0.4994284202653586\n",
      "Epoch 8442, Loss Train(MSE): 0.12514285378277368, R2 Train: 0.4994285848689053\n",
      "Epoch 8443, Loss Train(MSE): 0.125142812654273, R2 Train: 0.49942874938290804\n",
      "Epoch 8444, Loss Train(MSE): 0.12514277154813996, R2 Train: 0.49942891380744014\n",
      "Epoch 8445, Loss Train(MSE): 0.12514273046435664, R2 Train: 0.4994290781425734\n",
      "Epoch 8446, Loss Train(MSE): 0.12514268940290482, R2 Train: 0.4994292423883807\n",
      "Epoch 8447, Loss Train(MSE): 0.12514264836376657, R2 Train: 0.49942940654493373\n",
      "Epoch 8448, Loss Train(MSE): 0.12514260734692376, R2 Train: 0.499429570612305\n",
      "Epoch 8449, Loss Train(MSE): 0.12514256635235838, R2 Train: 0.4994297345905665\n",
      "Epoch 8450, Loss Train(MSE): 0.12514252538005247, R2 Train: 0.49942989847979014\n",
      "Epoch 8451, Loss Train(MSE): 0.12514248442998804, R2 Train: 0.4994300622800478\n",
      "Epoch 8452, Loss Train(MSE): 0.12514244350214712, R2 Train: 0.4994302259914115\n",
      "Epoch 8453, Loss Train(MSE): 0.12514240259651188, R2 Train: 0.4994303896139525\n",
      "Epoch 8454, Loss Train(MSE): 0.12514236171306425, R2 Train: 0.499430553147743\n",
      "Epoch 8455, Loss Train(MSE): 0.12514232085178648, R2 Train: 0.4994307165928541\n",
      "Epoch 8456, Loss Train(MSE): 0.1251422800126606, R2 Train: 0.49943087994935764\n",
      "Epoch 8457, Loss Train(MSE): 0.12514223919566883, R2 Train: 0.4994310432173247\n",
      "Epoch 8458, Loss Train(MSE): 0.1251421984007933, R2 Train: 0.4994312063968268\n",
      "Epoch 8459, Loss Train(MSE): 0.12514215762801625, R2 Train: 0.499431369487935\n",
      "Epoch 8460, Loss Train(MSE): 0.12514211687731985, R2 Train: 0.4994315324907206\n",
      "Epoch 8461, Loss Train(MSE): 0.12514207614868642, R2 Train: 0.49943169540525434\n",
      "Epoch 8462, Loss Train(MSE): 0.1251420354420981, R2 Train: 0.49943185823160763\n",
      "Epoch 8463, Loss Train(MSE): 0.12514199475753726, R2 Train: 0.49943202096985095\n",
      "Epoch 8464, Loss Train(MSE): 0.12514195409498616, R2 Train: 0.49943218362005537\n",
      "Epoch 8465, Loss Train(MSE): 0.1251419134544271, R2 Train: 0.4994323461822916\n",
      "Epoch 8466, Loss Train(MSE): 0.12514187283584247, R2 Train: 0.49943250865663014\n",
      "Epoch 8467, Loss Train(MSE): 0.12514183223921463, R2 Train: 0.4994326710431415\n",
      "Epoch 8468, Loss Train(MSE): 0.12514179166452588, R2 Train: 0.4994328333418965\n",
      "Epoch 8469, Loss Train(MSE): 0.1251417511117587, R2 Train: 0.49943299555296516\n",
      "Epoch 8470, Loss Train(MSE): 0.12514171058089554, R2 Train: 0.4994331576764178\n",
      "Epoch 8471, Loss Train(MSE): 0.1251416700719187, R2 Train: 0.4994333197123252\n",
      "Epoch 8472, Loss Train(MSE): 0.12514162958481082, R2 Train: 0.49943348166075674\n",
      "Epoch 8473, Loss Train(MSE): 0.12514158911955428, R2 Train: 0.4994336435217829\n",
      "Epoch 8474, Loss Train(MSE): 0.12514154867613156, R2 Train: 0.49943380529547377\n",
      "Epoch 8475, Loss Train(MSE): 0.12514150825452525, R2 Train: 0.499433966981899\n",
      "Epoch 8476, Loss Train(MSE): 0.12514146785471789, R2 Train: 0.49943412858112846\n",
      "Epoch 8477, Loss Train(MSE): 0.125141427476692, R2 Train: 0.49943429009323204\n",
      "Epoch 8478, Loss Train(MSE): 0.12514138712043016, R2 Train: 0.49943445151827937\n",
      "Epoch 8479, Loss Train(MSE): 0.125141346785915, R2 Train: 0.49943461285634005\n",
      "Epoch 8480, Loss Train(MSE): 0.12514130647312913, R2 Train: 0.4994347741074835\n",
      "Epoch 8481, Loss Train(MSE): 0.12514126618205523, R2 Train: 0.49943493527177907\n",
      "Epoch 8482, Loss Train(MSE): 0.1251412259126759, R2 Train: 0.4994350963492964\n",
      "Epoch 8483, Loss Train(MSE): 0.1251411856649739, R2 Train: 0.4994352573401044\n",
      "Epoch 8484, Loss Train(MSE): 0.12514114543893184, R2 Train: 0.4994354182442726\n",
      "Epoch 8485, Loss Train(MSE): 0.12514110523453248, R2 Train: 0.4994355790618701\n",
      "Epoch 8486, Loss Train(MSE): 0.12514106505175862, R2 Train: 0.4994357397929655\n",
      "Epoch 8487, Loss Train(MSE): 0.12514102489059295, R2 Train: 0.4994359004376282\n",
      "Epoch 8488, Loss Train(MSE): 0.12514098475101826, R2 Train: 0.49943606099592697\n",
      "Epoch 8489, Loss Train(MSE): 0.12514094463301736, R2 Train: 0.49943622146793054\n",
      "Epoch 8490, Loss Train(MSE): 0.12514090453657303, R2 Train: 0.49943638185370787\n",
      "Epoch 8491, Loss Train(MSE): 0.1251408644616682, R2 Train: 0.4994365421533272\n",
      "Epoch 8492, Loss Train(MSE): 0.12514082440828564, R2 Train: 0.49943670236685744\n",
      "Epoch 8493, Loss Train(MSE): 0.1251407843764083, R2 Train: 0.4994368624943668\n",
      "Epoch 8494, Loss Train(MSE): 0.12514074436601902, R2 Train: 0.4994370225359239\n",
      "Epoch 8495, Loss Train(MSE): 0.12514070437710073, R2 Train: 0.4994371824915971\n",
      "Epoch 8496, Loss Train(MSE): 0.12514066440963637, R2 Train: 0.49943734236145454\n",
      "Epoch 8497, Loss Train(MSE): 0.1251406244636089, R2 Train: 0.49943750214556437\n",
      "Epoch 8498, Loss Train(MSE): 0.12514058453900131, R2 Train: 0.49943766184399474\n",
      "Epoch 8499, Loss Train(MSE): 0.12514054463579652, R2 Train: 0.4994378214568139\n",
      "Epoch 8500, Loss Train(MSE): 0.12514050475397762, R2 Train: 0.49943798098408954\n",
      "Epoch 8501, Loss Train(MSE): 0.1251404648935276, R2 Train: 0.49943814042588963\n",
      "Epoch 8502, Loss Train(MSE): 0.1251404250544295, R2 Train: 0.49943829978228205\n",
      "Epoch 8503, Loss Train(MSE): 0.12514038523666646, R2 Train: 0.49943845905333417\n",
      "Epoch 8504, Loss Train(MSE): 0.12514034544022148, R2 Train: 0.49943861823911406\n",
      "Epoch 8505, Loss Train(MSE): 0.12514030566507772, R2 Train: 0.4994387773396891\n",
      "Epoch 8506, Loss Train(MSE): 0.12514026591121827, R2 Train: 0.49943893635512693\n",
      "Epoch 8507, Loss Train(MSE): 0.1251402261786263, R2 Train: 0.4994390952854948\n",
      "Epoch 8508, Loss Train(MSE): 0.12514018646728498, R2 Train: 0.4994392541308601\n",
      "Epoch 8509, Loss Train(MSE): 0.12514014677717747, R2 Train: 0.4994394128912901\n",
      "Epoch 8510, Loss Train(MSE): 0.12514010710828694, R2 Train: 0.49943957156685226\n",
      "Epoch 8511, Loss Train(MSE): 0.12514006746059667, R2 Train: 0.49943973015761334\n",
      "Epoch 8512, Loss Train(MSE): 0.1251400278340899, R2 Train: 0.4994398886636404\n",
      "Epoch 8513, Loss Train(MSE): 0.1251399882287498, R2 Train: 0.4994400470850008\n",
      "Epoch 8514, Loss Train(MSE): 0.12513994864455977, R2 Train: 0.49944020542176093\n",
      "Epoch 8515, Loss Train(MSE): 0.12513990908150296, R2 Train: 0.49944036367398814\n",
      "Epoch 8516, Loss Train(MSE): 0.12513986953956283, R2 Train: 0.4994405218417487\n",
      "Epoch 8517, Loss Train(MSE): 0.1251398300187226, R2 Train: 0.49944067992510965\n",
      "Epoch 8518, Loss Train(MSE): 0.1251397905189656, R2 Train: 0.4994408379241376\n",
      "Epoch 8519, Loss Train(MSE): 0.12513975104027533, R2 Train: 0.49944099583889867\n",
      "Epoch 8520, Loss Train(MSE): 0.12513971158263504, R2 Train: 0.49944115366945985\n",
      "Epoch 8521, Loss Train(MSE): 0.12513967214602825, R2 Train: 0.499441311415887\n",
      "Epoch 8522, Loss Train(MSE): 0.1251396327304383, R2 Train: 0.4994414690782468\n",
      "Epoch 8523, Loss Train(MSE): 0.1251395933358486, R2 Train: 0.49944162665660563\n",
      "Epoch 8524, Loss Train(MSE): 0.12513955396224266, R2 Train: 0.49944178415102936\n",
      "Epoch 8525, Loss Train(MSE): 0.12513951460960399, R2 Train: 0.49944194156158406\n",
      "Epoch 8526, Loss Train(MSE): 0.12513947527791602, R2 Train: 0.4994420988883359\n",
      "Epoch 8527, Loss Train(MSE): 0.12513943596716232, R2 Train: 0.49944225613135074\n",
      "Epoch 8528, Loss Train(MSE): 0.12513939667732632, R2 Train: 0.4994424132906947\n",
      "Epoch 8529, Loss Train(MSE): 0.1251393574083917, R2 Train: 0.49944257036643325\n",
      "Epoch 8530, Loss Train(MSE): 0.12513931816034193, R2 Train: 0.49944272735863227\n",
      "Epoch 8531, Loss Train(MSE): 0.12513927893316065, R2 Train: 0.4994428842673574\n",
      "Epoch 8532, Loss Train(MSE): 0.1251392397268314, R2 Train: 0.49944304109267434\n",
      "Epoch 8533, Loss Train(MSE): 0.12513920054133787, R2 Train: 0.4994431978346485\n",
      "Epoch 8534, Loss Train(MSE): 0.12513916137666362, R2 Train: 0.49944335449334554\n",
      "Epoch 8535, Loss Train(MSE): 0.1251391307131281, R2 Train: 0.4994434771474876\n",
      "Epoch 8536, Loss Train(MSE): 0.12513909742517218, R2 Train: 0.4994436102993113\n",
      "Epoch 8537, Loss Train(MSE): 0.12513906862899984, R2 Train: 0.4994437254840006\n",
      "Epoch 8538, Loss Train(MSE): 0.1251390373717899, R2 Train: 0.49944385051284035\n",
      "Epoch 8539, Loss Train(MSE): 0.12513900413425913, R2 Train: 0.4994439834629635\n",
      "Epoch 8540, Loss Train(MSE): 0.1251389759990554, R2 Train: 0.49944409600377837\n",
      "Epoch 8541, Loss Train(MSE): 0.12513894416526708, R2 Train: 0.4994442233389317\n",
      "Epoch 8542, Loss Train(MSE): 0.1251389109779923, R2 Train: 0.49944435608803084\n",
      "Epoch 8543, Loss Train(MSE): 0.1251388834865533, R2 Train: 0.4994444660537868\n",
      "Epoch 8544, Loss Train(MSE): 0.12513885109313835, R2 Train: 0.4994445956274466\n",
      "Epoch 8545, Loss Train(MSE): 0.12513881795595155, R2 Train: 0.4994447281761938\n",
      "Epoch 8546, Loss Train(MSE): 0.125138791091218, R2 Train: 0.499444835635128\n",
      "Epoch 8547, Loss Train(MSE): 0.12513875815498515, R2 Train: 0.4994449673800594\n",
      "Epoch 8548, Loss Train(MSE): 0.12513872547360996, R2 Train: 0.49944509810556015\n",
      "Epoch 8549, Loss Train(MSE): 0.12513869840665073, R2 Train: 0.49944520637339707\n",
      "Epoch 8550, Loss Train(MSE): 0.12513866534867663, R2 Train: 0.4994453386052935\n",
      "Epoch 8551, Loss Train(MSE): 0.1251386332765786, R2 Train: 0.49944546689368563\n",
      "Epoch 8552, Loss Train(MSE): 0.12513860568384552, R2 Train: 0.4994455772646179\n",
      "Epoch 8553, Loss Train(MSE): 0.1251385726755426, R2 Train: 0.4994457092978296\n",
      "Epoch 8554, Loss Train(MSE): 0.12513854119599416, R2 Train: 0.49944583521602337\n",
      "Epoch 8555, Loss Train(MSE): 0.12513851309396828, R2 Train: 0.49944594762412686\n",
      "Epoch 8556, Loss Train(MSE): 0.12513848013517245, R2 Train: 0.4994460794593102\n",
      "Epoch 8557, Loss Train(MSE): 0.1251384492315866, R2 Train: 0.4994462030736536\n",
      "Epoch 8558, Loss Train(MSE): 0.12513842063661004, R2 Train: 0.49944631745355983\n",
      "Epoch 8559, Loss Train(MSE): 0.12513838772715827, R2 Train: 0.49944644909136693\n",
      "Epoch 8560, Loss Train(MSE): 0.12513835738308735, R2 Train: 0.4994465704676506\n",
      "Epoch 8561, Loss Train(MSE): 0.12513832831136437, R2 Train: 0.4994466867545425\n",
      "Epoch 8562, Loss Train(MSE): 0.12513829545109473, R2 Train: 0.4994468181956211\n",
      "Epoch 8563, Loss Train(MSE): 0.1251382656502294, R2 Train: 0.4994469373990824\n",
      "Epoch 8564, Loss Train(MSE): 0.12513823611782768, R2 Train: 0.4994470555286893\n",
      "Epoch 8565, Loss Train(MSE): 0.1251382033065793, R2 Train: 0.4994471867736828\n",
      "Epoch 8566, Loss Train(MSE): 0.12513817403274707, R2 Train: 0.4994473038690117\n",
      "Epoch 8567, Loss Train(MSE): 0.1251381440555988, R2 Train: 0.49944742377760476\n",
      "Epoch 8568, Loss Train(MSE): 0.1251381112932119, R2 Train: 0.4994475548271524\n",
      "Epoch 8569, Loss Train(MSE): 0.12513808253037628, R2 Train: 0.49944766987849487\n",
      "Epoch 8570, Loss Train(MSE): 0.12513805212427934, R2 Train: 0.49944779150288265\n",
      "Epoch 8571, Loss Train(MSE): 0.12513801941059507, R2 Train: 0.4994479223576197\n",
      "Epoch 8572, Loss Train(MSE): 0.12513799114285437, R2 Train: 0.4994480354285825\n",
      "Epoch 8573, Loss Train(MSE): 0.12513796032347324, R2 Train: 0.49944815870610704\n",
      "Epoch 8574, Loss Train(MSE): 0.12513792765833404, R2 Train: 0.49944828936666386\n",
      "Epoch 8575, Loss Train(MSE): 0.12513789986991997, R2 Train: 0.49944840052032013\n",
      "Epoch 8576, Loss Train(MSE): 0.12513786865278723, R2 Train: 0.4994485253888511\n",
      "Epoch 8577, Loss Train(MSE): 0.12513783603603634, R2 Train: 0.49944865585585463\n",
      "Epoch 8578, Loss Train(MSE): 0.12513780871131328, R2 Train: 0.4994487651547469\n",
      "Epoch 8579, Loss Train(MSE): 0.12513777711183033, R2 Train: 0.4994488915526787\n",
      "Epoch 8580, Loss Train(MSE): 0.12513774454331214, R2 Train: 0.49944902182675144\n",
      "Epoch 8581, Loss Train(MSE): 0.12513771766677578, R2 Train: 0.49944912933289687\n",
      "Epoch 8582, Loss Train(MSE): 0.12513768570021427, R2 Train: 0.4994492571991429\n",
      "Epoch 8583, Loss Train(MSE): 0.1251376531797741, R2 Train: 0.4994493872809036\n",
      "Epoch 8584, Loss Train(MSE): 0.1251376267360505, R2 Train: 0.499449493055798\n",
      "Epoch 8585, Loss Train(MSE): 0.12513759441755307, R2 Train: 0.49944962232978773\n",
      "Epoch 8586, Loss Train(MSE): 0.12513756215757657, R2 Train: 0.4994497513696937\n",
      "Epoch 8587, Loss Train(MSE): 0.1251375357061934, R2 Train: 0.4994498571752264\n",
      "Epoch 8588, Loss Train(MSE): 0.12513750326183637, R2 Train: 0.4994499869526545\n",
      "Epoch 8589, Loss Train(MSE): 0.12513747141964754, R2 Train: 0.49945011432140984\n",
      "Epoch 8590, Loss Train(MSE): 0.1251374446309984, R2 Train: 0.4994502214760064\n",
      "Epoch 8591, Loss Train(MSE): 0.12513741223433567, R2 Train: 0.49945035106265734\n",
      "Epoch 8592, Loss Train(MSE): 0.1251373807948572, R2 Train: 0.4994504768205712\n",
      "Epoch 8593, Loss Train(MSE): 0.1251373536837917, R2 Train: 0.4994505852648332\n",
      "Epoch 8594, Loss Train(MSE): 0.12513732133467204, R2 Train: 0.49945071466131186\n",
      "Epoch 8595, Loss Train(MSE): 0.1251372902829531, R2 Train: 0.49945083886818764\n",
      "Epoch 8596, Loss Train(MSE): 0.1251372628641957, R2 Train: 0.49945094854321725\n",
      "Epoch 8597, Loss Train(MSE): 0.12513723056246887, R2 Train: 0.49945107775012454\n",
      "Epoch 8598, Loss Train(MSE): 0.1251371998836843, R2 Train: 0.4994512004652628\n",
      "Epoch 8599, Loss Train(MSE): 0.1251371721718352, R2 Train: 0.4994513113126592\n",
      "Epoch 8600, Loss Train(MSE): 0.12513713991735187, R2 Train: 0.4994514403305925\n",
      "Epoch 8601, Loss Train(MSE): 0.12513710959680113, R2 Train: 0.49945156161279547\n",
      "Epoch 8602, Loss Train(MSE): 0.12513708160633727, R2 Train: 0.4994516735746509\n",
      "Epoch 8603, Loss Train(MSE): 0.125137049398949, R2 Train: 0.49945180240420395\n",
      "Epoch 8604, Loss Train(MSE): 0.1251370194220551, R2 Train: 0.4994519223117796\n",
      "Epoch 8605, Loss Train(MSE): 0.12513699116733132, R2 Train: 0.4994520353306747\n",
      "Epoch 8606, Loss Train(MSE): 0.12513695900689079, R2 Train: 0.49945216397243686\n",
      "Epoch 8607, Loss Train(MSE): 0.1251369293591992, R2 Train: 0.4994522825632032\n",
      "Epoch 8608, Loss Train(MSE): 0.12513690085444903, R2 Train: 0.49945239658220386\n",
      "Epoch 8609, Loss Train(MSE): 0.12513686874080968, R2 Train: 0.4994525250367613\n",
      "Epoch 8610, Loss Train(MSE): 0.1251368394079875, R2 Train: 0.49945264236805\n",
      "Epoch 8611, Loss Train(MSE): 0.12513681066732443, R2 Train: 0.49945275733070227\n",
      "Epoch 8612, Loss Train(MSE): 0.12513677860034061, R2 Train: 0.49945288559863754\n",
      "Epoch 8613, Loss Train(MSE): 0.1251367495681754, R2 Train: 0.49945300172729845\n",
      "Epoch 8614, Loss Train(MSE): 0.12513672060559364, R2 Train: 0.49945311757762545\n",
      "Epoch 8615, Loss Train(MSE): 0.1251366885851208, R2 Train: 0.49945324565951676\n",
      "Epoch 8616, Loss Train(MSE): 0.12513665983951952, R2 Train: 0.4994533606419219\n",
      "Epoch 8617, Loss Train(MSE): 0.1251366306688951, R2 Train: 0.4994534773244196\n",
      "Epoch 8618, Loss Train(MSE): 0.1251365986947895, R2 Train: 0.49945360522084203\n",
      "Epoch 8619, Loss Train(MSE): 0.1251365702217778, R2 Train: 0.4994537191128888\n",
      "Epoch 8620, Loss Train(MSE): 0.1251365408568695, R2 Train: 0.49945383657252196\n",
      "Epoch 8621, Loss Train(MSE): 0.1251365089289882, R2 Train: 0.4994539642840472\n",
      "Epoch 8622, Loss Train(MSE): 0.1251364807147092, R2 Train: 0.4994540771411632\n",
      "Epoch 8623, Loss Train(MSE): 0.12513645116915964, R2 Train: 0.49945419532336144\n",
      "Epoch 8624, Loss Train(MSE): 0.1251364192873606, R2 Train: 0.4994543228505576\n",
      "Epoch 8625, Loss Train(MSE): 0.12513639131807408, R2 Train: 0.4994544347277037\n",
      "Epoch 8626, Loss Train(MSE): 0.12513636160541042, R2 Train: 0.49945455357835833\n",
      "Epoch 8627, Loss Train(MSE): 0.1251363297695527, R2 Train: 0.49945468092178924\n",
      "Epoch 8628, Loss Train(MSE): 0.1251363020316339, R2 Train: 0.4994547918734644\n",
      "Epoch 8629, Loss Train(MSE): 0.12513627216526899, R2 Train: 0.49945491133892406\n",
      "Epoch 8630, Loss Train(MSE): 0.12513624037521226, R2 Train: 0.49945503849915096\n",
      "Epoch 8631, Loss Train(MSE): 0.12513621285515128, R2 Train: 0.4994551485793949\n",
      "Epoch 8632, Loss Train(MSE): 0.12513618284838457, R2 Train: 0.4994552686064617\n",
      "Epoch 8633, Loss Train(MSE): 0.1251361511039895, R2 Train: 0.49945539558404195\n",
      "Epoch 8634, Loss Train(MSE): 0.12513612378839012, R2 Train: 0.4994555048464395\n",
      "Epoch 8635, Loss Train(MSE): 0.1251360936544085, R2 Train: 0.499455625382366\n",
      "Epoch 8636, Loss Train(MSE): 0.1251360619555365, R2 Train: 0.499455752177854\n",
      "Epoch 8637, Loss Train(MSE): 0.12513603483111535, R2 Train: 0.4994558606755386\n",
      "Epoch 8638, Loss Train(MSE): 0.12513600458299406, R2 Train: 0.49945598166802374\n",
      "Epoch 8639, Loss Train(MSE): 0.12513597292950757, R2 Train: 0.49945610828196974\n",
      "Epoch 8640, Loss Train(MSE): 0.12513594598309313, R2 Train: 0.49945621606762747\n",
      "Epoch 8641, Loss Train(MSE): 0.12513591563379686, R2 Train: 0.49945633746481255\n",
      "Epoch 8642, Loss Train(MSE): 0.12513588402555895, R2 Train: 0.4994564638977642\n",
      "Epoch 8643, Loss Train(MSE): 0.12513585724409065, R2 Train: 0.4994565710236374\n",
      "Epoch 8644, Loss Train(MSE): 0.12513582680647428, R2 Train: 0.4994566927741029\n",
      "Epoch 8645, Loss Train(MSE): 0.12513579524334903, R2 Train: 0.4994568190266039\n",
      "Epoch 8646, Loss Train(MSE): 0.1251357686138764, R2 Train: 0.4994569255444944\n",
      "Epoch 8647, Loss Train(MSE): 0.12513573810068584, R2 Train: 0.4994570475972566\n",
      "Epoch 8648, Loss Train(MSE): 0.12513570658253803, R2 Train: 0.4994571736698479\n",
      "Epoch 8649, Loss Train(MSE): 0.12513568009221973, R2 Train: 0.4994572796311211\n",
      "Epoch 8650, Loss Train(MSE): 0.12513564951609307, R2 Train: 0.4994574019356277\n",
      "Epoch 8651, Loss Train(MSE): 0.12513561804278836, R2 Train: 0.49945752782884656\n",
      "Epoch 8652, Loss Train(MSE): 0.1251355916788914, R2 Train: 0.4994576332844344\n",
      "Epoch 8653, Loss Train(MSE): 0.12513556105235943, R2 Train: 0.4994577557905623\n",
      "Epoch 8654, Loss Train(MSE): 0.12513552962376423, R2 Train: 0.4994578815049431\n",
      "Epoch 8655, Loss Train(MSE): 0.12513550337366303, R2 Train: 0.4994579865053479\n",
      "Epoch 8656, Loss Train(MSE): 0.12513547270915037, R2 Train: 0.4994581091633985\n",
      "Epoch 8657, Loss Train(MSE): 0.12513544132513196, R2 Train: 0.49945823469947215\n",
      "Epoch 8658, Loss Train(MSE): 0.12513541517630744, R2 Train: 0.49945833929477024\n",
      "Epoch 8659, Loss Train(MSE): 0.1251353844861333, R2 Train: 0.49945846205546685\n",
      "Epoch 8660, Loss Train(MSE): 0.12513535314655969, R2 Train: 0.49945858741376126\n",
      "Epoch 8661, Loss Train(MSE): 0.12513532708659839, R2 Train: 0.49945869165360646\n",
      "Epoch 8662, Loss Train(MSE): 0.12513529638297755, R2 Train: 0.4994588144680898\n",
      "Epoch 8663, Loss Train(MSE): 0.12513526508771752, R2 Train: 0.4994589396491299\n",
      "Epoch 8664, Loss Train(MSE): 0.12513523910431087, R2 Train: 0.4994590435827565\n",
      "Epoch 8665, Loss Train(MSE): 0.12513520839935424, R2 Train: 0.49945916640258303\n",
      "Epoch 8666, Loss Train(MSE): 0.1251351771482775, R2 Train: 0.49945929140689005\n",
      "Epoch 8667, Loss Train(MSE): 0.12513515122922078, R2 Train: 0.4994593950831169\n",
      "Epoch 8668, Loss Train(MSE): 0.12513512053493658, R2 Train: 0.49945951786025367\n",
      "Epoch 8669, Loss Train(MSE): 0.12513508932791342, R2 Train: 0.4994596426883463\n",
      "Epoch 8670, Loss Train(MSE): 0.12513506346110514, R2 Train: 0.4994597461555794\n",
      "Epoch 8671, Loss Train(MSE): 0.12513503278939953, R2 Train: 0.4994598688424019\n",
      "Epoch 8672, Loss Train(MSE): 0.12513500162630112, R2 Train: 0.49945999349479553\n",
      "Epoch 8673, Loss Train(MSE): 0.125134975799742, R2 Train: 0.499460096801032\n",
      "Epoch 8674, Loss Train(MSE): 0.1251349451624199, R2 Train: 0.49946021935032037\n",
      "Epoch 8675, Loss Train(MSE): 0.12513491404311808, R2 Train: 0.4994603438275277\n",
      "Epoch 8676, Loss Train(MSE): 0.1251348882449104, R2 Train: 0.4994604470203584\n",
      "Epoch 8677, Loss Train(MSE): 0.12513485765367638, R2 Train: 0.4994605693852945\n",
      "Epoch 8678, Loss Train(MSE): 0.12513482657804387, R2 Train: 0.49946069368782453\n",
      "Epoch 8679, Loss Train(MSE): 0.12513480079639036, R2 Train: 0.49946079681443856\n",
      "Epoch 8680, Loss Train(MSE): 0.12513477026284942, R2 Train: 0.49946091894860234\n",
      "Epoch 8681, Loss Train(MSE): 0.1251347392307596, R2 Train: 0.4994610430769616\n",
      "Epoch 8682, Loss Train(MSE): 0.125134713453963, R2 Train: 0.49946114618414805\n",
      "Epoch 8683, Loss Train(MSE): 0.1251346829896214, R2 Train: 0.49946126804151436\n",
      "Epoch 8684, Loss Train(MSE): 0.1251346520009483, R2 Train: 0.49946139199620676\n",
      "Epoch 8685, Loss Train(MSE): 0.12513462621741034, R2 Train: 0.49946149513035865\n",
      "Epoch 8686, Loss Train(MSE): 0.1251345958336764, R2 Train: 0.49946161666529443\n",
      "Epoch 8687, Loss Train(MSE): 0.12513456488829494, R2 Train: 0.49946174044682023\n",
      "Epoch 8688, Loss Train(MSE): 0.12513453908651545, R2 Train: 0.4994618436539382\n",
      "Epoch 8689, Loss Train(MSE): 0.1251345087947002, R2 Train: 0.4994619648211992\n",
      "Epoch 8690, Loss Train(MSE): 0.1251344778924859, R2 Train: 0.49946208843005635\n",
      "Epoch 8691, Loss Train(MSE): 0.12513445206106233, R2 Train: 0.4994621917557507\n",
      "Epoch 8692, Loss Train(MSE): 0.12513442187238044, R2 Train: 0.49946231251047823\n",
      "Epoch 8693, Loss Train(MSE): 0.12513439101320967, R2 Train: 0.49946243594716133\n",
      "Epoch 8694, Loss Train(MSE): 0.12513436514083595, R2 Train: 0.4994625394366562\n",
      "Epoch 8695, Loss Train(MSE): 0.12513433506640653, R2 Train: 0.49946265973437387\n",
      "Epoch 8696, Loss Train(MSE): 0.12513430425015626, R2 Train: 0.49946278299937497\n",
      "Epoch 8697, Loss Train(MSE): 0.1251342783256223, R2 Train: 0.4994628866975108\n",
      "Epoch 8698, Loss Train(MSE): 0.12513424837646947, R2 Train: 0.4994630064941221\n",
      "Epoch 8699, Loss Train(MSE): 0.1251342176030174, R2 Train: 0.4994631295879304\n",
      "Epoch 8700, Loss Train(MSE): 0.12513419161520822, R2 Train: 0.4994632335391671\n",
      "Epoch 8701, Loss Train(MSE): 0.12513416180226208, R2 Train: 0.49946335279095166\n",
      "Epoch 8702, Loss Train(MSE): 0.1251341310714866, R2 Train: 0.4994634757140536\n",
      "Epoch 8703, Loss Train(MSE): 0.12513410500938157, R2 Train: 0.4994635799624737\n",
      "Epoch 8704, Loss Train(MSE): 0.1251340753434788, R2 Train: 0.49946369862608475\n",
      "Epoch 8705, Loss Train(MSE): 0.12513404465525907, R2 Train: 0.4994638213789637\n",
      "Epoch 8706, Loss Train(MSE): 0.12513401850793107, R2 Train: 0.4994639259682757\n",
      "Epoch 8707, Loss Train(MSE): 0.12513398899981587, R2 Train: 0.4994640440007365\n",
      "Epoch 8708, Loss Train(MSE): 0.1251339583540316, R2 Train: 0.4994641665838736\n",
      "Epoch 8709, Loss Train(MSE): 0.12513393211064647, R2 Train: 0.49946427155741413\n",
      "Epoch 8710, Loss Train(MSE): 0.125133902770971, R2 Train: 0.499464388916116\n",
      "Epoch 8711, Loss Train(MSE): 0.12513387216750266, R2 Train: 0.49946451132998937\n",
      "Epoch 8712, Loss Train(MSE): 0.12513384581731832, R2 Train: 0.49946461673072673\n",
      "Epoch 8713, Loss Train(MSE): 0.1251338166566437, R2 Train: 0.49946473337342523\n",
      "Epoch 8714, Loss Train(MSE): 0.12513378609537246, R2 Train: 0.4994648556185102\n",
      "Epoch 8715, Loss Train(MSE): 0.12513375962773812, R2 Train: 0.4994649614890475\n",
      "Epoch 8716, Loss Train(MSE): 0.12513373065653505, R2 Train: 0.4994650773738598\n",
      "Epoch 8717, Loss Train(MSE): 0.12513370013734268, R2 Train: 0.4994651994506293\n",
      "Epoch 8718, Loss Train(MSE): 0.1251336735416983, R2 Train: 0.4994653058332068\n",
      "Epoch 8719, Loss Train(MSE): 0.1251336447703478, R2 Train: 0.4994654209186088\n",
      "Epoch 8720, Loss Train(MSE): 0.12513361429311676, R2 Train: 0.49946554282753297\n",
      "Epoch 8721, Loss Train(MSE): 0.12513358755899212, R2 Train: 0.4994656497640315\n",
      "Epoch 8722, Loss Train(MSE): 0.1251335589977862, R2 Train: 0.49946576400885523\n",
      "Epoch 8723, Loss Train(MSE): 0.12513352856239962, R2 Train: 0.4994658857504015\n",
      "Epoch 8724, Loss Train(MSE): 0.12513350167941378, R2 Train: 0.49946599328234487\n",
      "Epoch 8725, Loss Train(MSE): 0.12513347333855618, R2 Train: 0.4994661066457753\n",
      "Epoch 8726, Loss Train(MSE): 0.1251334429448979, R2 Train: 0.4994662282204084\n",
      "Epoch 8727, Loss Train(MSE): 0.12513341590275834, R2 Train: 0.49946633638896665\n",
      "Epoch 8728, Loss Train(MSE): 0.12513338779236527, R2 Train: 0.4994664488305389\n",
      "Epoch 8729, Loss Train(MSE): 0.12513335744031967, R2 Train: 0.4994665702387213\n",
      "Epoch 8730, Loss Train(MSE): 0.1251333302288217, R2 Train: 0.4994666790847132\n",
      "Epoch 8731, Loss Train(MSE): 0.12513330235892245, R2 Train: 0.4994667905643102\n",
      "Epoch 8732, Loss Train(MSE): 0.12513327204837454, R2 Train: 0.4994669118065018\n",
      "Epoch 8733, Loss Train(MSE): 0.1251332446574006, R2 Train: 0.49946702137039756\n",
      "Epoch 8734, Loss Train(MSE): 0.12513321703793837, R2 Train: 0.49946713184824654\n",
      "Epoch 8735, Loss Train(MSE): 0.12513318676877386, R2 Train: 0.49946725292490457\n",
      "Epoch 8736, Loss Train(MSE): 0.1251331591882927, R2 Train: 0.49946736324682917\n",
      "Epoch 8737, Loss Train(MSE): 0.12513313182912514, R2 Train: 0.49946747268349945\n",
      "Epoch 8738, Loss Train(MSE): 0.12513310160123034, R2 Train: 0.49946759359507864\n",
      "Epoch 8739, Loss Train(MSE): 0.12513307382129654, R2 Train: 0.49946770471481383\n",
      "Epoch 8740, Loss Train(MSE): 0.12513304673219633, R2 Train: 0.4994678130712147\n",
      "Epoch 8741, Loss Train(MSE): 0.1251330165454582, R2 Train: 0.49946793381816723\n",
      "Epoch 8742, Loss Train(MSE): 0.12513298855621133, R2 Train: 0.4994680457751547\n",
      "Epoch 8743, Loss Train(MSE): 0.1251329617468672, R2 Train: 0.4994681530125312\n",
      "Epoch 8744, Loss Train(MSE): 0.1251329316011733, R2 Train: 0.49946827359530677\n",
      "Epoch 8745, Loss Train(MSE): 0.12513290339283725, R2 Train: 0.499468386428651\n",
      "Epoch 8746, Loss Train(MSE): 0.12513287687285435, R2 Train: 0.4994684925085826\n",
      "Epoch 8747, Loss Train(MSE): 0.1251328467680929, R2 Train: 0.49946861292762845\n",
      "Epoch 8748, Loss Train(MSE): 0.12513281833097528, R2 Train: 0.4994687266760989\n",
      "Epoch 8749, Loss Train(MSE): 0.1251327921098759, R2 Train: 0.4994688315604964\n",
      "Epoch 8750, Loss Train(MSE): 0.1251327620459356, R2 Train: 0.4994689518162576\n",
      "Epoch 8751, Loss Train(MSE): 0.12513273337042716, R2 Train: 0.49946906651829137\n",
      "Epoch 8752, Loss Train(MSE): 0.12513270745765148, R2 Train: 0.49946917016939407\n",
      "Epoch 8753, Loss Train(MSE): 0.12513267743442172, R2 Train: 0.4994692902623131\n",
      "Epoch 8754, Loss Train(MSE): 0.12513264851099545, R2 Train: 0.4994694059560182\n",
      "Epoch 8755, Loss Train(MSE): 0.12513262291590216, R2 Train: 0.49946950833639137\n",
      "Epoch 8756, Loss Train(MSE): 0.12513259293327295, R2 Train: 0.4994696282669082\n",
      "Epoch 8757, Loss Train(MSE): 0.12513256375248363, R2 Train: 0.49946974499006547\n",
      "Epoch 8758, Loss Train(MSE): 0.1251325384843504, R2 Train: 0.49946984606259837\n",
      "Epoch 8759, Loss Train(MSE): 0.1251325085422123, R2 Train: 0.4994699658311508\n",
      "Epoch 8760, Loss Train(MSE): 0.12513247909469583, R2 Train: 0.4994700836212167\n",
      "Epoch 8761, Loss Train(MSE): 0.1251324541627202, R2 Train: 0.4994701833491192\n",
      "Epoch 8762, Loss Train(MSE): 0.1251324242609643, R2 Train: 0.4994703029561428\n",
      "Epoch 8763, Loss Train(MSE): 0.12513239453743705, R2 Train: 0.4994704218502518\n",
      "Epoch 8764, Loss Train(MSE): 0.12513236995073682, R2 Train: 0.49947052019705274\n",
      "Epoch 8765, Loss Train(MSE): 0.12513234008925492, R2 Train: 0.49947063964298033\n",
      "Epoch 8766, Loss Train(MSE): 0.1251323102441478, R2 Train: 0.49947075902340876\n",
      "Epoch 8767, Loss Train(MSE): 0.1251322856843344, R2 Train: 0.4994708572626624\n",
      "Epoch 8768, Loss Train(MSE): 0.12513225602808825, R2 Train: 0.499470975887647\n",
      "Epoch 8769, Loss Train(MSE): 0.12513222622309428, R2 Train: 0.4994710951076229\n",
      "Epoch 8770, Loss Train(MSE): 0.1251322013571308, R2 Train: 0.4994711945714768\n",
      "Epoch 8771, Loss Train(MSE): 0.12513217207589755, R2 Train: 0.4994713116964098\n",
      "Epoch 8772, Loss Train(MSE): 0.12513214231090988, R2 Train: 0.49947143075636047\n",
      "Epoch 8773, Loss Train(MSE): 0.12513211712981243, R2 Train: 0.4994715314807503\n",
      "Epoch 8774, Loss Train(MSE): 0.12513208823241312, R2 Train: 0.4994716470703475\n",
      "Epoch 8775, Loss Train(MSE): 0.12513205850732556, R2 Train: 0.49947176597069776\n",
      "Epoch 8776, Loss Train(MSE): 0.12513203300218773, R2 Train: 0.4994718679912491\n",
      "Epoch 8777, Loss Train(MSE): 0.12513200449736664, R2 Train: 0.49947198201053344\n",
      "Epoch 8778, Loss Train(MSE): 0.1251319748120736, R2 Train: 0.49947210075170556\n",
      "Epoch 8779, Loss Train(MSE): 0.12513194897406585, R2 Train: 0.4994722041037366\n",
      "Epoch 8780, Loss Train(MSE): 0.12513192087049121, R2 Train: 0.49947231651803514\n",
      "Epoch 8781, Loss Train(MSE): 0.12513189122488752, R2 Train: 0.4994724351004499\n",
      "Epoch 8782, Loss Train(MSE): 0.1251318650452567, R2 Train: 0.4994725398189732\n",
      "Epoch 8783, Loss Train(MSE): 0.12513183735152109, R2 Train: 0.49947265059391566\n",
      "Epoch 8784, Loss Train(MSE): 0.12513180774550223, R2 Train: 0.4994727690179911\n",
      "Epoch 8785, Loss Train(MSE): 0.12513178121557086, R2 Train: 0.49947287513771654\n",
      "Epoch 8786, Loss Train(MSE): 0.12513175394019213, R2 Train: 0.4994729842392315\n",
      "Epoch 8787, Loss Train(MSE): 0.12513172437365402, R2 Train: 0.4994731025053839\n",
      "Epoch 8788, Loss Train(MSE): 0.12513169748481973, R2 Train: 0.4994732100607211\n",
      "Epoch 8789, Loss Train(MSE): 0.12513167063624125, R2 Train: 0.499473317455035\n",
      "Epoch 8790, Loss Train(MSE): 0.1251316411090805, R2 Train: 0.49947343556367796\n",
      "Epoch 8791, Loss Train(MSE): 0.1251316138528154, R2 Train: 0.4994735445887384\n",
      "Epoch 8792, Loss Train(MSE): 0.12513158743940678, R2 Train: 0.4994736502423729\n",
      "Epoch 8793, Loss Train(MSE): 0.12513155795152048, R2 Train: 0.4994737681939181\n",
      "Epoch 8794, Loss Train(MSE): 0.12513153031937072, R2 Train: 0.49947387872251714\n",
      "Epoch 8795, Loss Train(MSE): 0.12513150434942846, R2 Train: 0.49947398260228615\n",
      "Epoch 8796, Loss Train(MSE): 0.12513147490071413, R2 Train: 0.49947410039714346\n",
      "Epoch 8797, Loss Train(MSE): 0.1251314468842992, R2 Train: 0.4994742124628032\n",
      "Epoch 8798, Loss Train(MSE): 0.12513142136604716, R2 Train: 0.49947431453581137\n",
      "Epoch 8799, Loss Train(MSE): 0.12513139195640294, R2 Train: 0.49947443217438825\n",
      "Epoch 8800, Loss Train(MSE): 0.12513136354741508, R2 Train: 0.4994745458103397\n",
      "Epoch 8801, Loss Train(MSE): 0.125131338489005, R2 Train: 0.49947464604397995\n",
      "Epoch 8802, Loss Train(MSE): 0.1251313091183296, R2 Train: 0.49947476352668163\n",
      "Epoch 8803, Loss Train(MSE): 0.12513128030853332, R2 Train: 0.4994748787658667\n",
      "Epoch 8804, Loss Train(MSE): 0.12513125571804554, R2 Train: 0.49947497712781785\n",
      "Epoch 8805, Loss Train(MSE): 0.12513122638623805, R2 Train: 0.4994750944550478\n",
      "Epoch 8806, Loss Train(MSE): 0.12513119716746954, R2 Train: 0.49947521133012185\n",
      "Epoch 8807, Loss Train(MSE): 0.1251311730529134, R2 Train: 0.49947530778834637\n",
      "Epoch 8808, Loss Train(MSE): 0.1251311437598736, R2 Train: 0.49947542496050557\n",
      "Epoch 8809, Loss Train(MSE): 0.12513111448256217, R2 Train: 0.4994755420697513\n",
      "Epoch 8810, Loss Train(MSE): 0.12513109013462076, R2 Train: 0.49947563946151696\n",
      "Epoch 8811, Loss Train(MSE): 0.12513106124018994, R2 Train: 0.49947575503924024\n",
      "Epoch 8812, Loss Train(MSE): 0.12513103200149706, R2 Train: 0.4994758719940118\n",
      "Epoch 8813, Loss Train(MSE): 0.12513100721733122, R2 Train: 0.4994759711306751\n",
      "Epoch 8814, Loss Train(MSE): 0.12513097882571017, R2 Train: 0.4994760846971593\n",
      "Epoch 8815, Loss Train(MSE): 0.12513094962553675, R2 Train: 0.499476201497853\n",
      "Epoch 8816, Loss Train(MSE): 0.12513092439725187, R2 Train: 0.4994763024109925\n",
      "Epoch 8817, Loss Train(MSE): 0.12513089651618342, R2 Train: 0.4994764139352663\n",
      "Epoch 8818, Loss Train(MSE): 0.12513086735443096, R2 Train: 0.49947653058227615\n",
      "Epoch 8819, Loss Train(MSE): 0.12513084167420144, R2 Train: 0.49947663330319425\n",
      "Epoch 8820, Loss Train(MSE): 0.12513081431135986, R2 Train: 0.49947674275456055\n",
      "Epoch 8821, Loss Train(MSE): 0.12513078518793044, R2 Train: 0.4994768592482782\n",
      "Epoch 8822, Loss Train(MSE): 0.12513075904799928, R2 Train: 0.4994769638080029\n",
      "Epoch 8823, Loss Train(MSE): 0.1251307322109912, R2 Train: 0.49947707115603523\n",
      "Epoch 8824, Loss Train(MSE): 0.1251307031257872, R2 Train: 0.4994771874968512\n",
      "Epoch 8825, Loss Train(MSE): 0.12513067651846535, R2 Train: 0.4994772939261386\n",
      "Epoch 8826, Loss Train(MSE): 0.12513065021482997, R2 Train: 0.4994773991406801\n",
      "Epoch 8827, Loss Train(MSE): 0.12513062116775447, R2 Train: 0.4994775153289821\n",
      "Epoch 8828, Loss Train(MSE): 0.1251305940854204, R2 Train: 0.4994776236583184\n",
      "Epoch 8829, Loss Train(MSE): 0.12513056832263017, R2 Train: 0.4994777267094793\n",
      "Epoch 8830, Loss Train(MSE): 0.12513053931358664, R2 Train: 0.49947784274565343\n",
      "Epoch 8831, Loss Train(MSE): 0.12513051174868575, R2 Train: 0.499477953005257\n",
      "Epoch 8832, Loss Train(MSE): 0.12513048653414682, R2 Train: 0.4994780538634127\n",
      "Epoch 8833, Loss Train(MSE): 0.12513045756303914, R2 Train: 0.4994781697478434\n",
      "Epoch 8834, Loss Train(MSE): 0.12513042950808323, R2 Train: 0.4994782819676671\n",
      "Epoch 8835, Loss Train(MSE): 0.1251304048491362, R2 Train: 0.4994783806034552\n",
      "Epoch 8836, Loss Train(MSE): 0.12513037591586879, R2 Train: 0.49947849633652486\n",
      "Epoch 8837, Loss Train(MSE): 0.12513034736343562, R2 Train: 0.49947861054625753\n",
      "Epoch 8838, Loss Train(MSE): 0.12513032326735551, R2 Train: 0.49947870693057794\n",
      "Epoch 8839, Loss Train(MSE): 0.12513029437183326, R2 Train: 0.49947882251266695\n",
      "Epoch 8840, Loss Train(MSE): 0.12513026549160078, R2 Train: 0.4994789380335969\n",
      "Epoch 8841, Loss Train(MSE): 0.1251302416112817, R2 Train: 0.4994790335548732\n",
      "Epoch 8842, Loss Train(MSE): 0.12513021293185175, R2 Train: 0.499479148272593\n",
      "Epoch 8843, Loss Train(MSE): 0.12513018408922297, R2 Train: 0.49947926364310813\n",
      "Epoch 8844, Loss Train(MSE): 0.1251301596860943, R2 Train: 0.49947936125562276\n",
      "Epoch 8845, Loss Train(MSE): 0.12513013159450778, R2 Train: 0.4994794736219689\n",
      "Epoch 8846, Loss Train(MSE): 0.1251301027893888, R2 Train: 0.4994795888424448\n",
      "Epoch 8847, Loss Train(MSE): 0.12513007785627747, R2 Train: 0.4994796885748901\n",
      "Epoch 8848, Loss Train(MSE): 0.12513005035956273, R2 Train: 0.49947979856174907\n",
      "Epoch 8849, Loss Train(MSE): 0.1251300215918601, R2 Train: 0.4994799136325596\n",
      "Epoch 8850, Loss Train(MSE): 0.1251299961216566, R2 Train: 0.4994800155133736\n",
      "Epoch 8851, Loss Train(MSE): 0.12512996922677913, R2 Train: 0.4994801230928835\n",
      "Epoch 8852, Loss Train(MSE): 0.1251299404963999, R2 Train: 0.49948023801440045\n",
      "Epoch 8853, Loss Train(MSE): 0.12512991448205768, R2 Train: 0.4994803420717693\n",
      "Epoch 8854, Loss Train(MSE): 0.12512988819592058, R2 Train: 0.4994804472163177\n",
      "Epoch 8855, Loss Train(MSE): 0.1251298595027722, R2 Train: 0.4994805619889112\n",
      "Epoch 8856, Loss Train(MSE): 0.1251298329373073, R2 Train: 0.49948066825077075\n",
      "Epoch 8857, Loss Train(MSE): 0.1251298072667518, R2 Train: 0.4994807709329928\n",
      "Epoch 8858, Loss Train(MSE): 0.12512977861074215, R2 Train: 0.4994808855570314\n",
      "Epoch 8859, Loss Train(MSE): 0.12512975148723282, R2 Train: 0.4994809940510687\n",
      "Epoch 8860, Loss Train(MSE): 0.12512972643903855, R2 Train: 0.4994810942438458\n",
      "Epoch 8861, Loss Train(MSE): 0.12512969782007602, R2 Train: 0.4994812087196959\n",
      "Epoch 8862, Loss Train(MSE): 0.12512967013166196, R2 Train: 0.49948131947335217\n",
      "Epoch 8863, Loss Train(MSE): 0.12512964571254773, R2 Train: 0.4994814171498091\n",
      "Epoch 8864, Loss Train(MSE): 0.1251296171305411, R2 Train: 0.4994815314778356\n",
      "Epoch 8865, Loss Train(MSE): 0.1251295888704232, R2 Train: 0.49948164451830723\n",
      "Epoch 8866, Loss Train(MSE): 0.12512956508704723, R2 Train: 0.4994817396518111\n",
      "Epoch 8867, Loss Train(MSE): 0.12512953654190576, R2 Train: 0.499481853832377\n",
      "Epoch 8868, Loss Train(MSE): 0.12512950801167672, R2 Train: 0.49948196795329314\n",
      "Epoch 8869, Loss Train(MSE): 0.12512948425369844, R2 Train: 0.49948206298520625\n",
      "Epoch 8870, Loss Train(MSE): 0.12512945605505885, R2 Train: 0.4994821757797646\n",
      "Epoch 8871, Loss Train(MSE): 0.12512942756155992, R2 Train: 0.4994822897537603\n",
      "Epoch 8872, Loss Train(MSE): 0.1251294032081646, R2 Train: 0.4994823871673416\n",
      "Epoch 8873, Loss Train(MSE): 0.1251293756686356, R2 Train: 0.4994824973254576\n",
      "Epoch 8874, Loss Train(MSE): 0.1251293472117772, R2 Train: 0.49948261115289116\n",
      "Epoch 8875, Loss Train(MSE): 0.12512932225639817, R2 Train: 0.49948271097440733\n",
      "Epoch 8876, Loss Train(MSE): 0.12512929538240783, R2 Train: 0.4994828184703687\n",
      "Epoch 8877, Loss Train(MSE): 0.1251292669621007, R2 Train: 0.4994829321515972\n",
      "Epoch 8878, Loss Train(MSE): 0.12512924139823017, R2 Train: 0.4994830344070793\n",
      "Epoch 8879, Loss Train(MSE): 0.1251292151961481, R2 Train: 0.49948313921540755\n",
      "Epoch 8880, Loss Train(MSE): 0.12512918681230348, R2 Train: 0.4994832527507861\n",
      "Epoch 8881, Loss Train(MSE): 0.12512916063349216, R2 Train: 0.49948335746603134\n",
      "Epoch 8882, Loss Train(MSE): 0.12512913510963022, R2 Train: 0.4994834595614791\n",
      "Epoch 8883, Loss Train(MSE): 0.12512910676215971, R2 Train: 0.49948357295136114\n",
      "Epoch 8884, Loss Train(MSE): 0.12512907996201642, R2 Train: 0.49948368015193434\n",
      "Epoch 8885, Loss Train(MSE): 0.1251290551226289, R2 Train: 0.4994837795094844\n",
      "Epoch 8886, Loss Train(MSE): 0.1251290268114445, R2 Train: 0.499483892754222\n",
      "Epoch 8887, Loss Train(MSE): 0.12512899938363553, R2 Train: 0.4994840024654579\n",
      "Epoch 8888, Loss Train(MSE): 0.1251289752349198, R2 Train: 0.49948409906032076\n",
      "Epoch 8889, Loss Train(MSE): 0.12512894695993404, R2 Train: 0.49948421216026384\n",
      "Epoch 8890, Loss Train(MSE): 0.12512891889818287, R2 Train: 0.49948432440726853\n",
      "Epoch 8891, Loss Train(MSE): 0.12512889544627975, R2 Train: 0.499484418214881\n",
      "Epoch 8892, Loss Train(MSE): 0.12512886720740546, R2 Train: 0.49948453117037817\n",
      "Epoch 8893, Loss Train(MSE): 0.1251288389831206, R2 Train: 0.4994846440675176\n",
      "Epoch 8894, Loss Train(MSE): 0.1251288152785574, R2 Train: 0.4994847388857704\n",
      "Epoch 8895, Loss Train(MSE): 0.12512878755472157, R2 Train: 0.49948484978111374\n",
      "Epoch 8896, Loss Train(MSE): 0.1251287593664185, R2 Train: 0.49948496253432595\n",
      "Epoch 8897, Loss Train(MSE): 0.12512873500556237, R2 Train: 0.4994850599777505\n",
      "Epoch 8898, Loss Train(MSE): 0.12512870800056167, R2 Train: 0.49948516799775333\n",
      "Epoch 8899, Loss Train(MSE): 0.12512867984815437, R2 Train: 0.4994852806073825\n",
      "Epoch 8900, Loss Train(MSE): 0.12512865482494726, R2 Train: 0.499485380700211\n",
      "Epoch 8901, Loss Train(MSE): 0.12512862854470608, R2 Train: 0.4994854858211757\n",
      "Epoch 8902, Loss Train(MSE): 0.1251286004281088, R2 Train: 0.4994855982875648\n",
      "Epoch 8903, Loss Train(MSE): 0.12512857473654773, R2 Train: 0.4994857010538091\n",
      "Epoch 8904, Loss Train(MSE): 0.12512854918693592, R2 Train: 0.49948580325225633\n",
      "Epoch 8905, Loss Train(MSE): 0.1251285211060634, R2 Train: 0.4994859155757464\n",
      "Epoch 8906, Loss Train(MSE): 0.12512849474020007, R2 Train: 0.4994860210391997\n",
      "Epoch 8907, Loss Train(MSE): 0.12512846992703336, R2 Train: 0.49948612029186656\n",
      "Epoch 8908, Loss Train(MSE): 0.12512844188180075, R2 Train: 0.499486232472797\n",
      "Epoch 8909, Loss Train(MSE): 0.12512841483574108, R2 Train: 0.4994863406570357\n",
      "Epoch 8910, Loss Train(MSE): 0.1251283907647815, R2 Train: 0.499486436940874\n",
      "Epoch 8911, Loss Train(MSE): 0.12513423099234122, R2 Train: 0.4994630760306351\n",
      "Epoch 8912, Loss Train(MSE): 0.12514162100771647, R2 Train: 0.4994335159691341\n",
      "Epoch 8913, Loss Train(MSE): 0.12514155717472092, R2 Train: 0.4994337713011163\n",
      "Epoch 8914, Loss Train(MSE): 0.12514148392765528, R2 Train: 0.49943406428937887\n",
      "Epoch 8915, Loss Train(MSE): 0.12514141796840564, R2 Train: 0.49943432812637745\n",
      "Epoch 8916, Loss Train(MSE): 0.1251413475498394, R2 Train: 0.49943460980064236\n",
      "Epoch 8917, Loss Train(MSE): 0.12514127881369136, R2 Train: 0.49943488474523456\n",
      "Epoch 8918, Loss Train(MSE): 0.12514121155089764, R2 Train: 0.49943515379640946\n",
      "Epoch 8919, Loss Train(MSE): 0.12514114001910395, R2 Train: 0.4994354399235842\n",
      "Epoch 8920, Loss Train(MSE): 0.12514107592879403, R2 Train: 0.49943569628482387\n",
      "Epoch 8921, Loss Train(MSE): 0.12514100343697912, R2 Train: 0.4994359862520835\n",
      "Epoch 8922, Loss Train(MSE): 0.12514093883427055, R2 Train: 0.4994362446629178\n",
      "Epoch 8923, Loss Train(MSE): 0.12514086840065125, R2 Train: 0.499436526397395\n",
      "Epoch 8924, Loss Train(MSE): 0.12514080093402302, R2 Train: 0.4994367962639079\n",
      "Epoch 8925, Loss Train(MSE): 0.12514073373600293, R2 Train: 0.4994370650559883\n",
      "Epoch 8926, Loss Train(MSE): 0.12514066338754107, R2 Train: 0.49943734644983573\n",
      "Epoch 8927, Loss Train(MSE): 0.12514059944106268, R2 Train: 0.4994376022357493\n",
      "Epoch 8928, Loss Train(MSE): 0.1251405276898546, R2 Train: 0.49943788924058163\n",
      "Epoch 8929, Loss Train(MSE): 0.12514046402388437, R2 Train: 0.4994381439044625\n",
      "Epoch 8930, Loss Train(MSE): 0.1251403939696294, R2 Train: 0.4994384241214824\n",
      "Epoch 8931, Loss Train(MSE): 0.12514032735597214, R2 Train: 0.49943869057611145\n",
      "Epoch 8932, Loss Train(MSE): 0.12514026061411482, R2 Train: 0.49943895754354073\n",
      "Epoch 8933, Loss Train(MSE): 0.12514019103565965, R2 Train: 0.4994392358573614\n",
      "Epoch 8934, Loss Train(MSE): 0.1251401276214007, R2 Train: 0.4994394895143972\n",
      "Epoch 8935, Loss Train(MSE): 0.12514005659660798, R2 Train: 0.49943977361356806\n",
      "Epoch 8936, Loss Train(MSE): 0.1251399934609126, R2 Train: 0.49944002615634964\n",
      "Epoch 8937, Loss Train(MSE): 0.12513992416788902, R2 Train: 0.4994403033284439\n",
      "Epoch 8938, Loss Train(MSE): 0.1251398580038802, R2 Train: 0.4994405679844792\n",
      "Epoch 8939, Loss Train(MSE): 0.12513979209712636, R2 Train: 0.49944083161149455\n",
      "Epoch 8940, Loss Train(MSE): 0.1251397228884676, R2 Train: 0.49944110844612954\n",
      "Epoch 8941, Loss Train(MSE): 0.1251396603824682, R2 Train: 0.49944135847012716\n",
      "Epoch 8942, Loss Train(MSE): 0.12513959007033063, R2 Train: 0.4994416397186775\n",
      "Epoch 8943, Loss Train(MSE): 0.12513952707134482, R2 Train: 0.4994418917146207\n",
      "Epoch 8944, Loss Train(MSE): 0.1251394589092724, R2 Train: 0.49944216436291045\n",
      "Epoch 8945, Loss Train(MSE): 0.12513939280437938, R2 Train: 0.49944242878248246\n",
      "Epoch 8946, Loss Train(MSE): 0.1251393280996204, R2 Train: 0.49944268760151844\n",
      "Epoch 8947, Loss Train(MSE): 0.12513925887323024, R2 Train: 0.49944296450707903\n",
      "Epoch 8948, Loss Train(MSE): 0.12513919763957843, R2 Train: 0.4994432094416863\n",
      "Epoch 8949, Loss Train(MSE): 0.125139128026747, R2 Train: 0.49944348789301196\n",
      "Epoch 8950, Loss Train(MSE): 0.12513906478338013, R2 Train: 0.49944374086647947\n",
      "Epoch 8951, Loss Train(MSE): 0.12513899811021917, R2 Train: 0.4994440075591233\n",
      "Epoch 8952, Loss Train(MSE): 0.1251389316862799, R2 Train: 0.4994442732548804\n",
      "Epoch 8953, Loss Train(MSE): 0.12513886853874223, R2 Train: 0.4994445258450311\n",
      "Epoch 8954, Loss Train(MSE): 0.12513879942254086, R2 Train: 0.49944480230983657\n",
      "Epoch 8955, Loss Train(MSE): 0.12513873881379212, R2 Train: 0.49944504474483153\n",
      "Epoch 8956, Loss Train(MSE): 0.12513867038749094, R2 Train: 0.49944531845003626\n",
      "Epoch 8957, Loss Train(MSE): 0.12513860654066747, R2 Train: 0.4994455738373301\n",
      "Epoch 8958, Loss Train(MSE): 0.12513854169302904, R2 Train: 0.49944583322788383\n",
      "Epoch 8959, Loss Train(MSE): 0.125138474593782, R2 Train: 0.499446101624872\n",
      "Epoch 8960, Loss Train(MSE): 0.12513841333744855, R2 Train: 0.4994463466502058\n",
      "Epoch 8961, Loss Train(MSE): 0.12513834489847703, R2 Train: 0.4994466204060919\n",
      "Epoch 8962, Loss Train(MSE): 0.12513828339847977, R2 Train: 0.49944686640608094\n",
      "Epoch 8963, Loss Train(MSE): 0.12513821706973735, R2 Train: 0.4994471317210506\n",
      "Epoch 8964, Loss Train(MSE): 0.12513815226187094, R2 Train: 0.4994473909525162\n",
      "Epoch 8965, Loss Train(MSE): 0.12513808957554465, R2 Train: 0.4994476416978214\n",
      "Epoch 8966, Loss Train(MSE): 0.1251380216176681, R2 Train: 0.4994479135293276\n",
      "Epoch 8967, Loss Train(MSE): 0.12513796224893425, R2 Train: 0.499448151004263\n",
      "Epoch 8968, Loss Train(MSE): 0.12513789464357478, R2 Train: 0.4994484214257009\n",
      "Epoch 8969, Loss Train(MSE): 0.1251378319130106, R2 Train: 0.49944867234795765\n",
      "Epoch 8970, Loss Train(MSE): 0.12513776799978296, R2 Train: 0.49944892800086815\n",
      "Epoch 8971, Loss Train(MSE): 0.1251377018942059, R2 Train: 0.4994491924231764\n",
      "Epoch 8972, Loss Train(MSE): 0.1251376416846687, R2 Train: 0.4994494332613252\n",
      "Epoch 8973, Loss Train(MSE): 0.12513757438302223, R2 Train: 0.4994497024679111\n",
      "Epoch 8974, Loss Train(MSE): 0.12513751351078428, R2 Train: 0.4994499459568629\n",
      "Epoch 8975, Loss Train(MSE): 0.12513744857888315, R2 Train: 0.4994502056844674\n",
      "Epoch 8976, Loss Train(MSE): 0.12513738427961085, R2 Train: 0.4994504628815566\n",
      "Epoch 8977, Loss Train(MSE): 0.1251373230992949, R2 Train: 0.4994507076028204\n",
      "Epoch 8978, Loss Train(MSE): 0.12513725626395542, R2 Train: 0.49945097494417834\n",
      "Epoch 8979, Loss Train(MSE): 0.12513719704522153, R2 Train: 0.4994512118191139\n",
      "Epoch 8980, Loss Train(MSE): 0.12513713128892118, R2 Train: 0.4994514748443153\n",
      "Epoch 8981, Loss Train(MSE): 0.12513706859253274, R2 Train: 0.49945172562986906\n",
      "Epoch 8982, Loss Train(MSE): 0.12513700663439342, R2 Train: 0.4994519734624263\n",
      "Epoch 8983, Loss Train(MSE): 0.12513694044826484, R2 Train: 0.4994522382069406\n",
      "Epoch 8984, Loss Train(MSE): 0.12513688229882494, R2 Train: 0.49945247080470023\n",
      "Epoch 8985, Loss Train(MSE): 0.12513681609971422, R2 Train: 0.4994527356011431\n",
      "Epoch 8986, Loss Train(MSE): 0.12513675479795153, R2 Train: 0.4994529808081939\n",
      "Epoch 8987, Loss Train(MSE): 0.12513669226000365, R2 Train: 0.4994532309599854\n",
      "Epoch 8988, Loss Train(MSE): 0.12513662741971096, R2 Train: 0.49945349032115616\n",
      "Epoch 8989, Loss Train(MSE): 0.12513656873531817, R2 Train: 0.4994537250587273\n",
      "Epoch 8990, Loss Train(MSE): 0.12513650298842022, R2 Train: 0.49945398804631913\n",
      "Epoch 8991, Loss Train(MSE): 0.1251364428866349, R2 Train: 0.49945422845346044\n",
      "Epoch 8992, Loss Train(MSE): 0.12513637995346857, R2 Train: 0.4994544801861257\n",
      "Epoch 8993, Loss Train(MSE): 0.1251363162656871, R2 Train: 0.49945473493725157\n",
      "Epoch 8994, Loss Train(MSE): 0.12513625722968444, R2 Train: 0.49945497108126224\n",
      "Epoch 8995, Loss Train(MSE): 0.1251361919292835, R2 Train: 0.49945523228286604\n",
      "Epoch 8996, Loss Train(MSE): 0.12513613283698982, R2 Train: 0.4994554686520407\n",
      "Epoch 8997, Loss Train(MSE): 0.12513606968922739, R2 Train: 0.49945572124309046\n",
      "Epoch 8998, Loss Train(MSE): 0.12513600696476354, R2 Train: 0.49945597214094584\n",
      "Epoch 8999, Loss Train(MSE): 0.12513594775655545, R2 Train: 0.4994562089737782\n",
      "Epoch 9000, Loss Train(MSE): 0.12513588289704589, R2 Train: 0.49945646841181646\n",
      "Epoch 9001, Loss Train(MSE): 0.12513582462782818, R2 Train: 0.49945670148868726\n",
      "Epoch 9002, Loss Train(MSE): 0.12513576144221092, R2 Train: 0.4994569542311563\n",
      "Epoch 9003, Loss Train(MSE): 0.12513569949591083, R2 Train: 0.4994572020163567\n",
      "Epoch 9004, Loss Train(MSE): 0.1251356402910488, R2 Train: 0.49945743883580485\n",
      "Epoch 9005, Loss Train(MSE): 0.12513557586693186, R2 Train: 0.49945769653227257\n",
      "Epoch 9006, Loss Train(MSE): 0.12513551823835478, R2 Train: 0.4994579270465809\n",
      "Epoch 9007, Loss Train(MSE): 0.12513545518782715, R2 Train: 0.4994581792486914\n",
      "Epoch 9008, Loss Train(MSE): 0.1251353938384876, R2 Train: 0.4994584246460496\n",
      "Epoch 9009, Loss Train(MSE): 0.12513533480875397, R2 Train: 0.49945866076498413\n",
      "Epoch 9010, Loss Train(MSE): 0.1251352708146349, R2 Train: 0.49945891674146037\n",
      "Epoch 9011, Loss Train(MSE): 0.12513521364815586, R2 Train: 0.49945914540737657\n",
      "Epoch 9012, Loss Train(MSE): 0.1251351509019478, R2 Train: 0.4994593963922088\n",
      "Epoch 9013, Loss Train(MSE): 0.12513508997222933, R2 Train: 0.4994596401110827\n",
      "Epoch 9014, Loss Train(MSE): 0.12513503128571893, R2 Train: 0.4994598748571243\n",
      "Epoch 9015, Loss Train(MSE): 0.12513496771630403, R2 Train: 0.4994601291347839\n",
      "Epoch 9016, Loss Train(MSE): 0.12513491083718764, R2 Train: 0.49946035665124944\n",
      "Epoch 9017, Loss Train(MSE): 0.12513484856089502, R2 Train: 0.49946060575641993\n",
      "Epoch 9018, Loss Train(MSE): 0.1251347878772373, R2 Train: 0.49946084849105077\n",
      "Epoch 9019, Loss Train(MSE): 0.125134729698437, R2 Train: 0.499461081206252\n",
      "Epoch 9020, Loss Train(MSE): 0.1251346665485307, R2 Train: 0.4994613338058772\n",
      "Epoch 9021, Loss Train(MSE): 0.12513460978576596, R2 Train: 0.49946156085693616\n",
      "Epoch 9022, Loss Train(MSE): 0.12513454814142871, R2 Train: 0.49946180743428514\n",
      "Epoch 9023, Loss Train(MSE): 0.12513448753396839, R2 Train: 0.49946204986412646\n",
      "Epoch 9024, Loss Train(MSE): 0.12513443002383462, R2 Train: 0.4994622799046615\n",
      "Epoch 9025, Loss Train(MSE): 0.12513436728833705, R2 Train: 0.4994625308466518\n",
      "Epoch 9026, Loss Train(MSE): 0.125134310474556, R2 Train: 0.499462758101776\n",
      "Epoch 9027, Loss Train(MSE): 0.1251342496207345, R2 Train: 0.499463001517062\n",
      "Epoch 9028, Loss Train(MSE): 0.12513418892322467, R2 Train: 0.4994632443071013\n",
      "Epoch 9029, Loss Train(MSE): 0.12513413223925923, R2 Train: 0.49946347104296307\n",
      "Epoch 9030, Loss Train(MSE): 0.1251340699131634, R2 Train: 0.49946372034734643\n",
      "Epoch 9031, Loss Train(MSE): 0.12513401288456247, R2 Train: 0.49946394846175013\n",
      "Epoch 9032, Loss Train(MSE): 0.12513395297641206, R2 Train: 0.4994641880943518\n",
      "Epoch 9033, Loss Train(MSE): 0.12513389202614408, R2 Train: 0.49946443189542367\n",
      "Epoch 9034, Loss Train(MSE): 0.12513383632246797, R2 Train: 0.49946465471012813\n",
      "Epoch 9035, Loss Train(MSE): 0.12513377440085754, R2 Train: 0.4994649023965698\n",
      "Epoch 9036, Loss Train(MSE): 0.12513371699712036, R2 Train: 0.49946513201151854\n",
      "Epoch 9037, Loss Train(MSE): 0.12513365818646388, R2 Train: 0.4994653672541445\n",
      "Epoch 9038, Loss Train(MSE): 0.1251335968241913, R2 Train: 0.49946561270323475\n",
      "Epoch 9039, Loss Train(MSE): 0.12513354225161666, R2 Train: 0.49946583099353337\n",
      "Epoch 9040, Loss Train(MSE): 0.12513348072966335, R2 Train: 0.4994660770813466\n",
      "Epoch 9041, Loss Train(MSE): 0.12513342279388598, R2 Train: 0.4994663088244561\n",
      "Epoch 9042, Loss Train(MSE): 0.12513336522928464, R2 Train: 0.4994665390828614\n",
      "Epoch 9043, Loss Train(MSE): 0.12513330395051775, R2 Train: 0.499466784197929\n",
      "Epoch 9044, Loss Train(MSE): 0.1251332493591203, R2 Train: 0.49946700256351884\n",
      "Epoch 9045, Loss Train(MSE): 0.12513318888153493, R2 Train: 0.49946724447386026\n",
      "Epoch 9046, Loss Train(MSE): 0.125133130268009, R2 Train: 0.49946747892796395\n",
      "Epoch 9047, Loss Train(MSE): 0.12513307408696175, R2 Train: 0.499467703652153\n",
      "Epoch 9048, Loss Train(MSE): 0.12513301320023132, R2 Train: 0.4994679471990747\n",
      "Epoch 9049, Loss Train(MSE): 0.12513295781480185, R2 Train: 0.4994681687407926\n",
      "Epoch 9050, Loss Train(MSE): 0.12513289883211504, R2 Train: 0.49946840467153986\n",
      "Epoch 9051, Loss Train(MSE): 0.1251328393904184, R2 Train: 0.49946864243832645\n",
      "Epoch 9052, Loss Train(MSE): 0.1251327847352948, R2 Train: 0.4994688610588208\n",
      "Epoch 9053, Loss Train(MSE): 0.12513272423599242, R2 Train: 0.4994691030560303\n",
      "Epoch 9054, Loss Train(MSE): 0.12513266790832628, R2 Train: 0.4994693283666949\n",
      "Epoch 9055, Loss Train(MSE): 0.1251326105607756, R2 Train: 0.4994695577568976\n",
      "Epoch 9056, Loss Train(MSE): 0.12513255029724726, R2 Train: 0.49946979881101095\n",
      "Epoch 9057, Loss Train(MSE): 0.1251324970053285, R2 Train: 0.499470011978686\n",
      "Epoch 9058, Loss Train(MSE): 0.12513243704069835, R2 Train: 0.4994702518372066\n",
      "Epoch 9059, Loss Train(MSE): 0.12513237963334578, R2 Train: 0.4994704814666169\n",
      "Epoch 9060, Loss Train(MSE): 0.12513232405053784, R2 Train: 0.49947070379784864\n",
      "Epoch 9061, Loss Train(MSE): 0.12513226416716028, R2 Train: 0.4994709433313589\n",
      "Epoch 9062, Loss Train(MSE): 0.1251322096850434, R2 Train: 0.4994711612598264\n",
      "Epoch 9063, Loss Train(MSE): 0.12513215159094782, R2 Train: 0.49947139363620874\n",
      "Epoch 9064, Loss Train(MSE): 0.12513209296182776, R2 Train: 0.499471628152689\n",
      "Epoch 9065, Loss Train(MSE): 0.12513203927814767, R2 Train: 0.4994718428874093\n",
      "Epoch 9066, Loss Train(MSE): 0.12513197977051838, R2 Train: 0.4994720809179265\n",
      "Epoch 9067, Loss Train(MSE): 0.12513192395822223, R2 Train: 0.4994723041671111\n",
      "Epoch 9068, Loss Train(MSE): 0.12513186786702635, R2 Train: 0.4994725285318946\n",
      "Epoch 9069, Loss Train(MSE): 0.12513180858808917, R2 Train: 0.4994727656476433\n",
      "Epoch 9070, Loss Train(MSE): 0.12513175551796826, R2 Train: 0.49947297792812695\n",
      "Epoch 9071, Loss Train(MSE): 0.12513169709109792, R2 Train: 0.4994732116356083\n",
      "Epoch 9072, Loss Train(MSE): 0.1251316398189783, R2 Train: 0.49947344072408684\n",
      "Epoch 9073, Loss Train(MSE): 0.12513158585282588, R2 Train: 0.49947365658869647\n",
      "Epoch 9074, Loss Train(MSE): 0.12513152694268107, R2 Train: 0.49947389222927574\n",
      "Epoch 9075, Loss Train(MSE): 0.12513147230769361, R2 Train: 0.49947411076922554\n",
      "Epoch 9076, Loss Train(MSE): 0.12513141610639508, R2 Train: 0.49947433557441967\n",
      "Epoch 9077, Loss Train(MSE): 0.12513135742072334, R2 Train: 0.4994745703171066\n",
      "Epoch 9078, Loss Train(MSE): 0.12513130535038575, R2 Train: 0.499474778598457\n",
      "Epoch 9079, Loss Train(MSE): 0.12513124698369116, R2 Train: 0.49947501206523537\n",
      "Epoch 9080, Loss Train(MSE): 0.1251311906588417, R2 Train: 0.4994752373646332\n",
      "Epoch 9081, Loss Train(MSE): 0.12513113680081056, R2 Train: 0.49947545279675776\n",
      "Epoch 9082, Loss Train(MSE): 0.1251310784771639, R2 Train: 0.4994756860913444\n",
      "Epoch 9083, Loss Train(MSE): 0.12513102461518041, R2 Train: 0.49947590153927834\n",
      "Epoch 9084, Loss Train(MSE): 0.12513096868914264, R2 Train: 0.49947612524342944\n",
      "Epoch 9085, Loss Train(MSE): 0.12513091058586376, R2 Train: 0.49947635765654497\n",
      "Epoch 9086, Loss Train(MSE): 0.12513085911636163, R2 Train: 0.4994765635345535\n",
      "Epoch 9087, Loss Train(MSE): 0.1251308011900418, R2 Train: 0.4994767952398328\n",
      "Epoch 9088, Loss Train(MSE): 0.1251307454159087, R2 Train: 0.49947701833636515\n",
      "Epoch 9089, Loss Train(MSE): 0.12513069204392455, R2 Train: 0.4994772318243018\n",
      "Epoch 9090, Loss Train(MSE): 0.12513063429608481, R2 Train: 0.49947746281566074\n",
      "Epoch 9091, Loss Train(MSE): 0.12513058081580306, R2 Train: 0.4994776767367878\n",
      "Epoch 9092, Loss Train(MSE): 0.12513052553788803, R2 Train: 0.49947789784844787\n",
      "Epoch 9093, Loss Train(MSE): 0.1251304680064201, R2 Train: 0.49947812797431956\n",
      "Epoch 9094, Loss Train(MSE): 0.1251304167516367, R2 Train: 0.49947833299345323\n",
      "Epoch 9095, Loss Train(MSE): 0.12513035963355337, R2 Train: 0.4994785614657865\n",
      "Epoch 9096, Loss Train(MSE): 0.12513030402632888, R2 Train: 0.4994787838946845\n",
      "Epoch 9097, Loss Train(MSE): 0.12513025150606066, R2 Train: 0.49947899397575735\n",
      "Epoch 9098, Loss Train(MSE): 0.12513019432362052, R2 Train: 0.4994792227055179\n",
      "Epoch 9099, Loss Train(MSE): 0.12513014084631693, R2 Train: 0.49947943661473226\n",
      "Epoch 9100, Loss Train(MSE): 0.12513008657729033, R2 Train: 0.4994796536908387\n",
      "Epoch 9101, Loss Train(MSE): 0.12513002960733127, R2 Train: 0.4994798815706749\n",
      "Epoch 9102, Loss Train(MSE): 0.12512997819356342, R2 Train: 0.4994800872257463\n",
      "Epoch 9103, Loss Train(MSE): 0.12512992223964017, R2 Train: 0.4994803110414393\n",
      "Epoch 9104, Loss Train(MSE): 0.12512986642784837, R2 Train: 0.4994805342886065\n",
      "Epoch 9105, Loss Train(MSE): 0.12512981511310417, R2 Train: 0.4994807395475833\n",
      "Epoch 9106, Loss Train(MSE): 0.12512975848592967, R2 Train: 0.49948096605628134\n",
      "Epoch 9107, Loss Train(MSE): 0.1251297046450512, R2 Train: 0.49948118141979525\n",
      "Epoch 9108, Loss Train(MSE): 0.12512965173397245, R2 Train: 0.4994813930641102\n",
      "Epoch 9109, Loss Train(MSE): 0.1251295953154898, R2 Train: 0.49948161873804076\n",
      "Epoch 9110, Loss Train(MSE): 0.12512954338104618, R2 Train: 0.4994818264758153\n",
      "Epoch 9111, Loss Train(MSE): 0.12512948893565243, R2 Train: 0.4994820442573903\n",
      "Epoch 9112, Loss Train(MSE): 0.12512943272444257, R2 Train: 0.4994822691022297\n",
      "Epoch 9113, Loss Train(MSE): 0.12512938263268855, R2 Train: 0.4994824694692458\n",
      "Epoch 9114, Loss Train(MSE): 0.12512932671429008, R2 Train: 0.49948269314283966\n",
      "Epoch 9115, Loss Train(MSE): 0.1251292721616707, R2 Train: 0.49948291135331724\n",
      "Epoch 9116, Loss Train(MSE): 0.1251292209396475, R2 Train: 0.49948311624140995\n",
      "Epoch 9117, Loss Train(MSE): 0.12512916506286442, R2 Train: 0.4994833397485423\n",
      "Epoch 9118, Loss Train(MSE): 0.12512911226427598, R2 Train: 0.4994835509428961\n",
      "Epoch 9119, Loss Train(MSE): 0.12512905965398816, R2 Train: 0.4994837613840474\n",
      "Epoch 9120, Loss Train(MSE): 0.12512900398082283, R2 Train: 0.4994839840767087\n",
      "Epoch 9121, Loss Train(MSE): 0.12512895287434522, R2 Train: 0.4994841885026191\n",
      "Epoch 9122, Loss Train(MSE): 0.12512889893533927, R2 Train: 0.49948440425864293\n",
      "Epoch 9123, Loss Train(MSE): 0.12512884346442252, R2 Train: 0.49948462614230993\n",
      "Epoch 9124, Loss Train(MSE): 0.12512879398883806, R2 Train: 0.4994848240446478\n",
      "Epoch 9125, Loss Train(MSE): 0.12512873877998076, R2 Train: 0.49948504488007694\n",
      "Epoch 9126, Loss Train(MSE): 0.12512868478470895, R2 Train: 0.4994852608611642\n",
      "Epoch 9127, Loss Train(MSE): 0.12512863432559898, R2 Train: 0.4994854626976041\n",
      "Epoch 9128, Loss Train(MSE): 0.12512857918104514, R2 Train: 0.49948568327581944\n",
      "Epoch 9129, Loss Train(MSE): 0.1251285267317998, R2 Train: 0.4994858930728008\n",
      "Epoch 9130, Loss Train(MSE): 0.1251284750839215, R2 Train: 0.499486099664314\n",
      "Epoch 9131, Loss Train(MSE): 0.1251284201380889, R2 Train: 0.49948631944764443\n",
      "Epoch 9132, Loss Train(MSE): 0.1251283691754001, R2 Train: 0.4994865232983996\n",
      "Epoch 9133, Loss Train(MSE): 0.1251283163959309, R2 Train: 0.49948673441627645\n",
      "Epoch 9134, Loss Train(MSE): 0.12512826164749782, R2 Train: 0.4994869534100087\n",
      "Epoch 9135, Loss Train(MSE): 0.12512821211256883, R2 Train: 0.4994871515497247\n",
      "Epoch 9136, Loss Train(MSE): 0.12512815825803492, R2 Train: 0.49948736696786034\n",
      "Epoch 9137, Loss Train(MSE): 0.1251281245453906, R2 Train: 0.4994875018184376\n",
      "Epoch 9138, Loss Train(MSE): 0.12512810105657474, R2 Train: 0.49948759577370105\n",
      "Epoch 9139, Loss Train(MSE): 0.12512807214773755, R2 Train: 0.4994877114090498\n",
      "Epoch 9140, Loss Train(MSE): 0.12512804486098683, R2 Train: 0.4994878205560527\n",
      "Epoch 9141, Loss Train(MSE): 0.12512802023904773, R2 Train: 0.49948791904380907\n",
      "Epoch 9142, Loss Train(MSE): 0.12512799137353078, R2 Train: 0.49948803450587687\n",
      "Epoch 9143, Loss Train(MSE): 0.12512796527356973, R2 Train: 0.4994881389057211\n",
      "Epoch 9144, Loss Train(MSE): 0.12512793953629112, R2 Train: 0.49948824185483554\n",
      "Epoch 9145, Loss Train(MSE): 0.1251279107139421, R2 Train: 0.49948835714423157\n",
      "Epoch 9146, Loss Train(MSE): 0.12512788578289685, R2 Train: 0.4994884568684126\n",
      "Epoch 9147, Loss Train(MSE): 0.1251278589479261, R2 Train: 0.49948856420829557\n",
      "Epoch 9148, Loss Train(MSE): 0.12512783016859377, R2 Train: 0.4994886793256249\n",
      "Epoch 9149, Loss Train(MSE): 0.12512780638872728, R2 Train: 0.4994887744450909\n",
      "Epoch 9150, Loss Train(MSE): 0.1251277784735766, R2 Train: 0.49948888610569364\n",
      "Epoch 9151, Loss Train(MSE): 0.1251277500047362, R2 Train: 0.4994889999810552\n",
      "Epoch 9152, Loss Train(MSE): 0.12512772682276158, R2 Train: 0.4994890927089537\n",
      "Epoch 9153, Loss Train(MSE): 0.1251276981113506, R2 Train: 0.4994892075545976\n",
      "Epoch 9154, Loss Train(MSE): 0.12512767077338974, R2 Train: 0.49948931690644105\n",
      "Epoch 9155, Loss Train(MSE): 0.1251276465311894, R2 Train: 0.49948941387524237\n",
      "Epoch 9156, Loss Train(MSE): 0.12512761786241855, R2 Train: 0.4994895285503258\n",
      "Epoch 9157, Loss Train(MSE): 0.12512759163792106, R2 Train: 0.49948963344831576\n",
      "Epoch 9158, Loss Train(MSE): 0.12512756635268993, R2 Train: 0.4994897345892403\n",
      "Epoch 9159, Loss Train(MSE): 0.1251275377264115, R2 Train: 0.499489849094354\n",
      "Epoch 9160, Loss Train(MSE): 0.12512751259809446, R2 Train: 0.49948994960762216\n",
      "Epoch 9161, Loss Train(MSE): 0.12512748628689582, R2 Train: 0.4994900548524167\n",
      "Epoch 9162, Loss Train(MSE): 0.12512745770296307, R2 Train: 0.4994901691881477\n",
      "Epoch 9163, Loss Train(MSE): 0.12512743365367562, R2 Train: 0.4994902653852975\n",
      "Epoch 9164, Loss Train(MSE): 0.1251274063334422, R2 Train: 0.49949037466623125\n",
      "Epoch 9165, Loss Train(MSE): 0.12512737779170938, R2 Train: 0.4994904888331625\n",
      "Epoch 9166, Loss Train(MSE): 0.12512735480443177, R2 Train: 0.49949058078227293\n",
      "Epoch 9167, Loss Train(MSE): 0.12512732649196648, R2 Train: 0.4994906940321341\n",
      "Epoch 9168, Loss Train(MSE): 0.12512729880631315, R2 Train: 0.4994908047747474\n",
      "Epoch 9169, Loss Train(MSE): 0.12512727523570888, R2 Train: 0.4994908990571645\n",
      "Epoch 9170, Loss Train(MSE): 0.12512724676062384, R2 Train: 0.49949101295750464\n",
      "Epoch 9171, Loss Train(MSE): 0.125127220117723, R2 Train: 0.49949111952910796\n",
      "Epoch 9172, Loss Train(MSE): 0.12512719557381166, R2 Train: 0.49949121770475335\n",
      "Epoch 9173, Loss Train(MSE): 0.1251271671405639, R2 Train: 0.4994913314377444\n",
      "Epoch 9174, Loss Train(MSE): 0.12512714152370258, R2 Train: 0.4994914339051897\n",
      "Epoch 9175, Loss Train(MSE): 0.12512711602298407, R2 Train: 0.4994915359080637\n",
      "Epoch 9176, Loss Train(MSE): 0.12512708763143127, R2 Train: 0.49949164947427493\n",
      "Epoch 9177, Loss Train(MSE): 0.12512706302402388, R2 Train: 0.4994917479039045\n",
      "Epoch 9178, Loss Train(MSE): 0.12512703658287205, R2 Train: 0.4994918536685118\n",
      "Epoch 9179, Loss Train(MSE): 0.1251270082328728, R2 Train: 0.49949196706850885\n",
      "Epoch 9180, Loss Train(MSE): 0.12512698461846034, R2 Train: 0.49949206152615866\n",
      "Epoch 9181, Loss Train(MSE): 0.12512695725312367, R2 Train: 0.4994921709875053\n",
      "Epoch 9182, Loss Train(MSE): 0.12512692894453756, R2 Train: 0.49949228422184977\n",
      "Epoch 9183, Loss Train(MSE): 0.1251269063067867, R2 Train: 0.49949237477285324\n",
      "Epoch 9184, Loss Train(MSE): 0.12512687803338962, R2 Train: 0.4994924878664415\n",
      "Epoch 9185, Loss Train(MSE): 0.1251268506894571, R2 Train: 0.49949259724217165\n",
      "Epoch 9186, Loss Train(MSE): 0.12512682716503543, R2 Train: 0.4994926913398583\n",
      "Epoch 9187, Loss Train(MSE): 0.12512679892186998, R2 Train: 0.4994928043125201\n",
      "Epoch 9188, Loss Train(MSE): 0.12512677253633703, R2 Train: 0.4994929098546519\n",
      "Epoch 9189, Loss Train(MSE): 0.12512674812179625, R2 Train: 0.499493007512815\n",
      "Epoch 9190, Loss Train(MSE): 0.1251267199196944, R2 Train: 0.4994931203212224\n",
      "Epoch 9191, Loss Train(MSE): 0.12512669447652083, R2 Train: 0.49949322209391667\n",
      "Epoch 9192, Loss Train(MSE): 0.1251266691876956, R2 Train: 0.4994933232492176\n",
      "Epoch 9193, Loss Train(MSE): 0.1251266410265201, R2 Train: 0.4994934358939196\n",
      "Epoch 9194, Loss Train(MSE): 0.1251266165097879, R2 Train: 0.4994935339608484\n",
      "Epoch 9195, Loss Train(MSE): 0.12512659036239188, R2 Train: 0.4994936385504325\n",
      "Epoch 9196, Loss Train(MSE): 0.12512656224200638, R2 Train: 0.49949375103197446\n",
      "Epoch 9197, Loss Train(MSE): 0.12512653863591885, R2 Train: 0.4994938454563246\n",
      "Epoch 9198, Loss Train(MSE): 0.12512651164554578, R2 Train: 0.4994939534178169\n",
      "Epoch 9199, Loss Train(MSE): 0.1251264835658148, R2 Train: 0.4994940657367408\n",
      "Epoch 9200, Loss Train(MSE): 0.12512646085469573, R2 Train: 0.4994941565812171\n",
      "Epoch 9201, Loss Train(MSE): 0.12512643303682008, R2 Train: 0.49949426785271966\n",
      "Epoch 9202, Loss Train(MSE): 0.12512640561322858, R2 Train: 0.4994943775470857\n",
      "Epoch 9203, Loss Train(MSE): 0.1251263825499519, R2 Train: 0.4994944698001924\n",
      "Epoch 9204, Loss Train(MSE): 0.12512635453445864, R2 Train: 0.49949458186216544\n",
      "Epoch 9205, Loss Train(MSE): 0.1251263279885243, R2 Train: 0.4994946880459028\n",
      "Epoch 9206, Loss Train(MSE): 0.12512630411474696, R2 Train: 0.4994947835410122\n",
      "Epoch 9207, Loss Train(MSE): 0.12512627613957125, R2 Train: 0.499494895441715\n",
      "Epoch 9208, Loss Train(MSE): 0.12512625045589804, R2 Train: 0.49949499817640786\n",
      "Epoch 9209, Loss Train(MSE): 0.1251262257868175, R2 Train: 0.49949509685272997\n",
      "Epoch 9210, Loss Train(MSE): 0.12512619785182694, R2 Train: 0.49949520859269225\n",
      "Epoch 9211, Loss Train(MSE): 0.12512617301513618, R2 Train: 0.4994953079394553\n",
      "Epoch 9212, Loss Train(MSE): 0.12512614756583407, R2 Train: 0.49949540973666373\n",
      "Epoch 9213, Loss Train(MSE): 0.125126119670897, R2 Train: 0.49949552131641195\n",
      "Epoch 9214, Loss Train(MSE): 0.12512609566602617, R2 Train: 0.4994956173358953\n",
      "Epoch 9215, Loss Train(MSE): 0.12512606945146906, R2 Train: 0.49949572219412375\n",
      "Epoch 9216, Loss Train(MSE): 0.12512604159645482, R2 Train: 0.4994958336141807\n",
      "Epoch 9217, Loss Train(MSE): 0.12512601840835677, R2 Train: 0.4994959263665729\n",
      "Epoch 9218, Loss Train(MSE): 0.12512599144339706, R2 Train: 0.49949603422641176\n",
      "Epoch 9219, Loss Train(MSE): 0.1251259636281758, R2 Train: 0.4994961454872968\n",
      "Epoch 9220, Loss Train(MSE): 0.12512594124191792, R2 Train: 0.4994962350323283\n",
      "Epoch 9221, Loss Train(MSE): 0.12512591354129476, R2 Train: 0.49949634583482094\n",
      "Epoch 9222, Loss Train(MSE): 0.12512588643616326, R2 Train: 0.49949645425534694\n",
      "Epoch 9223, Loss Train(MSE): 0.12512586349578236, R2 Train: 0.49949654601687055\n",
      "Epoch 9224, Loss Train(MSE): 0.12512583574345545, R2 Train: 0.4994966570261782\n",
      "Epoch 9225, Loss Train(MSE): 0.12512580942392698, R2 Train: 0.4994967623042921\n",
      "Epoch 9226, Loss Train(MSE): 0.12512578576382288, R2 Train: 0.49949685694470847\n",
      "Epoch 9227, Loss Train(MSE): 0.12512575805096593, R2 Train: 0.4994969677961363\n",
      "Epoch 9228, Loss Train(MSE): 0.12512573250237366, R2 Train: 0.49949706999050536\n",
      "Epoch 9229, Loss Train(MSE): 0.12512570813702253, R2 Train: 0.49949716745190986\n",
      "Epoch 9230, Loss Train(MSE): 0.12512568046350853, R2 Train: 0.4994972781459659\n",
      "Epoch 9231, Loss Train(MSE): 0.12512565567129738, R2 Train: 0.4994973773148105\n",
      "Epoch 9232, Loss Train(MSE): 0.12512563061506524, R2 Train: 0.49949747753973905\n",
      "Epoch 9233, Loss Train(MSE): 0.1251256029807679, R2 Train: 0.49949758807692834\n",
      "Epoch 9234, Loss Train(MSE): 0.12512557893049325, R2 Train: 0.499497684278027\n",
      "Epoch 9235, Loss Train(MSE): 0.12512555319763652, R2 Train: 0.4994977872094539\n",
      "Epoch 9236, Loss Train(MSE): 0.1251255256024305, R2 Train: 0.49949789759027796\n",
      "Epoch 9237, Loss Train(MSE): 0.12512550227975766, R2 Train: 0.49949799088096936\n",
      "Epoch 9238, Loss Train(MSE): 0.12512547588442413, R2 Train: 0.4994980964623035\n",
      "Epoch 9239, Loss Train(MSE): 0.1251254483281848, R2 Train: 0.49949820668726075\n",
      "Epoch 9240, Loss Train(MSE): 0.12512542571888796, R2 Train: 0.49949829712444815\n",
      "Epoch 9241, Loss Train(MSE): 0.12512539867511768, R2 Train: 0.49949840529952927\n",
      "Epoch 9242, Loss Train(MSE): 0.12512537134245594, R2 Train: 0.49949851463017625\n",
      "Epoch 9243, Loss Train(MSE): 0.12512534906269132, R2 Train: 0.4994986037492347\n",
      "Epoch 9244, Loss Train(MSE): 0.12512532156805808, R2 Train: 0.49949871372776766\n",
      "Epoch 9245, Loss Train(MSE): 0.12512529493355345, R2 Train: 0.4994988202657862\n",
      "Epoch 9246, Loss Train(MSE): 0.12512527202028575, R2 Train: 0.499498911918857\n",
      "Epoch 9247, Loss Train(MSE): 0.12512524456430932, R2 Train: 0.4994990217427627\n",
      "Epoch 9248, Loss Train(MSE): 0.12512521861398826, R2 Train: 0.49949912554404696\n",
      "Epoch 9249, Loss Train(MSE): 0.12512519508100806, R2 Train: 0.4994992196759678\n",
      "Epoch 9250, Loss Train(MSE): 0.12512516766356654, R2 Train: 0.4994993293457338\n",
      "Epoch 9251, Loss Train(MSE): 0.12512514238356165, R2 Train: 0.4994994304657534\n",
      "Epoch 9252, Loss Train(MSE): 0.1251251182445545, R2 Train: 0.499499527021782\n",
      "Epoch 9253, Loss Train(MSE): 0.12512509086552673, R2 Train: 0.4994996365378931\n",
      "Epoch 9254, Loss Train(MSE): 0.12512506624207603, R2 Train: 0.49949973503169587\n",
      "Epoch 9255, Loss Train(MSE): 0.12512504151062326, R2 Train: 0.49949983395750697\n",
      "Epoch 9256, Loss Train(MSE): 0.12512501416988883, R2 Train: 0.4994999433204447\n",
      "Epoch 9257, Loss Train(MSE): 0.1251249901893348, R2 Train: 0.49950003924266084\n",
      "Epoch 9258, Loss Train(MSE): 0.12512496487891425, R2 Train: 0.499500140484343\n",
      "Epoch 9259, Loss Train(MSE): 0.1251249375763536, R2 Train: 0.4995002496945856\n",
      "Epoch 9260, Loss Train(MSE): 0.1251249142251425, R2 Train: 0.49950034309943003\n",
      "Epoch 9261, Loss Train(MSE): 0.12512730131916258, R2 Train: 0.4994907947233497\n",
      "Epoch 9262, Loss Train(MSE): 0.1251274506813593, R2 Train: 0.4994901972745628\n",
      "Epoch 9263, Loss Train(MSE): 0.12512741739215816, R2 Train: 0.49949033043136737\n",
      "Epoch 9264, Loss Train(MSE): 0.1251273841194952, R2 Train: 0.4994904635220192\n",
      "Epoch 9265, Loss Train(MSE): 0.12512735086335705, R2 Train: 0.4994905965465718\n",
      "Epoch 9266, Loss Train(MSE): 0.12512731762373042, R2 Train: 0.49949072950507833\n",
      "Epoch 9267, Loss Train(MSE): 0.12512728440060214, R2 Train: 0.49949086239759144\n",
      "Epoch 9268, Loss Train(MSE): 0.12512725119395895, R2 Train: 0.4994909952241642\n",
      "Epoch 9269, Loss Train(MSE): 0.12512721800378768, R2 Train: 0.4994911279848493\n",
      "Epoch 9270, Loss Train(MSE): 0.12512718483007523, R2 Train: 0.49949126067969907\n",
      "Epoch 9271, Loss Train(MSE): 0.12512715167280838, R2 Train: 0.49949139330876646\n",
      "Epoch 9272, Loss Train(MSE): 0.12512711853197406, R2 Train: 0.49949152587210377\n",
      "Epoch 9273, Loss Train(MSE): 0.1251270854075593, R2 Train: 0.49949165836976284\n",
      "Epoch 9274, Loss Train(MSE): 0.1251270522995509, R2 Train: 0.4994917908017964\n",
      "Epoch 9275, Loss Train(MSE): 0.12512701920793587, R2 Train: 0.4994919231682565\n",
      "Epoch 9276, Loss Train(MSE): 0.12512698613270132, R2 Train: 0.49949205546919473\n",
      "Epoch 9277, Loss Train(MSE): 0.12512695307383415, R2 Train: 0.4994921877046634\n",
      "Epoch 9278, Loss Train(MSE): 0.1251269200313215, R2 Train: 0.499492319874714\n",
      "Epoch 9279, Loss Train(MSE): 0.12512688700515046, R2 Train: 0.4994924519793982\n",
      "Epoch 9280, Loss Train(MSE): 0.12512685399530804, R2 Train: 0.49949258401876784\n",
      "Epoch 9281, Loss Train(MSE): 0.1251268210017815, R2 Train: 0.49949271599287404\n",
      "Epoch 9282, Loss Train(MSE): 0.1251267880245579, R2 Train: 0.4994928479017684\n",
      "Epoch 9283, Loss Train(MSE): 0.12512675506362447, R2 Train: 0.4994929797455021\n",
      "Epoch 9284, Loss Train(MSE): 0.12512672211896841, R2 Train: 0.49949311152412634\n",
      "Epoch 9285, Loss Train(MSE): 0.12512668919057693, R2 Train: 0.4994932432376923\n",
      "Epoch 9286, Loss Train(MSE): 0.1251266562784373, R2 Train: 0.4994933748862508\n",
      "Epoch 9287, Loss Train(MSE): 0.12512662338253683, R2 Train: 0.4994935064698527\n",
      "Epoch 9288, Loss Train(MSE): 0.1251265905028628, R2 Train: 0.49949363798854884\n",
      "Epoch 9289, Loss Train(MSE): 0.12512655763940247, R2 Train: 0.4994937694423901\n",
      "Epoch 9290, Loss Train(MSE): 0.12512652479214334, R2 Train: 0.49949390083142664\n",
      "Epoch 9291, Loss Train(MSE): 0.12512649196107264, R2 Train: 0.4994940321557094\n",
      "Epoch 9292, Loss Train(MSE): 0.12512645914617782, R2 Train: 0.49949416341528874\n",
      "Epoch 9293, Loss Train(MSE): 0.1251264263474463, R2 Train: 0.4994942946102148\n",
      "Epoch 9294, Loss Train(MSE): 0.12512639356486555, R2 Train: 0.4994944257405378\n",
      "Epoch 9295, Loss Train(MSE): 0.125126360798423, R2 Train: 0.49949455680630805\n",
      "Epoch 9296, Loss Train(MSE): 0.12512632804810614, R2 Train: 0.49949468780757544\n",
      "Epoch 9297, Loss Train(MSE): 0.1251262953139025, R2 Train: 0.49949481874438995\n",
      "Epoch 9298, Loss Train(MSE): 0.12512626259579965, R2 Train: 0.4994949496168014\n",
      "Epoch 9299, Loss Train(MSE): 0.12512622989378505, R2 Train: 0.4994950804248598\n",
      "Epoch 9300, Loss Train(MSE): 0.12512619720784635, R2 Train: 0.4994952111686146\n",
      "Epoch 9301, Loss Train(MSE): 0.1251261645379711, R2 Train: 0.4994953418481156\n",
      "Epoch 9302, Loss Train(MSE): 0.12512613188414695, R2 Train: 0.4994954724634122\n",
      "Epoch 9303, Loss Train(MSE): 0.12512609924636153, R2 Train: 0.49949560301455387\n",
      "Epoch 9304, Loss Train(MSE): 0.12512606662460252, R2 Train: 0.49949573350158993\n",
      "Epoch 9305, Loss Train(MSE): 0.12512603401885763, R2 Train: 0.49949586392456946\n",
      "Epoch 9306, Loss Train(MSE): 0.1251260014291145, R2 Train: 0.49949599428354197\n",
      "Epoch 9307, Loss Train(MSE): 0.12512596885536087, R2 Train: 0.4994961245785565\n",
      "Epoch 9308, Loss Train(MSE): 0.12512593629758453, R2 Train: 0.49949625480966187\n",
      "Epoch 9309, Loss Train(MSE): 0.12512590375577323, R2 Train: 0.49949638497690707\n",
      "Epoch 9310, Loss Train(MSE): 0.12512587122991473, R2 Train: 0.4994965150803411\n",
      "Epoch 9311, Loss Train(MSE): 0.12512583871999686, R2 Train: 0.4994966451200126\n",
      "Epoch 9312, Loss Train(MSE): 0.1251258062260074, R2 Train: 0.49949677509597035\n",
      "Epoch 9313, Loss Train(MSE): 0.12512577374793438, R2 Train: 0.4994969050082625\n",
      "Epoch 9314, Loss Train(MSE): 0.1251257412857654, R2 Train: 0.4994970348569384\n",
      "Epoch 9315, Loss Train(MSE): 0.12512570883948856, R2 Train: 0.49949716464204574\n",
      "Epoch 9316, Loss Train(MSE): 0.1251256764090917, R2 Train: 0.4994972943636332\n",
      "Epoch 9317, Loss Train(MSE): 0.1251256439945627, R2 Train: 0.49949742402174924\n",
      "Epoch 9318, Loss Train(MSE): 0.12512561159588956, R2 Train: 0.49949755361644177\n",
      "Epoch 9319, Loss Train(MSE): 0.12512557921306028, R2 Train: 0.4994976831477589\n",
      "Epoch 9320, Loss Train(MSE): 0.12512554684606278, R2 Train: 0.4994978126157489\n",
      "Epoch 9321, Loss Train(MSE): 0.12512551449488507, R2 Train: 0.4994979420204597\n",
      "Epoch 9322, Loss Train(MSE): 0.12512548215951522, R2 Train: 0.49949807136193913\n",
      "Epoch 9323, Loss Train(MSE): 0.12512544983994128, R2 Train: 0.49949820064023487\n",
      "Epoch 9324, Loss Train(MSE): 0.12512541753615122, R2 Train: 0.4994983298553951\n",
      "Epoch 9325, Loss Train(MSE): 0.12512538524813321, R2 Train: 0.49949845900746714\n",
      "Epoch 9326, Loss Train(MSE): 0.12512535297587532, R2 Train: 0.4994985880964987\n",
      "Epoch 9327, Loss Train(MSE): 0.1251253207193657, R2 Train: 0.49949871712253724\n",
      "Epoch 9328, Loss Train(MSE): 0.12512528847859242, R2 Train: 0.49949884608563033\n",
      "Epoch 9329, Loss Train(MSE): 0.1251252562535437, R2 Train: 0.4994989749858252\n",
      "Epoch 9330, Loss Train(MSE): 0.12512522404420767, R2 Train: 0.4994991038231693\n",
      "Epoch 9331, Loss Train(MSE): 0.12512519185057253, R2 Train: 0.4994992325977099\n",
      "Epoch 9332, Loss Train(MSE): 0.12512515967262652, R2 Train: 0.4994993613094939\n",
      "Epoch 9333, Loss Train(MSE): 0.12512512751035784, R2 Train: 0.49949948995856863\n",
      "Epoch 9334, Loss Train(MSE): 0.1251250953637547, R2 Train: 0.49949961854498115\n",
      "Epoch 9335, Loss Train(MSE): 0.12512506323280545, R2 Train: 0.4994997470687782\n",
      "Epoch 9336, Loss Train(MSE): 0.12512503111749831, R2 Train: 0.49949987553000674\n",
      "Epoch 9337, Loss Train(MSE): 0.1251249990178216, R2 Train: 0.49950000392871363\n",
      "Epoch 9338, Loss Train(MSE): 0.1251249669337636, R2 Train: 0.4995001322649456\n",
      "Epoch 9339, Loss Train(MSE): 0.12512493486531265, R2 Train: 0.4995002605387494\n",
      "Epoch 9340, Loss Train(MSE): 0.1251249028124571, R2 Train: 0.49950038875017155\n",
      "Epoch 9341, Loss Train(MSE): 0.1251248707751854, R2 Train: 0.49950051689925845\n",
      "Epoch 9342, Loss Train(MSE): 0.12512483875348582, R2 Train: 0.49950064498605673\n",
      "Epoch 9343, Loss Train(MSE): 0.1251248067473468, R2 Train: 0.4995007730106128\n",
      "Epoch 9344, Loss Train(MSE): 0.12512477475675676, R2 Train: 0.49950090097297295\n",
      "Epoch 9345, Loss Train(MSE): 0.12512474278170413, R2 Train: 0.4995010288731835\n",
      "Epoch 9346, Loss Train(MSE): 0.12512471082217735, R2 Train: 0.4995011567112906\n",
      "Epoch 9347, Loss Train(MSE): 0.12512467887816492, R2 Train: 0.4995012844873403\n",
      "Epoch 9348, Loss Train(MSE): 0.12512464694965528, R2 Train: 0.4995014122013789\n",
      "Epoch 9349, Loss Train(MSE): 0.12512461503663697, R2 Train: 0.49950153985345214\n",
      "Epoch 9350, Loss Train(MSE): 0.1251245831390985, R2 Train: 0.499501667443606\n",
      "Epoch 9351, Loss Train(MSE): 0.12512455125702834, R2 Train: 0.4995017949718866\n",
      "Epoch 9352, Loss Train(MSE): 0.1251245193904151, R2 Train: 0.4995019224383396\n",
      "Epoch 9353, Loss Train(MSE): 0.12512448753924732, R2 Train: 0.4995020498430107\n",
      "Epoch 9354, Loss Train(MSE): 0.1251244557035136, R2 Train: 0.4995021771859456\n",
      "Epoch 9355, Loss Train(MSE): 0.12512442388320255, R2 Train: 0.4995023044671898\n",
      "Epoch 9356, Loss Train(MSE): 0.12512439207830273, R2 Train: 0.4995024316867891\n",
      "Epoch 9357, Loss Train(MSE): 0.12512436028880278, R2 Train: 0.4995025588447889\n",
      "Epoch 9358, Loss Train(MSE): 0.12512432851469132, R2 Train: 0.4995026859412347\n",
      "Epoch 9359, Loss Train(MSE): 0.12512429675595707, R2 Train: 0.4995028129761717\n",
      "Epoch 9360, Loss Train(MSE): 0.12512426501258872, R2 Train: 0.4995029399496451\n",
      "Epoch 9361, Loss Train(MSE): 0.12512423328457487, R2 Train: 0.49950306686170054\n",
      "Epoch 9362, Loss Train(MSE): 0.1251242015719043, R2 Train: 0.49950319371238283\n",
      "Epoch 9363, Loss Train(MSE): 0.12512416987456562, R2 Train: 0.4995033205017375\n",
      "Epoch 9364, Loss Train(MSE): 0.12512413819254767, R2 Train: 0.4995034472298093\n",
      "Epoch 9365, Loss Train(MSE): 0.12512410652583922, R2 Train: 0.4995035738966431\n",
      "Epoch 9366, Loss Train(MSE): 0.1251240748744289, R2 Train: 0.49950370050228443\n",
      "Epoch 9367, Loss Train(MSE): 0.12512404323830562, R2 Train: 0.49950382704677754\n",
      "Epoch 9368, Loss Train(MSE): 0.12512401161745812, R2 Train: 0.4995039535301675\n",
      "Epoch 9369, Loss Train(MSE): 0.12512398001187525, R2 Train: 0.499504079952499\n",
      "Epoch 9370, Loss Train(MSE): 0.12512394842154576, R2 Train: 0.499504206313817\n",
      "Epoch 9371, Loss Train(MSE): 0.12512391684645857, R2 Train: 0.4995043326141657\n",
      "Epoch 9372, Loss Train(MSE): 0.1251238852866024, R2 Train: 0.4995044588535904\n",
      "Epoch 9373, Loss Train(MSE): 0.12512385374196633, R2 Train: 0.4995045850321347\n",
      "Epoch 9374, Loss Train(MSE): 0.12512382221253904, R2 Train: 0.49950471114984385\n",
      "Epoch 9375, Loss Train(MSE): 0.1251237906983095, R2 Train: 0.499504837206762\n",
      "Epoch 9376, Loss Train(MSE): 0.1251237591992667, R2 Train: 0.49950496320293325\n",
      "Epoch 9377, Loss Train(MSE): 0.12512372771539945, R2 Train: 0.4995050891384022\n",
      "Epoch 9378, Loss Train(MSE): 0.1251236962466967, R2 Train: 0.4995052150132132\n",
      "Epoch 9379, Loss Train(MSE): 0.1251236647931475, R2 Train: 0.49950534082741005\n",
      "Epoch 9380, Loss Train(MSE): 0.12512363335474072, R2 Train: 0.4995054665810371\n",
      "Epoch 9381, Loss Train(MSE): 0.12512360193146532, R2 Train: 0.4995055922741387\n",
      "Epoch 9382, Loss Train(MSE): 0.1251235705233104, R2 Train: 0.49950571790675835\n",
      "Epoch 9383, Loss Train(MSE): 0.12512353913026492, R2 Train: 0.49950584347894034\n",
      "Epoch 9384, Loss Train(MSE): 0.12512350775231793, R2 Train: 0.4995059689907283\n",
      "Epoch 9385, Loss Train(MSE): 0.12512347638945837, R2 Train: 0.49950609444216654\n",
      "Epoch 9386, Loss Train(MSE): 0.1251234450416754, R2 Train: 0.49950621983329835\n",
      "Epoch 9387, Loss Train(MSE): 0.12512341370895802, R2 Train: 0.4995063451641679\n",
      "Epoch 9388, Loss Train(MSE): 0.1251233823912953, R2 Train: 0.4995064704348188\n",
      "Epoch 9389, Loss Train(MSE): 0.12512335108867645, R2 Train: 0.4995065956452942\n",
      "Epoch 9390, Loss Train(MSE): 0.1251233198010904, R2 Train: 0.4995067207956384\n",
      "Epoch 9391, Loss Train(MSE): 0.12512328852852642, R2 Train: 0.49950684588589433\n",
      "Epoch 9392, Loss Train(MSE): 0.1251232572709735, R2 Train: 0.49950697091610596\n",
      "Epoch 9393, Loss Train(MSE): 0.12512322602842096, R2 Train: 0.49950709588631614\n",
      "Epoch 9394, Loss Train(MSE): 0.1251231948008578, R2 Train: 0.49950722079656884\n",
      "Epoch 9395, Loss Train(MSE): 0.12512316358827325, R2 Train: 0.499507345646907\n",
      "Epoch 9396, Loss Train(MSE): 0.12512313239065648, R2 Train: 0.4995074704373741\n",
      "Epoch 9397, Loss Train(MSE): 0.12512310120799675, R2 Train: 0.499507595168013\n",
      "Epoch 9398, Loss Train(MSE): 0.1251230700402832, R2 Train: 0.4995077198388672\n",
      "Epoch 9399, Loss Train(MSE): 0.12512303888750503, R2 Train: 0.49950784444997987\n",
      "Epoch 9400, Loss Train(MSE): 0.12512300774965157, R2 Train: 0.4995079690013937\n",
      "Epoch 9401, Loss Train(MSE): 0.12512297662671198, R2 Train: 0.49950809349315206\n",
      "Epoch 9402, Loss Train(MSE): 0.12512294551867564, R2 Train: 0.4995082179252974\n",
      "Epoch 9403, Loss Train(MSE): 0.12512291442553172, R2 Train: 0.4995083422978731\n",
      "Epoch 9404, Loss Train(MSE): 0.1251228833472695, R2 Train: 0.49950846661092196\n",
      "Epoch 9405, Loss Train(MSE): 0.12512285228387834, R2 Train: 0.4995085908644866\n",
      "Epoch 9406, Loss Train(MSE): 0.12512282123534754, R2 Train: 0.49950871505860983\n",
      "Epoch 9407, Loss Train(MSE): 0.1251227902016664, R2 Train: 0.49950883919333444\n",
      "Epoch 9408, Loss Train(MSE): 0.12512275918282434, R2 Train: 0.49950896326870264\n",
      "Epoch 9409, Loss Train(MSE): 0.1251227281788106, R2 Train: 0.4995090872847576\n",
      "Epoch 9410, Loss Train(MSE): 0.12512269718961458, R2 Train: 0.4995092112415417\n",
      "Epoch 9411, Loss Train(MSE): 0.12512266621522566, R2 Train: 0.49950933513909734\n",
      "Epoch 9412, Loss Train(MSE): 0.12512263525563325, R2 Train: 0.499509458977467\n",
      "Epoch 9413, Loss Train(MSE): 0.12512260431082672, R2 Train: 0.4995095827566931\n",
      "Epoch 9414, Loss Train(MSE): 0.12512257338079555, R2 Train: 0.4995097064768178\n",
      "Epoch 9415, Loss Train(MSE): 0.1251225424655291, R2 Train: 0.49950983013788364\n",
      "Epoch 9416, Loss Train(MSE): 0.12512251156501678, R2 Train: 0.4995099537399329\n",
      "Epoch 9417, Loss Train(MSE): 0.12512248067924814, R2 Train: 0.49951007728300745\n",
      "Epoch 9418, Loss Train(MSE): 0.12512244980821252, R2 Train: 0.4995102007671499\n",
      "Epoch 9419, Loss Train(MSE): 0.12512241895189952, R2 Train: 0.49951032419240193\n",
      "Epoch 9420, Loss Train(MSE): 0.12512238811029852, R2 Train: 0.4995104475588059\n",
      "Epoch 9421, Loss Train(MSE): 0.1251223572833991, R2 Train: 0.4995105708664036\n",
      "Epoch 9422, Loss Train(MSE): 0.12512232647119073, R2 Train: 0.4995106941152371\n",
      "Epoch 9423, Loss Train(MSE): 0.1251222956736629, R2 Train: 0.4995108173053484\n",
      "Epoch 9424, Loss Train(MSE): 0.1251222648908052, R2 Train: 0.49951094043677924\n",
      "Epoch 9425, Loss Train(MSE): 0.1251222341226071, R2 Train: 0.49951106350957164\n",
      "Epoch 9426, Loss Train(MSE): 0.12512220336905827, R2 Train: 0.4995111865237669\n",
      "Epoch 9427, Loss Train(MSE): 0.12512217263014822, R2 Train: 0.49951130947940714\n",
      "Epoch 9428, Loss Train(MSE): 0.12512214190586654, R2 Train: 0.49951143237653384\n",
      "Epoch 9429, Loss Train(MSE): 0.12512211119620278, R2 Train: 0.49951155521518886\n",
      "Epoch 9430, Loss Train(MSE): 0.1251220805011466, R2 Train: 0.4995116779954136\n",
      "Epoch 9431, Loss Train(MSE): 0.12512204982068756, R2 Train: 0.49951180071724977\n",
      "Epoch 9432, Loss Train(MSE): 0.12512201915481538, R2 Train: 0.49951192338073847\n",
      "Epoch 9433, Loss Train(MSE): 0.12512198850351958, R2 Train: 0.4995120459859217\n",
      "Epoch 9434, Loss Train(MSE): 0.12512195786678992, R2 Train: 0.4995121685328403\n",
      "Epoch 9435, Loss Train(MSE): 0.125121927244616, R2 Train: 0.49951229102153605\n",
      "Epoch 9436, Loss Train(MSE): 0.1251218966369875, R2 Train: 0.49951241345205\n",
      "Epoch 9437, Loss Train(MSE): 0.12512186604389405, R2 Train: 0.4995125358244238\n",
      "Epoch 9438, Loss Train(MSE): 0.1251218354653255, R2 Train: 0.49951265813869805\n",
      "Epoch 9439, Loss Train(MSE): 0.12512180490127142, R2 Train: 0.4995127803949143\n",
      "Epoch 9440, Loss Train(MSE): 0.1251217743517216, R2 Train: 0.49951290259311365\n",
      "Epoch 9441, Loss Train(MSE): 0.12512174381666571, R2 Train: 0.49951302473333714\n",
      "Epoch 9442, Loss Train(MSE): 0.12512171329609356, R2 Train: 0.49951314681562575\n",
      "Epoch 9443, Loss Train(MSE): 0.12512168278999486, R2 Train: 0.49951326884002056\n",
      "Epoch 9444, Loss Train(MSE): 0.12512165229835934, R2 Train: 0.49951339080656265\n",
      "Epoch 9445, Loss Train(MSE): 0.12512162182117684, R2 Train: 0.49951351271529265\n",
      "Epoch 9446, Loss Train(MSE): 0.12512159135843715, R2 Train: 0.4995136345662514\n",
      "Epoch 9447, Loss Train(MSE): 0.12512156091013005, R2 Train: 0.4995137563594798\n",
      "Epoch 9448, Loss Train(MSE): 0.12512153047624527, R2 Train: 0.4995138780950189\n",
      "Epoch 9449, Loss Train(MSE): 0.12512150005677275, R2 Train: 0.499513999772909\n",
      "Epoch 9450, Loss Train(MSE): 0.12512146965170223, R2 Train: 0.49951412139319107\n",
      "Epoch 9451, Loss Train(MSE): 0.1251214392610236, R2 Train: 0.49951424295590563\n",
      "Epoch 9452, Loss Train(MSE): 0.1251214088847267, R2 Train: 0.4995143644610932\n",
      "Epoch 9453, Loss Train(MSE): 0.1251213785228014, R2 Train: 0.49951448590879444\n",
      "Epoch 9454, Loss Train(MSE): 0.12512134817523757, R2 Train: 0.49951460729904973\n",
      "Epoch 9455, Loss Train(MSE): 0.1251213178420251, R2 Train: 0.4995147286318996\n",
      "Epoch 9456, Loss Train(MSE): 0.12512128752315385, R2 Train: 0.4995148499073846\n",
      "Epoch 9457, Loss Train(MSE): 0.12512125721861372, R2 Train: 0.4995149711255451\n",
      "Epoch 9458, Loss Train(MSE): 0.1251212269283947, R2 Train: 0.4995150922864212\n",
      "Epoch 9459, Loss Train(MSE): 0.12512119665248667, R2 Train: 0.49951521339005334\n",
      "Epoch 9460, Loss Train(MSE): 0.12512116639087953, R2 Train: 0.4995153344364819\n",
      "Epoch 9461, Loss Train(MSE): 0.12512113614356332, R2 Train: 0.4995154554257467\n",
      "Epoch 9462, Loss Train(MSE): 0.1251211059105279, R2 Train: 0.49951557635788835\n",
      "Epoch 9463, Loss Train(MSE): 0.12512107569176337, R2 Train: 0.49951569723294653\n",
      "Epoch 9464, Loss Train(MSE): 0.12512104548725955, R2 Train: 0.4995158180509618\n",
      "Epoch 9465, Loss Train(MSE): 0.1251210152970066, R2 Train: 0.49951593881197365\n",
      "Epoch 9466, Loss Train(MSE): 0.12512098512099434, R2 Train: 0.4995160595160226\n",
      "Epoch 9467, Loss Train(MSE): 0.12512095495921294, R2 Train: 0.49951618016314825\n",
      "Epoch 9468, Loss Train(MSE): 0.1251209248116523, R2 Train: 0.4995163007533908\n",
      "Epoch 9469, Loss Train(MSE): 0.12512089467830254, R2 Train: 0.49951642128678986\n",
      "Epoch 9470, Loss Train(MSE): 0.12512086455915364, R2 Train: 0.49951654176338545\n",
      "Epoch 9471, Loss Train(MSE): 0.12512083445419572, R2 Train: 0.4995166621832171\n",
      "Epoch 9472, Loss Train(MSE): 0.1251208043634188, R2 Train: 0.4995167825463248\n",
      "Epoch 9473, Loss Train(MSE): 0.12512077428681298, R2 Train: 0.4995169028527481\n",
      "Epoch 9474, Loss Train(MSE): 0.1251207442243683, R2 Train: 0.49951702310252677\n",
      "Epoch 9475, Loss Train(MSE): 0.1251207141760749, R2 Train: 0.4995171432957004\n",
      "Epoch 9476, Loss Train(MSE): 0.12512068414192284, R2 Train: 0.4995172634323086\n",
      "Epoch 9477, Loss Train(MSE): 0.1251206541219023, R2 Train: 0.49951738351239083\n",
      "Epoch 9478, Loss Train(MSE): 0.12512062411600336, R2 Train: 0.49951750353598656\n",
      "Epoch 9479, Loss Train(MSE): 0.12512059412421614, R2 Train: 0.49951762350313544\n",
      "Epoch 9480, Loss Train(MSE): 0.1251205641465308, R2 Train: 0.4995177434138768\n",
      "Epoch 9481, Loss Train(MSE): 0.12512053418293753, R2 Train: 0.4995178632682499\n",
      "Epoch 9482, Loss Train(MSE): 0.12512050423342644, R2 Train: 0.49951798306629425\n",
      "Epoch 9483, Loss Train(MSE): 0.12512047429798778, R2 Train: 0.49951810280804887\n",
      "Epoch 9484, Loss Train(MSE): 0.12512044437661166, R2 Train: 0.4995182224935534\n",
      "Epoch 9485, Loss Train(MSE): 0.12512041446928832, R2 Train: 0.49951834212284674\n",
      "Epoch 9486, Loss Train(MSE): 0.12512038457600794, R2 Train: 0.49951846169596825\n",
      "Epoch 9487, Loss Train(MSE): 0.12512035469676078, R2 Train: 0.4995185812129569\n",
      "Epoch 9488, Loss Train(MSE): 0.12512032483153698, R2 Train: 0.49951870067385207\n",
      "Epoch 9489, Loss Train(MSE): 0.12512029498032687, R2 Train: 0.4995188200786925\n",
      "Epoch 9490, Loss Train(MSE): 0.12512026514312066, R2 Train: 0.49951893942751735\n",
      "Epoch 9491, Loss Train(MSE): 0.12512023531990857, R2 Train: 0.4995190587203657\n",
      "Epoch 9492, Loss Train(MSE): 0.12512020551068093, R2 Train: 0.4995191779572763\n",
      "Epoch 9493, Loss Train(MSE): 0.12512017571542794, R2 Train: 0.49951929713828824\n",
      "Epoch 9494, Loss Train(MSE): 0.12512014609248243, R2 Train: 0.4995194156300703\n",
      "Epoch 9495, Loss Train(MSE): 0.12512011653527633, R2 Train: 0.4995195338588947\n",
      "Epoch 9496, Loss Train(MSE): 0.12512008678087344, R2 Train: 0.49951965287650624\n",
      "Epoch 9497, Loss Train(MSE): 0.12512005704040965, R2 Train: 0.4995197718383614\n",
      "Epoch 9498, Loss Train(MSE): 0.12512002731387528, R2 Train: 0.4995198907444989\n",
      "Epoch 9499, Loss Train(MSE): 0.12511999760126064, R2 Train: 0.49952000959495746\n",
      "Epoch 9500, Loss Train(MSE): 0.125119967902556, R2 Train: 0.499520128389776\n",
      "Epoch 9501, Loss Train(MSE): 0.12511993821775177, R2 Train: 0.4995202471289929\n",
      "Epoch 9502, Loss Train(MSE): 0.1251199085468383, R2 Train: 0.4995203658126468\n",
      "Epoch 9503, Loss Train(MSE): 0.1251198788898059, R2 Train: 0.49952048444077635\n",
      "Epoch 9504, Loss Train(MSE): 0.125119849246645, R2 Train: 0.49952060301342005\n",
      "Epoch 9505, Loss Train(MSE): 0.1251198196173459, R2 Train: 0.49952072153061644\n",
      "Epoch 9506, Loss Train(MSE): 0.12511979000189902, R2 Train: 0.49952083999240393\n",
      "Epoch 9507, Loss Train(MSE): 0.1251197604002948, R2 Train: 0.4995209583988208\n",
      "Epoch 9508, Loss Train(MSE): 0.12511973081252356, R2 Train: 0.49952107674990576\n",
      "Epoch 9509, Loss Train(MSE): 0.12511970123857577, R2 Train: 0.4995211950456969\n",
      "Epoch 9510, Loss Train(MSE): 0.12511967167844187, R2 Train: 0.4995213132862325\n",
      "Epoch 9511, Loss Train(MSE): 0.12511964213211227, R2 Train: 0.4995214314715509\n",
      "Epoch 9512, Loss Train(MSE): 0.1251196125995774, R2 Train: 0.4995215496016904\n",
      "Epoch 9513, Loss Train(MSE): 0.12511958308082777, R2 Train: 0.49952166767668893\n",
      "Epoch 9514, Loss Train(MSE): 0.12511955357585378, R2 Train: 0.4995217856965849\n",
      "Epoch 9515, Loss Train(MSE): 0.1251195240846459, R2 Train: 0.49952190366141636\n",
      "Epoch 9516, Loss Train(MSE): 0.12511949460719468, R2 Train: 0.4995220215712213\n",
      "Epoch 9517, Loss Train(MSE): 0.125119469536486, R2 Train: 0.499522121854056\n",
      "Epoch 9518, Loss Train(MSE): 0.12511944496368752, R2 Train: 0.49952222014524994\n",
      "Epoch 9519, Loss Train(MSE): 0.12511942477615098, R2 Train: 0.49952230089539607\n",
      "Epoch 9520, Loss Train(MSE): 0.12511940020660695, R2 Train: 0.4995223991735722\n",
      "Epoch 9521, Loss Train(MSE): 0.12511937564915374, R2 Train: 0.49952249740338506\n",
      "Epoch 9522, Loss Train(MSE): 0.12511935504874933, R2 Train: 0.4995225798050027\n",
      "Epoch 9523, Loss Train(MSE): 0.12511933095895655, R2 Train: 0.4995226761641738\n",
      "Epoch 9524, Loss Train(MSE): 0.12511930643123034, R2 Train: 0.49952277427507863\n",
      "Epoch 9525, Loss Train(MSE): 0.12511928538302086, R2 Train: 0.49952285846791655\n",
      "Epoch 9526, Loss Train(MSE): 0.12511926179242344, R2 Train: 0.49952295283030623\n",
      "Epoch 9527, Loss Train(MSE): 0.12511923729435492, R2 Train: 0.4995230508225803\n",
      "Epoch 9528, Loss Train(MSE): 0.12511921579262328, R2 Train: 0.4995231368295069\n",
      "Epoch 9529, Loss Train(MSE): 0.12511919270683175, R2 Train: 0.499523229172673\n",
      "Epoch 9530, Loss Train(MSE): 0.12511916823835195, R2 Train: 0.4995233270465922\n",
      "Epoch 9531, Loss Train(MSE): 0.12511914627742834, R2 Train: 0.4995234148902866\n",
      "Epoch 9532, Loss Train(MSE): 0.1251191237020064, R2 Train: 0.4995235051919744\n",
      "Epoch 9533, Loss Train(MSE): 0.12511909926304665, R2 Train: 0.4995236029478134\n",
      "Epoch 9534, Loss Train(MSE): 0.12511907683730839, R2 Train: 0.49952369265076646\n",
      "Epoch 9535, Loss Train(MSE): 0.12511905477777305, R2 Train: 0.4995237808889078\n",
      "Epoch 9536, Loss Train(MSE): 0.12511903036826505, R2 Train: 0.4995238785269398\n",
      "Epoch 9537, Loss Train(MSE): 0.12511900747213595, R2 Train: 0.4995239701114562\n",
      "Epoch 9538, Loss Train(MSE): 0.1251189859339581, R2 Train: 0.4995240562641676\n",
      "Epoch 9539, Loss Train(MSE): 0.1251189615538338, R2 Train: 0.49952415378466475\n",
      "Epoch 9540, Loss Train(MSE): 0.12511893818178418, R2 Train: 0.49952424727286326\n",
      "Epoch 9541, Loss Train(MSE): 0.12511891717038873, R2 Train: 0.4995243313184451\n",
      "Epoch 9542, Loss Train(MSE): 0.12511889281958047, R2 Train: 0.4995244287216781\n",
      "Epoch 9543, Loss Train(MSE): 0.12511886896612656, R2 Train: 0.49952452413549375\n",
      "Epoch 9544, Loss Train(MSE): 0.1251188484868929, R2 Train: 0.49952460605242843\n",
      "Epoch 9545, Loss Train(MSE): 0.1251188241653332, R2 Train: 0.49952470333866716\n",
      "Epoch 9546, Loss Train(MSE): 0.125118799855614, R2 Train: 0.499524800577544\n",
      "Epoch 9547, Loss Train(MSE): 0.12511877985251182, R2 Train: 0.4995248805899527\n",
      "Epoch 9548, Loss Train(MSE): 0.12511875559181604, R2 Train: 0.49952497763273584\n",
      "Epoch 9549, Loss Train(MSE): 0.1251187313112451, R2 Train: 0.4995250747550196\n",
      "Epoch 9550, Loss Train(MSE): 0.1251187108076985, R2 Train: 0.49952515676920595\n",
      "Epoch 9551, Loss Train(MSE): 0.12511868709795193, R2 Train: 0.4995252516081923\n",
      "Epoch 9552, Loss Train(MSE): 0.12511866310552663, R2 Train: 0.49952534757789346\n",
      "Epoch 9553, Loss Train(MSE): 0.12511864227031488, R2 Train: 0.49952543091874047\n",
      "Epoch 9554, Loss Train(MSE): 0.12511861911675612, R2 Train: 0.4995255235329755\n",
      "Epoch 9555, Loss Train(MSE): 0.12511859490279895, R2 Train: 0.4995256203888042\n",
      "Epoch 9556, Loss Train(MSE): 0.125118573370406, R2 Train: 0.499525706518376\n",
      "Epoch 9557, Loss Train(MSE): 0.12511855079751474, R2 Train: 0.499525796809941\n",
      "Epoch 9558, Loss Train(MSE): 0.1251185266124434, R2 Train: 0.49952589355022636\n",
      "Epoch 9559, Loss Train(MSE): 0.12511850454450857, R2 Train: 0.4995259818219657\n",
      "Epoch 9560, Loss Train(MSE): 0.1251184825572874, R2 Train: 0.49952606977085035\n",
      "Epoch 9561, Loss Train(MSE): 0.1251184584010367, R2 Train: 0.49952616639585323\n",
      "Epoch 9562, Loss Train(MSE): 0.125118435792499, R2 Train: 0.49952625683000396\n",
      "Epoch 9563, Loss Train(MSE): 0.12511841439590798, R2 Train: 0.49952634241636806\n",
      "Epoch 9564, Loss Train(MSE): 0.12511839026841298, R2 Train: 0.49952643892634807\n",
      "Epoch 9565, Loss Train(MSE): 0.12511836711425425, R2 Train: 0.499526531542983\n",
      "Epoch 9566, Loss Train(MSE): 0.125118346313211, R2 Train: 0.499526614747156\n",
      "Epoch 9567, Loss Train(MSE): 0.1251183222144071, R2 Train: 0.49952671114237157\n",
      "Epoch 9568, Loss Train(MSE): 0.1251182985096515, R2 Train: 0.499526805961394\n",
      "Epoch 9569, Loss Train(MSE): 0.12511827830903177, R2 Train: 0.4995268867638729\n",
      "Epoch 9570, Loss Train(MSE): 0.12511825423885456, R2 Train: 0.49952698304458176\n",
      "Epoch 9571, Loss Train(MSE): 0.12511823018024765, R2 Train: 0.4995270792790094\n",
      "Epoch 9572, Loss Train(MSE): 0.1251182101812901, R2 Train: 0.4995271592748396\n",
      "Epoch 9573, Loss Train(MSE): 0.12511818634245536, R2 Train: 0.49952725463017855\n",
      "Epoch 9574, Loss Train(MSE): 0.1251181623123793, R2 Train: 0.4995273507504828\n",
      "Epoch 9575, Loss Train(MSE): 0.12511814174508532, R2 Train: 0.49952743301965874\n",
      "Epoch 9576, Loss Train(MSE): 0.12511811852417135, R2 Train: 0.4995275259033146\n",
      "Epoch 9577, Loss Train(MSE): 0.12511809452256262, R2 Train: 0.49952762190974953\n",
      "Epoch 9578, Loss Train(MSE): 0.1251180733821184, R2 Train: 0.49952770647152644\n",
      "Epoch 9579, Loss Train(MSE): 0.12511805078384025, R2 Train: 0.499527796864639\n",
      "Epoch 9580, Loss Train(MSE): 0.1251180268106354, R2 Train: 0.49952789275745835\n",
      "Epoch 9581, Loss Train(MSE): 0.12511800509226823, R2 Train: 0.4995279796309271\n",
      "Epoch 9582, Loss Train(MSE): 0.12511798312130037, R2 Train: 0.4995280675147985\n",
      "Epoch 9583, Loss Train(MSE): 0.12511795917643642, R2 Train: 0.4995281632942543\n",
      "Epoch 9584, Loss Train(MSE): 0.12511793687541414, R2 Train: 0.49952825249834343\n",
      "Epoch 9585, Loss Train(MSE): 0.1251179155363907, R2 Train: 0.4995283378544372\n",
      "Epoch 9586, Loss Train(MSE): 0.12511789161980483, R2 Train: 0.4995284335207807\n",
      "Epoch 9587, Loss Train(MSE): 0.1251178687314357, R2 Train: 0.49952852507425716\n",
      "Epoch 9588, Loss Train(MSE): 0.12511784802895093, R2 Train: 0.4995286078841963\n",
      "Epoch 9589, Loss Train(MSE): 0.12511782414058054, R2 Train: 0.49952870343767786\n",
      "Epoch 9590, Loss Train(MSE): 0.1251178006602129, R2 Train: 0.49952879735914835\n",
      "Epoch 9591, Loss Train(MSE): 0.12511778059882125, R2 Train: 0.499528877604715\n",
      "Epoch 9592, Loss Train(MSE): 0.12511775673860412, R2 Train: 0.4995289730455835\n",
      "Epoch 9593, Loss Train(MSE): 0.12511773288975422, R2 Train: 0.4995290684409831\n",
      "Epoch 9594, Loss Train(MSE): 0.12511771301746294, R2 Train: 0.49952914793014824\n",
      "Epoch 9595, Loss Train(MSE): 0.1251176894145586, R2 Train: 0.49952924234176554\n",
      "Epoch 9596, Loss Train(MSE): 0.12511766559376922, R2 Train: 0.49952933762492313\n",
      "Epoch 9597, Loss Train(MSE): 0.12511764511258763, R2 Train: 0.49952941954964947\n",
      "Epoch 9598, Loss Train(MSE): 0.12511762239791951, R2 Train: 0.49952951040832194\n",
      "Epoch 9599, Loss Train(MSE): 0.125117598891478, R2 Train: 0.499529604434088\n",
      "Epoch 9600, Loss Train(MSE): 0.1251175777960174, R2 Train: 0.4995296888159304\n",
      "Epoch 9601, Loss Train(MSE): 0.1251175555205692, R2 Train: 0.49952977791772324\n",
      "Epoch 9602, Loss Train(MSE): 0.12511753176373966, R2 Train: 0.49952987294504136\n",
      "Epoch 9603, Loss Train(MSE): 0.1251175100315702, R2 Train: 0.4995299598737192\n",
      "Epoch 9604, Loss Train(MSE): 0.1251174884414363, R2 Train: 0.4995300462342548\n",
      "Epoch 9605, Loss Train(MSE): 0.12511746471242466, R2 Train: 0.49953014115030137\n",
      "Epoch 9606, Loss Train(MSE): 0.1251174423392377, R2 Train: 0.49953023064304924\n",
      "Epoch 9607, Loss Train(MSE): 0.12511742143863222, R2 Train: 0.4995303142454711\n",
      "Epoch 9608, Loss Train(MSE): 0.12511739773737815, R2 Train: 0.4995304090504874\n",
      "Epoch 9609, Loss Train(MSE): 0.1251173747189023, R2 Train: 0.4995305011243908\n",
      "Epoch 9610, Loss Train(MSE): 0.12511735451200245, R2 Train: 0.4995305819519902\n",
      "Epoch 9611, Loss Train(MSE): 0.12511733083844587, R2 Train: 0.49953067664621653\n",
      "Epoch 9612, Loss Train(MSE): 0.12511730717605746, R2 Train: 0.49953077129577017\n",
      "Epoch 9613, Loss Train(MSE): 0.12511728765551083, R2 Train: 0.49953084937795666\n",
      "Epoch 9614, Loss Train(MSE): 0.12511726401629228, R2 Train: 0.4995309439348309\n",
      "Epoch 9615, Loss Train(MSE): 0.1251172403815119, R2 Train: 0.4995310384739524\n",
      "Epoch 9616, Loss Train(MSE): 0.12511722019975796, R2 Train: 0.49953111920096815\n",
      "Epoch 9617, Loss Train(MSE): 0.12511719726993584, R2 Train: 0.49953121092025665\n",
      "Epoch 9618, Loss Train(MSE): 0.12511717366270406, R2 Train: 0.4995313053491838\n",
      "Epoch 9619, Loss Train(MSE): 0.1251171528156163, R2 Train: 0.4995313887375348\n",
      "Epoch 9620, Loss Train(MSE): 0.1251171305992242, R2 Train: 0.49953147760310324\n",
      "Epoch 9621, Loss Train(MSE): 0.1251171070194818, R2 Train: 0.49953157192207276\n",
      "Epoch 9622, Loss Train(MSE): 0.12511708550296996, R2 Train: 0.49953165798812016\n",
      "Epoch 9623, Loss Train(MSE): 0.1251170640040056, R2 Train: 0.4995317439839776\n",
      "Epoch 9624, Loss Train(MSE): 0.12511704045169367, R2 Train: 0.49953183819322533\n",
      "Epoch 9625, Loss Train(MSE): 0.1251170182617033, R2 Train: 0.4995319269531868\n",
      "Epoch 9626, Loss Train(MSE): 0.12511699748412888, R2 Train: 0.49953201006348447\n",
      "Epoch 9627, Loss Train(MSE): 0.12511697395918867, R2 Train: 0.49953210416324534\n",
      "Epoch 9628, Loss Train(MSE): 0.12511695109170107, R2 Train: 0.49953219563319573\n",
      "Epoch 9629, Loss Train(MSE): 0.12511693103944338, R2 Train: 0.4995322758422265\n",
      "Epoch 9630, Loss Train(MSE): 0.1251169075418165, R2 Train: 0.49953236983273397\n",
      "Epoch 9631, Loss Train(MSE): 0.1251168840551933, R2 Train: 0.4995324637792268\n",
      "Epoch 9632, Loss Train(MSE): 0.1251168646071709, R2 Train: 0.49953254157131644\n",
      "Epoch 9633, Loss Train(MSE): 0.12511684120022773, R2 Train: 0.4995326351990891\n",
      "Epoch 9634, Loss Train(MSE): 0.1251168177408308, R2 Train: 0.4995327290366768\n",
      "Epoch 9635, Loss Train(MSE): 0.12511679760005753, R2 Train: 0.49953280959976987\n",
      "Epoch 9636, Loss Train(MSE): 0.12511677493346274, R2 Train: 0.49953290026614905\n",
      "Epoch 9637, Loss Train(MSE): 0.12511675166940842, R2 Train: 0.49953299332236634\n",
      "Epoch 9638, Loss Train(MSE): 0.125116731250988, R2 Train: 0.49953307499604804\n",
      "Epoch 9639, Loss Train(MSE): 0.1251167093279181, R2 Train: 0.4995331626883276\n",
      "Epoch 9640, Loss Train(MSE): 0.12511668593046318, R2 Train: 0.49953325627814726\n",
      "Epoch 9641, Loss Train(MSE): 0.12511666438084737, R2 Train: 0.4995333424766105\n",
      "Epoch 9642, Loss Train(MSE): 0.1251166432240916, R2 Train: 0.4995334271036336\n",
      "Epoch 9643, Loss Train(MSE): 0.12511661985363484, R2 Train: 0.49953352058546063\n",
      "Epoch 9644, Loss Train(MSE): 0.12511659758136062, R2 Train: 0.4995336096745575\n",
      "Epoch 9645, Loss Train(MSE): 0.12511657719453267, R2 Train: 0.4995336912218693\n",
      "Epoch 9646, Loss Train(MSE): 0.1251165538510174, R2 Train: 0.49953378459593045\n",
      "Epoch 9647, Loss Train(MSE): 0.1251165308524148, R2 Train: 0.49953387659034076\n",
      "Epoch 9648, Loss Train(MSE): 0.1251165112390954, R2 Train: 0.4995339550436184\n",
      "Epoch 9649, Loss Train(MSE): 0.12511648792246508, R2 Train: 0.4995340483101397\n",
      "Epoch 9650, Loss Train(MSE): 0.12511646461665152, R2 Train: 0.4995341415333939\n",
      "Epoch 9651, Loss Train(MSE): 0.12511644493457857, R2 Train: 0.4995342202616857\n",
      "Epoch 9652, Loss Train(MSE): 0.12511643081951643, R2 Train: 0.4995342767219343\n",
      "Epoch 9653, Loss Train(MSE): 0.12512857850900883, R2 Train: 0.4994856859639647\n",
      "Epoch 9654, Loss Train(MSE): 0.12512852412221784, R2 Train: 0.49948590351112865\n",
      "Epoch 9655, Loss Train(MSE): 0.1251284645094161, R2 Train: 0.49948614196233565\n",
      "Epoch 9656, Loss Train(MSE): 0.1251284077366657, R2 Train: 0.4994863690533372\n",
      "Epoch 9657, Loss Train(MSE): 0.1251283508016783, R2 Train: 0.4994865967932868\n",
      "Epoch 9658, Loss Train(MSE): 0.12512829162876485, R2 Train: 0.4994868334849406\n",
      "Epoch 9659, Loss Train(MSE): 0.12512823738433243, R2 Train: 0.4994870504626703\n",
      "Epoch 9660, Loss Train(MSE): 0.12512817675997728, R2 Train: 0.49948729296009087\n",
      "Epoch 9661, Loss Train(MSE): 0.12512812329849243, R2 Train: 0.49948750680603027\n",
      "Epoch 9662, Loss Train(MSE): 0.1251280637942198, R2 Train: 0.49948774482312075\n",
      "Epoch 9663, Loss Train(MSE): 0.12512800788049586, R2 Train: 0.4994879684780166\n",
      "Epoch 9664, Loss Train(MSE): 0.12512795111514646, R2 Train: 0.49948819553941415\n",
      "Epoch 9665, Loss Train(MSE): 0.12512789273556935, R2 Train: 0.4994884290577226\n",
      "Epoch 9666, Loss Train(MSE): 0.12512783872133915, R2 Train: 0.4994886451146434\n",
      "Epoch 9667, Loss Train(MSE): 0.12512777866817051, R2 Train: 0.49948888532731794\n",
      "Epoch 9668, Loss Train(MSE): 0.12512772581091847, R2 Train: 0.49948909675632613\n",
      "Epoch 9669, Loss Train(MSE): 0.12512766671800066, R2 Train: 0.49948933312799737\n",
      "Epoch 9670, Loss Train(MSE): 0.12512761134453512, R2 Train: 0.4994895546218595\n",
      "Epoch 9671, Loss Train(MSE): 0.1251275550495011, R2 Train: 0.4994897798019956\n",
      "Epoch 9672, Loss Train(MSE): 0.1251274971467777, R2 Train: 0.4994900114128892\n",
      "Epoch 9673, Loss Train(MSE): 0.12512744366129647, R2 Train: 0.49949022535481413\n",
      "Epoch 9674, Loss Train(MSE): 0.12512738416911956, R2 Train: 0.49949046332352176\n",
      "Epoch 9675, Loss Train(MSE): 0.1251273316044916, R2 Train: 0.4994906735820336\n",
      "Epoch 9676, Loss Train(MSE): 0.1251272732168382, R2 Train: 0.4994909071326472\n",
      "Epoch 9677, Loss Train(MSE): 0.12512721807425303, R2 Train: 0.49949112770298787\n",
      "Epoch 9678, Loss Train(MSE): 0.1251271625413626, R2 Train: 0.49949134983454957\n",
      "Epoch 9679, Loss Train(MSE): 0.12512710480832634, R2 Train: 0.49949158076669464\n",
      "Epoch 9680, Loss Train(MSE): 0.12512705214135805, R2 Train: 0.4994917914345678\n",
      "Epoch 9681, Loss Train(MSE): 0.12512699320027604, R2 Train: 0.49949202719889585\n",
      "Epoch 9682, Loss Train(MSE): 0.12512694062583615, R2 Train: 0.4994922374966554\n",
      "Epoch 9683, Loss Train(MSE): 0.12512688322870585, R2 Train: 0.4994924670851766\n",
      "Epoch 9684, Loss Train(MSE): 0.12512682801672384, R2 Train: 0.49949268793310464\n",
      "Epoch 9685, Loss Train(MSE): 0.125126773529219, R2 Train: 0.499492905883124\n",
      "Epoch 9686, Loss Train(MSE): 0.12512671566773306, R2 Train: 0.49949313732906775\n",
      "Epoch 9687, Loss Train(MSE): 0.12512666410051942, R2 Train: 0.4994933435979223\n",
      "Epoch 9688, Loss Train(MSE): 0.12512660570091952, R2 Train: 0.4994935771963219\n",
      "Epoch 9689, Loss Train(MSE): 0.1251265528231233, R2 Train: 0.4994937887075068\n",
      "Epoch 9690, Loss Train(MSE): 0.1251264966933801, R2 Train: 0.49949401322647957\n",
      "Epoch 9691, Loss Train(MSE): 0.12512644112054638, R2 Train: 0.49949423551781447\n",
      "Epoch 9692, Loss Train(MSE): 0.12512638795333691, R2 Train: 0.49949444818665234\n",
      "Epoch 9693, Loss Train(MSE): 0.12512632993871586, R2 Train: 0.49949468024513655\n",
      "Epoch 9694, Loss Train(MSE): 0.1251262792198, R2 Train: 0.49949488312080004\n",
      "Epoch 9695, Loss Train(MSE): 0.1251262216147166, R2 Train: 0.49949511354113363\n",
      "Epoch 9696, Loss Train(MSE): 0.12512616815635486, R2 Train: 0.4994953273745806\n",
      "Epoch 9697, Loss Train(MSE): 0.12512611355498932, R2 Train: 0.49949554578004274\n",
      "Epoch 9698, Loss Train(MSE): 0.1251260573461073, R2 Train: 0.4994957706155708\n",
      "Epoch 9699, Loss Train(MSE): 0.12512600575830027, R2 Train: 0.4994959769667989\n",
      "Epoch 9700, Loss Train(MSE): 0.12512594826923748, R2 Train: 0.4994962069230501\n",
      "Epoch 9701, Loss Train(MSE): 0.12512589674702926, R2 Train: 0.49949641301188297\n",
      "Epoch 9702, Loss Train(MSE): 0.1251258408816646, R2 Train: 0.49949663647334164\n",
      "Epoch 9703, Loss Train(MSE): 0.12512578656600173, R2 Train: 0.49949685373599306\n",
      "Epoch 9704, Loss Train(MSE): 0.12512573375399488, R2 Train: 0.49949706498402047\n",
      "Epoch 9705, Loss Train(MSE): 0.1251256766386938, R2 Train: 0.49949729344522475\n",
      "Epoch 9706, Loss Train(MSE): 0.12512562688546394, R2 Train: 0.49949749245814423\n",
      "Epoch 9707, Loss Train(MSE): 0.1251255699152636, R2 Train: 0.49949772033894557\n",
      "Epoch 9708, Loss Train(MSE): 0.12512551732671065, R2 Train: 0.4994979306931574\n",
      "Epoch 9709, Loss Train(MSE): 0.1251254634486633, R2 Train: 0.4994981462053468\n",
      "Epoch 9710, Loss Train(MSE): 0.12512540801455038, R2 Train: 0.49949836794179847\n",
      "Epoch 9711, Loss Train(MSE): 0.1251253572377164, R2 Train: 0.4994985710491344\n",
      "Epoch 9712, Loss Train(MSE): 0.12512530063279295, R2 Train: 0.4994987974688282\n",
      "Epoch 9713, Loss Train(MSE): 0.1251252496011567, R2 Train: 0.4994990015953732\n",
      "Epoch 9714, Loss Train(MSE): 0.12512519481947812, R2 Train: 0.4994992207220875\n",
      "Epoch 9715, Loss Train(MSE): 0.12512514090180915, R2 Train: 0.4994994363927634\n",
      "Epoch 9716, Loss Train(MSE): 0.12512508925882587, R2 Train: 0.4994996429646965\n",
      "Epoch 9717, Loss Train(MSE): 0.12512503301696265, R2 Train: 0.4994998679321494\n",
      "Epoch 9718, Loss Train(MSE): 0.12512498338279376, R2 Train: 0.49950006646882494\n",
      "Epoch 9719, Loss Train(MSE): 0.1251249278492876, R2 Train: 0.49950028860284956\n",
      "Epoch 9720, Loss Train(MSE): 0.1251248752896235, R2 Train: 0.49950049884150605\n",
      "Epoch 9721, Loss Train(MSE): 0.12512482293134145, R2 Train: 0.4995007082746342\n",
      "Epoch 9722, Loss Train(MSE): 0.12512476743673623, R2 Train: 0.4995009302530551\n",
      "Epoch 9723, Loss Train(MSE): 0.12512471826200097, R2 Train: 0.4995011269519961\n",
      "Epoch 9724, Loss Train(MSE): 0.12512466251602414, R2 Train: 0.49950134993590345\n",
      "Epoch 9725, Loss Train(MSE): 0.12512461115189763, R2 Train: 0.4995015553924095\n",
      "Epoch 9726, Loss Train(MSE): 0.12512455823335017, R2 Train: 0.4995017670665993\n",
      "Epoch 9727, Loss Train(MSE): 0.1251245038961422, R2 Train: 0.49950198441543125\n",
      "Epoch 9728, Loss Train(MSE): 0.12512445419642454, R2 Train: 0.49950218321430184\n",
      "Epoch 9729, Loss Train(MSE): 0.1251243988032854, R2 Train: 0.49950240478685837\n",
      "Epoch 9730, Loss Train(MSE): 0.12512434848259243, R2 Train: 0.49950260606963026\n",
      "Epoch 9731, Loss Train(MSE): 0.12512429514857737, R2 Train: 0.4995028194056905\n",
      "Epoch 9732, Loss Train(MSE): 0.1251242418176207, R2 Train: 0.49950303272951724\n",
      "Epoch 9733, Loss Train(MSE): 0.1251241917368135, R2 Train: 0.499503233052746\n",
      "Epoch 9734, Loss Train(MSE): 0.1251241366923597, R2 Train: 0.4995034532305612\n",
      "Epoch 9735, Loss Train(MSE): 0.12512408726600957, R2 Train: 0.4995036509359617\n",
      "Epoch 9736, Loss Train(MSE): 0.12512403365844652, R2 Train: 0.49950386536621394\n",
      "Epoch 9737, Loss Train(MSE): 0.12512398118558712, R2 Train: 0.4995040752576515\n",
      "Epoch 9738, Loss Train(MSE): 0.1251239308647248, R2 Train: 0.49950427654110074\n",
      "Epoch 9739, Loss Train(MSE): 0.12512387616488005, R2 Train: 0.4995044953404798\n",
      "Epoch 9740, Loss Train(MSE): 0.12512382748673267, R2 Train: 0.4995046900530693\n",
      "Epoch 9741, Loss Train(MSE): 0.12512377374472158, R2 Train: 0.49950490502111367\n",
      "Epoch 9742, Loss Train(MSE): 0.12512372198473568, R2 Train: 0.4995051120610573\n",
      "Epoch 9743, Loss Train(MSE): 0.1251236715620519, R2 Train: 0.4995053137517924\n",
      "Epoch 9744, Loss Train(MSE): 0.12512361720281376, R2 Train: 0.49950553118874497\n",
      "Epoch 9745, Loss Train(MSE): 0.1251235691296193, R2 Train: 0.4995057234815228\n",
      "Epoch 9746, Loss Train(MSE): 0.12512351538949712, R2 Train: 0.4995059384420115\n",
      "Epoch 9747, Loss Train(MSE): 0.1251234642000309, R2 Train: 0.4995061431998764\n",
      "Epoch 9748, Loss Train(MSE): 0.12512341381101513, R2 Train: 0.49950634475593947\n",
      "Epoch 9749, Loss Train(MSE): 0.12512335978845307, R2 Train: 0.4995065608461877\n",
      "Epoch 9750, Loss Train(MSE): 0.12512331217979225, R2 Train: 0.499506751280831\n",
      "Epoch 9751, Loss Train(MSE): 0.12512325857518897, R2 Train: 0.49950696569924413\n",
      "Epoch 9752, Loss Train(MSE): 0.12512320781669975, R2 Train: 0.499507168733201\n",
      "Epoch 9753, Loss Train(MSE): 0.12512315759415255, R2 Train: 0.4995073696233898\n",
      "Epoch 9754, Loss Train(MSE): 0.12512310390440573, R2 Train: 0.4995075843823771\n",
      "Epoch 9755, Loss Train(MSE): 0.12512305662263234, R2 Train: 0.4995077735094706\n",
      "Epoch 9756, Loss Train(MSE): 0.12512300328452497, R2 Train: 0.4995079868619001\n",
      "Epoch 9757, Loss Train(MSE): 0.12512295282022398, R2 Train: 0.4995081887191041\n",
      "Epoch 9758, Loss Train(MSE): 0.12512290289431077, R2 Train: 0.4995083884227569\n",
      "Epoch 9759, Loss Train(MSE): 0.12512284953358638, R2 Train: 0.4995086018656545\n",
      "Epoch 9760, Loss Train(MSE): 0.12512280244377072, R2 Train: 0.4995087902249171\n",
      "Epoch 9761, Loss Train(MSE): 0.12512274950053645, R2 Train: 0.4995090019978542\n",
      "Epoch 9762, Loss Train(MSE): 0.12512269919633276, R2 Train: 0.49950920321466896\n",
      "Epoch 9763, Loss Train(MSE): 0.12512264969463657, R2 Train: 0.4995094012214537\n",
      "Epoch 9764, Loss Train(MSE): 0.1251225966592079, R2 Train: 0.49950961336316835\n",
      "Epoch 9765, Loss Train(MSE): 0.12512254962908173, R2 Train: 0.4995098014836731\n",
      "Epoch 9766, Loss Train(MSE): 0.12512249720654997, R2 Train: 0.4995100111738001\n",
      "Epoch 9767, Loss Train(MSE): 0.12512244693099597, R2 Train: 0.4995102122760161\n",
      "Epoch 9768, Loss Train(MSE): 0.1251223979785688, R2 Train: 0.4995104080857248\n",
      "Epoch 9769, Loss Train(MSE): 0.12512234526477348, R2 Train: 0.4995106189409061\n",
      "Epoch 9770, Loss Train(MSE): 0.12512229816467618, R2 Train: 0.49951080734129527\n",
      "Epoch 9771, Loss Train(MSE): 0.125122246386179, R2 Train: 0.499511014455284\n",
      "Epoch 9772, Loss Train(MSE): 0.12512219601041707, R2 Train: 0.4995112159583317\n",
      "Epoch 9773, Loss Train(MSE): 0.1251221477298303, R2 Train: 0.4995114090806788\n",
      "Epoch 9774, Loss Train(MSE): 0.1251220953340686, R2 Train: 0.4995116186637256\n",
      "Epoch 9775, Loss Train(MSE): 0.12512204803689486, R2 Train: 0.49951180785242055\n",
      "Epoch 9776, Loss Train(MSE): 0.12512199702331672, R2 Train: 0.4995120119067331\n",
      "Epoch 9777, Loss Train(MSE): 0.12512194642102717, R2 Train: 0.4995122143158913\n",
      "Epoch 9778, Loss Train(MSE): 0.12512189893242057, R2 Train: 0.49951240427031773\n",
      "Epoch 9779, Loss Train(MSE): 0.12512184685115388, R2 Train: 0.4995126125953845\n",
      "Epoch 9780, Loss Train(MSE): 0.12512179923230235, R2 Train: 0.4995128030707906\n",
      "Epoch 9781, Loss Train(MSE): 0.1251217491021283, R2 Train: 0.49951300359148676\n",
      "Epoch 9782, Loss Train(MSE): 0.12512169814947863, R2 Train: 0.4995132074020855\n",
      "Epoch 9783, Loss Train(MSE): 0.1251216515706084, R2 Train: 0.4995133937175664\n",
      "Epoch 9784, Loss Train(MSE): 0.12512159980035759, R2 Train: 0.49951360079856966\n",
      "Epoch 9785, Loss Train(MSE): 0.12512155173768086, R2 Train: 0.49951379304927657\n",
      "Epoch 9786, Loss Train(MSE): 0.12512150260704397, R2 Train: 0.49951398957182414\n",
      "Epoch 9787, Loss Train(MSE): 0.1251214511826392, R2 Train: 0.4995141952694432\n",
      "Epoch 9788, Loss Train(MSE): 0.12512140562892496, R2 Train: 0.49951437748430016\n",
      "Epoch 9789, Loss Train(MSE): 0.12512135416626888, R2 Train: 0.4995145833349245\n",
      "Epoch 9790, Loss Train(MSE): 0.12512130554002485, R2 Train: 0.4995147778399006\n",
      "Epoch 9791, Loss Train(MSE): 0.1251212575227523, R2 Train: 0.4995149699089908\n",
      "Epoch 9792, Loss Train(MSE): 0.1251212062474089, R2 Train: 0.49951517501036435\n",
      "Epoch 9793, Loss Train(MSE): 0.1251211603563796, R2 Train: 0.4995153585744816\n",
      "Epoch 9794, Loss Train(MSE): 0.12512110993631345, R2 Train: 0.4995155602547462\n",
      "Epoch 9795, Loss Train(MSE): 0.1251210606351711, R2 Train: 0.49951575745931565\n",
      "Epoch 9796, Loss Train(MSE): 0.12512101383676608, R2 Train: 0.4995159446529357\n",
      "Epoch 9797, Loss Train(MSE): 0.12512096286360858, R2 Train: 0.4995161485455657\n",
      "Epoch 9798, Loss Train(MSE): 0.1251209162085219, R2 Train: 0.49951633516591243\n",
      "Epoch 9799, Loss Train(MSE): 0.12512086709297285, R2 Train: 0.4995165316281086\n",
      "Epoch 9800, Loss Train(MSE): 0.12512081700176922, R2 Train: 0.4995167319929231\n",
      "Epoch 9801, Loss Train(MSE): 0.12512077153167123, R2 Train: 0.4995169138733151\n",
      "Epoch 9802, Loss Train(MSE): 0.12512072085742348, R2 Train: 0.49951711657030606\n",
      "Epoch 9803, Loss Train(MSE): 0.12512067332468751, R2 Train: 0.49951730670124994\n",
      "Epoch 9804, Loss Train(MSE): 0.12512062562158044, R2 Train: 0.49951749751367824\n",
      "Epoch 9805, Loss Train(MSE): 0.1251205751293759, R2 Train: 0.49951769948249636\n",
      "Epoch 9806, Loss Train(MSE): 0.1251205300948776, R2 Train: 0.49951787962048955\n",
      "Epoch 9807, Loss Train(MSE): 0.12512048021690078, R2 Train: 0.4995180791323969\n",
      "Epoch 9808, Loss Train(MSE): 0.1251204317010318, R2 Train: 0.49951827319587283\n",
      "Epoch 9809, Loss Train(MSE): 0.1251203855102648, R2 Train: 0.4995184579589408\n",
      "Epoch 9810, Loss Train(MSE): 0.12512033531178793, R2 Train: 0.4995186587528483\n",
      "Epoch 9811, Loss Train(MSE): 0.12512028920909207, R2 Train: 0.4995188431636317\n",
      "Epoch 9812, Loss Train(MSE): 0.12512024092515528, R2 Train: 0.4995190362993789\n",
      "Epoch 9813, Loss Train(MSE): 0.1251201913169062, R2 Train: 0.49951923473237525\n",
      "Epoch 9814, Loss Train(MSE): 0.1251201467422389, R2 Train: 0.4995194130310444\n",
      "Epoch 9815, Loss Train(MSE): 0.12512009683434963, R2 Train: 0.4995196126626015\n",
      "Epoch 9816, Loss Train(MSE): 0.1251200495557031, R2 Train: 0.4995198017771876\n",
      "Epoch 9817, Loss Train(MSE): 0.12512000296812503, R2 Train: 0.49951998812749987\n",
      "Epoch 9818, Loss Train(MSE): 0.12511995323722783, R2 Train: 0.4995201870510887\n",
      "Epoch 9819, Loss Train(MSE): 0.12511990823054, R2 Train: 0.49952036707784\n",
      "Epoch 9820, Loss Train(MSE): 0.12511985968568926, R2 Train: 0.499520561257243\n",
      "Epoch 9821, Loss Train(MSE): 0.1251198111311582, R2 Train: 0.49952075547536723\n",
      "Epoch 9822, Loss Train(MSE): 0.12511976633451413, R2 Train: 0.49952093466194347\n",
      "Epoch 9823, Loss Train(MSE): 0.12511971688923545, R2 Train: 0.4995211324430582\n",
      "Epoch 9824, Loss Train(MSE): 0.1251196705254939, R2 Train: 0.49952131789802445\n",
      "Epoch 9825, Loss Train(MSE): 0.1251196238495179, R2 Train: 0.49952150460192835\n",
      "Epoch 9826, Loss Train(MSE): 0.1251195745782143, R2 Train: 0.4995217016871428\n",
      "Epoch 9827, Loss Train(MSE): 0.12511953034916504, R2 Train: 0.49952187860333985\n",
      "Epoch 9828, Loss Train(MSE): 0.12511948184799856, R2 Train: 0.49952207260800574\n",
      "Epoch 9829, Loss Train(MSE): 0.12511943403058626, R2 Train: 0.49952226387765497\n",
      "Epoch 9830, Loss Train(MSE): 0.12511938931487684, R2 Train: 0.4995224427404926\n",
      "Epoch 9831, Loss Train(MSE): 0.12511934032434782, R2 Train: 0.4995226387026087\n",
      "Epoch 9832, Loss Train(MSE): 0.12511929456278764, R2 Train: 0.49952282174884943\n",
      "Epoch 9833, Loss Train(MSE): 0.12511924809757938, R2 Train: 0.4995230076096825\n",
      "Epoch 9834, Loss Train(MSE): 0.12511919927808401, R2 Train: 0.49952320288766394\n",
      "Epoch 9835, Loss Train(MSE): 0.12511915551778, R2 Train: 0.49952337792888\n",
      "Epoch 9836, Loss Train(MSE): 0.12511910735576123, R2 Train: 0.49952357057695507\n",
      "Epoch 9837, Loss Train(MSE): 0.12511905996828623, R2 Train: 0.49952376012685507\n",
      "Epoch 9838, Loss Train(MSE): 0.12511901562734346, R2 Train: 0.4995239374906262\n",
      "Epoch 9839, Loss Train(MSE): 0.12511896708389936, R2 Train: 0.49952413166440257\n",
      "Epoch 9840, Loss Train(MSE): 0.1251189216210997, R2 Train: 0.4995243135156012\n",
      "Epoch 9841, Loss Train(MSE): 0.12511887565685575, R2 Train: 0.499524497372577\n",
      "Epoch 9842, Loss Train(MSE): 0.1251188272815767, R2 Train: 0.4995246908736932\n",
      "Epoch 9843, Loss Train(MSE): 0.12511878369031368, R2 Train: 0.4995248652387453\n",
      "Epoch 9844, Loss Train(MSE): 0.12511873615404603, R2 Train: 0.49952505538381586\n",
      "Epoch 9845, Loss Train(MSE): 0.12511868889845956, R2 Train: 0.49952524440616175\n",
      "Epoch 9846, Loss Train(MSE): 0.12511864521730873, R2 Train: 0.4995254191307651\n",
      "Epoch 9847, Loss Train(MSE): 0.12511859711347406, R2 Train: 0.49952561154610375\n",
      "Epoch 9848, Loss Train(MSE): 0.1251185516550356, R2 Train: 0.4995257933798576\n",
      "Epoch 9849, Loss Train(MSE): 0.12511850647325254, R2 Train: 0.49952597410698985\n",
      "Epoch 9850, Loss Train(MSE): 0.12511845853478462, R2 Train: 0.4995261658608615\n",
      "Epoch 9851, Loss Train(MSE): 0.12511841482177039, R2 Train: 0.49952634071291846\n",
      "Epoch 9852, Loss Train(MSE): 0.1251183681892624, R2 Train: 0.49952652724295044\n",
      "Epoch 9853, Loss Train(MSE): 0.12511832077637344, R2 Train: 0.49952671689450623\n",
      "Epoch 9854, Loss Train(MSE): 0.12511827803149675, R2 Train: 0.499526887874013\n",
      "Epoch 9855, Loss Train(MSE): 0.12511823035997857, R2 Train: 0.4995270785600857\n",
      "Epoch 9856, Loss Train(MSE): 0.1251181846202524, R2 Train: 0.4995272615189904\n",
      "Epoch 9857, Loss Train(MSE): 0.12511814049398673, R2 Train: 0.4995274380240531\n",
      "Epoch 9858, Loss Train(MSE): 0.1251180929851047, R2 Train: 0.4995276280595812\n",
      "Epoch 9859, Loss Train(MSE): 0.1251180488681917, R2 Train: 0.4995278045272332\n",
      "Epoch 9860, Loss Train(MSE): 0.12511800340911344, R2 Train: 0.49952798636354623\n",
      "Epoch 9861, Loss Train(MSE): 0.1251179560618437, R2 Train: 0.4995281757526252\n",
      "Epoch 9862, Loss Train(MSE): 0.12511791351792337, R2 Train: 0.4995283459283065\n",
      "Epoch 9863, Loss Train(MSE): 0.12511786677409573, R2 Train: 0.4995285329036171\n",
      "Epoch 9864, Loss Train(MSE): 0.12511782048108194, R2 Train: 0.49952871807567223\n",
      "Epoch 9865, Loss Train(MSE): 0.12511777767003204, R2 Train: 0.4995288893198718\n",
      "Epoch 9866, Loss Train(MSE): 0.12511773058368178, R2 Train: 0.4995290776652729\n",
      "Epoch 9867, Loss Train(MSE): 0.12511768579426083, R2 Train: 0.4995292568229567\n",
      "Epoch 9868, Loss Train(MSE): 0.12511764176503093, R2 Train: 0.4995294329398763\n",
      "Epoch 9869, Loss Train(MSE): 0.12511759483765234, R2 Train: 0.49952962064939066\n",
      "Epoch 9870, Loss Train(MSE): 0.12511755150332343, R2 Train: 0.49952979398670627\n",
      "Epoch 9871, Loss Train(MSE): 0.12511750630269833, R2 Train: 0.4995299747892067\n",
      "Epoch 9872, Loss Train(MSE): 0.125117459533301, R2 Train: 0.499530161866796\n",
      "Epoch 9873, Loss Train(MSE): 0.12511741760607206, R2 Train: 0.49953032957571175\n",
      "Epoch 9874, Loss Train(MSE): 0.12511737128034306, R2 Train: 0.49953051487862776\n",
      "Epoch 9875, Loss Train(MSE): 0.12511732555770963, R2 Train: 0.4995306977691615\n",
      "Epoch 9876, Loss Train(MSE): 0.12511728320712814, R2 Train: 0.4995308671714874\n",
      "Epoch 9877, Loss Train(MSE): 0.1251172366928173, R2 Train: 0.4995310532287308\n",
      "Epoch 9878, Loss Train(MSE): 0.1251171923106303, R2 Train: 0.49953123075747885\n",
      "Epoch 9879, Loss Train(MSE): 0.1251171488988586, R2 Train: 0.49953140440456556\n",
      "Epoch 9880, Loss Train(MSE): 0.1251171025399742, R2 Train: 0.49953158984010315\n",
      "Epoch 9881, Loss Train(MSE): 0.12511705945150928, R2 Train: 0.4995317621939629\n",
      "Epoch 9882, Loss Train(MSE): 0.1251170150236104, R2 Train: 0.49953193990555844\n",
      "Epoch 9883, Loss Train(MSE): 0.12511696881919399, R2 Train: 0.49953212472322406\n",
      "Epoch 9884, Loss Train(MSE): 0.12511692697821572, R2 Train: 0.4995322920871371\n",
      "Epoch 9885, Loss Train(MSE): 0.1251168815787783, R2 Train: 0.49953247368488685\n",
      "Epoch 9886, Loss Train(MSE): 0.1251168358988637, R2 Train: 0.49953265640454525\n",
      "Epoch 9887, Loss Train(MSE): 0.1251167945142892, R2 Train: 0.4995328219428432\n",
      "Epoch 9888, Loss Train(MSE): 0.12511674855931462, R2 Train: 0.4995330057627415\n",
      "Epoch 9889, Loss Train(MSE): 0.12511670406286984, R2 Train: 0.4995331837485206\n",
      "Epoch 9890, Loss Train(MSE): 0.12511666176812036, R2 Train: 0.49953335292751855\n",
      "Epoch 9891, Loss Train(MSE): 0.12511661596514098, R2 Train: 0.4995335361394361\n",
      "Epoch 9892, Loss Train(MSE): 0.12511657260714878, R2 Train: 0.49953370957140486\n",
      "Epoch 9893, Loss Train(MSE): 0.1251165294456323, R2 Train: 0.49953388221747075\n",
      "Epoch 9894, Loss Train(MSE): 0.12511648379372017, R2 Train: 0.4995340648251193\n",
      "Epoch 9895, Loss Train(MSE): 0.1251164415296336, R2 Train: 0.49953423388146556\n",
      "Epoch 9896, Loss Train(MSE): 0.12511639754430184, R2 Train: 0.4995344098227926\n",
      "Epoch 9897, Loss Train(MSE): 0.12511635204253718, R2 Train: 0.49953459182985127\n",
      "Epoch 9898, Loss Train(MSE): 0.12511631082827432, R2 Train: 0.4995347566869027\n",
      "Epoch 9899, Loss Train(MSE): 0.12511626606162785, R2 Train: 0.4995349357534886\n",
      "Epoch 9900, Loss Train(MSE): 0.12511622095491098, R2 Train: 0.49953511618035606\n",
      "Epoch 9901, Loss Train(MSE): 0.12511619393956344, R2 Train: 0.49953522424174623\n",
      "Epoch 9902, Loss Train(MSE): 0.12511616990000654, R2 Train: 0.49953532039997384\n",
      "Epoch 9903, Loss Train(MSE): 0.12511614696170048, R2 Train: 0.4995354121531981\n",
      "Epoch 9904, Loss Train(MSE): 0.12511612668671498, R2 Train: 0.49953549325314006\n",
      "Epoch 9905, Loss Train(MSE): 0.12511610267986556, R2 Train: 0.49953558928053776\n",
      "Epoch 9906, Loss Train(MSE): 0.12511608060125068, R2 Train: 0.4995356775949973\n",
      "Epoch 9907, Loss Train(MSE): 0.12511605952070387, R2 Train: 0.4995357619171845\n",
      "Epoch 9908, Loss Train(MSE): 0.12511603554645886, R2 Train: 0.49953585781416454\n",
      "Epoch 9909, Loss Train(MSE): 0.1251160143145754, R2 Train: 0.49953594274169844\n",
      "Epoch 9910, Loss Train(MSE): 0.12511599244127358, R2 Train: 0.49953603023490567\n",
      "Epoch 9911, Loss Train(MSE): 0.1251159684995307, R2 Train: 0.49953612600187725\n",
      "Epoch 9912, Loss Train(MSE): 0.1251159481015102, R2 Train: 0.4995362075939592\n",
      "Epoch 9913, Loss Train(MSE): 0.12511592544816935, R2 Train: 0.4995362982073226\n",
      "Epoch 9914, Loss Train(MSE): 0.12511590153882676, R2 Train: 0.49953639384469295\n",
      "Epoch 9915, Loss Train(MSE): 0.12511588196189152, R2 Train: 0.4995364721524339\n",
      "Epoch 9916, Loss Train(MSE): 0.12511585854113777, R2 Train: 0.4995365658354489\n",
      "Epoch 9917, Loss Train(MSE): 0.12511583498243223, R2 Train: 0.4995366600702711\n",
      "Epoch 9918, Loss Train(MSE): 0.12511581557693321, R2 Train: 0.49953673769226714\n",
      "Epoch 9919, Loss Train(MSE): 0.12511579171878778, R2 Train: 0.49953683312484887\n",
      "Epoch 9920, Loss Train(MSE): 0.12511576896687218, R2 Train: 0.4995369241325113\n",
      "Epoch 9921, Loss Train(MSE): 0.12511574880802281, R2 Train: 0.49953700476790874\n",
      "Epoch 9922, Loss Train(MSE): 0.12511572498202428, R2 Train: 0.4995371000719029\n",
      "Epoch 9923, Loss Train(MSE): 0.12511570302433275, R2 Train: 0.499537187902669\n",
      "Epoch 9924, Loss Train(MSE): 0.1251156821245496, R2 Train: 0.4995372715018016\n",
      "Epoch 9925, Loss Train(MSE): 0.12511565833059834, R2 Train: 0.4995373666776066\n",
      "Epoch 9926, Loss Train(MSE): 0.1251156371546534, R2 Train: 0.4995374513813864\n",
      "Epoch 9927, Loss Train(MSE): 0.12511561552626563, R2 Train: 0.4995375378949375\n",
      "Epoch 9928, Loss Train(MSE): 0.12511559176426265, R2 Train: 0.4995376329429494\n",
      "Epoch 9929, Loss Train(MSE): 0.12511557135767465, R2 Train: 0.4995377145693014\n",
      "Epoch 9930, Loss Train(MSE): 0.12511554901292435, R2 Train: 0.4995378039483026\n",
      "Epoch 9931, Loss Train(MSE): 0.12511552528277123, R2 Train: 0.4995378988689151\n",
      "Epoch 9932, Loss Train(MSE): 0.12511550563323778, R2 Train: 0.49953797746704887\n",
      "Epoch 9933, Loss Train(MSE): 0.12511548258428073, R2 Train: 0.49953806966287706\n",
      "Epoch 9934, Loss Train(MSE): 0.12511545894801354, R2 Train: 0.49953816420794583\n",
      "Epoch 9935, Loss Train(MSE): 0.12511543991878948, R2 Train: 0.49953824032484206\n",
      "Epoch 9936, Loss Train(MSE): 0.12511541623897443, R2 Train: 0.4995383350441023\n",
      "Epoch 9937, Loss Train(MSE): 0.1251153933461548, R2 Train: 0.49953842661538084\n",
      "Epoch 9938, Loss Train(MSE): 0.12511537362610578, R2 Train: 0.4995385054955769\n",
      "Epoch 9939, Loss Train(MSE): 0.12511534997789558, R2 Train: 0.49953860008841766\n",
      "Epoch 9940, Loss Train(MSE): 0.12511532781642423, R2 Train: 0.49953868873430307\n",
      "Epoch 9941, Loss Train(MSE): 0.12511530741750498, R2 Train: 0.49953877032998006\n",
      "Epoch 9942, Loss Train(MSE): 0.12511528380080322, R2 Train: 0.4995388647967871\n",
      "Epoch 9943, Loss Train(MSE): 0.1251152623586662, R2 Train: 0.49953895056533515\n",
      "Epoch 9944, Loss Train(MSE): 0.12511524129274715, R2 Train: 0.4995390348290114\n",
      "Epoch 9945, Loss Train(MSE): 0.12511521770745812, R2 Train: 0.4995391291701675\n",
      "Epoch 9946, Loss Train(MSE): 0.1251151969727258, R2 Train: 0.4995392121090968\n",
      "Epoch 9947, Loss Train(MSE): 0.12511517525159369, R2 Train: 0.49953929899362526\n",
      "Epoch 9948, Loss Train(MSE): 0.1251151516976221, R2 Train: 0.4995393932095116\n",
      "Epoch 9949, Loss Train(MSE): 0.12511513165844895, R2 Train: 0.4995394733662042\n",
      "Epoch 9950, Loss Train(MSE): 0.12511510929380748, R2 Train: 0.4995395628247701\n",
      "Epoch 9951, Loss Train(MSE): 0.12511508577105873, R2 Train: 0.4995396569157651\n",
      "Epoch 9952, Loss Train(MSE): 0.12511506641568235, R2 Train: 0.4995397343372706\n",
      "Epoch 9953, Loss Train(MSE): 0.1251150434191528, R2 Train: 0.4995398263233888\n",
      "Epoch 9954, Loss Train(MSE): 0.1251150200718784, R2 Train: 0.49953991971248635\n",
      "Epoch 9955, Loss Train(MSE): 0.12511500109969365, R2 Train: 0.4995399956012254\n",
      "Epoch 9956, Loss Train(MSE): 0.12511497762630458, R2 Train: 0.4995400894947817\n",
      "Epoch 9957, Loss Train(MSE): 0.12511495495000052, R2 Train: 0.4995401801999979\n",
      "Epoch 9958, Loss Train(MSE): 0.12511493535853732, R2 Train: 0.4995402585658507\n",
      "Epoch 9959, Loss Train(MSE): 0.1251149119161355, R2 Train: 0.499540352335458\n",
      "Epoch 9960, Loss Train(MSE): 0.12511488989923308, R2 Train: 0.49954044040306766\n",
      "Epoch 9961, Loss Train(MSE): 0.1251148696999211, R2 Train: 0.49954052120031556\n",
      "Epoch 9962, Loss Train(MSE): 0.1251148462884137, R2 Train: 0.49954061484634515\n",
      "Epoch 9963, Loss Train(MSE): 0.12511482491942552, R2 Train: 0.4995407003222979\n",
      "Epoch 9964, Loss Train(MSE): 0.1251148041236139, R2 Train: 0.49954078350554443\n",
      "Epoch 9965, Loss Train(MSE): 0.12511478074290883, R2 Train: 0.4995408770283647\n",
      "Epoch 9966, Loss Train(MSE): 0.1251147600104282, R2 Train: 0.4995409599582872\n",
      "Epoch 9967, Loss Train(MSE): 0.12511473862938613, R2 Train: 0.4995410454824555\n",
      "Epoch 9968, Loss Train(MSE): 0.12511471527939175, R2 Train: 0.499541138882433\n",
      "Epoch 9969, Loss Train(MSE): 0.1251146951720922, R2 Train: 0.4995412193116312\n",
      "Epoch 9970, Loss Train(MSE): 0.12511467321700953, R2 Train: 0.4995413071319619\n",
      "Epoch 9971, Loss Train(MSE): 0.12511464989763474, R2 Train: 0.49954140040946104\n",
      "Epoch 9972, Loss Train(MSE): 0.12511463040426932, R2 Train: 0.49954147838292273\n",
      "Epoch 9973, Loss Train(MSE): 0.12511460788625706, R2 Train: 0.4995415684549718\n",
      "Epoch 9974, Loss Train(MSE): 0.12511458459741137, R2 Train: 0.4995416616103545\n",
      "Epoch 9975, Loss Train(MSE): 0.1251145657068122, R2 Train: 0.4995417371727512\n",
      "Epoch 9976, Loss Train(MSE): 0.12511454263690316, R2 Train: 0.49954182945238734\n",
      "Epoch 9977, Loss Train(MSE): 0.12511451974963603, R2 Train: 0.4995419210014559\n",
      "Epoch 9978, Loss Train(MSE): 0.1251145007082304, R2 Train: 0.49954199716707837\n",
      "Epoch 9979, Loss Train(MSE): 0.125114477467662, R2 Train: 0.499542090129352\n",
      "Epoch 9980, Loss Train(MSE): 0.12511445517119205, R2 Train: 0.4995421793152318\n",
      "Epoch 9981, Loss Train(MSE): 0.1251144355896511, R2 Train: 0.4995422576413956\n",
      "Epoch 9982, Loss Train(MSE): 0.12511441237938634, R2 Train: 0.49954235048245466\n",
      "Epoch 9983, Loss Train(MSE): 0.1251143906627288, R2 Train: 0.49954243734908477\n",
      "Epoch 9984, Loss Train(MSE): 0.12511437055190414, R2 Train: 0.49954251779238346\n",
      "Epoch 9985, Loss Train(MSE): 0.12511434737185426, R2 Train: 0.499542610512583\n",
      "Epoch 9986, Loss Train(MSE): 0.12511432622410162, R2 Train: 0.4995426951035935\n",
      "Epoch 9987, Loss Train(MSE): 0.12511430559476833, R2 Train: 0.4995427776209267\n",
      "Epoch 9988, Loss Train(MSE): 0.12511428244484515, R2 Train: 0.4995428702206194\n",
      "Epoch 9989, Loss Train(MSE): 0.1251142618551663, R2 Train: 0.4995429525793348\n",
      "Epoch 9990, Loss Train(MSE): 0.12511424071802374, R2 Train: 0.49954303712790504\n",
      "Epoch 9991, Loss Train(MSE): 0.12511421759813954, R2 Train: 0.49954312960744185\n",
      "Epoch 9992, Loss Train(MSE): 0.1251141975557796, R2 Train: 0.4995432097768816\n",
      "Epoch 9993, Loss Train(MSE): 0.12511417592145177, R2 Train: 0.4995432963141929\n",
      "Epoch 9994, Loss Train(MSE): 0.1251141528315193, R2 Train: 0.4995433886739228\n",
      "Epoch 9995, Loss Train(MSE): 0.1251141333257989, R2 Train: 0.4995434666968044\n",
      "Epoch 9996, Loss Train(MSE): 0.12511411120483493, R2 Train: 0.4995435551806603\n",
      "Epoch 9997, Loss Train(MSE): 0.12511408814476752, R2 Train: 0.49954364742092994\n",
      "Epoch 9998, Loss Train(MSE): 0.12511406916508222, R2 Train: 0.4995437233396711\n",
      "Epoch 9999, Loss Train(MSE): 0.12511404656795705, R2 Train: 0.4995438137281718\n",
      "Matriz de Confusi√≥n:\n",
      "[[2 0]\n",
      " [1 1]]\n",
      "\n",
      "Reporte de Clasificaci√≥n:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80         2\n",
      "           1       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.83      0.75      0.73         4\n",
      "weighted avg       0.83      0.75      0.73         4\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from neural_network.DenseLayer import DenseLayer\n",
    "from neural_network.Activation import Sigmoid, ReLU\n",
    "from neural_network.NeuralNetwork import NeuralNetwork\n",
    "\n",
    "# Generaci√≥n de datos XOR\n",
    "def generate_xor_data():\n",
    "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    y = np.array([[0], [1], [1], [0]])\n",
    "    return X, y\n",
    "\n",
    "# Crear y entrenar la red neuronal\n",
    "X, y = generate_xor_data()\n",
    "network = NeuralNetwork(DenseLayer(2, 4), ReLU(), DenseLayer(4, 1), Sigmoid())\n",
    "network.train(X, y, epochs=10000, learning_rate=0.1)\n",
    "\n",
    "# Funci√≥n para graficar la curva ROC\n",
    "def plot_roc_curve(y_true, y_scores):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# Realizar predicciones y evaluar\n",
    "predictions = network.predict(X)\n",
    "predictions_rounded = np.round(predictions)\n",
    "\n",
    "# Matriz de confusi√≥n y reporte de clasificaci√≥n\n",
    "print(\"Matriz de Confusi√≥n:\")\n",
    "print(confusion_matrix(y.flatten(), predictions_rounded.flatten()))\n",
    "print(\"\\nReporte de Clasificaci√≥n:\")\n",
    "print(classification_report(y.flatten(), predictions_rounded.flatten()))\n",
    "\n",
    "# Graficar la curva ROC\n",
    "plot_roc_curve(y.flatten(), predictions.flatten())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T20:39:24.103557700Z",
     "start_time": "2024-01-07T20:39:17.573833300Z"
    }
   },
   "id": "cee0753616edb1eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Datos MNIST"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15ef4b49ad6a1a3d"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss Train(MSE): 2.524838472993198, R2 Train: -0.017062228807567248\n",
      ",Loss Valid(MSE): 2.672333790365825, R2 Valid: -0.03601223896068362\n",
      "Epoch 1, Loss Train(MSE): 2.288095715160645, R2 Train: 0.003997013527534565\n",
      ",Loss Valid(MSE): 2.310880466378037, R2 Valid: 0.0007444085709308323\n",
      "Epoch 2, Loss Train(MSE): 2.252288756561062, R2 Train: 0.011587476776695582\n",
      ",Loss Valid(MSE): 2.2798826479889973, R2 Valid: 0.003644218460859272\n",
      "Epoch 3, Loss Train(MSE): 2.234804545652136, R2 Train: 0.01613428910765724\n",
      ",Loss Valid(MSE): 2.2306831016122213, R2 Valid: 0.01729396476462408\n",
      "Epoch 4, Loss Train(MSE): 2.2171480467916225, R2 Train: 0.01968097245200706\n",
      ",Loss Valid(MSE): 2.24758980192722, R2 Valid: 0.010796246449303992\n",
      "Epoch 5, Loss Train(MSE): 2.2041308191142175, R2 Train: 0.023930821305041095\n",
      ",Loss Valid(MSE): 2.1967391669260095, R2 Valid: 0.026066407507383538\n",
      "Epoch 6, Loss Train(MSE): 2.188263499029087, R2 Train: 0.026614714821341257\n",
      ",Loss Valid(MSE): 2.2198474669375976, R2 Valid: 0.016874498801924043\n",
      "Epoch 7, Loss Train(MSE): 2.1759380175813954, R2 Train: 0.03145238132711958\n",
      ",Loss Valid(MSE): 2.1640976671894365, R2 Valid: 0.0350117176295861\n",
      "Epoch 8, Loss Train(MSE): 2.162847065355912, R2 Train: 0.03287476429944447\n",
      ",Loss Valid(MSE): 2.1964257971755834, R2 Valid: 0.021871092237244816\n",
      "Epoch 9, Loss Train(MSE): 2.1498609901697545, R2 Train: 0.03889980058044862\n",
      ",Loss Valid(MSE): 2.131935375454874, R2 Valid: 0.04440027531996549\n",
      "Epoch 10, Loss Train(MSE): 2.1390502427041818, R2 Train: 0.03914083280952352\n",
      ",Loss Valid(MSE): 2.174449364630669, R2 Valid: 0.026895222525623264\n",
      "Epoch 11, Loss Train(MSE): 2.1210317613358227, R2 Train: 0.04779233923638293\n",
      ",Loss Valid(MSE): 2.097541482659038, R2 Valid: 0.055211544451034045\n",
      "Epoch 12, Loss Train(MSE): 2.106133395820842, R2 Train: 0.04797465775341936\n",
      ",Loss Valid(MSE): 2.1412036649077, R2 Valid: 0.0352221911618531\n",
      "Epoch 13, Loss Train(MSE): 2.081206939594922, R2 Train: 0.06013160330746159\n",
      ",Loss Valid(MSE): 2.055070828427614, R2 Valid: 0.06899844954907863\n",
      "Epoch 14, Loss Train(MSE): 2.0578868154023207, R2 Train: 0.06083947770744169\n",
      ",Loss Valid(MSE): 2.0907597265121973, R2 Valid: 0.0481894121130203\n",
      "Epoch 15, Loss Train(MSE): 2.0301099589474845, R2 Train: 0.07590565992329212\n",
      ",Loss Valid(MSE): 2.002834766428927, R2 Valid: 0.0861133202445864\n",
      "Epoch 16, Loss Train(MSE): 2.000227819213897, R2 Train: 0.07681181834638662\n",
      ",Loss Valid(MSE): 2.0305068278533005, R2 Valid: 0.06443649564956966\n",
      "Epoch 17, Loss Train(MSE): 1.9716924059708871, R2 Train: 0.09445093507940527\n",
      ",Loss Valid(MSE): 1.942920511979405, R2 Valid: 0.10626451314887864\n",
      "Epoch 18, Loss Train(MSE): 1.9363066359888246, R2 Train: 0.09543800055951446\n",
      ",Loss Valid(MSE): 1.963715645458775, R2 Valid: 0.0835710648446133\n",
      "Epoch 19, Loss Train(MSE): 1.9080763617750967, R2 Train: 0.11544416757052134\n",
      ",Loss Valid(MSE): 1.8775001369718418, R2 Valid: 0.12913332063018212\n",
      "Epoch 20, Loss Train(MSE): 1.86743042744072, R2 Train: 0.11659778071301441\n",
      ",Loss Valid(MSE): 1.8915729453411687, R2 Valid: 0.10557866350693057\n",
      "Epoch 21, Loss Train(MSE): 1.840134199274517, R2 Train: 0.138775248593356\n",
      ",Loss Valid(MSE): 1.807729700766423, R2 Valid: 0.1545007553834008\n",
      "Epoch 22, Loss Train(MSE): 1.7935550007716832, R2 Train: 0.14061244255996186\n",
      ",Loss Valid(MSE): 1.8138163362905586, R2 Valid: 0.1309184258267153\n",
      "Epoch 23, Loss Train(MSE): 1.7672738703188844, R2 Train: 0.1646345470347118\n",
      ",Loss Valid(MSE): 1.7334712200256601, R2 Valid: 0.18234704725055828\n",
      "Epoch 24, Loss Train(MSE): 1.7146176593525895, R2 Train: 0.16789449370686083\n",
      ",Loss Valid(MSE): 1.730336453486601, R2 Valid: 0.16005856767696913\n",
      "Epoch 25, Loss Train(MSE): 1.6888236338312763, R2 Train: 0.19321881195680912\n",
      ",Loss Valid(MSE): 1.6542599302627026, R2 Valid: 0.21267927119138075\n",
      "Epoch 26, Loss Train(MSE): 1.631459074016761, R2 Train: 0.19853451643268127\n",
      ",Loss Valid(MSE): 1.642148665856682, R2 Valid: 0.19303394290979115\n",
      "Epoch 27, Loss Train(MSE): 1.6050930613276015, R2 Train: 0.22446538447907705\n",
      ",Loss Valid(MSE): 1.5703015402318055, R2 Valid: 0.24532677586397922\n",
      "Epoch 28, Loss Train(MSE): 1.5458958529801559, R2 Train: 0.23207560480737066\n",
      ",Loss Valid(MSE): 1.551432425277608, R2 Valid: 0.22921622468278413\n",
      "Epoch 29, Loss Train(MSE): 1.5178809567235705, R2 Train: 0.257842676852706\n",
      ",Loss Valid(MSE): 1.4829955936215018, R2 Valid: 0.27978266777614236\n",
      "Epoch 30, Loss Train(MSE): 1.4605425358608373, R2 Train: 0.26741187742444217\n",
      ",Loss Valid(MSE): 1.4613416389074718, R2 Valid: 0.2672069467203777\n",
      "Epoch 31, Loss Train(MSE): 1.4305927098412834, R2 Train: 0.2921650905447719\n",
      ",Loss Valid(MSE): 1.3951036033815583, R2 Valid: 0.31505393236964063\n",
      "Epoch 32, Loss Train(MSE): 1.3789247929933612, R2 Train: 0.3026630319338246\n",
      ",Loss Valid(MSE): 1.376164976639275, R2 Valid: 0.3046975895258396\n",
      "Epoch 33, Loss Train(MSE): 1.348399781448445, R2 Train: 0.3253593136034638\n",
      ",Loss Valid(MSE): 1.310980716198998, R2 Valid: 0.34943568713394313\n",
      "Epoch 34, Loss Train(MSE): 1.3058236797170486, R2 Train: 0.3350246289402862\n",
      ",Loss Valid(MSE): 1.3016847486251084, R2 Valid: 0.33835424318527163\n",
      "Epoch 35, Loss Train(MSE): 1.278305013781796, R2 Train: 0.35430957668263474\n",
      ",Loss Valid(MSE): 1.2368269437176034, R2 Valid: 0.3802912189470269\n",
      "Epoch 36, Loss Train(MSE): 1.2470456128123746, R2 Train: 0.36088523287455776\n",
      ",Loss Valid(MSE): 1.2447109987892417, R2 Valid: 0.36411949027554635\n",
      "Epoch 37, Loss Train(MSE): 1.2281634186091868, R2 Train: 0.37538080203393265\n",
      ",Loss Valid(MSE): 1.1801439473997017, R2 Valid: 0.40434002765792254\n",
      "Epoch 38, Loss Train(MSE): 1.2061654624439306, R2 Train: 0.3776109654639199\n",
      ",Loss Valid(MSE): 1.2088975387383063, R2 Valid: 0.37942336649903186\n",
      "Epoch 39, Loss Train(MSE): 1.2003774489073566, R2 Train: 0.387723920444365\n",
      ",Loss Valid(MSE): 1.1444118499743692, R2 Valid: 0.4203464401978112\n",
      "Epoch 40, Loss Train(MSE): 1.175694417736578, R2 Train: 0.38902370582619716\n",
      ",Loss Valid(MSE): 1.1839291026722698, R2 Valid: 0.3892855630523955\n",
      "Epoch 41, Loss Train(MSE): 1.1780753572407112, R2 Train: 0.39893857680535094\n",
      ",Loss Valid(MSE): 1.1163677266531433, R2 Valid: 0.4340707884631181\n",
      "Epoch 42, Loss Train(MSE): 1.1387493686732555, R2 Train: 0.4051328189361004\n",
      ",Loss Valid(MSE): 1.1473818201767114, R2 Valid: 0.40590602923096786\n",
      "Epoch 43, Loss Train(MSE): 1.1376589081429835, R2 Train: 0.41856416901379645\n",
      ",Loss Valid(MSE): 1.0761575957519238, R2 Valid: 0.45304560382658365\n",
      "Epoch 44, Loss Train(MSE): 1.0929387632603504, R2 Train: 0.42761192291812167\n",
      ",Loss Valid(MSE): 1.0960371832928553, R2 Valid: 0.43157922763834256\n",
      "Epoch 45, Loss Train(MSE): 1.0847673482008322, R2 Train: 0.44418717816113207\n",
      ",Loss Valid(MSE): 1.0282230191288895, R2 Valid: 0.4752777091286402\n",
      "Epoch 46, Loss Train(MSE): 1.0464396831228568, R2 Train: 0.4508776519558326\n",
      ",Loss Valid(MSE): 1.0425043717279705, R2 Valid: 0.4590052991243967\n",
      "Epoch 47, Loss Train(MSE): 1.0348223002262826, R2 Train: 0.4687690783842645\n",
      ",Loss Valid(MSE): 0.984176780577908, R2 Valid: 0.49574568740206515\n",
      "Epoch 48, Loss Train(MSE): 1.0033756247800174, R2 Train: 0.4722098062030854\n",
      ",Loss Valid(MSE): 0.9935596133805571, R2 Valid: 0.4840747185242187\n",
      "Epoch 49, Loss Train(MSE): 0.9916144850116422, R2 Train: 0.4904416964070919\n",
      ",Loss Valid(MSE): 0.945693490007393, R2 Valid: 0.5139818991265445\n",
      "Epoch 50, Loss Train(MSE): 0.9634863629935113, R2 Train: 0.4920521831439191\n",
      ",Loss Valid(MSE): 0.950151689324511, R2 Valid: 0.506538432454739\n",
      "Epoch 51, Loss Train(MSE): 0.9529194998980494, R2 Train: 0.5103482246680028\n",
      ",Loss Valid(MSE): 0.9096426744612174, R2 Valid: 0.5317777245239794\n",
      "Epoch 52, Loss Train(MSE): 0.9253499693647766, R2 Train: 0.5113656758675877\n",
      ",Loss Valid(MSE): 0.911392850713853, R2 Valid: 0.5271357449327756\n",
      "Epoch 53, Loss Train(MSE): 0.9179091276729946, R2 Train: 0.5285474318269872\n",
      ",Loss Valid(MSE): 0.874647455153252, R2 Valid: 0.5495266261318972\n",
      "Epoch 54, Loss Train(MSE): 0.8909543702105979, R2 Train: 0.5289787046027177\n",
      ",Loss Valid(MSE): 0.8795773153546091, R2 Valid: 0.5446204740861678\n",
      "Epoch 55, Loss Train(MSE): 0.8895273108107759, R2 Train: 0.5425705166370686\n",
      ",Loss Valid(MSE): 0.8431560066596318, R2 Valid: 0.5650463197550206\n",
      "Epoch 56, Loss Train(MSE): 0.8668345223704788, R2 Train: 0.5413109098117301\n",
      ",Loss Valid(MSE): 0.8614698383287194, R2 Valid: 0.5553230073735724\n",
      "Epoch 57, Loss Train(MSE): 0.872980062562505, R2 Train: 0.5485687406580309\n",
      ",Loss Valid(MSE): 0.8203586209495513, R2 Valid: 0.5745029596014051\n",
      "Epoch 58, Loss Train(MSE): 0.8604899089830707, R2 Train: 0.5452094142237822\n",
      ",Loss Valid(MSE): 0.8640007860592758, R2 Valid: 0.5562500525093561\n",
      "Epoch 59, Loss Train(MSE): 0.8673688590695153, R2 Train: 0.5477142673854466\n",
      ",Loss Valid(MSE): 0.8077886648332517, R2 Valid: 0.5775815554883955\n",
      "Epoch 60, Loss Train(MSE): 0.864079637135054, R2 Train: 0.5465148228735422\n",
      ",Loss Valid(MSE): 0.8758634871540144, R2 Valid: 0.55454132372189\n",
      "Epoch 61, Loss Train(MSE): 0.8519613981124269, R2 Train: 0.554998735600583\n",
      ",Loss Valid(MSE): 0.7907888385891959, R2 Valid: 0.5857438272612556\n",
      "Epoch 62, Loss Train(MSE): 0.8465993392372032, R2 Train: 0.5596767587974759\n",
      ",Loss Valid(MSE): 0.8611490631948329, R2 Valid: 0.5663093230024597\n",
      "Epoch 63, Loss Train(MSE): 0.8126664524146722, R2 Train: 0.5784610186324195\n",
      ",Loss Valid(MSE): 0.7571742801696952, R2 Valid: 0.6062663029935217\n",
      "Epoch 64, Loss Train(MSE): 0.8056258374464679, R2 Train: 0.583642982615227\n",
      ",Loss Valid(MSE): 0.8191413655972878, R2 Valid: 0.5894858819333253\n",
      "Epoch 65, Loss Train(MSE): 0.7691427588197586, R2 Train: 0.6046457700016594\n",
      ",Loss Valid(MSE): 0.7210617254540543, R2 Valid: 0.6288393204048737\n",
      "Epoch 66, Loss Train(MSE): 0.7640708825735218, R2 Train: 0.607346355426795\n",
      ",Loss Valid(MSE): 0.7765074050887147, R2 Valid: 0.612158225338298\n",
      "Epoch 67, Loss Train(MSE): 0.7327501255692567, R2 Train: 0.6263248171906068\n",
      ",Loss Valid(MSE): 0.6910601972929366, R2 Valid: 0.6476154805166696\n",
      "Epoch 68, Loss Train(MSE): 0.72898434184634, R2 Train: 0.6272464564491946\n",
      ",Loss Valid(MSE): 0.741066180514479, R2 Valid: 0.6309042325875205\n",
      "Epoch 69, Loss Train(MSE): 0.702963195115326, R2 Train: 0.643663672434633\n",
      ",Loss Valid(MSE): 0.6665609389783796, R2 Valid: 0.6627318279422253\n",
      "Epoch 70, Loss Train(MSE): 0.6996789978577244, R2 Train: 0.6437298591509826\n",
      ",Loss Valid(MSE): 0.7121481218173293, R2 Valid: 0.6461766500454635\n",
      "Epoch 71, Loss Train(MSE): 0.6777367972708015, R2 Train: 0.6579861030001866\n",
      ",Loss Valid(MSE): 0.645865033590594, R2 Valid: 0.6752635853635466\n",
      "Epoch 72, Loss Train(MSE): 0.6745332805755685, R2 Train: 0.6577450960614104\n",
      ",Loss Valid(MSE): 0.6879757938094208, R2 Valid: 0.658952066385582\n",
      "Epoch 73, Loss Train(MSE): 0.6555604302206234, R2 Train: 0.6703231715053835\n",
      ",Loss Valid(MSE): 0.6277595487410668, R2 Valid: 0.686046400067687\n",
      "Epoch 74, Loss Train(MSE): 0.652278818623188, R2 Train: 0.6700587731178043\n",
      ",Loss Valid(MSE): 0.6671225126417191, R2 Valid: 0.6700066577478516\n",
      "Epoch 75, Loss Train(MSE): 0.6354917224971324, R2 Train: 0.681330359968216\n",
      ",Loss Valid(MSE): 0.6115074934711644, R2 Valid: 0.6956112174185394\n",
      "Epoch 76, Loss Train(MSE): 0.6320726226661156, R2 Train: 0.6811914565261297\n",
      ",Loss Valid(MSE): 0.6486228692692859, R2 Valid: 0.6798622590886426\n",
      "Epoch 77, Loss Train(MSE): 0.6169583410368186, R2 Train: 0.6914066255475041\n",
      ",Loss Valid(MSE): 0.5966708567208844, R2 Valid: 0.7042810323358668\n",
      "Epoch 78, Loss Train(MSE): 0.613379551907954, R2 Train: 0.6914771490822743\n",
      ",Loss Valid(MSE): 0.6318552650121011, R2 Valid: 0.6888505455444098\n",
      "Epoch 79, Loss Train(MSE): 0.5996101974829502, R2 Train: 0.7007934758691028\n",
      ",Loss Valid(MSE): 0.5829846827967738, R2 Valid: 0.7122508804691923\n",
      "Epoch 80, Loss Train(MSE): 0.5958651598045415, R2 Train: 0.7011240294939806\n",
      ",Loss Valid(MSE): 0.6164270917492509, R2 Valid: 0.6971759920617104\n",
      "Epoch 81, Loss Train(MSE): 0.5832300183861622, R2 Train: 0.709637577857307\n",
      ",Loss Valid(MSE): 0.5702836653372915, R2 Valid: 0.7196383930269179\n",
      "Epoch 82, Loss Train(MSE): 0.5793216387852067, R2 Train: 0.7102585886062042\n",
      ",Loss Valid(MSE): 0.6020925067956688, R2 Valid: 0.7049613632819154\n",
      "Epoch 83, Loss Train(MSE): 0.5676805825242643, R2 Train: 0.7180285662227982\n",
      ",Loss Valid(MSE): 0.5584605385526008, R2 Valid: 0.7265146024695263\n",
      "Epoch 84, Loss Train(MSE): 0.5636195909745585, R2 Train: 0.7189555148061164\n",
      ",Loss Valid(MSE): 0.5886975096763442, R2 Valid: 0.712278457102757\n",
      "Epoch 85, Loss Train(MSE): 0.5528730283059383, R2 Train: 0.7260219974121112\n",
      ",Loss Valid(MSE): 0.5474419946636477, R2 Valid: 0.7329229959642701\n",
      "Epoch 86, Loss Train(MSE): 0.5486769663713211, R2 Train: 0.7272576774635\n",
      ",Loss Valid(MSE): 0.5761439873186508, R2 Valid: 0.7191685359800685\n",
      "Epoch 87, Loss Train(MSE): 0.5387473784466075, R2 Train: 0.7336534465274513\n",
      ",Loss Valid(MSE): 0.5371743085593048, R2 Valid: 0.7388915138953283\n",
      "Epoch 88, Loss Train(MSE): 0.5344391805332201, R2 Train: 0.7351894182304993\n",
      ",Loss Valid(MSE): 0.5643665063326849, R2 Valid: 0.7256557281029257\n",
      "Epoch 89, Loss Train(MSE): 0.5252604189315676, R2 Train: 0.7409471792582375\n",
      ",Loss Valid(MSE): 0.5276145863864142, R2 Valid: 0.7444401183294577\n",
      "Epoch 90, Loss Train(MSE): 0.5208666000631988, R2 Train: 0.7427652469553245\n",
      ",Loss Valid(MSE): 0.5533176593910806, R2 Valid: 0.7317555895820376\n",
      "Epoch 91, Loss Train(MSE): 0.5123781821452653, R2 Train: 0.7479214078548923\n",
      ",Loss Valid(MSE): 0.5187254571071508, R2 Valid: 0.749585455681477\n",
      "Epoch 92, Loss Train(MSE): 0.507926932363817, R2 Train: 0.7499953323523068\n",
      ",Loss Valid(MSE): 0.5429591421735713, R2 Valid: 0.7374803168579527\n",
      "Epoch 93, Loss Train(MSE): 0.5000713998102887, R2 Train: 0.7545913600175334\n",
      ",Loss Valid(MSE): 0.5104719689326254, R2 Valid: 0.7543435637844653\n",
      "Epoch 94, Loss Train(MSE): 0.49559087016700626, R2 Train: 0.7568887557549014\n",
      ",Loss Valid(MSE): 0.5332566289460123, R2 Valid: 0.7428416643990198\n",
      "Epoch 95, Loss Train(MSE): 0.48831290276134953, R2 Train: 0.7609709454792362\n",
      ",Loss Valid(MSE): 0.5028199318752805, R2 Valid: 0.7587312590030482\n",
      "Epoch 96, Loss Train(MSE): 0.48382985743653556, R2 Train: 0.7634552304985498\n",
      ",Loss Valid(MSE): 0.5241771131903011, R2 Valid: 0.7478523353594659\n",
      "Epoch 97, Loss Train(MSE): 0.477076295713599, R2 Train: 0.7670735391833028\n",
      ",Loss Valid(MSE): 0.4957352054829344, R2 Valid: 0.7627666519600831\n",
      "Epoch 98, Loss Train(MSE): 0.4726151893934148, R2 Train: 0.7697058067732006\n",
      ",Loss Valid(MSE): 0.5156877934248231, R2 Valid: 0.7525264058041805\n",
      "Epoch 99, Loss Train(MSE): 0.4663354467921851, R2 Train: 0.7729122326949657\n",
      ",Loss Valid(MSE): 0.4891835805988417, R2 Valid: 0.766469118675097\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x500 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x800 with 2 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxkAAAK7CAYAAACeQEKuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/SrBM8AAAACXBIWXMAAA9hAAAPYQGoP6dpAADKHklEQVR4nOzdd1hT1x8G8JchGwQZKg5U3IjIUNwTd63WvfcW9564UZYLt1i1Wvdo62rrqKNOQHBbBQRUZCkoAgmQ/P6gpqTanyMXLiTv53nyaO69Sd7DzTr5nnOvllwul4OIiIiIiEgg2mIHICIiIiIi9cJOBhERERERCYqdDCIiIiIiEhQ7GUREREREJCh2MoiIiIiISFDsZBARERERkaDYySAiIiIiIkGxk0FERERERIJiJ4OIiATFc7wSERE7GURUZN25cwfTp09H8+bNUbt2bXh4eGD+/PmIjY3Nt8fcsWMHGjVqhNq1a2PDhg2C3Of169dRrVo1XL9+XZD7+5zHqlatGi5fvvzRbSIiIhTbPHv27LPvWyqVYvny5fjll18+uW21atWwbt26z75vIiIqWtjJIKIiac+ePejduzeSk5MxdepUbN26FSNHjsSNGzfQvXt3PHz4UPDHTEtLw8qVK1G7dm0EBQXhu+++E+R+HRwcsH//fjg4OAhyf59DW1sbp0+f/ui6kydPftV9JiQkYOfOncjOzv7ktvv370ePHj2+6nGIiKjwYyeDiIqckJAQLFu2DH379sX27dvRqVMnuLu7o2fPnti7dy/09fUxZ84cwR83NTUVMpkMHh4eqFu3LkqXLi3I/ZqYmKBOnTowMTER5P4+h4uLC37//fePdghOnjyJGjVq5Ovj16lTB6VKlcrXxyAiIvGwk0FERU5QUBBMTU0xZcqUD9aVKFECs2bNQqtWrZCeng4AyMnJwZ49e9CpUyfUrl0bzZs3h5+fHyQSieJ2s2bNwuDBg3H48GG0bdsWtWrVQufOnXHx4kUAwJEjR9CyZUsAwJw5c1CtWjUAQMuWLTFr1iylDEeOHFEaapSZmYmFCxeiadOmqFWrFtq1a4egoCDF9h8bLnXnzh0MGzYM7u7ucHFxwejRo/H48eMPbnP16lUMHToUTk5OaNSoEXx9fZGTk/PJv2GHDh2QkpKCa9euKS1/+PAhnj59ivbt239wmzNnzqBv375wdnZWtGPPnj0AgGfPnqFVq1YAgNmzZyv+VrNmzcKgQYPg5eUFFxcXdOjQATk5OUrDpTw9PeHo6IjIyEjFY61btw41atTAjRs3PtkWIiIqfNjJIKIiRS6X4/Lly2jQoAEMDQ0/uk2HDh0wbtw4GBkZAQAWLFgAb29veHh4YOPGjejXrx92796NsWPHKk1Svnv3LoKCgjBhwgSsX78eOjo6GD9+PFJTU9G8eXMEBgYCAMaMGYP9+/d/dubly5fj4sWLmDlzJoKCgtCqVSv4+Pjg8OHDH93+2rVr6NOnj+K2S5cuRVxcHHr37o2IiAilbadNmwZXV1ds2rQJ33zzDbZt24aDBw9+MlPlypVRpUqVD4ZMnThxAvXq1YO1tbXS8j/++APjxo2Dg4MDNmzYgHXr1qFcuXJYvHgxwsPDYWNjo/T3ef9/AAgODkZcXBzWr1+PqVOnQkdHR+m+Fy5cCCMjI3h5eQHI3Q+bNm3C0KFDUa9evU+2hYiICh9dsQMQEX2J169fQyKRoGzZsp+1/ZMnT3Do0CFMnToVI0eOBAA0atQINjY2mDFjBi5evIhmzZoBAN6+fYsjR46gfPnyAAAjIyP0798f165dQ9u2bRVDiMqXL486dep8duYbN26gUaNG6NixIwDA3d0dRkZGsLS0/Oj2/v7+sLOzw5YtWxRfyBs3bozWrVtj7dq1WLNmjWLbHj16YNy4cQCABg0a4MyZM/jjjz/Qu3fvT+Zq3749du3ahYULF0JXN/fj4OTJkxg9evQH2z558gTfffcd5s6dq1jm7OwMd3d3XL9+HU5OTkp/n5o1ayq2y87OxuLFi/9zeJSVlRW8vLwwefJkHDx4EDt37kTVqlUxceLET7aBiIgKJ1YyiKhIef+l+3OGBAFQDLd5/wX/vY4dO0JHR0dpiFKJEiUUHQwAii/FGRkZKmV2d3fHgQMHMGLECOzevRuxsbEYN24cmjdv/sG26enpuHPnDtq3b6/0i7+ZmRlatGjxwfAhZ2dnpeulSpVSDBP7lH8PmQoPD0d8fDzatGnzwbbDhw/HihUr8O7dO9y9excnT57E5s2bAeQeVer/MTc3/+T8iw4dOqBt27ZYsGABYmNj4efnBz09vc9qBxERFT7sZBBRkVK8eHEYGxvjxYsX/7lNeno6UlNTAUDx77+H/+jq6sLCwgJv375VLPv38CstLS0AgEwmUynz3LlzMWnSJDx79gxLliyBh4cHevfu/dEjYL19+xZyuRxWVlYfrLOyslLKCwAGBgZK17W1tT/7PBUVK1ZEjRo1FEOmTp48icaNG6N48eIfbPvq1SuMHz8ebm5u6NmzJ9atW4e0tDQAnz4vhrGx8Wfl+e677yCTyVChQgVUrFjxs25DRESFEzsZRFTkNG7cGNevX1eauJ3XgQMHUL9+fdy7d0/xhTkxMVFpm6ysLLx+/RoWFhYq5/l3VeXflQQ9PT2MGTMGp06dwvnz5xW/1k+dOvWD+zI1NYWWlhaSkpI+WJeYmAhzc3OV8+bVoUMH/P7778jKysLp06c/qPi8N23aNNy5cwc7duxAWFgYTp06JegRvDIyMuDt7Y2qVavir7/+wvbt2wW7byIiKnjsZBBRkTN06FCkpKRg9erVH6xLTEzE9u3bUblyZTg4OCgmDp84cUJpuxMnTiAnJweurq4qZTExMcHLly+VloWEhCj+n5mZibZt2yq+NNva2qJfv37o2LHjR6sxRkZGqFWrFk6dOqXUeXn79i3++OMPlfP+W/v27ZGSkoJNmzYhNTVVcYSofwsJCUGbNm3g7u6uGMb0/shb7ys9/57Q/SX8/f3x8uVLrFu3Dv3798fatWs/mORORERFByd+E1GRU6dOHUycOBGrV69GREQEunTpAgsLCzx+/BhBQUGQSCSKDkjlypXx3XffYe3atcjIyEDdunXx4MEDBAYGwt3dHU2aNFEpS4sWLbB582Zs3rwZTk5OOHfunNJhYQ0MDODg4IDAwEAUK1YM1apVQ1RUFI4ePYq2bdt+9D6nTp2KYcOGYeTIkejbty+ysrKwZcsWSKVSxSRvoZQrVw6Ojo7YvHkzWrdurTgi17/Vrl0bv/zyCxwcHFCqVCmEhoZiy5Yt0NLSUsxZMTU1BQBcvXoV9vb2cHJy+qwMN27cwO7duzF58mRUqFABkyZNwu+//45Zs2Zh3759KnVeiIhIHOxkEFGRNGbMGNSsWRN79uzB8uXLkZqaitKlS6N58+YYPXq00onyli1bBjs7Oxw+fBhbt26FjY0NBg4ciLFjx0JbW7WC7qhRo/Dq1SsEBQUhKysLzZs3x7JlyzBmzBjFNosXL8bq1auxfft2JCYmwtLSEt27d//Poyc1aNAA33//PdauXYspU6ZAT08Pbm5uWLlyJapUqaJS3o/p0KED7ty5859DpQBgxYoVWLJkCZYsWQIAqFChAhYtWoSff/4ZwcHBAHKrOkOGDMH+/ftx4cIF/Pnnn5987PT0dMyePRtVq1bFsGHDAOTO4ViwYAHGjBmDbdu2YdSoUQK0koiICpKW/HNnCBIREREREX0GzskgIiIiIiJBsZNBRERERESCYieDiIiIiIgExU4GEREREREJip0MIiIiIiISFDsZREREREQkKHYyiIiIiIhIUGp5Mj7TXjvFjiCKxD2DxI4gCp7pRbNoaYmdQBzZOZr5RNfR1swdrqnPc9IsBoX4W6ihs6doj51xK1C0xxYSKxlERERERCSoQtyHJCIiIiISgRZ/h1cV/4JERERERCQodjKIiIiIiEhQHC5FRERERJQXj76gMlYyiIiIiIhIUKxkEBERERHlxYnfKuNfkIiIiIiIBMVKBhERERFRXpyToTJWMoiIiIiISFDsZBARERERkaA4XIqIiIiIKC9O/FYZ/4JERERERCQoVjKIiIiIiPLixG+VsZJBRERERESCYieDiIiIiIgExeFSRERERER5ceK3yvgXJCIiIiIiQbGSQURERESUFyd+q4yVDCIiIiIiEhQrGUREREREeXFOhsr4FyQiIiIiIkGxk0FERERERIJiJ+MzWZkZ4IfJzRC7vQ/C1nyHfs3sFetaOdniik8nJPzQD1d8OqF1nTJKt+3fvDJCArogbmdfnFvaAfWrWRd0/HwjkUjgNX8OGtd3Q6tmjbFzx3axI+W7n44dQZ1a1T64ODtWFztavpNKpVi+dBGaNKyLlk0bYu3qAMjlcrFjFSjPMSMxf84ssWPki4T4eMyYMgEtGrujnUdTBPh6QyKRAACu/HkJvbt3RsO6TujdvTP+vHRR5LT559yZ3z94fU+bPEHsWPkuJjoao0cMQ303Z7Rt1Rw7tm8TO1KB0MTPMUBz2/3ZtLTEu6gJzsn4THuntYCOthY6Lv4VtiWMsHlcY7zNyMLd6Nf4cWoLLN53CyeCY/BN3fLYO60FXCYfRUziO3g42cJ/qDvGb7mC4MdJ6NvMHodmecBtyjG8fJ0hdrNUFuDng/t372Lr9p148eIF5s+ZCdvStmjdtp3Y0fJN23Yd0KhxE8X17KxsjBg2CE2bNRcvVAHx8V6KGzeuY8PmIKS/e4dZ0yfD1tYW3Xv2FjtagTh18gQuXbyAbzt/J3YUwcnlcsyYOgFmZsWxbcduvElNxSKvudDW1kG3Hr0wbfJ4jBs/Cc1atMIf585g6qRxOPLzKdiWKSt2dMFFRDxBs+YtMH/hEsUyPT19ERPlP5lMBs+xI+FQyxH7Dx9FTHQ0Zk2fAhubkujwTSex4+UrTfwcAzS33VRwWMn4DM6VLFG/mg2Grr2I209f4XToM6z+6S4mdnKAraURvj/7F9afvI+nCWkIPHEf7yTZcLW3AgD0a14ZP16IwIHLUYiMf4ulB8KQkJKBds5F/4M5PT0dRw8fxIzZc1GjpgNaebTG4KHDsW/vHrGj5SsDAwNYWVkrLieO/wzI5Zg4eZrY0fJVamoKjh09jAULl8DRsTbc6zfAgEFDced2uNjRCkRqSgpW+fvAoZaj2FHyxdOnUbhzOxxeS5bDvnIVOLu6YfTY8Th96jji41+ia7ee6DdgMMqWLYf+A4fA0NAId+/eETt2voiKjIB95apKr3MzMzOxY+Wr5OQkVKteA/MWLISdXQU0adoM9eo3wK3QELGj5StN/RzT1HZ/ES1t8S5qgpWMz1DBxgSJqRl4mpCmWHY35jXm93LGtUcJuHw/HgCgq6OFvk3toa+rjZCIJADA6p/vIi0j64P7NDPSK5jw+eivRw+RnZ2NOnWcFcucXVyxbcsmyGQyaGurzwvlv6SmpuD77VvhtWgp9PSK/j79f26FhsDExARudesplg0dPlLERAXL328lvunUGYkJCWJHyRdWllZYt3ErLC2tlJanvU2DW113uNV1BwBkZWXhxC8/QSqVopaadrgiIyPg3qCh2DEKlLW1DXz9VwPIrWqF3QpFaPBNzJnvJW6wfKapn2Oa2m4qWIWik/H69WtIpVIYGhoWyl+LElIzUdxYD4Z6OsiQ5gAAyloaoZiuNoob6SH5rQSVSpoiZFUX6OpoY8GeEMQkvgMAhEe9UrovDydbVLEtjgv34gq8HUJLSkyEubkFiuX5cm1paQWJRIKUlBSUKFFCxHQF48C+vbCxtkHrNupfXn72LBa2tmXwy0/HELRtE7KystC5S1cMHzlG7T+Qrl+7itDgYBw69guWLV4odpx8YWpmhoaN/hkGKJPJcGDfHtRzr69YFhsTjW6dOyAnJwfjJ01Vy6FScrkcT59G4eqflxG0dTNkOTlo3bYdxnpOQLFi6v1DwnvtW7dEXNwLNG3WAh6t24odJ19p6ueYprabCpZonYzffvsNu3fvxu3btxUTC4HcoSi1atXCoEGD4OHhIVY8JcFPEhH3KgN+Q9wxfccNlDI3hGdHBwBAMd3cL1dJbzLRbM4J1KtqDe8BdRHx8g1+vhGjdD8VS5pi09jG2Hcp4oPOR1GUkZnxwa/3769nSaViRCpQcrkcR48cxOAhw8WOUiAy0tMRExONQwf3YdESbyQlJmLp4gUwMDDEwMFDxY6XbyQSCZYu8sLseQtgYGAgdpwCsybAFw8f3MeuHw8qlplblMCuHw/idngYVvmtQLly5dFKzb6ExsW9QGZGBorp6cHHfzVePHuGld5LkZmZiZmz54kdr0D4r16LpKQkLFuyEL4rvTFrjvq2W1M/xzS13V9EjSZgi0WUTsb333+PwMBADB8+HJ6enrC0tISenh6kUimSkpIQHByMWbNmYeLEiRgwYIAYEZVIsmQYuPoP7JzUDC929EFiaiZW/3wPKwbVxdv03KFQbzKycPvpK9x++grVy5hjdLsaSp2MyqXN8PO81oiKf4vxm6+K1RRB6evrQ/qvN6P31zXhy9i9u3eQEB+Pdu07ih2lQOjo6CItLQ3ePv6wtc09glrcyxc4sG+vWncyNm0IRE2HWkqT/dXd2lV+2LtnF7x9AlC5SlXFclNTU1SvURPVa9REVOQT7N+7W+06Gba2ZXDhz+swMysOLS0tVK9eAzK5DHNnTce0GbOho6MjdsR8937ekVQiweyZ0zB12gylX7zViaZ+jmlqu6lgidLJ2L59O1auXPnRSoW9vT3c3d1RrVo1LFmypFB0MgAgNCIZjuOPwKa4AZLfStCqti2S3mSinLUJSpjo4crDf8ZpP3yegiYOJRXXq5c1x/H5bfA0/i26ep9BZlaOGE0QnI1NSaSkvEZ2djZ0dXOfSklJiTAwMIBpIRz2JrQrf16Ci6sbzIoXFztKgbCytoa+vr6igwEAFSpURPzLoj/07/85feoEkpOSUN8td+xyVlbuB/Hvv/2Ka8G3xIyWL3y8l+DQgX1YstxH0YGIePIYb1JT4ezqptiuYqXKCLl5U6yY+ap4cXOl6xUr2UMikSA1NVVth5EkJyUhPDwMLVv987lcyb4ysrKykPYuDRZ66tluTf0c09R2fxE1moAtFlH+gpmZmShb9v+P5S1ZsiTevn1bQIn+PwtjPfy2qB1KmOgjITUTOTI52rqUxaX7L9HBtSzWjVSeIOhc0RKPnqcCAEqaG+Knua0REfcGnZf9jrcfmQReVFWrXgO6urq4HR6mWHYrNAQOtRzVfow+ANy5fRt1nF3EjlFgatd2gkQiQfTTKMWyqMhIpU6HOgra8QMOHf0FBw4fw4HDx9CseUs0a94SBw4fEzua4LZsDMShg/uxfKU/2uap0F28cB5LFs1XOifKwwf3UKFSJTFi5qsrf15Cs0buyMj45xDjjx4+gLm5udp2MADg+fNnmDLRE/Hx8Ypl9+/fhUWJErCwUN92a+rnmKa2mwqWKM+k1q1bY9asWQgODkZ2drbSOplMhtDQUMyZMwdt2xaOMvzrd1IYGxTDkn6uqGBjgkEtq2BAi8pY/fNd7LsUiZIWhljc1wX2pUwxok019GpSCf7Hcg/tuGyAG3S0tTBu8xUYG+jCprgBbIobwFi/UMy5V4mhoSE6de6CpYsX4u6d2zh39gx27diOvv0Hih2tQDx58hiVKlUWO0aBqVCxEpo0bY75c2fj0cOHuPLnJWwP2oIevfqIHS1f2dqWQXk7O8XF2NgYxsbGKG9nJ3Y0QUVFRmDblo0YPHQE6ri4IikpUXHp0PFbJCUlYt1qf8REP8WBfXtw8vjPGDJM/Y4u5lTHGfoG+ljkNQ9PoyJx+dIFrPL3waCh6j33yqGWI2rWdIDXvDmIePIEly5ewCo/X4wYOVrsaPlKUz/HNLXdX4SHsFWZllyE0/VKpVKsXLkShw4dQk5ODszNzRVzMlJSUqCrq4vOnTtj9uzZXzU20LTXTsEzVylthjUjGsDF3hLRiWnw+jEUp0OfAQDqVrHCykH14FDeAjF/rzsZEgsAiN/VD0Yf6VAsPxgG70PCnl8gcc8gQe/vc2RkZGDZ4oU48/tvMDE1weAhw9B/4OACzSDWCafdXWtj1dr1SkfkUXdv377FyuVLcO7s7zAwMESvPn0xcvQ4aBXgBDmx5+K9P9v3kuUrCvRxs3Py94n+fdAWBK4J+Oi6kNsPcSc8DH4+3nj8+BFsbctg/MSpaNaiZb5mAgAd7YLf4U+ePIbviuW4czsMxsbG6NajN0aNUf/neUJCPLyXLcGNa1dhaGiI3n37Y9iIUQXabjEUhs8xMRSGdhsU4t9bDZstFu2xMy4sEO2xhSRKJ+O9jIwMPHz4EImJicjIyIC+vj5KliyJGjVqqDTxKD86GUWBGJ2MwkC8ZzCJQc2/7/yn/O5kFFZidDIKA019npNmYSfj49SlkyHq7jU0NISzs/OnNyQiIiIiKiga+gOHkNRn4BcRERERERUKhbhQRUREREQkAjWagC0W/gWJiIiIiEhQ7GQQEREREZGgOFyKiIiIiCgvHuJNZaxkEBERERGRoFjJICIiIiLKixO/Vca/IBERERERCYqVDCIiIiKivDgnQ2WsZBARERERkaDYySAiIiIiIkGxk0FERERElJeWtniXryCVSvHNN9/g+vXrimWxsbEYPHgw6tSpgw4dOuDy5ctKt7ly5Qq++eYbODk5YeDAgYiNjVVav2PHDjRp0gTOzs6YM2cOMjIyvigTOxlEREREREWURCLBlClT8PjxY8UyuVyOcePGwcrKCocPH0bnzp3h6emJFy9eAABevHiBcePGoWvXrjh06BBKlCiBsWPHQi6XAwB+/fVXBAYGYvHixdi5cyfCw8Ph6+v7RbnYySAiIiIiyktLS7zLF3jy5Al69uyJmJgYpeXXrl1DbGwsFi9eDHt7e4waNQp16tTB4cOHAQAHDx5ErVq1MHToUFSpUgXe3t54/vw5bty4AQDYtWsXBg0ahBYtWqB27dpYtGgRDh8+/EXVDHYyiIiIiIiKoBs3bsDd3R379+9XWh4eHo6aNWvCyMhIsczV1RVhYWGK9W5ubop1hoaGcHBwQFhYGHJycnDnzh2l9XXq1EFWVhYePnz42dl4CFsiIiIiokJCKpVCKpUqLdPT04Oent4H2/bt2/ej95GYmAgbGxulZZaWlnj58uUn17958wYSiURpva6uLszNzRW3/xzsZBARERER5SXiGb83b96MwMBApWWenp4YP378Z99HRkbGB50SPT09Refl/63PzMxUXP+v238OdjKIiIiIiAqJUaNGYciQIUrLPlbF+H/09fWRkpKitEwqlcLAwECx/t8dBqlUCjMzM+jr6yuu/3u9oaHhZ2fgnAwiIiIiorxEnPitp6cHExMTpcuXdjJKliyJpKQkpWVJSUmKIVD/td7a2hrm5ubQ19dXWp+dnY2UlBRYW1t/dgZ2MoiIiIiI1IiTkxPu3bunGPoEACEhIXByclKsDwkJUazLyMjA/fv34eTkBG1tbTg6OiqtDwsLg66uLqpXr/7ZGdjJICIiIiLKq4idjO/f6tWrh9KlS2P27Nl4/PgxtmzZgtu3b6N79+4AgG7duiE0NBRbtmzB48ePMXv2bJQtWxbu7u4AcieUBwUF4cyZM7h9+zYWLlyInj17crgUEREREZGm0tHRwYYNG5CYmIiuXbvi559/xvr162FrawsAKFu2LNatW4fDhw+je/fuSElJwfr166H193k6OnbsiFGjRmHBggUYOnQoateujenTp39RBi35+1P7qRHTXjvFjiCKxD2DxI4gCvV7BtP/84XnKVIb2Tma+UTX0dbMHa6pz3PSLAaF+PBDhh3WiPbYGScnivbYQirEu5eIiIiISATs6atMLTsZmvqLvkVdT7EjiOL1zcBPb0RUxOnq8AOP1F9WtkzsCKIopsvR66R+1LKTQURERET01UQ8GZ+64F+QiIiIiIgExU4GEREREREJisOliIiIiIjy4nAplfEvSEREREREgmIlg4iIiIgoLx7CVmWsZBARERERkaDYySAiIiIiIkFxuBQRERERUV6c+K0y/gWJiIiIiEhQrGQQEREREeXFid8qYyWDiIiIiIgExUoGEREREVFenJOhMv4FiYiIiIhIUOxkEBERERGRoDhcioiIiIgoL078VhkrGUREREREJChWMoiIiIiI8tBiJUNlrGQQEREREZGg2MkgIiIiIiJBcbgUEREREVEeHC6lOlYyiIiIiIhIUKxkEBERERHlxUKGyljJICIiIiIiQbGTIRCJRAKv+XPQuL4bWjVrjJ07tosdSSV6xXQRfHAOmrhWUSyzs7XEiU2eSLrij9DDc9GqfnWl2/TpWBfhR+cj/pIv9vuPQElL04/e9+SBrfDwxKJ8zZ/f4uPjMXXSBDRpUA8eLZrAd6U3JBKJ2LHyXUx0NEaPGIb6bs5o26o5dmzfJnakAqFur+/Ppant1tTn+XtSqRRdO3+Dmzeuix0lX8TGRMNz9HA0qe+Kjm1bYteOoI9u06henYIPJwJ1399fS0tLS7SLuuBwKYEE+Png/t272Lp9J168eIH5c2bCtrQtWrdtJ3a0L6avp4udywfDobKt0vIDq0bg3uMXaNTPB51aOGF/wAg4d12K2Jev4dGgBrYs7I/pfodx7vojzBzWFscCx6JhXx/I5XLFfVQoY4m5ozsg6XVaQTdLMHK5HNMmT4CZmRm+/2EP3qSmwmveHOjoaGPKtJlix8s3MpkMnmNHwqGWI/YfPoqY6GjMmj4FNjYl0eGbTmLHy1fq9Pr+EprYbk1+ngO5HctZM6Yi4sljsaPkC5lMhomeo+Hg4Ig9+w8jJiYac2dNg41NSbTr8A0A4OXLOEwaP0YjfjhS9/1N4mIlQwDp6ek4evggZsyeixo1HdDKozUGDx2OfXv3iB3ti1WvVAoXdk1DxXJWSsub1a2KSmWt4bl0Hx5FxcNv+2+4fjsKAzs3AACM6d0M+04FY9P+i/jraTzGLd2LcqVKfFDtWDe3N8IfPiuw9uSHp1GRuB0ehsVLvVG5chW4uLphrOcEnDxxXOxo+So5OQnVqtfAvAULYWdXAU2aNkO9+g1wKzRE7Gj5Sp1e319CU9utqc9zAIh48gQD+vTEs5gYsaPkm1fJSahWrTpmzfNCebsKaNykGerVq4+wW7n7949zZzCgd3fo6emJnDT/acL+JnGxkyGAvx49RHZ2NurUcVYsc3ZxxZ3b4ZDJZCIm+3JNXCvj4s2/0HyQv9Lyeo4VEPYwFumZUsWyK7ci4V67IgCgYllL3LzzVLEuU5KFyNhExXoA6PtNPRgZ6GHHsav524h8ZmlljQ2bt8HSSrkjlva26FZnPoe1tQ18/VfD2NgEcrkct0JDEBp8E2716okdLV+p0+v7S2hquzX1eQ4AIcE3ULeeO3b9uF/sKPnGytoG3r6rYGxsDLlcjrBboQgNDYarW+7+vXzpAkaPm4CpM+aInDT/acL+VgWHS6mOw6UEkJSYCHNzCxTL88uHpaUVJBIJUlJSUKJECRHTfZmtBy9/dHlp6+KIS0xVWpbw6g3KlDTP/X/yW9jamCvWaWlpwdamOKzMjQEAVhYmWDqhMzqOXgdXB7t8yV5QzMzM0KhxE8V1mUyGfT/uhnv9+iKmKljtW7dEXNwLNG3WAh6t24odJ1+p0+v7S2hqu/PSpOc5APTs3VfsCAWqU/tWeBkXhyZNm6OlRxsAwDyvJQCA4Js3xIxWIDRtf1PBYyVDABmZGR+UVt9fz5JKP3aTIsfQoBgk0mylZRJpNvSL5fZTD/0WihE9GsO9dkXo6mpjxrA2sClhhmJ/r/eZ2hW7f7mOB5EvCzx7flvl74sHD+7Dc+JksaMUGP/Va7F2/SY8evQAviu9xY6TrzTh9f0xmtruvDTpea6JfPzXYtXaDfjr0UME+K4QOw4VMqxkqI6VDAHo6+tD+q8P3ffXDQwMxIgkuExJNizN9ZWW6evpKoZPbT/yJxwq2+JM0CQAwNGzYTj95z28fZcJjwY14F67Isb2WF7QsfPdKn9f7PlhJ3z8VqFKlapixykwDrUcAQBSiQSzZ07D1GkzlH7xViea8Pr+GE1td16a9DzXRDUdagEAJFIp5s+ejklTp6NYMe5fIqGI1sm4efPmZ29bt27dfEyiOhubkkhJeY3s7Gzo6ub+SZOSEmFgYABTMzOR0wnjRUIKatqXVlpW0tIML5PeAABkMjkmrziAOauPwkCvGF6/ScelH6bh3LWH6NHWFWVLWSD2XO4vRbo62tArpoPEP/3RxXMD/rwVUeDtEYL3siU4uH8vlq3whUcb9R9KkZyUhPDwMLRs5aFYVsm+MrKyspD2Lg0Weuo5fEYTXt8fo6nt1tTnuaZITk7CnfAwNG+ZZ/9WskdWVhbepb2DuQU7GURCEa2TsXjxYjx58gQAlA5x+m9aWlp48OBBQcX6KtWq14Curi5uh4fBxdUNAHArNAQOtRyhra0eI9Ju3HmKaUNaw0C/GDIlWQCAhnXscSUst4Mwvl8L6Ovpwu/735GRmYVSVmZwqlYOoxbuweHfQrEy6LTivrq0rIOxfZqhzYg1eJGQ+tHHK+w2bQjEoQP7sNI3QK0P55nX8+fPMGWiJ349ewElS5YEANy/fxcWJUrAwkJ9v3hpwuv7YzS13Zr6PNcUL54/w/QpE3Di1/Ow+Xv/Prh/DxYWJWBuYSFyOipM1GnYklhE+6Q4fPgwWrVqhWrVqiE8PBwPHz786KWwdzAAwNDQEJ06d8HSxQtx985tnDt7Brt2bEff/gPFjiaYSyGP8Sw+BVsW9UeNSqUwbUhruNWyw86/jxT19HkypgxujaZuVVCjUin86DsMpy/fxf2IOCS+TkNkbJLikvDqLbJzZIiMTVJ0WIqSyIgIbNm0AUOGjYCziyuSEhMVF3XmUMsRNWs6wGveHEQ8eYJLFy9glZ8vRowcLXa0fKUJr++P0dR2a+rzXFPUdHBEjZoOWOw1F5ERT3D50gWsXeWHoSNGiR2NSO2IVsnQ09NDQEAAevbsidWrV2PmzKJ9ErNpM2Zj2eKFGD5kEExMTTBm3Hh4tG4jdizByGRy9Ji8GZu8+uHKjzMREZuIXlO3IvblawDAL3/cRrWdZ7Bj+WAY6BfDL+dvY6rPQZFT54/z584iJycHWzdvxNbNG5XWhd97JFKq/Kejo4PVgRvgvWwJBvbrBUNDQ/TtP0Dtv3QC6v/6/i+a2G5Nfp5rAh0dHfivXg8f7yUYMrAPDA0N0atvf/TuO0DsaFTYsJChMi35/xurVAAiIiJw48YN9OnTR7D7zMz+9DbqyKKup9gRRPH6ZqDYEYiISABZ2ep7Dpb/p5iu+g5B/H8MCvHhh4r3/UG0x079UT06vaLvXnt7e9jb24sdg4iIiIgIAOdkCEEzu85ERERERJRv2MkgIiIiIiJBiT5cioiIiIioMOFwKdWxkkFERERERIJiJYOIiIiIKA9WMlTHSgYREREREQmKnQwiIiIiIhIUh0sREREREeXB4VKqYyWDiIiIiIgExUoGEREREVFeLGSojJUMIiIiIiISFCsZRERERER5cE6G6ljJICIiIiIiQbGTQUREREREguJwKSIiIiKiPDhcSnWsZBARERERkaBYySAiIiIiyoOVDNWxkkFERERERIJiJ4OIiIiIiATF4VJERERERHlxtJTKWMkgIiIiIiJBsZJBRERERJQHJ36rjpUMIiIiIiISFCsZRERERER5sJKhOrXsZMhkcrEjiCLx2jqxI4ii4tjDYkcQxZPArmJHoAKko62ZH3g5Gvp+rqnfb3Lkmrm/i4kdgCgfcLgUEREREREJSi0rGUREREREX4vDpVTHSgYREREREQmKlQwiIiIiojxYyVAdKxlERERERCQodjKIiIiIiEhQHC5FRERERJQXR0upjJUMIiIiIiISFCsZRERERER5cOK36ljJICIiIiIiQbGSQURERESUBysZqmMlg4iIiIiIBMVOBhERERERCYrDpYiIiIiI8uBwKdWxkkFERERERIJiJYOIiIiIKC8WMlTGSgYREREREQmKnQwiIiIiIhIUh0sREREREeXBid+qYyWDiIiIiIgExUoGEREREVEerGSojpUMIiIiIiISFDsZREREREQkKHYyVCSVStH9u04IvnldscxnxTI4O1ZXuuz7cbeIKYWTEB+PGVMmoEVjd7TzaIoAX29IJBIAgO+KZXCtXV3psn9v0Wy3rYUhdnk2xF9rvsWN5e0wolVlxbrqZczw04xmiAzsgnNeHmhYzVqxTl9XG0t7O+GOX0fc8esIn/7OMNTTEaMJgkiIj8f0KRPQvJE72rZqCn+ff/b3ezEx0Wjg5iRSwvyhqe3+fzzHjMT8ObPEjpEvuL9zvUpOxrTJE9CkQV18274Nfj52ROxI+W6y52gsnj9HcX3apHFwr1NT6XL54h/iBcxHMdHRGD1iGOq7OaNtq+bYsX2b2JEKFS0tLdEu6oJzMlQgkUgwZ+Y0RDx5rLQ8MiIC4ydOwbddvlMsMzY2Keh4gpPL5ZgxdQLMzIpj247deJOaikVec6GtrYNJU2cgKjICnhOnoFPnot/uzSPd8exVOtouO4eqpU2xYXg9PEtOx+VHCdg/qQl+C4/DxO+D0b1+eWwfUx+N5v+G5LcSTO1UAw2qWqP/uj8BaGHtEDfM+a4W5u8PF7tJX0wul2P6lNz9HbRzN1JTU7FowVxo6+hg8tQZAICXL+MwadzoD76QFWWa2u7/59TJE7h08QK+zfPaVhfc37nkcjmmTPSETCbD1u07kRAfj/lzZsHY2AStWrcRO16++O30SVy5fBEdO3VRLIuKiMCiZStR172+YpmpWXER0uUvmUwGz7Ej4VDLEfsPH0VMdDRmTZ8CG5uS6PBNJ7HjkZpgJeMrRUQ8wcB+vRAbG/PBuqjICNSoWRNWVtaKi6GhoQgphfX0aRTu3A6H15LlsK9cBc6ubhg9djxOnzoOILfd1WsU/XYXNyoGN3tLrD7xEFEJafg1PA7n78WjcQ0b9Gxgh3eSbMzcE4qnie/g98sDRCWkwcnOAgDQslYp7L4UhfDoFIRHv8bOC5FoXN36E49YOD2Nyt3fC//e3y6ubhgzbjxOn8zd3+fPnkG/Xt1QTE9P5KTC0tR2/5fUlBSs8veBQy1HsaPkC+7vXPfv3UV42C0sX+mH6jVqomnzFhg8bDh27ggSO1q+SE1NwbpVfqjp8M/zWiqVIu7Fc9Ss5QhLK2vFRU8N931ychKqVa+BeQsWws6uApo0bYZ69RvgVmiI2NEKDVYyVMdOxlcKCb6JunXdsXP3PqXlaWlpSEiIh51dBXGC5SMrSyus27gVlpZWSsvT3qapVbszpTlIl2Sjd0M76Opowb6kCeraW+JuTAoaVrXGr2EvIJP/s3375edx7u5LAMDrd1J841IGxY2KobhRMXRwscXd2BRxGqIiKysrBG7aCkurD/c3AFy+dAFjxk3A9FlzPnbzIktT2/1f/P1W4ptOnWFvX/nTGxdB3N+5nj97BosSJVC2XDnFsipVq+HBvXvIysoSMVn+WBvgi/YdO6FiJXvFsuinUYCWFmzLlBUxWcGwtraBr/9qGBubQC6X41ZoCEKDb8KtXj2xo5EaYSfjK/Xs1QfTZs7+4Jf6qMgIaGlpYdvWzWjbqhl6duuMn386KlJKYZmamaFhoyaK6zKZDAf27UE99/qKdgdt3Yz2Hs3Qu3tn/FJE2y3JlmH2j2EY0LQSogK74PKStjh39yX2/vkU5a2NkZwmgW9/F4T7dsTxWc1R195Scdslh+6gvJUR7gd0wv2ATjA30sOsPWHiNUYFH9vf+/fm7m8AmL9wCbr37C1WvHyjqe3+mOvXriI0OBgjR48VO0q+4f7OVcLSEm/fvEVGRoZiWfzLOGRnZyMtLU3EZMILvnENYaHBGDpyjNLyp1GRMDExwcJ5s9DBoymG9OuFK5cvipSy4LRv3RKDB/RFbSdneLRuK3acwkNLxIuaEKWTIZVK4evri2bNmsHFxQWenp6IiIhQ2iYpKQk1atQQI55KoqIioaWlhQoVK2Ldhi34rmt3LF20AOfO/i52NMGtCfDFwwf3MXb8JDzN0+61G7agS9fuWLa46La7SmlT/HY7Dt+sOI+J3wfjG9cy6FqvHIz1deHZrhriUzPQb+1lXP0rCfsmNYatRW5ns4KNCZ6/ykD3gIvos+Yy9IvpYFHP2iK3Rhjv9/e4CZPEjlKgNLXdEokESxd5Yfa8BTAwMBA7ToHR1P3tWNsJ1jbWWLl8KTLS0xETE43du3YAALKypOKGE5BEIoH3koWYPnv+B8/r6KhIZGZmon6DRli9fgsaNm6CaRPH4cG9uyKlLRj+q9di7fpNePToAXxXeosdh9SIKBO/AwICcP78ecyYMQNyuRy7d+9Gt27d4OfnBw8PD8V2crn8/9xL4dTp2y5o1rwFihc3BwBUrVYN0dFPcXD/XrRs1VrccAJau8oPe/fsgrdPACpXqQr7ylXQNE+7q1Sthpjopzh0oOi1u3F1a/RtXAGuM08iM0uG8OgUlLYwwKSO1ZEjk+NubAr8fnkAALgbm4pmNUvmTgA/H4GAga7oseoibkW9BgBM2RmCo9Obwefn+0hIzRSzWSpZE+CHH3fvwgrf3P2tKTS13QCwaUMgajrUQqPGTT69sZrQ5P2tr68PX/81mDFtEhrXd0OJEpYYNHQY/H1WwMSkaB7A42O2bV6PGg61UL9h4w/WDR05Bj379ofZ3xO9q1arjocP7uPo4YOo4VCroKMWmPfzraQSCWbPnIap02ao/RwkKhiidDJOnTqFgIAAuLq6AgA6duwIHx8fTJo0Cb6+vmjfvj2Aonm2RS0tLcUX7fcqVbLHzRvXP36DIsjHewkOHdiHJct90Orv0urH2l2hYtFsd207C0QlpCEzS6ZYdicmFRM7GCE06hWexCkPHYiMfwtbCyNULmUKYwNd3I9NVay7G5sCHW0t2FoYFtlOxsrluft7qfc/+1sTaGq73zt96gSSk5JQ380ZwD+/Zv/+26+4FnxLzGj5QtP3NwA4ODrixK9nkZSUCHNzC1y98ifMLSxgZGQsdjTB/H76FF4lJ6F5g9zvH9K/55ucO/Mr/rgaouhgvFehYiVERjwp8Jz5LTkpCeHhYWjZ6p8fdivZV0ZWVhbS3qXBQq+EiOkKh6L4HbSwEaWTkZmZCXNzc8V1LS0tzJw5E9ra2pg+fTp0dXXh7OwsRjSVbQhci/CwW9i87XvFskcPH6BCxYoiphLOlo2BOHRwP5av9IdHm3aK5RvXr8XtsFvYuPWfdv/16AEqVCh67Y5PyUBFGxMU09FCVk5uNa1KKVPEJKUjNPIV6ldVPlpU5VKmOHojFvEpuZ2IqrZmuBOTolgHALFJ7wquAQLavDEQhw/uh7eP8v5Wd5ra7ryCdvyA7KxsxfXVAX4AgElTpokVKd9wf+cebWmS51isWrceVla573GXL16AW131mgi8cdsOZGf/87wOXBMAAPCcOAWL58+BlrYW5i9aplj/16OHqFylSoHnzG/Pnz/DlIme+PXsBZQsWRIAcP/+XViUKAELC3YwSBiizMlwd3eHj48PXr16pbR8+vTp6NWrFyZPnowff/xRjGgqa9a8BUJDbmLXjiDExsbgwP69OP7LTxg4aKjY0VQWFRmBbVs2YvDQEajj4oqkpETFpWmzFgjJ0+6D+/fixC8/YcDgotfu327HIStHBv+BrqhkY4LWtUtjQodqCDr3BLsuRKFmmeKY2qkGKlgbY/q3NWFnbYzD12MQl5KBc3dfwre/C2qXN4eTnTl8+7vg6I1YJKcVvTHNkZER2Lb54/tbnWlqu//N1rYMytvZKS7GxsYwNjZGeTs7saMJivs7V/Hi5khPT8dqfz88i43FkUMH8dPRwxg8ZLjY0QRV2rYMypW3U1yMjYxhbGSMcuXt0KR5C5w+8QtO/vITYmOisW3zBoSHhaJH7/5ixxacQy1H1KzpAK95cxDx5AkuXbyAVX6+GDFytNjRCg0ewlZ1olQy5s6diwkTJqBRo0bYtm0bGjVqpFg3f/58WFhYYOPGjWJEU5lDLUf4+K/BxvVrsSFwLWxty2D5Sj841SmalZm8/jh/Fjk5OQjashFBW5T3T8jth/DxX4NN69di0/q1KG1bBstW+KG2U9Fr99uMbPQMuITFvZxwam5LJL+VYPWJh/jhYhQAoM+ay1ja2wme7arhcdxbDFh3BS//rmKM3XYDXj1qY/eERpDLgdNhL7Do0G0xm/PVLpzL3d/btmzEtn/t79A7D0VKlf80td2aivv7Hyv9ArB0kRd6dP0WZcqUhY//ajg4quf5UT6mRavWmDFnAbZv3YT4l3GoaF8Za9ZvgW2ZMmJHE5yOjg5WB26A97IlGNivFwwNDdG3/wD07T9Q7GikRrTkIs6ujoyMhLW1NUxNTT9YFxERgbNnz2LkyJFffL/p0qI3YVwIMs1sNqqMPyJ2BFE8CewqdgQqQDra6vPr1pfI0dA3NjX6MfOLSLNln95IDRkU0xE7gigMRPmp+/PYTz0l2mNH+LcX7bGFJOrurVSp0n+us7e3h729/X+uJyIiIiLKD5ra0RcST8ZHRERERFQExcXFYdSoUXBxcUHLli2xY8cOxbr79++jR48ecHJyQrdu3XD3rvI5X44fPw4PDw84OTlh3LhxH8yVVhU7GUREREREeRSVid+TJk2CkZERjhw5gjlz5mD16tX4/fffkZ6ejpEjR8LNzQ1HjhyBs7MzRo0ahfT0dADA7du3MXfuXHh6emL//v148+YNZs+eLejfkJ0MIiIiIqIiJjU1FWFhYRgzZgwqVKgADw8PNGnSBFevXsXJkyehr6+PGTNmwN7eHnPnzoWxsTFOnz4NANi9ezfat2+PLl26oHr16vDx8cGFCxcQGxsrWD52MoiIiIiI8tDSEu/yuQwMDGBoaIgjR44gKysLkZGRCA0NRY0aNRAeHg5XV1dFZURLSwsuLi4ICwsDAISHh8PNzU1xX6VLl4atrS3Cw8MF+xuyk0FEREREVEhIpVKkpaUpXaTSD8+3pa+vjwULFmD//v1wcnJC+/bt0bRpU/To0QOJiYmwsbFR2t7S0hIvX74EACQkJPzf9UIoxAcPIyIiIiLSLJs3b0ZgYKDSMk9PT4wfP/6DbSMiItCiRQsMGTIEjx8/xpIlS9CgQQNkZGRAT09PaVs9PT1FZyUzM/P/rhcCOxlERERERHmIeebtUaNGYciQIUrL/t0hAICrV6/i0KFDuHDhAgwMDODo6Ij4+Hhs3LgR5cqV+6DDIJVKYWBgACC3CvKx9YaGhoK1g8OliIiIiIgKCT09PZiYmChdPtbJuHv3Luzs7BQdBwCoWbMmXrx4gZIlSyIpKUlp+6SkJMUQqf9ab21tLVg72MkgIiIiIsqjKEz8trGxQXR0tFJFIjIyEmXLloWTkxNu3boFuVwOAJDL5QgNDYWTkxMAwMnJCSEhIYrbxcXFIS4uTrFeCOxkEBEREREVMS1btkSxYsUwb948REVF4dy5c9i0aRMGDBiAdu3a4c2bN1i2bBmePHmCZcuWISMjA+3btwcA9OnTBz/99BMOHjyIhw8fYsaMGWjevDnKlSsnWD52MoiIiIiIihhTU1Ps2LEDiYmJ6N69O7y9vTFmzBj06tULJiYm2Lx5M0JCQtC1a1eEh4djy5YtMDIyAgA4Oztj8eLFWL9+Pfr06YPixYvD29tb0Hxa8vd1FDWSLlW7Jn0WmWY2G1XGHxE7giieBHYVOwIVIB1t8SYhiilHQ9/YRJxzKipptkzsCKIwKKYjdgRRGBTiww/VnPObaI99f3kb0R5bSKxkEBERERGRoApxH5KIiIiIqOBpajVRSKxkEBERERGRoFjJICIiIiLKQ8yT8akLVjKIiIiIiEhQ7GQQEREREZGgOFyKiIiIiCgPjpZSHSsZREREREQkKFYyiIiIiIjy4MRv1bGSQUREREREgmIng4iIiIiIBMXhUkREREREeXC4lOpYySAiIiIiIkGpZSVDW1sze5+a2mN8vK6r2BFE4TTntNgRRHF3RXuxI4gi8Y1E7AiisDbTFzuCKGQyudgRRKGjoZ/fMrlm7m+g8O5vFjJUp6nfS4mIiIiIKJ+oZSWDiIiIiOhrcU6G6ljJICIiIiIiQbGTQUREREREguJwKSIiIiKiPDhaSnWsZBARERERkaBYySAiIiIiyoMTv1XHSgYREREREQmKnQwiIiIiIhIUh0sREREREeXB0VKqYyWDiIiIiIgExUoGEREREVEenPitOlYyiIiIiIhIUKxkEBERERHlwUKG6ljJICIiIiIiQbGTQUREREREguJwKSIiIiKiPDjxW3WsZBARERERkaBYySAiIiIiyoOFDNWxkkFERERERIJiJ4OIiIiIiATF4VJERERERHlw4rfqWMkgIiIiIiJBsZORDzzHjMT8ObPEjlHg1LndCfHxmDFlAlo0dkc7j6YI8PWGRCIBAPiuWAbX2tWVLvv37hY58dcpXdwAW4a6Imxpa/wxpxkGN6mgWFfT1gyHJjTAneVtcGRiAziUMVO67YQ2lXF5XguELPbAmv51UMJYr4DT5z+pVIqunb/BzRvXxY4iOKlUiuH9vkNY6E2l5c9jY9ChWd0Ptv/lyAH079Ye37ZqgFmTRuPF82cFFTXfSSQSeM2fg8b13dCqWWPs3LFd7Ej5SiqVovt3nRB885/n9ZU/L6Fnt86o7+aEnt064/KliyImFFbu+/lEtGxcH+09miHAd4Xi/fxl3AtMGDsSjeo5o0vHtvj911Mip80fUqkU3bt0QvC/3stiYqJR39VJpFSFi5aWeBd1wU6GwE6dPIFLFy+IHaPAqXO75XI5ZkydgMzMTGzbsRveKwNw8cIf2Bi4BgAQFRkBz4lT8Ou5S4rLt126iZz666wdUAfpkmx0Xv0nlv70AFPaV0HrWiVhqKeDbcNdERz5Cl1W/4nQpynYNswNhno6AIDe9cuhR72ymPJjOHpvuAYbM30s71FL5NYISyKRYOb0KYh48ljsKIKTSiRYvmAmnkZGKC1PiH+JedM8IZVKlJbfvPYntq5fBc/Js7Dh+70wMDTEwlmTCjBx/grw88H9u3exdftOzJnvhc0bAvH7r6fFjpUvJBIJZs+YqvS8jomJxtRJ4/Ft5+9w6OhxdPq2C6ZMHKcWHUm5XI6ZUyciMzMDW3f8gOUr/XHpwnlsClyL7OxsTBw3Grq6xbBn/xEMGDwU82fPxJPHf4kdW1ASiQSzp0/94L3sZVwcJo4brehwEamKnQwBpaakYJW/DxxqOYodpUCpe7ufPo3Cndvh8FqyHPaVq8DZ1Q2jx47H6VPHAeR2MqrXqAkrK2vFxdDQUOTUX87MUBfOFSyw/mwEopPSceZeAi4+TELDKpbo6FQamVkyrDj+CBEJ77D0pwd4J8lG+9qlAADNq1vjRFgcbkS+wuOXadj6RxQaVLEUuUXCiXjyBAP69MSzmBixowguOioC40f0x4vnsUrL/7xwDmOH9EIxvQ8rUjeuXoKrewPUb9wMZctXwMBhYxD55C+kprwuqNj5Jj09HUcPH8SM2XNRo6YDWnm0xuChw7Fv7x6xowkuIuIJBvbrhdhY5ed1QvxLdO3eE/0HDkbZcuUwYNAQGBoa4e6dOyIlFU70R97PR42dgNOnjuPPSxcRH/8Si5evRIWKFdGtRy80atIUt8PDxI4tmIiIJxjY98N9fv7sGfTt1Q3FiqlfBfpraWlpiXZRF+xkCMjfbyW+6dQZ9vaVxY5SoNS93VaWVli3cSssLa2Ulqe9TUNaWhoSEuJhZ1dBnHACysySIV2aje51y0JXWwsVrY3hWsEC95+/QR274giJUv4CGfL0NZztzAEAr9Oz0LyGDUqa6UNfVxvf1CmN+8/fiNCK/BESfAN167lj14/7xY4iuPBbwXByqYu1W39QWn7tykUMHuGJsZNnfnAbMzNz3AkLQczTKORkZ+P3U7+gVGlbmJiafbBtUfPXo4fIzs5GnTrOimXOLq64czscMplMxGTCCwm+ibp13bFz9z6l5W513TF95hwAQFZWFo4eOQRplhS1HIv+D0mW/+f9PPd1Xh8mJiaK5f5rAtG1e8+CjplvQm7eRN167ti5R3mfX7p4AWM9J2DGrDkiJSN1VKiOLpWdnY20tDSYm5uLHeWLXb92FaHBwTh07BcsW7xQ7DgFRhPabWpmhoaNmiiuy2QyHNi3B/Xc6yMqMgJaWloI2roZVy5fRHFzc/QbMBidOn8nYuKvI82WYeGR+/D6riYGNbaDro42Dt18hoM3nqF1rZJ4/PKt0vZJaVJULZX7YRz4+xNsGeqKPxe0RHaODIlvJeix7poYzcgXPXv3FTtCvvm2a6+PLp86eyEAfDBHAwC69OiL0OBrGNqnM7R1dGBgYIjVm3ZAR0cnP6MWiKTERJibWyhVcCwtrSCRSJCSkoISJUqImE5YPXv1+b/rY2Ki0fXbDsjJycGESVNhW6ZsASXLP6ZmZmjQqLHi+vv387ru9fH82TPY2tpi3Wp/nDz+M8zNLTBqrCeat/QQMbGwevb++D5fsGgJAHwwR4NIFaJVMk6cOIHFixfj119/hVwux9KlS+Hi4oIGDRqgUaNG2L276EyclUgkWLrIC7PnLYCBgYHYcQqMprZ7TYAvHj64j7HjJ+FpVCS0tLRQoWJFrN2wBV26dseyxQtw7uzvYsf8KvYlTXDufgK6r7uKGftuo51jKXzrbAuDYtqQZiv/iivNlkFPJ/ctpGwJQ2Rk5WBEUDD6bryOlymZWNGz6P/qSR+XnJQAqUSK2Qu9sXbzLjg5u8J74WxI1WAsd0ZmBvT+NUTs/fUsqVSMSKKxsCiB3XsPYvbcBdi0YR3O/P6r2JEEtzbAD48e3MfY8RORkf4Ov/x8DG/evMGqtRvQsVNnzJw6Cffv3RU7JomAE79VJ0olIygoCBs3bkSDBg3g5eWFY8eO4cGDB/D19UXlypVx584d+Pn5IT09HSNHjhQj4hfZtCEQNR1qoVHjJp/eWI1oYrvXrvLD3j274O0TgMpVqsK+chU0bd4CxYubAwCqVK2GmOinOHRgL1q2ai1u2C/UoLIletYri8ZLzkOSLcPdZ29QsrgBxnrYI/ZVOvR0lX+T0NPVRmZWDgDAt3dtrDj+EOcfJAIAJvwQhgtzm8OpfHGEx6QWeFsof632WYImLTzQqm1HAMCcRSvRp0tr/HnpPFp4tBM5nWr09fUh/Vdn4v11TfoxBQBMTU1RvUZNVK9RE5ERT7Dvx93waN1W7FiCef9+vvzv93MdHV0UL26O2fO8oK2tjeo1HXArNARHDx1ATQf1OpAFUUEQpZOxZ88eBAQEoGnTpggJCUH//v2xadMmNGvWDABgb28PCwsLzJ8/v0h0Mk6fOoHkpCTUd8sdw5uVlfuB9Ptvv+Ja8C0xo+UrTWu3j/cSHDqwD0uW+6DV3x+0Wlpaig7GexUq2hfJQ5zWKmuG6KR0SPJULO4/f4OxrewRHPUKVqb6Sttbm+oh4Y0EJUz0YGthiIcv/hlOFZeaidfpUpSxMGQnQw09fvgAfQeNUFw3NDJCmbJ2SHj5QsRUwrCxKYmUlNfIzs6Grm7uR2RSUiIMDAxgalb055x8jognj5GamgoXVzfFskr2lREc/OHQuaLKx3spDh/Yh8XLV6JV6zYAACtra2hpaUFb+58fVOwqVMCTv9Tr6FL0edRpArZYROlkvH79GhUqVAAAuLq6onTp0rCyUp6EVbZsWWRkZIiQ7ssF7fgB2VnZiuurA/wAAJOmTBMrUoHQpHZv2RiIQwf3Y/lKf3i0+eeX2o3r1+J22C1s3Pq9Ytlfjx6gQoWKYsRUScIbCcpbGaGYjhaycuQAAHsbYzx7lY6w6FSMallJaXvXChbYcDYCqelZkGTloHJJE0QmvgMAWBgVg7mRHmJfFY3XMH0ZSytrxDyNRL0GuWPbpVIpXsY9R6nSRX/MfrXqNaCrq4vb4WGKL9m3QkPgUMtR6cunOrvwx3n88tNRHPn5pOKL1v3791CxYqVP3LJo2LJxPQ4f3I9lK/3h0eafykyt2k7YvmUTcnJyFPOLoiIjUdrWVqyoREWaKO+YLi4uWL9+PdLT0wEA586dg4ODg2J9QkICvL290aBBAzHifTFb2zIob2enuBgbG8PY2Bjl7ezEjpavNKXdUZER2LZlIwYPHYE6Lq5ISkpUXJo2a4GQkJvYtSMIsbExOLh/L0788hMGDB4qduwvdvZ+ArJzZFjewxEVrIzQsqYNRreyx87L0Th9+yXMDHUxr3MNVC5pgnmda8BQTwcnw18iRybHoZvPMatTddStZIEqpUzg39cJYdEpuBPLKoY66tC5G/bs2Iprly8gNjoKq1YsgqGRERo0biZ2NJUZGhqiU+cuWLp4Ie7euY1zZ89g147t6Nt/oNjRCkzHb75FUlIi1q7yR3T0U+zfuwcnj/+MocML/8iCT4mKjEDQlo0YPHQ46ri4KL2ft23fETK5DCuWLUZsTDQO7vsRV/68hO+69RA7NlGRJEolw8vLCyNHjsS8efMQEBCgtO7MmTMYP348atWqBW9vbzHiESn54/xZ5OTkIGjLRgRt2ai0LuT2Q/j4r8Gm9Wuxaf1alLYtg2Ur/FDbyfk/7q3wSsvMxsDNNzGvcw0cndgQr95JseFMBPZdyz1/woigECzp5oDe9cvhYdxbDN8Wggxp7pyMZT8/wJR2VbCqbx3oF9PGn38lYdre22I2h/JRj76DIJfLEbhqBd6kpsLB0Qk+a7dCT1//0zcuAqbNmI1lixdi+JBBMDE1wZhx4+Hx95AaTVCyVCms37QNfj7e2Ld3N0rbloGP/xrUqOnw6RsXchfOn/v7/XwTgrZsUloXfPsB1m8Owoqli9Cr67coVdoW3j7+qK4G7aYvx+FSqtOSy+VyMR5YLpcjKSkJ1tbWSsuTk5Px7NkzODp+fWk6M/vT25D6yM4R5Sksujpz1fMMxJ9yd0V7sSOIIvFN0T9y09ewNlOPjsuXksk0830tR5yvJKLT0dbML7RGxQpvu5sG/CnaY1+c0ki0xxaSaOfJ0NLS+qCDAQCWlpawtFSfMwUTERERUdHCQobqNGMWGxERERERFRh2MoiIiIiISFCiDZciIiIiIiqMOPFbdaxkEBERERGRoFjJICIiIiLKg4UM1bGSQUREREREgmIlg4iIiIgoD87JUB0rGUREREREJCh2MoiIiIiISFAcLkVERERElAdHS6mOlQwiIiIiIhIUKxlERERERHlos5ShMlYyiIiIiIhIUOxkEBERERGRoDhcioiIiIgoD46WUh0rGUREREREJChWMoiIiIiI8uAZv1XHSgYREREREQmKlQwiIiIiojy0WchQGSsZREREREQkKHYyiIiIiIhIUBwuRURERESUByd+q46VDCIiIiIiEhQrGUREREREebCQoTp2MqjI09HQQ0DcXdFe7Aii6Ln9ptgRRHFgaF2xI4giRyYXO4IotDX0G46OZjYbwVGvxY4giqZVS4gdgfIRh0sREREREZGgWMkgIiIiIspDCxpaVhMQKxlERERERCQoVjKIiIiIiPLQ0OmegmIlg4iIiIiIBMVKBhERERFRHjwZn+pYySAiIiIiIkGxk0FERERERILicCkiIiIiojw4Wkp1rGQQEREREZGgWMkgIiIiIspDm6UMlbGSQUREREREgmIng4iIiIiIBMXhUkREREREeXC0lOpYySAiIiIiIkGxkkFERERElAfP+K06VjKIiIiIiEhQrGQQEREREeXBQobqWMkgIiIiIiJBsZNBRERERESC4nApIiIiIqI8eMZv1bGSQUREREREgmIlg4iIiIgoD9YxVMdKBhERERERCYqdDIFJpVJ07fwNbt64LnaUAqVp7ZZKpVi+dBGaNKyLlk0bYu3qAMjlcrFj5TuJRAKv+XPQuL4bWjVrjJ07tosdSRC62loY1ag8fhzkjF3962BA3TIfbGNjoof9Q1xQq7Sp0vIONW0Q1Lc29g12wUwPe5jo6xRU7Hynrvv73xLi4zF9ygQ0b+SOtq2awt/HGxKJRGmbmJhoNHBzEilhwdDU97VXycmYNnkCmjSoi2/bt8HPx46IHUkQr5MTsNF7Dib2aYPpgzph/7Y1yJLmPq+jnzyE97QR8OzREsunDUfEw7tKt/3zzHHMH90rd/3UYXhyP1yMJlARx+FSApJIJJg1YyoinjwWO0qB0sR2+3gvxY0b17FhcxDS373DrOmTYWtri+49e4sdLV8F+Png/t272Lp9J168eIH5c2bCtrQtWrdtJ3Y0lYxoWB61bc3gdfIvGBbTxvRW9khIk+LXB4mKbcY0toNhMeUORONKJTCkflkEnI/C85RMjG9WAaMb2cHvXGRBNyFfqOv+zksul2P6lAkwMyuOoJ27kZqaikUL5kJbRweTp84AALx8GYdJ40Z/0PFQN5r4viaXyzFloidkMhm2bt+JhPh4zJ8zC8bGJmjVuo3Y8b6aXC7HJu+5MDIxxYwVm/Au7Q12rlkGbW1ttO3aD/7zxsOtcSsMnjgXd0OuYtWCiVgUuAeWNqVwN+QqftzohwHjZ6NS1Zq4cu4k1i6aisUb9sLc0lrsphUYnvFbdaxkCCTiyRMM6NMTz2JixI5SoDSx3ampKTh29DAWLFwCR8facK/fAAMGDcWd2+r9S096ejqOHj6IGbPnokZNB7TyaI3BQ4dj3949YkdTiYm+DlpXt0Lgpad4nPgOt1+8xbHbL1HNxlixTbPKJWCo92GFoludUjgc9hJXo14j5nUGdlyLhV0JQ2irwWeTuu7vf3saFYU7t8OxcMly2FeuAhdXN4wZNx6nTx4HAJw/ewb9enVDMT09kZPmL019X7t/7y7Cw25h+Uo/VK9RE02bt8DgYcOxc0eQ2NFU8vJZNCIf3cWQifNQxq4SqjrUwbf9RuDGhd9w9dwpmJgWR/8x01G6XAW07tIHVWrUxoVTuRWcK2dPokGrDqjfvC1sbMuhS/9RMLOwxO3gKyK3ij5GKpVi0aJFqFu3Lho2bIiAgH8qkPfv30ePHj3g5OSEbt264e5d5YrV8ePH4eHhAScnJ4wbNw6vXr0SNBs7GQIJCb6BuvXcsevH/WJHKVCa2O5boSEwMTGBW916imVDh4/EoqXeIqbKf389eojs7GzUqeOsWObs4oo7t8Mhk8lETKaamqVMkS7Nwb24t4plh8NfYu2FpwAAU30dDHYvh/WXnirdzrCYNuytjHE16rVi2b2XaRh/6B5kajDCRF33979ZWVkhcNNWWFpZKS1Pe5sGALh86QLGjJuA6bPmiBGvwGjq+9rzZ89gUaIEypYrp1hWpWo1PLh3D1lZWSImU01xC0tMXLQKZhYllJZnpL9D0ssXKG9fDdo6//xwUqZiZcWQqbZd+6N1lz4f3GfGu7T8DV3IaGuJd/kSS5cuxZUrVxAUFAR/f38cOHAA+/fvR3p6OkaOHAk3NzccOXIEzs7OGDVqFNLT0wEAt2/fxty5c+Hp6Yn9+/fjzZs3mD17tqB/w0I3XMrFxQU//fQTyuV5wRcFPXv3FTuCKDSx3c+excLWtgx++ekYgrZtQlZWFjp36YrhI8dAW1t9++1JiYkwN7dQ+kXX0tIKEokEKSkpKFGixP+5deFVylQf8W+laFHFEj2cS0NXWwtnHyXhwK04yAEMa1Ae5/5KQuzrTOXbmekDAIob6mJlk+qwMdVH+PM32HolBu+kOSK0RFjqur//zdTMDA0bNVFcl8lk2L93D+q51wcAzF+4BAAQfFO955tp6vtaCUtLvH3zFhkZGTA0NAQAxL+MQ3Z2NtLS0mBhYSFywq9jZGKKWi71FddlMhnOnziE6k5uMLUogdinysObXyfGI+1NKgDArnI1pXV3Q64i/nkMqtd2y//g9EVSUlJw+PBhfP/996hduzYAYOjQoQgPD4euri709fUxY8YMaGlpYe7cubh48SJOnz6Nrl27Yvfu3Wjfvj26dOkCAPDx8UGLFi0QGxsr2HdwUToZ/6+nJJVK4evrC2Pj3KEK3t7q/SsKFT0Z6emIiYnGoYP7sGiJN5ISE7F08QIYGBhi4OChYsfLNxmZGdD715CR99ezpFIxIgnCoJg2bIvro10Na6z5IwoljPQwtokdJNkyRL3KQI1SJhh/8O6Ht9PN/RVwVCM77LzxDG8zszG8YXlMblEJS38t+vOT1HV/f8qaAF88fHAfP+w9KHaUAqWp72uOtZ1gbWONlcuXYubsuUhMSsTuXTsAAFlZ6vM8P/x9IGIiHmFuQO7BG07s+x4Xf/0JjTw64kF4MMKuX/rofIuEuGf4fvVSuDdv+0HnQ90VhTkZISG5Fch69f6pQI4cORIAMH/+fLi6uiraoaWlBRcXF4SFhaFr164IDw/HiBEjFLcrXbo0bG1tER4eXrCdjOrVq3/2H/vBgwef3CY5ORkXL15E7dq1YW9v/1n3S1RY6OjoIi0tDd4+/rC1zT0KUdzLFziwb69afxjr6+tD+q8vl++vGxgYiBFJEDkyOYz1dOF3LhKJaVIA72BtoocODjbQ0gI2XY6GNOfD8U+yv8e8Hg6Lw43oFABA4MUorOlWCyWMiuFVetEdagGo7/7+f9YE+OHH3buwwjcAlatUFTtOgdLk9zVf/zWYMW0SGtd3Q4kSlhg0dBj8fVbAxMRE7HiCOLRjPc78fAAjZyxBGbvc71wDx8/C3i2rsHuDD8pVrILmHbri0Z1Qpdu9fB6DVfMmwLp0GQz0nCVGdI0llUo/eP/V09P74Ief2NhYlClTBseOHcOmTbkVyK5du2LMmDFITExE5cqVlba3tLTE48e5P4IlJCTAxsbmg/UvX74UrB2f1cnYtWuXYA8IAFu2bMGJEyfg6+uLBg0aYNy4cYo/3OnTpzF9+vQiN1yKNIeVtTX09fUVH8QAUKFCRcS/jBMxVf6zsSmJlJTXyM7Ohq5u7ltHUlIiDAwMYGpmJnK6r/c6PQuSbNnfHYxcz1IzYVs894v0rNbKb9Je7avi3F9JOBSWu7+fpfwzjOr53/+3MtYr8p0Mdd3f/2Xl8iU4dGAflnr7oFXrtmLHKXCa+r4GAA6Ojjjx61kkJeUOEbx65U+YW1jAyMj40zcu5H7c7I8LJ49i2FQvuDZqoVjeyOMbNGjRHm9SX8O8hBUOfR8IS5vSivXPoyMRMG88rEvZYuLCVdDTV88fFgqrzZs3IzAwUGmZp6cnxo8fr7QsPT0d0dHR2LdvH7y9vZGYmIgFCxbA0NAQGRkfr0a/77xkZmb+3/VC+KxORt4yzHtpaWmIiYlB5cqVIZVKv7jH37FjRzRu3BgrV65Ep06d4OXlhYYNG37RfRCJoXZtJ0gkEkQ/jYJdhYoAgKjISKUPZ3VUrXoN6Orq4nZ4GFxcc8fm3goNgUMtxyI9ZvtRwjvo6+YOmXqRmnuI0nLmBoh/I8GCk4+Utt3cuzYCL0Yh7NkbpGZmI/mdFBUtjfBX4jsAQFlzQ8jkciSkFf1Dnarr/v6YzRsDcfjgfnj7+MOjjfocnvdLaOr7WmpqCiZ5jsWqdethZZU7XOjyxQtKE+CLqp/3BuHiqaMYOWMxXBu1VCx/eDsEF08fw8gZS2BewgpyuRx3Q66iWfvvAAApr5KwesFE2NiWw8SFATAwNBKrCaISc7TUqFGjMGTIEKVl/+4QAICubm4F0t/fH2XK5L5WX7x4gb1798LOzu6j1ej3lej/qla/n5skhC/+pJBKpZg3bx7q1auH7t27Iz4+HrNmzcKwYcOQmpr6RfdVvHhxLF++HAsXLsSiRYswdepUjTjxDxVtFSpWQpOmzTF/7mw8evgQV/68hO1BW9Cj14dH41AnhoaG6NS5C5YuXoi7d27j3Nkz2LVjO/r2Hyh2NJU8T83EzegUTGxWCRVKGMK5rBm61SmNn+68RNwbidIFAJLfZSE1MxsA8NOdePR1K4M6ZcxQoYQhxjSxw/WnKUjJyBazSYJQ1/39b5GREdi2eSMGDx2BOi6uSEpKVFw0iaa+rxUvbo709HSs9vfDs9hYHDl0ED8dPYzBQ4aLHU0lcbFPcWLf92jXfQAq13RC6utkxaVkmXIIv3EZf5w8gsSXz/HjRj+8S3uLBi07AAAObl8HmUyGQRPmQJKZobhdZka6yK3SHHp6ejAxMVG6fKyTYf13BfJ9BwMAKlasiLi4OJQsWRJJSUlK2yclJSmGSP3Xemtr4c6F8sUTv318fPDkyRMcPXoUvXvnnqBn/PjxmD17NpYuXQpfX98vDtGgQQP88ssvWLduHSwtLRWleaLCavlKP6xcvgRDBvaBgYEhevfphz79BogdK99NmzEbyxYvxPAhg2BiaoIx48bDowifsOo9/3ORGNmoPFZ8WwPSbBlO3EvA8XsJn7zdsdsvoaejhcktKsGgmDZuRKdg46XoAkhcMNR1f+d14dxZ5OTkYNuWjdi2ZaPSutA7D0VKJQ5NfV9b6ReApYu80KPrtyhTpix8/FfDwdFR7FgqCbt2ETJZDk7s34ET+3cordv6y1WMmrkUB7evw8Ht61CpmgOmLFkLA0MjyOVyhF29AKlUgvmjeyndrlOfYfi2b9HufH2JojDx28kptwIZFRWFihVzK5CRkZEoU6YMnJycsHXrVsjlcmhpaUEulyM0NBSjR49W3DYkJARdu3YFAMTFxSEuLg5OTk6C5dOSf2HpoGnTpli/fj0cHR3h7OyMn3/+GeXKlcO9e/cwdOhQXL8u/mH+Mov+j4j0BTS1+FUE3v/yRc/tN8WOIIoDQ+uKHUEUOepw0pGvoK2hL3A5NHN/B+c5344maVq18B4Ke+CPt0V77F19a3/2tqNGjUJqaioWLlyIxMREzJgxA2PGjEHXrl3RunVrdOzYEb1798a+fftw+vRp/PbbbzAyMsKtW7cwYMAAeHl5wdHREcuWLYOxsTE2bdokWDu+eLjUu3fvPjpeSyaTISen6B8bnoiIiIioKPDz80P58uXRp08fzJw5E/369cOAAQNgYmKCzZs3K6oV4eHh2LJlC4yMcufYODs7Y/HixVi/fj369OmD4sWLC37aiC8el9SyZUusWrUKK1euVCyLjY3F0qVL0axZM0HDEREREREVtC8987ZYTE1N4ePj89F1tWvXxtGjR//ztl27dlUMl8oPX1zJWLBgAbS1tVGvXj1kZGSgW7duaNOmDczMzDB//vz8yEhEREREREXIF1cyTE1NsW7dOsTGxiIiIgLZ2dmoWLEiT6pHRERERGqhKEz8Luy+6mDncrkc0dHRiI6ORkJCwgeHwCIiIiIiIs31xZWMR48ewdPTE8nJyahQoQLkcjmePn2KChUqYN26dShbtmx+5CQiIiIiKhCsY6juiysZXl5ecHJywqVLl3DkyBEcPXoUFy5cQJkyZTgng4iIiIiIvryTcf/+fYwbNw7GxsaKZWZmZpg8eTJCQ0MFDUdEREREREXPFw+XcnJywtWrVxVnFnwvNDQUNWrUECwYEREREZEYNPWEmEL6rE5GYGCg4v92dnZYvnw5bty4gdq1a0NbWxt//fUXjh8/jv79++dbUCIiIiIiKho+q5Nx/fp1pevOzs5ITk7G+fPnFcucnJxw9+5dYdMRERERERUwFjJU91mdjB9++CG/cxARERERkZr44jkZAPDgwQM8fvwYMpkMQO55M6RSKe7fv49FixYJGpCIiIiIiIqWL+5kBAYGIjAwEFZWVkhOTkbJkiWRlJSEnJwctG7dOj8yEhEREREVGJ7xW3VffAjb/fv3Y9GiRbh8+TJKly6NH374AVeuXEHDhg1Rvnz5/MhIRERERERFyBd3Ml6/fo0mTZoAAGrUqIFbt24pzpNx8uRJwQMSERERERUkLS3xLuriizsZJUuWRGxsLADA3t4e9+/fBwCYmJjg1atXwqYjIiIiIqIi54vnZPTo0QNTpkzB8uXL4eHhgcGDB8PGxgZXrlxB9erV8yMjEREREREVIV/cyRg9ejRKlSoFQ0ND1K5dG7Nnz8a+fftgbm6O5cuX50dGIiIiIqICwzN+q+6rDmHbpUsXxf979OiBHj16IDMzE4mJiULlIiIiIiKiIuqL52T8l5s3b6JNmzZC3R0RERERkSg48Vt1gnUyiIiIiIiIgK8cLkVEREREpK54Mj7VsZJBRERERESC+qxKxs2bNz+5zaNHj1QOQ0RERERERd9ndTIGDBjwWXdWWEpLOTK52BFEoaNdOP7+BU0Ozdzf7zJzxI4gin2D3cSOIIrvtl4XO4IoDg+rJ3YEUWjq51hWjkzsCKJwLm8udgT6Fw71Ud1ndTIePnyY3zmIiIiIiEhNcOI3EREREVEehWV0TlHGahAREREREQmKnQwiIiIiIhIUh0sREREREeWhocfSEdRXVTJycnLwxx9/YMeOHXjz5g3Cw8Px9u1bobMREREREVER9MWVjLi4OAwbNgwpKSlITU1Fq1atsG3bNty6dQtBQUGoVq1afuQkIiIiIioQrGSo7osrGYsXL4arqysuXboEPT09AEBAQAAaNmyIpUuXCh6QiIiIiIiKli+uZAQHB+PAgQPQ0dFRLCtWrBjGjh2L7777TtBwREREREQFjYewVd0XVzIMDAyQnJz8wfKoqCiYmJgIEoqIiIiIiIquL+5k9O7dGwsWLMAff/wBILdzcfjwYcyfPx/du3cXOh8RERERERUxXzxcaty4cTAzM8PChQuRkZGBkSNHwtLSEoMHD8awYcPyIyMRERERUYHhxG/VfdV5MgYMGIABAwYgPT0dOTk5MDU1FToXEREREREVUV/cyTh27Nj/Xd+lS5evjEJEREREJD7O+1bdF3cy1q5dq3Q9JycHycnJ0NXVRe3atdnJICIiIiLScF/cyTh37twHy969e4cFCxbwRHxERERERPTlR5f6GGNjY4wfPx7ff/+9EHdHRERERCQabS0t0S7qQpBOBgA8fPgQMplMqLsjIiIiIqIi6ouHSw0YMOCDsyC+e/cOjx49wuDBg4XKRUREREQkCsF+hddgX9zJcHd3/2CZnp4epk2bhgYNGggSioiIiIiIiq4v7mSkpKRg4MCBKF++fH7kISIiIiISlRpNjRDNF1eDfv75Z2hrs4hEREREREQf98WVjMGDB2PRokUYPHgwbG1toa+vr7Te1tZWsHBERERERFT0fHFJYu3atbh06RKGDRuG9u3bo1WrVmjVqhVatmyJVq1a5UfGQk0qlaLHd50QfPO6Ytn9e3cxqF8vNKrngoH9euF2eJh4AfNZfHw8pk6agCYN6sGjRRP4rvSGRCIRO1a+e5WcjGmTJ6BJg7r4tn0b/HzsiNiR8oVUKoX/iiVo17wBvmndFJsCV0MulwMAZk7xRCNXB6XLnxf/EDewwKRSKbrneX0vmDsLzo7VP7iMHDZI5KRfp5i2FsY2qYADQ13x4yAXDHIvq1hXoYQh/LrUxLERdbGhpyNq25p99D661SmNHf3qFFDi/PXv/Q0APiuWfbC/9/24W8SUwkmIj8eMKRPQorE72nk0RYDvP+/fd8LDMGRAbzR2d0HXTu1w9PBBkdPmjynjR2PxgjmK63+cO4NeXb9Bi4auGDmkPx4+uC9iOmHFxkTDc/RwNKnvio5tW2LXjqCPbtOoXp2CD1cI8RC2qvusSsbNmzfh7OwMXV1dnD17Nr8zFRkSiQRzZk5DxJPHimWvkpMxesQQtG7TDguXeOPPyxcxduRQHDx2HKVLq1eVRy6XY9rkCTAzM8P3P+zBm9RUeM2bAx0dbUyZNlPsePlGLpdjykRPyGQybN2+Ewnx8Zg/ZxaMjU3QqnUbseMJarWfN0JvXkdA4Gakp6fDa/Y0lCptiy7deuJpZAQWLFkJt3r/HAzC1Ky4iGmF9bHX9/RZczFh8lTF9RfPn2PE0IHo03eAGBFVNqqxHZzKmGHe8YcwKqaDWa0rI+GtFBeeJGNZp+q4/jQF/uci0KqaFea3q4Lhe8ORmpGtuH0pU330dyujtKyo+tj+BoDIiAiMnzgF33b5TrHM2NikoOMJTi6XY8bUCTAzK45tO3bjTWoqFnnNhba2DvoPGoLxY0eie8/eWLR0BR7cv4dFC+bAytoaTZo2Fzu6YH4/fRJXLl9Eh05dAACREY/hNWc6Zs5diNp1nLFvzy5MnTAah3/+FQaGhuKGVZFMJsNEz9FwcHDEnv2HERMTjbmzpsHGpiTadfgGAPDyZRwmjR+jET8UUsH4rE7GwIEDcfnyZVhaWqJMmTL5nalIiIx4gjkzpyl+1X3v+C/HYF7cHHPmL4SOjg4qVqqEa1f+xKH9ezF+0tT/uLei6WlUJG6Hh+HchT9haWUFABjrOQH+fivVupNx/95dhIfdwi+nfkfZcuVQvUZNDB42HDt3BKlVJ+NNagqOHzuCNRu3oWat2gCA3v0H4f7d2+jQqQviXjxHDYdasLSyFjmp8CL+4/VtamoKU1NTxfUFc2ehdZt2aNHKo6AjqsxEXwdtq1tjzi8P8VfCOwDA4fA4VLcxRjEdLWRmyRB4MQoyObD75nPULW+OqtbGuBmTqriP8c0qIiIpHVbGemI1QxD/tb8BICoyAoOGDIWVmj3Pnz6Nwp3b4fjt/GVYWua+f48eOx6rA3xQtlx5WFpZwXPiFABAebsKCL55HadPHlebTkZqagrWrfZDTQdHxbLrV6+gYqXK6NCpMwBgzPjJOLT/R0RFRqCGQy2xogriVXISqlWrjlnzvGBsbIzydhVQr159hN0KQbsO3+CPc2ewbLEXrKzV63muCjUqKIjms4ZLfeyNV9OFBN+EW1137Ni9T2n582fPUKOmA3R0dBTLqlStppZDpiytrLFh8zZFB+O9tLdpIiUqGM+fPYNFiRIoW66cYlmVqtXw4N49ZGVliZhMWOFhoTAxMYGza13FsgFDRmCO11LEREcB0IJtmbL/fQdFWEjwTdSt646d/3p953X92lWEhgTDc+LkAkwmHIfSpngnzcGduLeKZQdvxWHVH1GobWuGq1GvIcvz1j/x8D2lDkarqlbQ19XGrw8SCjJ2vviv/Z2WloaEhHjY2VUQJ1g+srK0wrqNWxUdjPfS3qahYaPGWLh4+Qe3SUt7+8GyomrdKl+079gJFSrZK5YVL26OqMgnCA8LhUwmw/GfjsDYxARl8rzXF1VW1jbw9l0FY2NjyOVyhN0KRWhoMFzd6gEALl+6gNHjJmDqjDmfuCeiz/fZE7//fQK+/CCXy5GSkgILC4t8fyxV9ejV56PLS1ha4q9HD5WWxb+MQ0pKSgGkKlhmZmZo1LiJ4rpMJsO+H3fDvX59EVPlvxKWlnj75i0yMjJg+HcJPf5lHLKzs5GWllYknr+f48XzZyhlWwanjv+EXdu3Ijs7Cx06dcGgYaPwNCoSJiYmWLJgFm4F34RNqVIYNsoTDRo1+fQdFwE9/+P1ndf3QVvRqfN3KFWqdAEkEl5pMwPEv5WgVVUr9HKxha6OFn5/mIh9IS9QykwfjxLSMKFZRbhXMEfCWym2XonG/Ze5PyAUN9DF0PrlMPuXh6hqYyxyS1T3X/s7KjICWlpa2LZ1M/68dBHFzc3Rf+BgfNv5u49uX5SYmpmhYSPl9+8D+/agnnt92JYpq/QDwqvkZPx6+iRGjfEUI6rggm9cQ1hoMHYf+Ak+yxcrlnu0bY9LF85j1JD+0NHRgZaWFvzXbYKZGg0DBYBO7VvhZVwcmjRtjpYeudX3eV5LAADBN2+IGY3UzGd3Mrp16/ZZh679nDkbEydOxLJly2BikjuuNSsrC76+vjhw4AAkEgnMzc0xYsQIDB069HPjFRqtPNpg2+aNOHLoAL7t0hU3rl/FH3+cg42NjdjR8t0qf188eHAfe/YfEjtKvnKs7QRrG2usXL4UM2fPRWJSInbv2gEAyMqSihtOQBnp6XgWE42fjhzA3IVLkZSUCN9li2BgYIiMjHRkZmaiXv1G6D94OC6eP4uZk8dh844fUaNm0R5W8Dmexcbi5o1rmD6r6P7qZ6CrjTLFDdC+pg0CzkeihFExTGhWEZJsGQyL6aCnsy1+uvMSC048QrPKllj2TXWM2HsbSe+kGNnIDr8/SkLM6wy16GT8l6ioSGhpaaFCxYro3ac/QoJvYOmiBTAxMUHLVq3FjieoNQG+ePjgPnb9qDzBOzMzE9OnTICVpRW6du8lUjrhSCQSrFi6ENNmzYeBgYHSutSUFCQnJ2HarHmo5eiEwwf3YanXXOzcewglSliKlFh4Pv5rkZyUiBXLFiPAdwWmz5ordqRCSZvDpVT22Z2MIUOGKI1FVsVvv/2GBQsWKDoZa9euxW+//QYfHx/Y29vj/v378PX1RWZmJsaOHSvIYxaUylWqYp7XYviuWIblSxaiarXq6NGrD4JvXP/0jYuwVf6+2PPDTvj4rUKVKlXFjpOv9PX14eu/BjOmTULj+m4oUcISg4YOg7/PCsVzWh3o6Ojg3bs0LFzmi1J/H7Qg/mUcjh7chx8PH0f33v0Uv/BVqVodjx7cw89HDmpEJ+Psmd9QrVp12NtXFjvKV8uRy2GsrwufM0+QkJbbObYx1UdHBxvkyOSISErH7pvPAQARSelwKVscrapZ4XHiO9QoaYI1f9wWM36B6PRtFzRr3gLFi5sDAKpWq4bo6Kc4uH+vWnUy1q7yw949u+DtE4DKed6/09PfYcqEcYiJfoqgnXsUlduiLGjzetSoWQv1Gzb+YN36tf6wr1wF3Xv1BQDMnr8Ivbt+g+M/HcXAIcMLOmq+qfn3/BKJVIr5s6dj0tTpKFasaM+rosLpszoZWlpa6NixIywthenJ/3uOx+nTpzFv3jx4eOROnrS3t4eZmRnmz59f5DoZAND5u2745tsuePUqGdbWNlgd4AtbNZ4w771sCQ7u34tlK3zh0aat2HEKhIOjI078ehZJSYkwN7fA1St/wtzCAkZG6vOrrqWVNfT09RUdDAAob1cR8fEvoa2t/cEQAruKlRAVEVHQMUVx5c9LaN6y6E32zut1ehYk2TJFBwMAnqVkwNpEHw/j0/AsJUNp+2epmbA20YNtcQNYmehh3xBXAICOlhZ0dbRwZLgb5p94hHtx6jNuX0tLS9HBeK9SJXvcVKMfjXy8l+DQgX1YstwHrVr/8/6dlpaGCWNHIDYmBpu27UB5NZmX8vuvp/AqOQktGuY+f6V/z6M7f+ZXlCxVGj379Fdsq62tjSpVq+Fl3AtRsgopOTkJd8LDlN63KlWyR1ZWFt6lvYO5BTsZ/6ZOh5IViygTv7W0tJTmeGhra6NsWeUJpOXLl8e7d+8EfdyCcPPGNcyaPgU6OjqwtraBXC7HlUsX4VbX/dM3LoI2bQjEoQP7sNI3AO07dBQ7ToFITU3BkAF9kZLyGlZW1tDV1cXlixfgVree2NEE5eDoBKlEgpjop4pl0VERKF26DJZ6zcHyRfOUtn/86BHsKlQs4JQFTy6X497dO6jj7CJ2FJU8iE+D/t9Dpt4rZ26I+LcSPIxPQ0VLI6Xty5kbIP6NBNuvxWDUvtsYd+AOxh24gx9uPsOrd1kYd+AOHieo10EfNgSuxajhQ5SWPXr4ABUqqsfzfMvGQBw6uB/LV/qjbft/3r9lMhmmTx6PZ8+eYcv3P8C+chURUwprw9Yd2H3gGHbtO4Jd+46gSdMWaNK0BXbtOwIraxtERSr/UBL99Kla/Ej44vkzTJ8yAQnx8YplD+7fg4VFCZiryTxCKnw+q5Px3XfffXBmb1XI5XLMmzcPq1atwrFjx1CrVi3s2rVLsV4ikWD9+vWoU6eOYI9ZUOzsKuLihfM4uH8vnsXGYsWyxXjz5g06de4idjTBRUZEYMumDRgybAScXVyRlJiouKiz4sXNkZ6ejtX+fngWG4sjhw7ip6OHMViNyukAYFehIho2boZlC+fi8V8Pcf3KZfywIwjfde+Fxs1a4NeTv+DU8Z/wLDYa27dswO2wUHTv3U/s2Pku7sVzvHv3DpXs7T+9cSH2PCUT15++xpSWlVDR0ggu5Yqjp4stTtyLx8l78ahoaYR+bmVQ2kwfA+qWQSkzA5x7nITUjGzEvZEoLikZWciRyRH3RgJpjnodibBZ8xYIDbmJXTuCEBsbgwP79+L4Lz9h4KCiN1/w36IiI7Bty0YMHjoCdVxckZSUqLgcO3IIwTevY/7CJTA1NVUsT01NETu2ykrblkG58naKi5GxMYyMjVGuvB06d+2Bn48ewqnjPyM2Jhrr1wTgZdwLxXk0irKaDo6oUdMBi73mIjLiCS5fuoC1q/wwdMQosaMVWlpa4l3UxWcNl/L29hb0QQMDA/HkyRNERETg0qVLiIqKQmZmJmbNmgUzMzM0bdoUhoaGCAr68GyUhZ1NyZJY6bcKq/x8sMrfB461nbBx6/dqNYzmvfPnziInJwdbN2/E1s0bldaF33skUqqCsdIvAEsXeaFH129RpkxZ+PivhoOj46dvWMR4LV2JVb7LMWbYABgYGKJbzz7o3rsftLS0MHXWfOwM2oz4l3GoWKkyAgI3o7Rt0f/F71OSk5MBQC2OOONzNgJjGtvBr0tNSLJl+OVOPH6+k/tL57zjDzG6cQX0dLZF7OsMeJ18hOR36nOI5s/hUMsRPv5rsHH9WmwIXAtb2zJYvtIPTnWcxY6msj/O575/B23ZiKAtyu/fDRo2hkwmwyTP0UrLXd3qYsv2HwoyZoFq3bY9MtLTsWP7FiTGv0SVatURuOV7tZj0raOjA//V6+HjvQRDBvaBoaEhevXtj95F9ESiVDRoyQvJSTBevHgBW9vcsd+XL1+Gs7MzjI2/7ov5O2mhaFKB09HQQyHICsdTuMClS3LEjiAKIz2dT2+khroFaeahJQ8PU69hiJ9Lpplva8jKkYkdQRS6Gvr5bWrwWQNqRLHkzBPRHnu+R9E9qEhen310qfz2voMBAI0bf3jUByIiIiKigqCh/T5BFd4uJBERERERFUmFppJBRERERFQYaIGlDFWxkkFERERERIJiJ4OIiIiIiATF4VJERERERHlw4rfqWMkgIiIiIiJBsZJBRERERJQHKxmqYyWDiIiIiIgExUoGEREREVEeWlosZaiKlQwiIiIiIhIUOxlERERERCQoDpciIiIiIsqDE79Vx0oGEREREREJipUMIiIiIqI8OO9bdaxkEBERERGRoNjJICIiIiIiQXG4FBERERFRHtocL6UyVjKIiIiIiEhQrGQQEREREeXBQ9iqjpUMIiIiIiISFCsZRERERER5cEqG6ljJICIiIiIiQbGTQUREREREguJwKSIiIiKiPLTB8VKqUstOhlwudgIqSNJsmdgRRGFioJYv30/KkWnmC3z/kLpiRxCFw8yTYkcQxQPfjmJHEIWujo7YEYhIIJr5LYWIiIiI6D9w4rfqOCeDiIiIiIgExU4GEREREREJisOliIiIiIjy4Bm/VcdKBhERERERCYqVDCIiIiKiPLQ581tlrGQQEREREZGg2MkgIiIiIiJBcbgUEREREVEeHC2lOlYyiIiIiIhIUKxkEBERERHlwYnfqmMlg4iIiIiIBMVKBhERERFRHixkqI6VDCIiIiIiEhQ7GUREREREJCgOlyIiIiIiyoO/wquOf0MiIiIioiJu5MiRmDVrluL6/fv30aNHDzg5OaFbt264e/eu0vbHjx+Hh4cHnJycMG7cOLx69UrQPOxkEBERERHloaWlJdrla5w4cQIXLlxQXE9PT8fIkSPh5uaGI0eOwNnZGaNGjUJ6ejoA4Pbt25g7dy48PT2xf/9+vHnzBrNnzxbkb/ceOxlEREREREVUSkoKfHx84OjoqFh28uRJ6OvrY8aMGbC3t8fcuXNhbGyM06dPAwB2796N9u3bo0uXLqhevTp8fHxw4cIFxMbGCpaLnQwiIiIiokJCKpUiLS1N6SKVSv9z+5UrV6Jz586oXLmyYll4eDhcXV0VlREtLS24uLggLCxMsd7NzU2xfenSpWFra4vw8HDB2sFOBhERERFRHloiXjZv3gxXV1ely+bNmz+a8+rVqwgODsbYsWOVlicmJsLGxkZpmaWlJV6+fAkASEhI+L/rhcCjSxERERERFRKjRo3CkCFDlJbp6el9sJ1EIoGXlxcWLFgAAwMDpXUZGRkf3EZPT09REcnMzPy/64XATgYRERERUR7aIp7yW09P76Odin8LDAxErVq10KRJkw/W6evrf9BhkEqlis7If603NDRUIbkydjKIiIiIiIqYEydOICkpCc7OzgCg6DT8+uuv+Oabb5CUlKS0fVJSkmKIVMmSJT+63traWrB8nJPxlRLi4zFjygS0aOyOdh5NEeDrDYlEorTN27dv0c6jKX7+6YhIKfNfTHQ0Ro8Yhvpuzmjbqjl2bN8mdqR8N9lzNBbPnwMAGDNsENzr1PzgssRrrsgp80d8fDymTpqAJg3qwaNFE/iu/PB5rw4S4uMxfcoENG/kjratmsLf5592Pn/2DKOHD0HDes7o1rkjrl65LHJa4cTGRGP86OFoWt8V37RtiR92BAEAFs6fjbpONT64jBk+WNzAX6m0uQG2DXfDbe82uDS/BYY0raBY16KmDU5Ma4y7K9ri1PQm8HCw+eh9jPOoDN8+tQsoccGQSCTwmj8Hjeu7oVWzxti5Y7vYkQqc55iRmD9n1qc3VAPc3/+fmHMyPtcPP/yAX375BceOHcOxY8fQsmVLtGzZEseOHYOTkxNu3boFuVwOAJDL5QgNDYWTkxMAwMnJCSEhIYr7iouLQ1xcnGK9EFjJ+ApyuRwzpk6AmVlxbNuxG29SU7HIay60tXUwaeoMxXbrVvshMSFBxKT5SyaTwXPsSDjUcsT+w0cREx2NWdOnwMamJDp800nsePnit9MnceXyRXTs1AUAsCJgDbKzshTr7965jbkzpqB7zz4iJcw/crkc0yZPgJmZGb7/YQ/epKbCa94c6OhoY8q0mWLHE4xcLsf0Kbmv76Cdu5GamopFC+ZCW0cHk6ZMx5SJ41C5SlXs3ncIf5w7g6mTxuPwTydQurSt2NFVIpPJMMlzNGo6OGL3/sOIjYnG3FnTYG1TEtNmzIHnxCmKbeNePMfoYYPQq29/ERN/vcBBLnj+KgPf+l9GlVKmWN2/Dp6/zkBMUjo2DnHBip8f4vyDBDStZo31g13RZdVlPHjxVnH7Ts62mNSuCo6FPBexFcIL8PPB/bt3sXX7Trx48QLz58yEbWlbtG7bTuxoBeLUyRO4dPECvu38ndhRCoSm7291UKZMGaXrxsbGAAA7OztYWlrC398fy5YtQ+/evbFv3z5kZGSgffv2AIA+ffpgwIABqFOnDhwdHbFs2TI0b94c5cqVEywfOxlf4enTKNy5HY7fzl+GpaUVAGD02PFYHeCj6GTcCg3BjevXYGklXNmpsElOTkK16jUwb8FCGBubwM6uAurVb4BboSFq2clITU3BulV+qOnwz3Goixc3V/w/JycHG9etRv/Bw1DDoZYICfPX06hI3A4Pw7kLf8LSKvd5P9ZzAvz9VqpVJ+NpVO7r+/fzlxXtHDNuPFb5+6BR46Z4FhuLHT/shaGRESpVsseNa9fw09HDGD12vMjJVfMqOQlVq1XHrHleMDY2Rnm7Cqhbrz7CboWgXYdvYGJqqth20bzZaNW6LZq39BAx8dcxM9SFSwULzN5/G0+T0vE0KR0XHiaiURUr1LHLxtXHydhx6SkA4IekaHjUKomOdWzx4MUj6GhrYWFXB3SvWxYxyeniNkRg6enpOHr4INZv2ooaNR1Qo6YDIp48xr69ezTiS2dqSgpW+fvAoZbjpzdWA5q+vzWBiYkJNm/eDC8vLxw4cADVqlXDli1bYGRkBABwdnbG4sWLsXbtWqSmpqJRo0ZYsmSJoBnYyfgKVpZWWLdxq6KD8V7a2zQAuWPili6aj5lz5mPZ4gViRCwQ1tY28PVfDSD319+wW6EIDb6JOfO9xA2WT9YG+KJ9x05ISkz86PoTPx/DmzepGDhkWAEnKxiWVtbYsHmb4ov3e++f9+rCysoKgZu2frSdd26HoXqNmjD8+00aAOq4uOB2eFgBpxSelbUNvH1XAch9Pd8Ou4VbocGYOUf5PezG9au4FRqMQz+fEiOmyjKzZEiXZKNHvXJYefwhylsawa2iBfxO/oWQqNfQ033xwW1MDXI/Ko30dFDd1hTfrf4Tw5pXLOjo+eqvRw+RnZ2NOnWcFcucXVyxbcsmyGQyaGur9+hqf7+V+KZTZ7UefZCXpu/vzyHivO+vtmLFCqXrtWvXxtGjR/9z+65du6Jr1675lofPoq9gamaGho3+mckvk8lwYN8e1HOvDwDYvnUTqlWvgQYNG4sVscC1b90Sgwf0RW0nZ3i0bit2HMEF37iGsNBgDB055qPr5XI5dn2/Db37DYSRkXEBpysYZmZmaNRY+Xm/78fdcK9fX8RUwvvY63v/3tzXd1JiIqw/OK64FRLi4ws6Zr76tn0rDB/cD46166ClRxuldTu3b8U333ZBqVKlRUqnGmm2DAsO30OfhuXxwKcdzs5pjj8eJOLA9VhEJKQpDYuqUsoEDatY4s/HuZMj32Zmo8faq3gY9/a/7r7ISkpMhLm5BYrlOaKNpaUVJBIJUlJSxAtWAK5fu4rQ4GCMHD320xurCU3e31RwROtkHDhwAHPn5k6Olcvl2LFjB9q1a4c6deqgY8eO2LNnj1jRvtiaAF88fHAfY8dPQmTEExw+uB9TZ8wWO1aB8l+9FmvXb8KjRw/gu9Jb7DiCkkgk8F6yENNnz//gONTvhQTfQEJCPDp37V7A6cSzyt8XDx7ch+fEyWJHyVfvX9/jJkz6+7jixZTWFxP4uOKFwUr/tQhYuwF/PXqIVb7//DL27Fksgm9cR88+RXMuxnuVS5rg7L14dF1zBdN+DEd7p9Lo7KI8p8bCuBg2DnZFSNRr/H5XvTqRH5OR+fFj6gNAlpo9v/OSSCRYusgLs+d9eJ4Bdaap+/tLaGlpiXZRF6IMl1q1ahUOHDiAoUOHAgA2btyIH374AaNHj0bFihURERGB9evX482bNxgz5uO/HBcWa1f5Ye+eXfD2CYB95SoYNqgvRo8b/8FQKnX3fhyrVCLB7JnTMHXaDKVfSIqybZvXo4ZDLdT/P5Wpc7//hgaNmijN0VBnq/x9seeHnfDxW4UqVaqKHSffrAnww4+7d2GFbwAqV6kKPT19pKZmKG2Tlee44+qi5t9ziqRSKebPno6JU6ejWDE9nD/zG6pWq45K9pVFTvj1GlaxRK/65dBg0VlIsmS4E5uKUsUN4NmmCn4KzR0qZWWihx/GuENbCxi7IxR/H5xFrf3XMfMBqN3zO69NGwJR06GWUpVWE2jq/qaCJUon4/Dhw1i1ahXq/z3M4siRI1iyZAk8PHInETZt2hSVK1fG7NmzC3Unw8d7CQ4d2Icly33QqnVbxL14jvCwW/jr0SOs8vMBAGRmZsB7yUL8fvoU1m3cKnJiYSUnJSE8PAwtW/0z+bOSfWVkZWUh7V0aLPRKiJhOOL+fPoVXyUlo3sAVACD9+2hS5878ij+u5h7+7dqVyxg+epxoGQuS97IlOLh/L5at8IVHG/UbGvfeyuW5r++l3rmvbwCwKWmDyIjHStslJSXCSsDjioslOTkJd8LDlCZzV6xkj6ysLLxLewdzCz1c+fMymrVoJWJK1TmWK46nie8gyZIplt17nopxrXM7TiWL6+PHsbmfTb3XX8Ord5rxq66NTUmkpLxGdnY2dHVzvxokJSXCwMAApmZmIqfLP6dPnUByUhLqu+XOTcjKyt3fv//2K64F3xIzWr7S1P1NBUuUToZUKoWJiYnierFixT44+Ye1tTUyMjL+fdNCY8vGQBw6uB/LV/rDo03ukRisbUri2PFflbYbOXQgevcdgPYd1e9oS8+fP8OUiZ749ewFlCxZEgBw//5dWJQoAQsL9ehgAMDGbTuQnZ2tuB64JgAAFIf0THn9Gs+fxcIpzwQ6dbVpQyAOHdiHlb4Ban0Eks0bA3H44H54+/zz+gYAx9p1sCNoKzIzMxW/9oXdCkUdZxexogrmxfNnmDFlAo7/eh42f7+eH96/BwuLEjC3sIBcLseDe3cwdPgokZOqJj5VAjsrYxTT0UJWTm6Jwt7GBLGv0mGop4MdI+tBJgf6rL+GpLfqdw6Y/1Kteg3o6uridngYXFzdAOQeJdGhlqNaTwIO2vEDsrP+eX9fHeAHAJg0ZZpYkQqEpu7vL8G/gupE+Rt27NgR06ZNQ3BwMABg1KhRWLlyJV6+fAkAiI6OxqJFi9C6dWsx4n1SVGQEtm3ZiMFDR6COiyuSkhKRlJSIlJTXKFfeTumio6sDC8sSig9tdeJQyxE1azrAa94cRDx5gksXL2CVny9GjBwtdjRBlbYto7RPjY2MYWxkjHLl7QAAEU8eQ19fH7ZlyoqcNH9FRkRgy6YNGDJsBJxdXJGUmKi4qJPIyAhs2/zh6zspKRGubnVRslRpLJw/BxFPHuP7bVtw785tdFGDuTg1HRxRvaYDlnjNRWTEE/x56QLWrPLDkBG5nYq4Fy/w7t07VLS3Fzmpas7ei0e2TIYVvWqjorUxWjnYYKxHZey4+BTjPCrDzsoY034MAwBYmerDylRfcXQpdWZoaIhOnbtg6eKFuHvnNs6dPYNdO7ajb/+BYkfLV7a2ZVDezk5xMTY2/vsQznZiR8tXmrq/qWCJ8s45e/ZsLF26FIMHD4apqSnKlCmDp0+fokWLFtDX14dEIkGzZs0wb948MeJ90h/nzyInJwdBWzYiaMtGpXUhtx+KlKrg6ejoYHXgBngvW4KB/XrB0NAQffsP0Lg3qVevkmFiaqpWk7U+5vy53Of91s0bsXWz8vM+/N4jkVIJ78Lf7dy2ZSO2/ev1HXrnIQLWrsfiBXPRr1c3lCtvB//VgUX+RHxA7uvZf/V6+HgvwdCBfWBoaIjeffujd98BAHLPowEAZmbFxYypsreZ2ei34Tq8vquJY5Mb4VWaFIG/P8beqzE4M6sZDPV0cGyy8vyrQzdiMX3vbZESF5xpM2bjf+3dd1wT9x8G8IcNgoDMAio4kSUiOKgbZ91Wq6h1a91UKw5woagoqKh1r7oXztb+nHVbShVFxM0URSlYoaKQAMnvD2tKqnbIJVfC8/aV18vcXchzOS7kc5/v5ebPDcHwIYNgUtEEo8eOR5u27f72cVQ2cXv/NU3/m64OWnK5eKe05ebmIjY2Funp6Xj16hV0dHRgY2MDT09PVKv24d9BnicpB2fpvYOuTvncIQoKi8WOIApDPR2xI4iiWFY+9+/yut6eQWXzehyldSeik9gRiFTuv9wk3Bf39jVz1KV3vbJ/4AoQ+WJ8ZmZm8PPzEzMCEREREZGS8nnYVlg8r4WIiIiIiATFIoOIiIiIiAT1Hx4NR0RERESkfjzxu/TYySAiIiIiIkGxk0FEREREVAKPwpceX0MiIiIiIhIUiwwiIiIiIhIUh0sREREREZXAE79Lj50MIiIiIiISFDsZREREREQlsI9ReuxkEBERERGRoNjJICIiIiIqgadklB47GUREREREJCgWGUREREREJCgOlyIiIiIiKkGbp36XGjsZREREREQkKHYyiIiIiIhK4InfpcdOBhERERERCYpFBhERERERCYrDpYiIiIiIStDiid+lxk4GEREREREJip0MIiIiIqISeOJ36bGTQUREREREgmIng4iIiIioBF6Mr/Q0ssjQ1SmfvxiFRTKxI4hCT4cNufJEu5z2sHV0y+d634noJHYEUVQevkfsCKJI3+AvdgRRvJQUiR1BFIYmGvkxlH7HT2dERERERCQolpBERERERCWU06a5oNjJICIiIiIiQbGTQURERERUAjsZpcdOBhERERERCYpFBhERERERCYrDpYiIiIiIStDidTJKjZ0MIiIiIiISFDsZREREREQlaLORUWrsZBARERERkaDYySAiIiIiKoHnZJQeOxlERERERCQoFhlERERERCQoDpciIiIiIiqBV/wuPXYyiIiIiIhIUOxkEBERERGVwBO/S4+dDCIiIiIiEhSLDCIiIiIiEhSHSxERERERlcArfpceOxlERERERCQodjKIiIiIiErgid+lx04GEREREREJikUGEREREREJikWGCowb/QVmBk8TO4ZKpD9Mw7hRw9GssTc6tffDti2bFPMeP3qEMV8MQdNG9fFZj8746cfLIiYV1i+ZmZj8VQBaNmmE9q2bY0l4GCQSidIyDx+mwdfHU6SE6pGZmYlJEwLQzLch2rRqhohFb78OmujM6VOo5+6sdAucGCB2LJWTSCSYPTMYTRv7oHWLpti6ZbPYkdRCU9fb3qICdk1ohpQ1PXFtcReMbFdbMa+l20c4N7c9Utf2xIHJLVHzo4rv/BldG1RB9hZ/dUVWuSOHD761b9dzd4aXRx2xowlOKpViycJQdGjpi85tm2PtymWQy+UAgKlfjUMTbzel2+UL58QNLDItLfFumoLnZAjs2P++x8UL59G1Ww+xowhOJpPhy3Gj4ObmgZ17D+DhwzRMnxYIGxtbtP+kEwInjkPNmrWxfXcUzp39AYETx2P/4aP4yM5e7OilIpfLMfmrAJiammHT1h3Izc3FnFnToa2jg4mTpgAAnj59ggljR2n0B265XI7AiQEwNTXFN9t34rfcXMyeEQwdHW18FThV7HgqlZSUiBYtW2FmSKhimr6+gYiJ1GPp4nDcTkjAhs1bkZGRgZnBU2FvZ4+27TuIHU2lNHW9N435GI+evUTrkBNwtjfD2lG+SM9+iaSnL7B7YnMs//429kenoX/z6jg0tRUaT/sfXkqKFI83raCHsP71RVwD4bXv0BFNmjZT3C8qLMKIYYPQvEVL8UKpyLLFYbh2JQZLV67Dq1evMDsoEB/Z2aN7z95ITU7CrNBF8GnYSLF8RVMzEdOSJmCRIaDcnBxELgmHm7uH2FFU4tdn2XB2roNpM2bD2NgYVR2d0LBhY8Rdj4WlpRUepadj89ZdMKpQAdWq18CVmGgcOXwQI0ePEzt6qaSmpOBm/A2cOnsJllZWAIDRY8cjckk4Jk6agrM/nMa8ubNgZWUtclLVSk1JRvyNOJw5f1nxOowZF4AlixdpfJGRkpyEGjVra/w2LunVq1c4dCAKq9ZugIurG1xc3ZCU+AB7du8s8x+2/4qmrrdZBT00qGmFid9cQXJmHpIz83Dm5hM0d7VFc1dbXEnMxsJDCQCAOftuoJ2nPXr5OmLruSTFz5jTpx5SfsmDrbmRWKshOENDQxgaGirub9qwDpDL8eXEQBFTCe+33BwcPXwQy9dshKt7XQCA/+eDcDshHh27dMeTjMdwcXOHZTl6j/s7GtRQEA2HSwloyeJF6NylG2rUqCl2FJWwsrZBWEQkjI2NIZfLEXf9Gq5duwpvn4a4efMG6ri4wKhCBcXynl7euHkjTrzAArGyssLKtRsUH6zfyHuRBwC4dPE8Ro8NwORpwWLEUxtLK2usXrfxva+DJktOToKjk5PYMdTq/r27KCoqQr16XoppXvW9cTP+BmQymYjJVEtT17ugsBgvJUXo16wadHW0UPOjimhYywo3H+bA0doEscnPlJa/8ygXPjX/2Nc/drZGE2cbRH53W93R1SY3NwffbN6AgImToK+vL3YcQd2IuwYTExN4eTdQTBswZASCZ8/Dw7QUAFqwd6gsXkDSSCwyBBLzUzSuXb2KL0aNETuKWnT5pDWGD+6PunXrwa9NO2RnZcHK2kZpGUtLS/yS+VSkhMKpaGqKj5v80U6XyWTYu3snGjZqDACYGRKKXr01Z4zy+5iamioNK5DJZNizawcaNW4sYirVk8vlSE1NQfTlS+jaqT06d2iD5ZGLUVgoFTuaSmVnZcHcvBL0SnzYsrS0gkQiQU5OjnjBVExT11tSKMPU7bEY2LIGHq3/DD8t7IQf4p9g54VkZP1WALtKyt0Je4sKsDR5/Rro62pj6eAGmLI9FgWFxWLEV4t9e3bDxtoGbduV3Y7V+2Q8foSP7B1w7OgR9P20Mz7r2h7fbFgDmUyG1JRkmJiYIHTWNHRt1wLDB/ZB9OWLYkcWnbaWlmg3TSFKkeHq6opFixahsLBQjKcXnEQiwbw5sxE0Y5ZS21WThS9ZgcgVq3H/3l0sjViIgoL8t4786OnpQ6oh27ik5UsjcPfObYwNmCB2FFFFLonAnTu3Me7LiWJHUaknTzJQkJ8PPX19hC9Zhq8Cp+J/R7/D0sXhYkdTqfx37NNv7hdKNbfA0uT1rm1nipNxGegQegrjNsagq08V9PJ1xOGfH6Jrgypo52kPHW0t9GniBK9qFtDT1QEATOrqhvi05zh3q+wfNHofuVyOQwej4N/vc7GjqET+q1d49DANRw7uw/SQeRg7IRD79+zE3p3b8DA1BQUFBWjYuAmWrFwH3ybNMXXiWNy5nSB2bCrjRDknQyaT4cyZMzhz5gwCAwPRtm1bMWIIZu3qlXB1c1c6yqvpXN3cAQASqRQzgyaja/dPkZ+fr7RMYaFU44qu5UsXY9eObVgYsRQ1a9X++wdoqMglEdi5fSvCF0eiloa/Dvb2Djh/OQampmbQ0tJCnToukMllmD5tMgKnBEFHR0fsiCphYGAA6Z8+VL+5r2n7dUmaut7NXGzxeYvqqDvxWxQUFiMu9TnsKhnhqy6u+Dj4GCKO3MI345pAV0cLl+78gr0/psLUSA91HMwwsGUNNJ9xTOxVUKlbCTfxS2YmOnzSSewoKqGjo4OXL/MQMj9C8WUsmU+f4FDUHuw6cBS9/PvD9PcTvWvVroN7d27h24NRcHF1FzM2lXGiFBlaWlrYunUrjhw5guDgYCxfvhwDBgxAx44dUbHiu78277/s+LHv8Sw7G419Xo/hfTOM4tTJE/jp6nUxownq2bNs3LwRh5Z+bRTTqlevgcLCQlhZWSMlJVl5+exsjTpRdtGCUOzftwfzwsLRum17seOIJmx+KKL27sb8hRFo0658vA5mZuZK96tVrwGJRILc3FxYWFiIE0rFbGxskZPzHEVFRdDVff2nIjs7C4aGhqhoaipyOtXR1PWu51QJyZkvlIY73Ux7jomdXQEAkd/dxqpjd2FqpIfsFxJsHPMx0rNfootPZVQy1seV8M4AAB3t10M5Utf2RODWq9gfnab+lVGBHy9fRH1vH5iaaeY3KllaWUPfwEDp2x6rOlZDZuZTaGtrKwqMNxyrVUdKUtKff0y5ojmDlsQjSpEhl8uhp6eHkSNHwt/fH7t27cL69esRGhqKBg0aoH79+qhRowbMzMzQpEkTMSL+K5u2bEdR4R9f87ds6WIAwISvNOvbKTIeP8LkrwLw/YmzsLG1BQDcuX0LlSpZoJ6XN3Zs+wYFBQWKo31x16+hnpdmfN3hujUrcSBqL8LCl6CNBo7X/afWrl6J/fv2YFHE0jL9TTv/xo+XLyJoSiCOnz4HI6PX49bv3b0Dc3NzjS0wAMC5jgt0dXURfyMO9b19AADXr8XCzd0D2tqaezqfpq7305x8VLOpCD0dbRQWvz6BvZadKR5mv8Snjaqifg1LzNh1HdkvJDDU00HTOjYYvykGVxKfKRUS3jUssXakL1rNOoGs3wrEWh3B3YyP15i/V+/i5uEJqUSCh2mpqOroBABIS0mCnZ0D5s0Ohra2NoJnz1Ms/+DePdSoWUuktKQpRHnH1CpxUouZmRlGjx6NH374Abt27UKDBg1w69YtLF68GGPHjhUj3r9mb++Aqo6OipuxsfHvX/HqKHY0Qbm6ecDF1Q1zZ09HclIiLl08jxWRizF0xEjU92kAW9uPMGdWMJISH2DLpg24lRCPbj16ih271JKTk7Bx3RoMHjoC9ep7Izs7S3ErT5KTkrB+7WoMGTYCXvW9kZ2VpbhpMs96XjAwNMCc2TOQmpKMSxfPI3JJOAYNHS52NJUyMjJCl27dMW9uCBJuxuPMD6exbctm9Pt8oNjRVEpT1/t4XAaKimVYNrQBathWRPt69pjQ2RUbTt1H0tMXGNyqJjp5V0Z1WxOsG+WLjF9f4XT8E+S8lCLllzzF7cnz18NiU37JQ15B0d88a9mRmPgA1atr5jdDAoCjUzV83LQF5odMx4P7dxHz4yVs37IJPXr1QdMWrXDif9/h2NEjeJSehs3rVyM+7hp6+fcXO7a4tES8aQjROhnvUrduXdStW1fNaeif0tHRwZJlqxAeFoohA/vCyMgIffp9Dv9+A6ClpYUly1chNGQGBvTthcpVqiIi8usyfyE+ADh/5gcUFxdj4/o12Lh+jdK8azfvipRK/c7+/jpsWLcGG9Ypvw43bt0TKZXqGRubYPW6TYhYuAD9+vSEsbExen7mj8FDNLvIAIDAKUGYPzcEw4cMgklFE4weOx5t2rYTO5bKaeJ6v8gvRI/ws1jQrz5OzW6L7BcSLP3uluI6GJO3XsVc/3qwMDHAhduZ6Bt5Ae/5U62Rfn2WDVOzsjsc7p+YPW8RIiMWYPSwATA0NELP3n3Ry78/tLS0MGnaTGzdtA6ZT5+gWvWaWLpyHezsHcSOTGWclvx9n/hV6NChQ+jUqZPKvodagw6u/CuFRWX3O9xLQ1tbg8r+f0GnnK53efrgU5IGfash/QOVh+8RO4Io0jdo/teBv0vJK6uXJ1Ym/91rQv+UlCPaczeuYS7acwtJlK3bo0cPMZ6WiIiIiOhvaWnSuCWRlN2z2IiIiIiI6D/pv9unIiIiIiISAYeolh47GUREREREJCh2MoiIiIiISmAjo/TYySAiIiIiIkGxyCAiIiIiIkFxuBQRERERUUkcL1Vq7GQQEREREZGg2MkgIiIiIiqBF+MrPXYyiIiIiIhIUCwyiIiIiIhIUBwuRURERERUAq/4XXrsZBARERERkaDYySAiIiIiKoGNjNJjJ4OIiIiIiATFTgYRERERUUlsZZQaOxlERERERCQoFhlERERERCQoDpciIiIiIiqBV/wuPXYyiIiIiIhIUOxkEBERERGVwIvxlR47GUREREREJCgWGUREREREJCgOlyIiIiIiKoGjpUqPnQwiIiIiIhKUllwul4sdQmivpBq3Sv9MeS27y+nmlpXT9dYup4dGpEUysSOIQk+nfG5w7XJ61mmj0NNiRxDF+WmtxI4gikoVdMSO8F430l+I9tyeVSqK9txCKp/v3kREREREpDI8J4OIiIiIqARejK/02MkgIiIiIiJBscggIiIiIiJBscggIiIiIipBS0u827+RmZmJgIAANGzYEM2aNUNYWBgkEgkAID09HYMHD0a9evXQsWNHXLp0SemxP/74Izp37gxPT08MHDgQ6enpQr18AFhkEBERERGVOXK5HAEBAcjPz8fOnTsRGRmJs2fPYtmyZZDL5Rg7diysrKxw4MABdOvWDePGjUNGRgYAICMjA2PHjsWnn36K/fv3w8LCAmPGjIGQXzrLE7+JiIiIiEooC6d9JycnIy4uDpcvX4aVlRUAICAgAIsWLULz5s2Rnp6OPXv2oEKFCqhRowaio6Nx4MABjB8/HlFRUXB3d8fQoUMBAGFhYWjSpAl+/vlnNGrUSJB87GQQEREREZUx1tbW2Lhxo6LAeCMvLw83btyAq6srKlSooJju7e2NuLg4AMCNGzfg4+OjmGdkZAQ3NzfFfCGwk0FERERE9B8hlUohlUqVpunr60NfX19pmqmpKZo1a6a4L5PJsGPHDjRu3BhZWVmwsbFRWt7S0hJPnz4FgL+dLwR2MoiIiIiIStIS77Zu3Tp4e3sr3datW/e3kSMiInD79m1MnDgR+fn5bxUl+vr6iuLl7+YLgZ0MIiIiIqL/iJEjR2LIkCFK0/5cEPxZREQEtm7disjISNSuXRsGBgbIyclRWkYqlcLQ0BAAYGBg8FZBIZVKYWpqWvoV+B2LDCIiIiKiEsS84ve7hkb9ldDQUOzevRsRERFo3749AMDW1haJiYlKy2VnZyuGSNna2iI7O/ut+S4uLqVM/wcOlyIiIiIiKoNWrlyJPXv2YOnSpejUqZNiuqenJ27duoWCggLFtNjYWHh6eirmx8bGKubl5+fj9u3bivlCYJFBRERERFRCWbgYX1JSElavXo0RI0bA29sbWVlZilvDhg1hZ2eHoKAgPHjwAOvXr0d8fDx69eoFAOjZsyeuXbuG9evX48GDBwgKCkLlypUF+/pagEUGEREREVGZ88MPP6C4uBhr1qxB06ZNlW46OjpYvXo1srKy8Omnn+Lbb7/FqlWrYG9vDwCoXLkyvv76axw4cAC9evVCTk4OVq1aBa1/e8nxv6AlF/LSfv8Rr6Qat0r/TFm4cowqlNPNLSun661dTg+NSItkYkcQhZ5O+dzg2gL+oS9LGoWeFjuCKM5PayV2BFFUqqAjdoT3up3xUrTndrU3Fu25hcQTv4mIiIiISiifZb6wyuchIiIiIiIiUhl2MoiIiIiISmIro9TYySAiIiIiIkGxyCAiIiIiIkFxuBQRERERUQliXvFbU7CTQUREREREgmKRUUpSqRS9enTB1Ssxb8178eIF2rVujm8PHxQhmWpJpVL06t4FV3/+Y73jb8RhUH9/fNygPrp37oCD+6NETKga5W17/5KZiSlfBaBV00bo0KY5lkaEQSKRKC3z4sULdGjTHN8e0Zz1Luldv+sA8PBhGhp7e4qUSvUmjhuFuTODFfcDJ4xFo3quSrdLF86JF1AFpFIpPvvT/n37VgIG9e+DJg3rY2D/Poi/ESdeQBV7+uQJxo8ZiSaN6uOTdn7YsX2L2JFKrWs9O8TPbfPWLS6kNTYN8X7nvDndXQEAejpa+KpdTZya1BSXglog0r8ubE0NRF6jD3fuzGk09nJVugUFTgAATJ4w9q15mrZ//1tl4Yrf/3UcLlUKEokEwVMDkZT44J3zl0cuRtYvv6g5lepJJBIET1Fe7+zsLIwb/QU+6+2PuQsW4s6tWwiZGQxra2s0a9FSvLACKm/bWy6XY8qkAJiammHjlh34LTcXc2ZPh7a2DiZMmqJY7utlmrXeJb3rdx14/WHsy7Gj3iq4NMXJ4//Dj5cuoFOX7oppKUlJmDN/ERo0aqyYVtHUTIR0qvGu/fvXZ88wasQQtG3XASGhYbh86QLGfDEUUYePws7OXsS0qjElcALs7Oyxa99BJCclImhqIOztHODXpq3Y0T7YiYRMXE58privq62FjUO8ceFeNtadT1a62KNHZTMs7u2BvT+nAwDG+NWAn4sNph1IwPOXhZjYriaW+tdF//VX1L4eQkhJTkTT5q0QNDNEMU3fwOD3eUkImb8IDRpq5v5N4mCR8YGSkhIRPDUQ77tg+vVrsfg55idYWVmrOZlqJSUlInjK2+t99ocfYGVphfETvgIAODo64eqVGBz731GNKDLK4/ZOTU3BzfgbOHn2EiwtrQAAo8aMx7Kl4Yoi4816W2rQer/x/t/10widM0ujtnVJubk5+DpyMVzdPBTTpFIpnmQ8hqu7h0Zu6+T37N9HvzsMczNzBM8MgY6ODqpVr46ffryM/Xt3Y/yESSKlVY3fcnMRfyMOs0JC4ejoBEdHJzRp0gwxMdFlusiQFMkgyZMq7g9r5gQtAMtOPUBh8R/bW1sLCGhTA99cSsXtjBcAgG717LDo2H3EpuYAAOYcuYMzU5qjqoURHv6ar87VEERqSjJq1Kz51j6s2L/dNHP//lAa1FAQDYdLfaDYq1fQoEEjbN2x5615UqkUoSEzETR9JvT09URIpzqxV66gQcNG2LpTeb2bNG2KkHkL3lo+78ULdUVTqfK4va0srfD1mg2KAuONvBd5AF6v97w5MzE1eCb0NWi933jf7/rFC+cxZlwApkwLfs8jy7YVSyPwSacuqFa9hmJaWmoKoKUFe4fKIiZTndirV+DToBG2/Gn/fvzoEVxc3aCjo6OYVqu2s0YOmTIwNIShkRGOHD6IwsJCpKYkI+76NdSp4yJ2NMGYGuliSFNHLD+VqFRgAEA3L3uYGelh86U0AK+HrAQduIXopGdv/RwTw7J5fDYlOQlVHJ3emq7p+zeJp2zuKf8Bvfv0fe+8TRvWwrmOC3w/bqrGROrR2//d623vUFnpDerXZ89w4tj/MHLMOHVFU6nyuL0rmpri4ybNFPdlMhn27dmJhr8Pl9msoev9xvt+12fNCQWAt87R0ARXf/4JcdeuYmfUEYTPn6uYnpqSDBMTE4TMmIZrV3+Gra0dRowei4+bNhcxrXA+e8/+bWFpifv37ipNy3z6BDk5OWpIpV4GBgYImj4LC+eHYteObSguLkbX7p+iR8/PxI4mmD4NKiPrhQSnbr89vHNIU0fsiH6IfGkxAEAuB2KSf1Vapr9vVfz6Uor7mXlqySskuVyOh6mpiPnxMrZuWg+ZTAa/Nu3xxZhxiv17zoxpuBb7ev8ePkpz9m8Sj2idjNOnT2PevHk4ePD1yaJHjx5Fp06d4OXlhS5duiAqqmyeNJyUlIj9+/YicEqQ2FFEU1BQgMCJAbC0skLPz/qIHUelytP2Xr40Anfv3MaY8ROQnJSIA1F7MakcrHd5IZFIEBYagslBM2FoaKg0Ly0lGQUFBWjs2wTLVq3Hx02bIfDLsbhzK0GktOrRuk07JNyMx8H9+1BUVIQfL1/EuXNnUFgo/fsHl0EpyUlo0bIVtu3ciznzwnD65HF8f/RbsWMJ5lNvB+yOSX9reoNqlWBraogDsY/f+9iWdawx6OOqWHEqEUXF7x42+1/29EkGCgryoaevj/nhSzF+4mScOHYUX0cuRlrq6/270cdNsGzlevg2bYbJEzR///5bWiLeNIQonYytW7di2bJlaNasGY4fP46rV6/ixIkTGDFiBFxcXJCcnIwlS5agoKAAAwYMECPiB5HL5QgNmYnRY8fD0srq7x+ggV69eomJ48ciLTUVm7fvhJGRkdiRVKY8be8VkYuxe+c2hIUvRY2atTBsUD+MGjv+raFUVHZtXLcKLm7uaPyOztTQL0ajd7/PYfr7iaC1nevg7p3bOHQgCi5u7uqOqjY1a9XGjNlzEbFwPhaEhqC2cx181qevRnaxYn6KxqED+3Hih/MwNDSEm7sHfsnMxMZ1a9Cpc1ex45Wam70pbEwNcOxm5lvz2rra4PKDZ/gtv+idj21VxxoRvT2wO+YRDl7LUHVUlbCzd8CJcz/C1NQMWlpaqO3sAplMhjkzpuLM5avo3feP/bvW7/v34YOavX+T6olSZGzbtg2LFy9G69atkZycjI4dO2LhwoXo3r07AKBFixZwdHTEokWLylSR8eRJBm7EXcf9e/ewdHE4AKCgIB/zQ0Nw4vgxrFq7QeSEqpWXl4dxo0Yg/eFDrN+8BY7vGPupScrL9g4PC8X+fXsQuiAcrdu2x5OMx4r1jiyx3mGhITh1/Bi+XqMZ613enDp+DL8+y0ZLX28AgLSwEABw5vQJnIuOVXwAecOpWnUkJyWqPae6devRE527dsevvz6DtbUNli2NgL2Dg9ixBHf7dgKqOjoqdbHquLhi04a1IqYSTpNalriWloMXBW8XEk1qWWLN2eR3Pq6Duy3m93RD1JXHiDh+X9UxVcrMzFzpfrVq1SGRSPBbbi4qWVgozXOqVh0p5WD//iu8GF/piVJk5OTkoFatWgCAqlWrQkdHB7Vr11Zapnr16vj111/f9fD/LBsbWxz5/oTStBFDBqJv/wHo2KmLSKnUQyaTYdKE8Xj86BE2btmOatWrix1J5crD9l6/ZiX2R+3FgkVL0KZdBwCAtY0tDh9VXu8vhg6Ef78B+ERD1rs8WrNxC4qK/vgAtnL5UgDAuC+/wtyZwdDS1sLMOfMV8+/fu4uav7+Pa6orP/+EA1H7sDBiKaytbSCXy/HjxQvo2dtf7GiCs7G2QfrDNBQWSqGnpw/g9bk4mnIysEdlU1x/mPPWdPMKeqhiUQHXH+a+Na9R9UqY39MNe2IelfkC46cfL2FW8GR8e+wMDH8fYXD//l2YmZvj62WLoa2thRkhf+zfD+7dRY2amr1/k+qJUmQ0aNAAy5cvx+jRo3HgwAHo6+tj06ZNCAsLg76+PoqKirB27VrUrVtXjHgfTFdXF1WrOipN09HVgYWFBWxsbUVKpR6HD+7H1Z9jsOzr1ahoWhHZ2VkAAD09vbeOnmgKTd/eKclJ2Lh+DQYP+wL16nsrtikAVHnHeley1Iz1Lq/s7JWPzhtXMAbwels3a9kKM6ZOgrdPQ3h41sOJY9/jRtw1BM2cI0ZUtXF0rIYL588iau9u+H7cFNu3bsZvv/2GLt26ix1NcM1b+iFySQTmzJqBESNHIzU1BZs2rMW4gIliRxNETRsTfH/j6TunFxQW4/Fz5a+k1dHWwpzurohNfY7Nl1JhaaKvmJebX1jmzsvw8PSCgYEhFsydhWEjx+Dxo3SsjFyMzwcNQ+UqVTFz2iTU9369f5/8ff+epuH7N6meKEVGSEgIvvzyS3Tu3BlGRkaYNWsWkpKS0Lx5czg5OSEtLQ26urrYsmWLGPHoA/xw6iRkMhkCxo5Smu7t0wAbt2wXKRWVxrmzP6C4uBib1q/BpvVrlObFxt99z6NIE7Vq3RZTgmdh84a1yHz6BNVq1MTyVes1cthQSTa2tli0OBKRi8MRuSQcHnU9sWbDN6jwewGmSSpWrIh1m7YgPGw++vv3QqVKFhg+crTGfHmHpYk+fisofOf0dw2hcrOvCHtzI9ibG+HsFOVvWRq6ORZXU5+rLKsqGBsbY/nq9YiMWIgh/T9DhQrG6N6rNz4fNBRaWlqYHDQL32z8Y/9etnI97O01e//+O5p05W2xaMnfd3UxNfjtt99gaGgIff3XRwiio6Nx69Yt2NjYwM/PDyYmJh/0c19Jy9YRBsGU1x2inG5uWTldb+1yenUfaZFM7AiiKHlF5vJEu5x+wmkUelrsCKI4P62V2BFEUamCzt8vJJLEX8S74GJNG8340hxRr5NhamqqdN/X1xe+vr4ipSEiIiIiKr/HbYVUPg8RERERERGRyrDIICIiIiIiQYk6XIqIiIiI6D+H46VKjZ0MIiIiIiISFDsZREREREQl8IrfpcdOBhERERERCYqdDCIiIiKiEsrppWoExU4GEREREREJikUGEREREREJisOliIiIiIhK4Gip0mMng4iIiIiIBMVOBhERERFRSWxllBo7GUREREREJCgWGUREREREJCgOlyIiIiIiKoFX/C49djKIiIiIiEhQ7GQQEREREZXAK36XHjsZREREREQkKHYyiIiIiIhKYCOj9NjJICIiIiIiQbHIICIiIiIiQXG4FBERERFRCTzxu/TYySAiIiIiIkGxk0FEREREpIStjNLSksvlcrFDCO1Vocat0j+iXU57ezLN+xX+Z8rpaucXFosdQRQV9MvnMaFy+rYGSaFM7Aii0NctnwMsak84InYEUaSv6iZ2hPd69Fwq2nNXrqQv2nMLqXzuzUREREREpDLl89AYEREREdF7lNcuqpDYySAiIiIiIkGxk0FEREREVAIbGaXHTgYREREREQmKnQwiIiIiohJ4TkbpsZNBRERERESCYpFBRERERESC4nApIiIiIqIStHjqd6mxk0FERERERIJiJ4OIiIiIqCQ2MkqNnQwiIiIiIhIUiwwiIiIiIhIUh0sREREREZXA0VKlx04GEREREREJip0MIiIiIqISeMXv0mMng4iIiIiIBMVOBhERERFRCbwYX+mxk0FERERERIJikUFERERERILicCkiIiIiopI4WqrU2MkgIiIiIiJBsZNBRERERFQCGxmlx04GEREREREJikWGAKRSKXp174KrP8coTX/4MA2NvT1FSqV+40Z/gZnB08SOoVLv2taPHz3CyOFD4NvAC5927YToy5dETKgaUqkUvXp0wdUrf6x3+ML58PKoo3Tbs2uHiCmFI5VKsTgsFO1b+KJTm+ZY+/UyyOVypWWeZDxG6yY+uHb1Z5FSqt6Z06dQz91Z6RY4MUDsWGojlUrxabfOuPKn93ZNNHHcSMyZGaS4f+nCOfTv3QMtfL3R77NuuHDujIjpVOvpkycYP2YkmjSqj0/a+WHH9i1iRxKEnbkhvhnVCLcXd8SPc9tiWKvqby1T2cIId5d2QuNalopputpaCOrmiqvz2yNuYQfM6OEGHW0e16d/j8OlSkkikSB4SiCSEh8oTX/65Am+HDsKEolEpGTqdex/3+PihfPo2q2H2FFU5l3bWi6XY2LAWNSqVRs79+zH2TOn8dWE8Tj47fews7MXMa1wJBIJgqe+/TuenJSE8V9+ha7d/9jmxsYm6o6nEssiwhB7JQaRq9bh1ctXmBUUiI/s7NG9V2/FMhEL5iI/P1/ElKqXlJSIFi1bYWZIqGKavr6BiInURyKRYNqUSW/93muik8e/x+VLF9CpS3cAwIP79zB1UgACJk7Gx02b46cfL2Fa4ARs2bkPtZ3riBtWBaYEToCdnT127TuI5KREBE0NhL2dA/zatBU7WqmsGdYAj399hY6LzqO2XUV8Pdgbj3/Nx/EbTxTLLPD3hLGB8kfBSZ3roFfjKpi0/TqyX0gQ0b8eZvV0x+yom+peBVHxit+lx05GKSQlJWJgvz5IT3+oNP3sD6fRr09P6Onpi5RMvXJzchC5JBxu7h5iR1GZ923rKz/H4FF6OmbMnoPqNWpg2IiRqOtZD0cOHhApqbCSkhIxsP/b6w0AKclJcHF1hZWVteJmZGQkQkph/Zabg++OHMS0mXPg6l4XPo0aw3/AINxKiFcsc+J/R/Hq1UsRU6pHSnISatSsrbSNTU1NxY6lckmJiRjQtzcePXz7917T5ObmYEXkYri6/fH+feLYUfg0bIw+/QagSlVHfObfH94NGuL0yeMiJlWN33JzEX8jDiNGjoajoxNa+bVBkybNEBMTLXa0UjEz0oN3dQusOH4fqVkvcTL+Kc7d/gVNnK0Uy3RvUBnGhm8fax7UvBoWHbmDc7d/QUJ6LoL2xOPzpk6oYKCjzlUgDSB6kVFcXIycnBxkZWWVuaOCsVeuoEHDRti6c4/S9IsXzmPMuABMmRYsUjL1WrJ4ETp36YYaNWqKHUVl3retb96IQx1XVxhVqKCY5uVVH/E34tScUDVir15BgwaNsHWH8nrn5eXhl18y4ejoJE4wFboRdw0mJibw8m6gmDZwyAhMD5kH4HVRvWr5EkyZHiJSQvVJTk6Co5OT2DHULvbqz2jQsBG27dordhSVW7E0Ah07dUW16jUU0zp16Y6xAV+9tezLvBfqjKYWBoaGMDQywpHDB1FYWIjUlGTEXb+GOnVcxI5WKgWFxXglKUJv36rQ1dZCdRsT+NSwQEJ6LgDA3FgP07u7Imj3DaXHWZroo6KRHq6nPldMu/s4F/q62qhb1VydqyA6LRH/aQrRhkudPn0aGzduREJCAoqLixXTK1WqhIYNG2LEiBFwc3MTK94/0tu/7zunz5rzemjBn8/R0EQxP0Xj2tWr2H/4O8yfGyJ2HJV537bOys6CtbWN0jQLSytkZmaqI5bK9e7z7vVOSU6ClpYWNm5Yh8sXL8DM3ByfDxysEcPlMh49gp2dA44dPYKtmzegqLAQnbp2x6BhI6GtrY0VSxehY+duqK7BRTXweihgamoKoi9fwqYN6yArLkbb9h0wZlyAxndpe/v3EzuCWlz5+Sdcv3YVu6KOYNH8OYrpJQsOAEhKfICrP/+ETz/ro+6IKmdgYICg6bOwcH4odu3YhuLiYnTt/il69PxM7GilIimSYca+eIT2rouhLatDV0cb+6IfYm/06+7crE/dsT8mHfefKBeOOa8KIS2S4SNzQzx4+nqeXaXXHWoLE83e70l4ohQZhw4dwsKFCzF8+HCMGTMGT548wZYtW+Dv7w8nJyecO3cO/fv3x/Lly9GiRQsxItI/IJFIMG/ObATNmAVDQ0Ox44iiIL8A+vp6StP09fVRKJWKlEg9UlKSoaWlBadq1eDf93PEXv0Z8+bMgomJCfxal+1xzK/yXyE9PQ2HD+zD9Nnz8Cw7C+Hz58DA0Ai1ajvjRtx17Nx3WOyYKvfkSQYK8vOhp6+P8CXLkPHoERaFzUNBQQGmBs0QOx6VkkQiwcLQ2ZgcNPMv379znj/HtMAvUbeeF1q0bK3GhOqTkpyEFi1bYcCgIUhMfIBFC0LRqLEvOnXuKna0UqlpWxGnbz7F+h+S4GxfEXM/q4uL97KQ/ZsEDWtYos38t0/mL5bJcTwuA1O7uuDB0xd4WVCEmT3cUFgsg76O6INf1IrnZJSeKEXG2rVrER4erlRANG7cGJ9//jnOnz+PFi1awNXVFYsXL2aR8R+2dvVKuLq5o0nTZmJHEY2BgQFycpSH+UmlUo0vurp07Y4WLVvBzMwcAFDb2RlpaamI2ru7zBcZOjo6eJmXh5D5EbCzf33yfubTJziwbzdkcjkCp82AgYZvXwCwt3fA+csxMDU1g5aWFurUcYFMLsP0aZMROCUIOjocn12WbVy3Ci5u7vD9uOl7l3n2LBvjRw2DTCbDwojl0NbWvA+ZMT9F49CB/Tjxw3kYGhrCzd0Dv2RmYuO6NWW6yGjibIW+Hzui4YwTKCiUIf5hDj4yM8LkznUglwPT995AQaHsnY+dFXUTq4b64Mr89ngpKcKKY/dRz6kSXhQUqXktqKwTpcj49ddfYWtrqzTNxsYGz549w/Pnz2FpaYnGjRtjwYIFYsSjf+j4se/xLDsbjX28AACFha+P3p86eQI/Xb0uZjS1sbGxeevbZ55lZ8HK2lqkROqhpaWlKDDeqF69hkZ81aeVlTX0DQwUBQYAVHWqhke/n/w+ffIEpeW/Gj8KHTt3w5Tps9UZUy3+vI2rVa8BiUSC3NxcWFhYiBOKBHHy+P/w67NstPD1BgBIf3//PnP6JM5Hx+KXzEyM+WIwAGDtxm2opKHb+/btBFR1dFQ6MFTHxRWbNqwVMVXpeVQxR0pWnlIhkfAoB1WtXp9rsm5EQ6Xlt4/xRVTMQwTvicezPCn8V/wI8wp6KCiUQUsLCOruikfPXql1HajsE6XI8PX1RUhICJYsWQIHB4fXw27mzYO9vT0sLS2Rm5uLdevWwd3dXYx49A9t2rIdRYV/HNlYtnQxAGDCV4FiRVI7D896+GbTBhQUFCj+SMVdv4Z6XvVFTqZaq1euwI2461i38RvFtHt378CpWjURUwnDzcMTUokED9NSUfX3E9tTU5JgZ++A5as3KC3bu3tHBM2ciwaNfUVIqlo/Xr6IoCmBOH76nOJbw+7dvQNzc3MWGBpg7catKCr64/175fIlAIBxX05Cfv4rfDl2BLS1tbF6wxZYWWnuQRMbaxukP0xDYaFUca5Rakoy7B0qi5ysdDJzC+BkbQw9HS0UFr++xk9N24pIy3qJz1cpf3PWxZA2mLwrDhfv/AIAWDaoPg7GpOPC3SwAQCcve2T9VoD7TzXvxH9SLVF6nyEhIQCANm3aoEmTJvDx8UF0dDSWLVsGABg9ejRu3bqF0NDQ9/8QEp29vQOqOjoqbsbGxjA2NkZVR0exo6mNt08D2H5kh9kzgpGU+ACbN65Hws14dO/ZS+xoKtWiZStci72CbVs2IT39Ifbt3Y2j3x3BwEFDxY5Wao5O1fBx0xaYN3s6Hty/i59+vITt32xCn/4DUbmqo9INAKxtbGBhYfk3P7Xs8aznBQNDA8yZPQOpKcm4dPE8IpeEY9DQ4WJHIwHY2TugSlVHxa1CBWNUqGCMKlUd8c3G9Xj0KB2z54YBALKzs5CdnYW8F5r3IbN5Sz/o6uphzqwZSEtNwflzZ7Bpw1r06z9A7GilcvrmUxQWyxHe3wvVbIzRxt0W49rXwprTD5Ca9VLpBgBPc/LxLO91N+t5nhRTurrA2a4iGteyRGhvD6w6+QB/uh4p0d8SpZNhYWGBPXv2ICEhAenp6bCysoKnpyf09V8fRVizZg3MzMzEiEb0r+jo6CDy61WYM2s6+vXuiSpVHbFk+UqNuRDf+7i5eyB8yXKsWbUCq1eugL29AxYsWgzPel5iRxNEyPxFWBq+AKOHDoCBoRF69emLz/z7ix1LrYyNTbB63SZELFyAfn16wtjYGD0/88fgISwyNN3ZH05CUlCAIQOUv02qU5fumB0aJlIq1ahYsSLWbdqC8LD56O/fC5UqWWD4yNHoWca/SetFQRH6rvgRIb3ccXRKC/yaJ8GK4/ex81La3z424ugdzO/jiQNfNcMrSRE2nknGprPJakj938ITv0tPSy7XvNr0VaHGrdI/ol1O9wiZ5v0K/zPldLXzC4v/fiENVEFftG8cF1U5fVuD5D0n5Wo6fV3NO7n8n6g94YjYEUSRvqqb2BHeKydfvL815kaa8cUa5XNvJiIiIiIilSmfh8aIiIiIiN5Dk668LRZ2MoiIiIiISFDsZBARERERlVBezwcTEjsZREREREQkKHYyiIiIiIhKYCOj9NjJICIiIiIiQbHIICIiIiIiQXG4FBERERFRSRwvVWrsZBARERERkaDYySAiIiIiKoEX4ys9djKIiIiIiEhQLDKIiIiIiEhQHC5FRERERFQCr/hdeuxkEBERERGRoNjJICIiIiIqgY2M0mMng4iIiIiIBMUig4iIiIiIBMXhUkREREREJXG8VKmxk0FERERERIJikUFEREREVIKWiP/+DYlEguDgYPj4+KBp06bYvHmzil6Rf4/DpYiIiIiIyqDw8HAkJCRg69atyMjIwNSpU2Fvb48OHTqIHY1FBhERERFRSWXhYnyvXr1CVFQUNmzYADc3N7i5ueHBgwfYuXPnf6LI4HApIiIiIqIy5u7duygqKoKXl5dimre3N27cuAGZTCZistfYySAiIiIi+o+QSqWQSqVK0/T19aGvr680LSsrC5UqVVKabmVlBYlEgpycHFhYWKgl7/toZJFRQa8M9LhIQNze5UkFfY182yJSYqjLgQblSfqqbmJHoD8xFPFPzddfr8PKlSuVpo0bNw7jx49Xmpafn/9W4fHm/p+LFDHwrzURERER0X/EyJEjMWTIEKVpfy4mAMDAwOCtYuLNfUNDQ9UF/IdYZBARERER/Ue8a2jUu9ja2uL58+coKiqCru7rj/RZWVkwNDSEqampqmP+LfZjiYiIiIjKGBcXF+jq6iIuLk4xLTY2Fh4eHtDWFv8jvvgJiIiIiIjoXzEyMkL37t0REhKC+Ph4nD59Gps3b8bAgQPFjgYA0JLL5XKxQxARERER0b+Tn5+PkJAQnDx5EiYmJhg2bBgGDx4sdiwALDKIiIiIiEhgHC5FRERERESCYpFBRERERESCYpFBRERERESCYpEhEIlEguDgYPj4+KBp06bYvHmz2JHUSiqVonPnzoiJiRE7ilpkZmYiICAADRs2RLNmzRAWFgaJRCJ2LJVLS0vDsGHD4OXlhZYtW2Ljxo1iR1K7L774AtOmTRM7hlqcOnUKzs7OSreAgACxY6mcVCrFnDlz0KBBA3z88cdYunQpNP30xYMHD761rZ2dnVGnTh2xo6nckydPMHLkSNSvXx9+fn7YsmWL2JHU4tmzZwgICICPjw/atm2LgwcPih2JNAwvxieQ8PBwJCQkYOvWrcjIyMDUqVNhb2+PDh06iB1N5SQSCSZNmoQHDx6IHUUt5HI5AgICYGpqip07dyI3NxfBwcHQ1tbG1KlTxY6nMjKZDF988QU8PDxw6NAhpKWl4auvvoKtrS26dOkidjy1+P7773H+/Hn06NFD7ChqkZiYiFatWiE0NFQxzcDAQMRE6jFv3jzExMRg06ZNePnyJSZOnAh7e3v4+/uLHU1lOnbsiGbNminuFxUVYdCgQWjZsqV4odRkwoQJsLe3x8GDB5GYmIjAwEA4ODigbdu2YkdTGblcjrFjx0Imk2Hbtm3IzMzE1KlTYWJignbt2okdjzQEiwwBvHr1ClFRUdiwYQPc3Nzg5uaGBw8eYOfOnRpfZCQmJmLSpEkaf5SvpOTkZMTFxeHy5cuwsrICAAQEBGDRokUaXWRkZ2fDxcUFISEhMDExgZOTE3x9fREbG1suioycnByEh4fDw8ND7Chqk5SUhNq1a8Pa2lrsKGqTk5ODAwcO4JtvvkHdunUBAEOHDsWNGzc0usgwNDSEoaGh4v66desgl8sRGBgoYirVy83NRVxcHEJDQ+Hk5AQnJyc0a9YM0dHRGl1kJCQk4Pr16zh9+jSqVKkCV1dXDB8+HJs2bWKRQYLhcCkB3L17F0VFRfDy8lJM8/b2xo0bNyCTyURMpno///wzGjVqhL1794odRW2sra2xceNGRYHxRl5enkiJ1MPGxgbLli2DiYkJ5HI5YmNjceXKFTRs2FDsaGqxaNEidOvWDTVr1hQ7itokJSXByclJ7BhqFRsbCxMTE6Xf6y+++AJhYWEiplKvnJwcbNiwAZMmTYK+vr7YcVTK0NAQRkZGOHjwIAoLC5GcnIxr167BxcVF7GgqlZ6eDgsLC1SpUkUxzdnZGQkJCSgsLBQxGWkSFhkCyMrKQqVKlZTejK2srCCRSJCTkyNeMDXo168fgoODYWRkJHYUtTE1NVUaViCTybBjxw40btxYxFTq5efnh379+sHLywvt27cXO47KRUdH4+rVqxgzZozYUdRGLpcjJSUFly5dQvv27dGmTRssXrwYUqlU7GgqlZ6eDgcHBxw+fBgdOnRA69atsWrVKo0/YFTS7t27YWNjo/GdeOD18L9Zs2Zh79698PT0xCeffILmzZvjs88+EzuaSllZWeHFixfIz89XTHv69CmKiorw4sULEZORJmGRIYD8/Py3jva8ua/pf5AJiIiIwO3btzFx4kSxo6jNihUrsHbtWty5c0fjj/BKJBLMnj0bs2bNUhpOoukyMjIU723Lli3D1KlT8d133yE8PFzsaCr16tUrpKWlYc+ePQgLC8PUqVOxffv2cnMysFwuR1RUFD7//HOxo6hNUlISWrVqhb179yIsLAzHjx/Ht99+K3YslfL09ISNjQ1CQ0MVv/PffPMNALCTQYLhORkCMDAweKuYeHO/PH0oKY8iIiKwdetWREZGonbt2mLHUZs35yVIJBIEBgZiypQpGjusYuXKlXB3d1fqXpUHDg4OiImJgZmZGbS0tODi4gKZTIbJkycjKCgIOjo6YkdUCV1dXeTl5WHJkiVwcHAA8Lrg2r17N4YOHSpyOtW7efMmMjMz0alTJ7GjqEV0dDT279+P8+fPw9DQEB4eHsjMzMSaNWvQtWtXseOpjIGBAZYtW4YJEybA29sblpaWGD58OMLCwmBiYiJ2PNIQLDIEYGtri+fPn6OoqAi6uq9f0qysLBgaGsLU1FTkdKQqoaGh2L17NyIiIsrFkKHs7GzExcWhTZs2imk1a9ZEYWEh8vLyYGFhIWI61fn++++RnZ2tOOfqzQGEEydO4Pr162JGUzlzc3Ol+zVq1IBEIkFubq7Gbm9ra2sYGBgoCgwAqFatGp48eSJiKvW5ePEifHx8YGZmJnYUtUhISICjo6PSAUFXV1esXbtWxFTqUbduXZw5c0Yx5Pvy5cuoVKkSjI2NxY5GGoLDpQTg4uICXV1dxMXFKabFxsbCw8MD2tp8iTXRypUrsWfPHixdurTcHPF79OgRxo0bh8zMTMW0hIQEWFhYaOwHTgDYvn07vvvuOxw+fBiHDx+Gn58f/Pz8cPjwYbGjqdTFixfRqFEjpTHbd+7cgbm5uUZvb09PT0gkEqSkpCimJScnKxUdmiw+Ph7169cXO4ba2NjYIC0tTWk0QnJyMipXrixiKtXLyclB37598fz5c1hbW0NXVxfnzp0rN1/kQerBT8ACMDIyQvfu3RESEoL4+HicPn0amzdvxsCBA8WORiqQlJSE1atXY8SIEfD29kZWVpbipsk8PDzg5uaG4OBgJCYm4vz584iIiMCoUaPEjqZSDg4OcHR0VNyMjY1hbGwMR0dHsaOplJeXFwwMDDBjxgwkJyfj/PnzCA8Px/Dhw8WOplLVq1dHy5YtERQUhLt37+LixYtYv349+vbtK3Y0tXjw4EG5+gY1Pz8/6OnpYcaMGUhJScGZM2ewdu1aDBgwQOxoKmVubo5Xr14hIiIC6enpiIqKwoEDBzR+/yb10pKXpwscqFB+fj5CQkJw8uRJmJiYYNiwYRg8eLDYsdTK2dkZ27ZtQ6NGjcSOolLr16/HkiVL3jnv3r17ak6jXpmZmQgNDUV0dDSMjIzw+eefY+TIkdDS0hI7mtq8udr3woULRU6ieg8ePMCCBQsQFxcHY2Nj+Pv7Y+zYsRq/vV+8eIHQ0FCcOnUKRkZG6NevX7lYb+D1EJpVq1aVq3OQEhMTMX/+fMTHx8PCwgL9+/fHoEGDNH57JycnY/bs2bh58yYqV66MSZMmoVWrVmLHIg3CIoOIiIiIiATF4VJERERERCQoFhlERERERCQoFhlERERERCQoFhlERERERCQoFhlERERERCQoFhlERERERCQoFhlERERERCQoFhlERERERCQoFhlEVG74+fnB2dlZcXNzc0OHDh2wZcsWQZ9nwIAB+PrrrwG8vkL4m6uE/xWpVIp9+/Z98HMePHgQfn5+75wXExMDZ2fnD/7Zzs7OiImJ+aDHfv311xgwYMAHPzcREZVNumIHICJSp+DgYHTs2BEAUFRUhJ9++gnTp0+Hubk5unfvLvjzTZ8+/R8t9/3332Pt2rXo3bu34BmIiIjUjZ0MIipXKlasCGtra1hbW8POzg49evSAr68vTp48qbLnq1ix4t8uJ5fLVfL8REREYmCRQUTlnq6uLvT09AC8HuoUGhqK1q1bo2XLlsjLy8OTJ08watQoeHp6ws/PDytXrkRxcbHi8adOnUL79u1Rr149zJ07V2nen4dLHTlyBB06dICnpyf8/f1x+/ZtxMTEICgoCI8fP4azszMePXoEuVyOVatWoWnTpvDx8cGoUaOQkZGh+DmZmZkYPnw46tWrhx49euDhw4cfvP55eXkICgqCr68v3N3d0aFDB5w+fVppmStXrqBdu3bw9PTEl19+idzcXMW8+/fvY8CAAahbty7at2+PnTt3fnAWIiLSDCwyiKjcKiwsxMmTJ3H58mW0bt1aMf3gwYOIiIjAypUrYWxsjHHjxsHS0hKHDh1CWFgYvvvuO6xduxYAkJiYiAkTJqBv3744cOAAioqKEBsb+87nu3jxIqZPn45Bgwbh22+/hbu7O0aOHAkvLy8EBwfjo48+wqVLl2BnZ4cdO3bgu+++w5IlS7B3715YWlpi6NChKCwsBAB8+eWXkMlkiIqKwogRI7B169YPfh3mz5+PlJQUbN68GUePHoWPjw+mT58OqVSqWGbnzp2YPn06du7ciZSUFISFhQEACgoKMGLECHh7e+Pbb7/F1KlTsXr1ahw+fPiD8xARUdnHczKIqFyZPXs2QkNDAbz+gGxoaIhBgwaha9euimVatmyJ+vXrAwCio6ORkZGBqKgoaGtro3r16pg6dSqCgoIwduxYHDhwAD4+Phg8eDAAYObMmTh79uw7n3vv3r3o3Lkz+vbtCwCYMmUK9PT0kJubi4oVK0JHRwfW1tYAgI0bN2L27Nlo1KgRAGDu3Llo2rQpLl68iCpVquD69es4e/Ys7O3tUatWLSQkJOD48eMf9Jo0aNAAQ4YMQe3atQEAQ4cORVRUFJ49ewY7OzsAwLhx49CiRQsAwIwZMzBkyBDMmDEDx44dg6WlJSZMmAAAcHJywuPHj7Ft2zaVnONCRERlA4sMIipXAgIC0K5dOwCAgYEBrK2toaOjo7SMg4OD4v9JSUnIycmBt7e3YppMJkNBQQGeP3+OpKQkuLi4KObp6ekp3S8pJSUF/v7+ivv6+vqYOnXqW8u9fPkST58+xcSJE6Gt/UfDuaCgAKmpqZBIJDA3N4e9vb1inoeHxwcXGd27d8fp06exb98+JCcn49atWwCgNOzLw8ND8X9XV1cUFRXh4cOHSE5Oxt27d+Hl5aWYX1xc/NZrSkRE5QuLDCIqVywtLeHo6PiXyxgYGCj+X1RUhOrVq2P16tVvLffmhO4/n7T95vyOP9PV/WdvuW8+3C9fvhzVqlVTmmdmZobo6Oh//Jz/xJQpU3D9+nV069YNffv2hbW1Nfr06aO0TMmi4c1z6+npoaioCL6+vpg1a9YHPz8REWkenpNBRPQXqlWrhoyMDFhYWMDR0RGOjo549OgRVqxYAS0tLdSqVQs3b95ULC+TyXD37t13/ixHR0elecXFxfDz80NsbCy0tLQU001NTWFpaYmsrCzFc9rZ2SEiIgIpKSmoXbs2cnNzkZaWpnjMnTt3Pmj98vLycPToUURGRiIgIABt27ZVnNRdspC5f/++4v/x8fHQ09ND5cqVUa1aNaSkpKBy5cqKrHFxcdi+ffsH5SEiIs3AIoOI6C80bdoUDg4OmDx5Mu7du4erV69i5syZMDIygo6ODnr37o2EhASsWbMGycnJWLRokdK3QJU0YMAAfPvttzh06BDS0tIQFhYGuVwONzc3GBkZITc3F6mpqSgqKsLgwYOxbNkynDlzBqmpqZgxYwauXbuG6tWro0aNGvD19UVwcDDu3r2L06dPY8eOHX+7LhcuXFC6xcTEQF9fH0ZGRjh58iQePXqEixcvYu7cuQCgdOJ3ZGQkoqOjERcXh3nz5sHf3x9GRkbo2rUrCgoKMGvWLCQlJeH8+fOYP38+LC0thdkARERUJnG4FBHRX9DR0cGaNWsQGhqK3r17o0KFCujQoYPiXApHR0esWbMGYWFhWLNmDdq0aaM4QfrPGjRogNmzZ2PVqlXIysqCu7s71q5dC0NDQzRu3BiOjo7o0qULdu3ahWHDhuHly5eYNWsW8vLy4O7ujk2bNsHMzAzA6w/9M2fOhL+/P+zt7TFgwAAcPHjwL9dlxIgRSvdtbW1x4cIFREREYNGiRdi+fTsqV66M0aNHY9myZbhz5w5q1KgBABgyZAimT5+O58+f45NPPkFgYCAAwMTEBBs2bMCCBQvQvXt3mJubo3///hg5cmSpXnciIirbtOS8AhQREREREQmIw6WIiIiIiEhQLDKIiIiIiEhQLDKIiIiIiEhQLDKIiIiIiEhQLDKIiIiIiEhQLDKIiIiIiEhQLDKIiIiIiEhQLDKIiIiIiEhQLDKIiIiIiEhQLDKIiIiIiEhQLDKIiIiIiEhQ/wfXTO2WV1iH2wAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93       980\n",
      "           1       0.93      0.96      0.95      1135\n",
      "           2       0.86      0.84      0.85      1032\n",
      "           3       0.85      0.80      0.83      1010\n",
      "           4       0.82      0.66      0.73       982\n",
      "           5       0.74      0.74      0.74       892\n",
      "           6       0.91      0.86      0.88       958\n",
      "           7       0.90      0.87      0.88      1028\n",
      "           8       0.81      0.79      0.80       974\n",
      "           9       0.65      0.84      0.73      1009\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.84      0.83      0.83     10000\n",
      "weighted avg       0.84      0.83      0.84     10000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from neural_network.DenseLayer import DenseLayer\n",
    "from neural_network.Activation import ReLU, Softmax\n",
    "from neural_network.NeuralNetwork import NeuralNetwork\n",
    "from neural_network.LossFunctions import CrossEntropy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_mnist_data():\n",
    "    train_data = pd.read_csv('mnist_train.csv')\n",
    "    test_data = pd.read_csv('mnist_test.csv')\n",
    "    return train_data, test_data\n",
    "\n",
    "def prepare_data(data):\n",
    "    data = data.dropna()\n",
    "    y = data.iloc[:, 0].values\n",
    "    X = data.iloc[:, 1:].values / 255.0\n",
    "    y_one_hot = np.zeros((y.size, 10))\n",
    "    y_one_hot[np.arange(y.size), y] = 1\n",
    "    return X, y_one_hot\n",
    "\n",
    "train_data, test_data = load_mnist_data()\n",
    "X_train, y_train = prepare_data(train_data)\n",
    "X_test, y_test = prepare_data(test_data)\n",
    "\n",
    "network = NeuralNetwork(\n",
    "        DenseLayer(784, 128),\n",
    "        Sigmoid(),\n",
    "        DenseLayer(128, 64),\n",
    "        Sigmoid(),\n",
    "        DenseLayer(64, 10),\n",
    "        Softmax())\n",
    "\n",
    "metrics = network.train(X_train, y_train, epochs=100, learning_rate=0.001, loss='crossentropy', patience = 100)\n",
    "\n",
    "# Plot the learning curves for R2 Score\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(metrics[\"r2_history_train\"], label='R2 Train')\n",
    "plt.plot(metrics[\"r2_history_valid\"], label='R2 Valid', linestyle='--')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('R2 Score')\n",
    "plt.title('R2 Score Learning Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the learning curves for MSE\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(metrics[\"loss_history_train\"], label='Training Loss (MSE)')\n",
    "plt.plot(metrics[\"loss_history_valid\"], label='Valid Loss (MSE)', linestyle='--')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE Learning Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Hacer predicciones y evaluar\n",
    "predictions = network.predict(X_test)\n",
    "predictions_rounded = np.argmax(predictions, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_labels, predictions_rounded)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test_labels, predictions_rounded))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T20:39:30.957248100Z",
     "start_time": "2024-01-07T20:39:24.072556600Z"
    }
   },
   "id": "a77977dc8e6eaa62"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4ef9630d5d6f3fdc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
